{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning Homework IV\n",
    "\n",
    "### 2. Long Short Term Memory (LSTM) (bh2821)\n",
    "\n",
    "* This document is open sourced under Apeche License Version 2.0\n",
    "* Author: Nathmath Huang (bh2821)\n",
    "* Date  : May 12, 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Prior to Anything`:\n",
    "\n",
    "....* Even though I used torch, I just used its capability of computing on CUDA but `NEVER used its Autograd functionality`.\n",
    "\n",
    "....* In my implementation, the base class `nn_Parameter` is completely compatible for `numpy` and `torch` backend,\n",
    "\n",
    "....* so you can see I have manually programmed the gradient system and even a mimic scalable Autograd framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`P. Libraries`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from typing import List, Dict, Tuple, Any, Optional, Union, Literal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "import lzma\n",
    "import random\n",
    "from collections import deque\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I have cuda\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whether using `numpy` as the backend or `torch`\n",
    "backend = \"torch\"; _backend = torch\n",
    "device = \"cuda\"    \n",
    "\n",
    "# Feel free to change those parameters since I have debugged and ensured all of them are useful.\n",
    "# Though, their result may NOT be completely the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`P. Matrix Wrapper Library (self-implemented)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This object class is self implemented and open-sourced\n",
    "# Available at https://github.com/dof-studio/MML/\n",
    "# By Nathmath Huang (bh2821)\n",
    "# License: Apache License Version 2.0\n",
    "\n",
    "class Object:\n",
    "    \"\"\"\n",
    "    Base Type for all advanced n-dimensional data types.\n",
    "    \"\"\"\n",
    "   \n",
    "    __attr__ = \"MML.Object\"  \n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "   \n",
    "    def __repr__(self):\n",
    "        return f\"Object (Abstract Data Type).\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This matrix class is self implemented and open-sourced\n",
    "# Available at https://github.com/dof-studio/MML/\n",
    "# By Nathmath Huang (bh2821)\n",
    "# License: Apache License Version 2.0\n",
    "\n",
    "class Matrix(Object):\n",
    "    \"\"\"\n",
    "    A production-level Matrix class that provides a unified interface for common matrix operations used in machine learning.\n",
    "    The underlying data is stored as either a numpy.ndarray or a torch.Tensor depending on the chosen backend.\n",
    "    This class supports element-wise arithmetic, matrix multiplication, transpose, determinant, inverse, trace, and SVD.\n",
    "    Internal optimizations avoid repeated string comparisons by setting boolean flags during initialization.\n",
    "    \"\"\"\n",
    "    __attr__ = \"MML.Matrix\"    \n",
    "    \n",
    "    def e(self):\n",
    "        \"\"\"\n",
    "        Returns natural exponent value as a single-value Matrix.\n",
    "        \n",
    "        Returns:\n",
    "            -------\n",
    "            Matrix with 0 shape. exp value.\n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            return Matrix(np.e, backend = self._backend, dtype = self.dtype, device = self.device)\n",
    "        else:\n",
    "            return Matrix(torch.e, backend = self._backend, dtype = self.dtype, device = self.device)\n",
    "    \n",
    "    def pi(self):\n",
    "        \"\"\"\n",
    "        Returns pi value as a single-value Matrix.\n",
    "        \n",
    "        Returns:\n",
    "            -------\n",
    "            Matrix with 0 shape. pi value.\n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            return Matrix(np.pi, backend = self._backend, dtype = self.dtype, device = self.device)\n",
    "        else:\n",
    "            return Matrix(torch.pi, backend = self._backend, dtype = self.dtype, device = self.device)\n",
    "    \n",
    "    def __init__(self, data, backend=\"numpy\", *, dtype=None, device=None):\n",
    "        \"\"\"\n",
    "        Initializes a Matrix instance with the specified backend.\n",
    "        \n",
    "        Args:\n",
    "            data (array-like): Input data to be converted into a matrix.\n",
    "            backend (str): The backend to use (\"numpy\" or \"torch\").\n",
    "            dtype(str): The type of data to be stored in (any type or None).\n",
    "            device (str): Device where the data is stored on (\"cpu\" or \"cuda\", or None).\n",
    "            \n",
    "        Raises:\n",
    "            ValueError: If an unsupported backend is provided.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self._backend = backend.lower()\n",
    "        if self._backend not in (\"numpy\", \"torch\"):\n",
    "            raise ValueError(\"Unsupported backend. Please choose 'numpy' or 'torch'.\")\n",
    "        self._is_numpy = (self._backend == \"numpy\")\n",
    "        self._is_torch = (self._backend == \"torch\")\n",
    "        \n",
    "        # Convert input data to the appropriate type.\n",
    "        # By Nathmath Huang\n",
    "        if self._is_numpy:\n",
    "            self.data = np.array(data, dtype=dtype)\n",
    "        else:\n",
    "            if torch is None:\n",
    "                raise ImportError(\"PyTorch is not installed but backend 'torch' was requested.\")\n",
    "            self.data = data.to(device, dtype=dtype) if isinstance(data, torch.Tensor) else torch.tensor(data, device=device, dtype=dtype)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        Returns a string representation of the Matrix showing the backend, shape, and data.\n",
    "        \"\"\"\n",
    "        return f\"Matrix(backend={self._backend}, shape={self.shape}, data=\\n{self.data})\"\n",
    "    \n",
    "    def __getitem__(self, key):\n",
    "       \"\"\"\n",
    "       Allows subscription using a[i, j]. \n",
    "       If the result is an array, it returns a new Matrix; otherwise, the scalar value.\n",
    "       \"\"\"\n",
    "       result = self.data[key]\n",
    "       if (self._is_numpy and isinstance(result, np.ndarray)) or (self._is_torch and torch is not None and isinstance(result, torch.Tensor)):\n",
    "           return Matrix(result, backend=self._backend)\n",
    "       return result\n",
    "   \n",
    "    def __setitem__(self, key, value):\n",
    "        \"\"\"\n",
    "        Allows assignment using a[i, j] = value.\n",
    "        If the value is a Matrix instance, its underlying data is used.\n",
    "        \"\"\"\n",
    "        if isinstance(value, Matrix):\n",
    "            value = value.data\n",
    "        self.data[key] = value\n",
    "   \n",
    "    def submatrix(self, key):\n",
    "      \"\"\"\n",
    "      Retrieves a sub-array of the matrix using the given key while ensuring the result remains two-dimensional.\n",
    "      For example, using a[:, 1] will return a Matrix of shape (m, 1) rather than (m,).\n",
    "      \n",
    "      Args:\n",
    "          key: Indexing key (can be an int, slice, or tuple of such) for sub-array extraction.\n",
    "      \n",
    "      Returns:\n",
    "          Matrix: A new Matrix instance representing the sub-array with two dimensions.\n",
    "      \"\"\"\n",
    "      result = self.data[key]\n",
    "      # For numpy backend: if result is 1D but the original matrix is 2D, adjust the shape.\n",
    "      if self._is_numpy and isinstance(result, np.ndarray):\n",
    "          if result.ndim == 1 and len(self.data.shape) == 2:\n",
    "              if isinstance(key, tuple):\n",
    "                  if len(key) == 2:\n",
    "                      if isinstance(key[0], slice) and isinstance(key[1], int):\n",
    "                          # Selecting a column -> reshape to (m, 1)\n",
    "                          result = result[:, np.newaxis]\n",
    "                      elif isinstance(key[0], int) and isinstance(key[1], slice):\n",
    "                          # Selecting a row -> reshape to (1, n)\n",
    "                          result = result[np.newaxis, :]\n",
    "                      else:\n",
    "                          result = np.atleast_2d(result)\n",
    "                  else:\n",
    "                      result = np.atleast_2d(result)\n",
    "              else:\n",
    "                  # key is a single index (e.g., a[1]) -> treat as row selection.\n",
    "                  result = result[np.newaxis, :]\n",
    "      # For torch backend: similar adjustments using unsqueeze.\n",
    "      elif self._is_torch and torch is not None and isinstance(result, torch.Tensor):\n",
    "          if result.dim() == 1 and len(self.data.shape) == 2:\n",
    "              if isinstance(key, tuple):\n",
    "                  if len(key) == 2:\n",
    "                      if isinstance(key[0], slice) and isinstance(key[1], int):\n",
    "                          result = result.unsqueeze(1)  # Make column vector.\n",
    "                      elif isinstance(key[0], int) and isinstance(key[1], slice):\n",
    "                          result = result.unsqueeze(0)  # Make row vector.\n",
    "                      else:\n",
    "                          result = result.unsqueeze(0)\n",
    "                  else:\n",
    "                      result = result.unsqueeze(0)\n",
    "              else:\n",
    "                  result = result.unsqueeze(0)\n",
    "      return Matrix(result, backend=self._backend) \n",
    "   \n",
    "    @property\n",
    "    def shape(self):\n",
    "        \"\"\"\n",
    "        Returns the shape of the matrix.\n",
    "        \"\"\"\n",
    "        return self.data.shape\n",
    "    \n",
    "    @property\n",
    "    def dtype(self):\n",
    "        \"\"\"\n",
    "        Returns the data type of the matrix elements.\n",
    "        \"\"\"\n",
    "        return self.data.dtype\n",
    "    \n",
    "    @property\n",
    "    def device(self):\n",
    "        \"\"\"\n",
    "        Returns the data device of the matrix elements.\n",
    "        \"\"\"\n",
    "        if self._backend == \"numpy\":\n",
    "            return \"cpu\"\n",
    "        else:\n",
    "            return self.data.device.type\n",
    "\n",
    "    def reshape(self, shape):\n",
    "        \"\"\"\n",
    "        Converts the matrix into a new shape.\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: A new Matrix object with the specified shape.\n",
    "        \"\"\"\n",
    "        return Matrix(self.data.reshape(shape), backend=self._backend)\n",
    "    \n",
    "    def astype(self, dtype):\n",
    "        \"\"\"\n",
    "        Converts the underlying data to the specified type.\n",
    "        \n",
    "        For the numpy backend, it uses np.ndarray.astype.\n",
    "        For the torch backend, it maps the input (which can be a torch.dtype, a string, or a numpy type)\n",
    "        to the corresponding torch dtype and uses tensor.to(dtype=...).\n",
    "        \n",
    "        Args:\n",
    "            dtype: The desired data type. For numpy, any valid numpy dtype is accepted.\n",
    "                   For torch, this can be a torch.dtype, a string (e.g., \"float32\", \"int64\"),\n",
    "                   or a numpy dtype.\n",
    "                   \n",
    "        Returns:\n",
    "            A new Matrix instance with the data converted to the specified type.\n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            new_data = self.data.astype(dtype)\n",
    "            return Matrix(new_data, backend=\"numpy\")\n",
    "        else:\n",
    "            # Map the input dtype to a torch dtype.\n",
    "            torch_dtype = None\n",
    "            if isinstance(dtype, torch.dtype):\n",
    "                torch_dtype = dtype\n",
    "            elif isinstance(dtype, str):\n",
    "                mapping = {\n",
    "                    \"float32\": torch.float32,\n",
    "                    \"float\": torch.float32,\n",
    "                    \"float64\": torch.float64,\n",
    "                    \"double\": torch.float64,\n",
    "                    \"int32\": torch.int32,\n",
    "                    \"int\": torch.int32,\n",
    "                    \"int64\": torch.int64,\n",
    "                    \"long\": torch.int64,\n",
    "                    \"bool\": torch.bool,\n",
    "                    \"complex64\": torch.complex64,\n",
    "                    \"complex128\": torch.complex128\n",
    "                }\n",
    "                if dtype in mapping:\n",
    "                    torch_dtype = mapping[dtype]\n",
    "                else:\n",
    "                    raise ValueError(f\"Unsupported dtype string: {dtype}\")\n",
    "            elif isinstance(dtype, (np.dtype, type)):\n",
    "                np_dtype = np.dtype(dtype)\n",
    "                mapping = {\n",
    "                    np.dtype(\"float32\"): torch.float32,\n",
    "                    np.dtype(\"float64\"): torch.float64,\n",
    "                    np.dtype(\"int32\"): torch.int32,\n",
    "                    np.dtype(\"int64\"): torch.int64,\n",
    "                    np.dtype(\"bool\"): torch.bool,\n",
    "                    np.dtype(\"complex64\"): torch.complex64,\n",
    "                    np.dtype(\"complex128\"): torch.complex128,\n",
    "                }\n",
    "                if np_dtype in mapping:\n",
    "                    torch_dtype = mapping[np_dtype]\n",
    "                else:\n",
    "                    raise ValueError(f\"Unsupported numpy dtype: {np_dtype}\")\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported dtype argument: {dtype}\")\n",
    "            new_data = self.data.to(dtype=torch_dtype)\n",
    "            return Matrix(new_data, backend=\"torch\")\n",
    "\n",
    "    def to(self, backend, *, dtype=None, device=None):\n",
    "        \"\"\"\n",
    "        Converts the matrix to the specified backend and optionally sets the device for torch tensors.\n",
    "        \n",
    "        Args:\n",
    "            backend (str): The target backend (\"numpy\" or \"torch\").\n",
    "            dtype (str, optional): The target type (any numpy or torch type nor None for auto inferenence).\n",
    "            device (str, optional): The target device for torch tensors (e.g., \"cpu\" or \"cuda\").\n",
    "                                    Ignored if converting to numpy.\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: A new Matrix object with data in the target backend and on the specified device.\n",
    "        \"\"\"\n",
    "        target = backend.lower()\n",
    "        if target == self._backend:\n",
    "            # Already in the target backend: for torch, adjust device if specified.\n",
    "            if self._is_torch:\n",
    "                new_data = self.data.to(device = device, dtype = dtype)\n",
    "                return Matrix(new_data, backend=\"torch\")\n",
    "            return Matrix(self.data, backend=self._backend, device=device, dtype=dtype)\n",
    "        if target == \"numpy\":\n",
    "            # Converting from torch to numpy: always bring to CPU.\n",
    "            if self._is_torch:\n",
    "                return Matrix(self.data.cpu().to(dtype = dtype).numpy(), backend=\"numpy\")\n",
    "        elif target == \"torch\":\n",
    "            if torch is None:\n",
    "                raise ImportError(\"PyTorch is not installed.\")\n",
    "            if self._is_numpy:\n",
    "                new_data = torch.tensor(self.data, device = device, dtype = dtype)\n",
    "                return Matrix(new_data, backend=\"torch\")\n",
    "        raise ValueError(\"Unsupported backend conversion.\")\n",
    "\n",
    "    def to_rational(self):\n",
    "        \"\"\"\n",
    "        Converts all values stored in the matrix to rational numbers (fractions.Fraction).\n",
    "        For the numpy backend, returns a new Matrix with a numpy array of Fraction objects (dtype=object).\n",
    "        For the torch backend, due to limitations of torch tensors with non-numeric types,\n",
    "        the conversion is performed via numpy and the underlying data becomes a numpy array of Fraction objects,\n",
    "        while the backend attribute is preserved as 'torch'.\n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            vec_func = np.vectorize(lambda x: float(x.real))\n",
    "            new_data = vec_func(self.data)\n",
    "            return Matrix(new_data, backend=\"numpy\")\n",
    "        else:\n",
    "            # Convert torch tensor to numpy, then to Fraction.\n",
    "            np_data = self.data.cpu().numpy()\n",
    "            vec_func = np.vectorize(lambda x: float(x.real))\n",
    "            new_data = vec_func(np_data)\n",
    "            # Although backend remains 'torch', data is now a numpy array.\n",
    "            return Matrix(new_data, backend=\"torch\")\n",
    "\n",
    "    def to_complex(self):\n",
    "        \"\"\"\n",
    "        Converts all values stored in the matrix to complex numbers.\n",
    "        For the numpy backend, returns a new Matrix with a numpy array of complex numbers.\n",
    "        For the torch backend, returns a new Matrix with data converted to a torch complex tensor.\n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            new_data = self.data.astype(complex)\n",
    "            return Matrix(new_data, backend=\"numpy\")\n",
    "        else:\n",
    "            if not torch.is_complex(self.data):\n",
    "                new_data = self.data.to(torch.complex64)\n",
    "                return Matrix(new_data, backend=\"torch\")\n",
    "            return Matrix(self.data, backend=\"torch\")\n",
    "        \n",
    "    def _apply_op(self, other, op):\n",
    "        \"\"\"\n",
    "        Internal helper to apply an element-wise binary operation.\n",
    "        \n",
    "        Args:\n",
    "            other (Matrix or scalar): The other operand.\n",
    "            op (callable): A function that applies the desired operation element-wise.\n",
    "            \n",
    "        Returns:\n",
    "            Matrix: A new Matrix instance with the operation applied.\n",
    "        \"\"\"\n",
    "        other_val = other.data if isinstance(other, Matrix) else other\n",
    "        result = op(self.data, other_val)\n",
    "        return Matrix(result, backend=self._backend)\n",
    "    \n",
    "    def copy(self, *, backend=None, dtype=None, device=None):\n",
    "        \"\"\"\n",
    "        Creates a deep copy of the current matrix with the specified backend and data type.\n",
    "        \n",
    "        Args:\n",
    "            backend (str): The backend for the copied matrix. Default is None.\n",
    "            dtype: Desired data type for the result. Default is None.\n",
    "            device: Device to which the tensor should be moved if applicable. Default is None.\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: A deep copy of the current matrix with the specified parameters.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            if backend is None:\n",
    "                return Matrix(self.data.copy(), backend=self._backend, dtype=dtype, device=device)\n",
    "            else:\n",
    "                return Matrix(self.data.copy(), backend=backend, dtype=dtype, device=device)\n",
    "        else:\n",
    "            if backend is None:\n",
    "                return Matrix(self.data.clone().detach(), backend=self._backend, dtype=dtype, device=device)\n",
    "            else:\n",
    "                return Matrix(self.data.clone().detach(), backend=backend, dtype=dtype, device=device)\n",
    "    \n",
    "    def append(self, to_append, axis=0):\n",
    "        \"\"\"\n",
    "        Append a scalar (broadcasted) or an array to the matrix along the specified axis.\n",
    "        \n",
    "        Args:\n",
    "            to_append: A scalar value or an array-like object (or Matrix) to append.\n",
    "            axis (int): Axis along which to append. For 2D matrices, use axis=0 (append row)\n",
    "                        or axis=1 (append column).\n",
    "                        \n",
    "        Returns:\n",
    "            Matrix: A new Matrix instance with the appended data.\n",
    "        \"\"\"\n",
    "        orig_shape = self.data.shape\n",
    "    \n",
    "        # If to_append is an instance of self, extract its underlying data.\n",
    "        if isinstance(to_append, type(self)):\n",
    "            append_data = to_append.data\n",
    "        else:\n",
    "            # If to_append is a scalar, create an array of appropriate shape.\n",
    "            if np.isscalar(to_append):\n",
    "                if axis == 0:\n",
    "                    new_shape = (1, orig_shape[1])\n",
    "                elif axis == 1:\n",
    "                    new_shape = (orig_shape[0], 1)\n",
    "                else:\n",
    "                    raise ValueError(\"Axis out of bounds. Only axis=0 or axis=1 are supported.\")\n",
    "                if self._is_numpy:\n",
    "                    append_data = np.full(new_shape, to_append, dtype=self.data.dtype)\n",
    "                else:\n",
    "                    append_data = torch.full(new_shape, to_append, dtype=self.data.dtype, device=self.data.device.type)\n",
    "            # Not a scalar\n",
    "            else:\n",
    "                # If the input is a matrix.\n",
    "                if isinstance(to_append, Matrix):\n",
    "                    if self._is_numpy:\n",
    "                        append_data = np.array(to_append.data.copy(), dtype=self.data.dtype)\n",
    "                    else:\n",
    "                        append_data = torch.tensor(to_append.data.clone().detach(), dtype=self.data.dtype, device=self.data.device.type)\n",
    "                    \n",
    "                # Otherwise, assume to_append is array-like.\n",
    "                else:\n",
    "                    if self._is_numpy:\n",
    "                        append_data = np.array(to_append.copy(), dtype=self.data.dtype)\n",
    "                    else:\n",
    "                        append_data = torch.tensor(to_append.clone().detach(), dtype=self.data.dtype, device=self.data.device.type)\n",
    "                    \n",
    "                # Validate dimensions (assuming 2D matrices)\n",
    "                if len(append_data.shape) != len(orig_shape):\n",
    "                    raise ValueError(\"Dimension mismatch: appended data must match dimensions of the matrix.\")\n",
    "                if axis == 0:\n",
    "                    if append_data.shape[1:] != orig_shape[1:]:\n",
    "                        raise ValueError(\"Shape mismatch for axis 0: appended array must have the same number of columns.\")\n",
    "                elif axis == 1:\n",
    "                    if append_data.shape[0] != orig_shape[0]:\n",
    "                        raise ValueError(\"Shape mismatch for axis 1: appended array must have the same number of rows.\")\n",
    "                else:\n",
    "                    raise ValueError(\"Axis out of bounds. Only axis=0 or axis=1 are supported.\")\n",
    "        \n",
    "        # Concatenate along the specified axis using the appropriate backend.\n",
    "        if self._is_numpy:\n",
    "            new_data = np.concatenate((self.data, append_data), axis=axis)\n",
    "        else:\n",
    "            new_data = torch.cat((self.data, append_data), dim=axis)\n",
    "        \n",
    "        return Matrix(new_data, backend=self._backend)\n",
    "\n",
    "    def __add__(self, other):\n",
    "        \"\"\"\n",
    "        Element-wise addition.\n",
    "        \"\"\"\n",
    "        other_val = other.data if isinstance(other, Object) else other\n",
    "        return Matrix(self.data + other_val, backend=self._backend)\n",
    "    \n",
    "    def __radd__(self, other):\n",
    "        \"\"\"\n",
    "        Right-hand element-wise addition.\n",
    "        \"\"\"\n",
    "        return self.__add__(other)\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        \"\"\"\n",
    "        Element-wise subtraction.\n",
    "        \"\"\"\n",
    "        other_val = other.data if isinstance(other, Object) else other\n",
    "        return Matrix(self.data - other_val, backend=self._backend)\n",
    "    \n",
    "    def __rsub__(self, other):\n",
    "        \"\"\"\n",
    "        Right-hand element-wise subtraction.\n",
    "        \"\"\"\n",
    "        other_val = other.data if isinstance(other, Object) else other\n",
    "        return Matrix(other_val - self.data, backend=self._backend)\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        \"\"\"\n",
    "        Element-wise multiplication.\n",
    "        \"\"\"\n",
    "        other_val = other.data if isinstance(other, Object) else other\n",
    "        return Matrix(self.data * other_val, backend=self._backend)\n",
    "    \n",
    "    def __rmul__(self, other):\n",
    "        \"\"\"\n",
    "        Right-hand element-wise multiplication.\n",
    "        \"\"\"\n",
    "        other_val = other.data if isinstance(other, Object) else other\n",
    "        return Matrix(other_val * self.data, backend=self._backend)\n",
    "    \n",
    "    def __truediv__(self, other):\n",
    "        \"\"\"\n",
    "        Element-wise true division.\n",
    "        \"\"\"\n",
    "        other_val = other.data if isinstance(other, Object) else other\n",
    "        return Matrix(self.data / other_val, backend=self._backend)\n",
    "    \n",
    "    def __rtruediv__(self, other):\n",
    "        \"\"\"\n",
    "        Right-hand element-wise true division.\n",
    "        \"\"\"\n",
    "        other_val = other.data if isinstance(other, Object) else other\n",
    "        return Matrix(other_val / self.data, backend=self._backend)\n",
    "    \n",
    "    def __matmul__(self, other):\n",
    "        \"\"\"\n",
    "        Matrix multiplication using the @ operator.\n",
    "        \n",
    "        Args:\n",
    "            other (Matrix or array-like): The matrix to multiply with.\n",
    "            \n",
    "        Returns:\n",
    "            Matrix: The result of the matrix multiplication.\n",
    "        \"\"\"\n",
    "        other_val = other.data if isinstance(other, Object) else other\n",
    "        result = self.data @ other_val\n",
    "        return Matrix(result, backend=self._backend)\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        \"\"\" Equals operator. \"\"\"\n",
    "        if isinstance(other, Object):\n",
    "            if np.prod(self.data.shape) == 1 and np.prod(other.data.shape) == 1:\n",
    "                # Scalar to scalar, output a scalar\n",
    "                return bool(self.flatten().data == other.flatten().data)\n",
    "            else:\n",
    "                return Matrix(self.data == other.data, backend=self._backend)\n",
    "                \n",
    "        elif np.prod(self.data.shape) == 1 and hasattr(other, \"__len__\") == False:\n",
    "            # Scalar to scalar, output a scalar\n",
    "            return bool(self.data == other)\n",
    "        else:\n",
    "            return Matrix(self.data == other, backend=self._backend)\n",
    "    \n",
    "    def __pow__(self, to_power):\n",
    "        \"\"\"Element-wise power.\"\"\"\n",
    "        return self._apply_op(to_power, lambda a, b: a ** b)\n",
    "\n",
    "    def __rpow__(self, other):\n",
    "        \"\"\"Right-hand element-wise power.\"\"\"\n",
    "        return self.__pow__(other)\n",
    "    \n",
    "    def __neg__(self):\n",
    "        \"\"\"\n",
    "        Returns the negation of the matrix.\n",
    "        \"\"\"\n",
    "        return Matrix(-self.data, backend=self._backend) \n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the length of the matrix.\n",
    "        \"\"\"\n",
    "        return len(self.data)\n",
    "    \n",
    "    def unique(self):\n",
    "        \"\"\"\n",
    "        Returns the unique values of the elements that are non-zero.\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: The unique value matrix.\n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.unique(self.data)\n",
    "        else:\n",
    "            result = torch.unique(self.data)\n",
    "        return Matrix(result, backend=self._backend)\n",
    "    \n",
    "    def nonzero(self):\n",
    "        \"\"\"\n",
    "        Returns the indices of the elements that are non-zero.\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: The indices matrix.\n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = self.data.nonzero()\n",
    "        else:\n",
    "            result = self.data.nonzero()\n",
    "        return Matrix(result, backend=self._backend)\n",
    "    \n",
    "    def any(self, axis = None):\n",
    "        \"\"\"\n",
    "        Computes the element-wise logical OR along a specified axis.\n",
    "        \n",
    "        Args: \n",
    "            axis (int or None): Axis along which to apply the `any` operation. If not provided,\n",
    "                                it applies over all elements of the matrix.\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: A new matrix containing the result of the logical OR operation along the specified axis.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.any(self.data, axis=axis)\n",
    "        else:\n",
    "            result = self.data.any(dim=axis)\n",
    "        return Matrix(result, backend=self._backend)\n",
    "    \n",
    "    def all(self, axis = None):\n",
    "        \"\"\"\n",
    "        Computes the element-wise logical AND along a specified axis.\n",
    "    \n",
    "        Args: \n",
    "            axis (int or None): Axis along which to apply the `all` operation. If not provided,\n",
    "                                it applies over all elements of the matrix.\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: A new matrix containing the result of the logical AND operation along the specified axis.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.all(self.data, axis=axis)\n",
    "        else:\n",
    "            result = self.data.all(dim=axis)\n",
    "        return Matrix(result, backend=self._backend)\n",
    "    \n",
    "    def round(self, digits = 0):\n",
    "        \"\"\"\n",
    "        Rounds the data to a specified number of decimal places.\n",
    "    \n",
    "        Args:\n",
    "            digits (int): The number of decimal places to round the data. Default is 0.\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: A new matrix containing the rounded values of the original data.\n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.round(self.data, decimals = digits)\n",
    "        else:\n",
    "            result = torch.round(self.data, decimals = digits)\n",
    "        return Matrix(result, backend=self._backend)\n",
    "    \n",
    "    def mean(self, axis = None):\n",
    "        \"\"\"\n",
    "        Computes the mean of the matrix along a specified axis.\n",
    "    \n",
    "        Args:\n",
    "            axis (Optional[int]): Axis along which to compute the mean. If None, computes the mean across all dimensions.\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: A new instance containing the computed mean values.\n",
    "    \n",
    "        Raises:\n",
    "            AttributeError: If no data attribute exists in the instance.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.mean(self.data, axis=axis)\n",
    "        else:\n",
    "            result = torch.mean(self.data, dim=axis)\n",
    "        return Matrix(result, backend=self._backend)\n",
    "    \n",
    "    def median(self, axis = None):\n",
    "        \"\"\"\n",
    "        Computes the median along a given axis.\n",
    "    \n",
    "        Args:\n",
    "            axis (Optional[int]): Axis along which to compute the median. Default is None, which computes over all dimensions.\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: A new matrix containing the computed medians.\n",
    "    \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.median(self.data, axis=axis)\n",
    "        else:\n",
    "            if axis is None:\n",
    "                result = torch.median(self.data)\n",
    "            else:\n",
    "                result, _ = torch.median(self.data, dim=axis)\n",
    "        return Matrix(result, backend=self._backend)\n",
    "    \n",
    "    def std(self, axis = None):\n",
    "        \"\"\"\n",
    "        Computes the standard deviation of the matrix along a specified axis.\n",
    "        \n",
    "        Args:\n",
    "            axis (Optional[int]): Axis along which to compute the standard deviation. If None, computes across all dimensions.\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: A new instance containing the computed standard deviation values.\n",
    "        \n",
    "        Raises:\n",
    "            AttributeError: If no data attribute exists in the instance.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.std(self.data, axis=axis)\n",
    "        else:\n",
    "            result = torch.std(self.data, dim=axis)\n",
    "        return Matrix(result, backend=self._backend)\n",
    "    \n",
    "    def var(self, axis = None):\n",
    "        \"\"\"\n",
    "        Computes the variance of the matrix along a specified axis.\n",
    "        \n",
    "        Args:\n",
    "            axis (Optional[int]): Axis along which to compute the variance. If None, computes across all dimensions.\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: A new instance containing the computed variance values.\n",
    "        \n",
    "        Raises:\n",
    "            AttributeError: If no data attribute exists in the instance.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.var(self.data, axis=axis)\n",
    "        else:\n",
    "            result = torch.var(self.data, dim=axis)\n",
    "        return Matrix(result, backend=self._backend)\n",
    "    \n",
    "    def min(self, axis = None):\n",
    "        \"\"\"\n",
    "        Computes the minimum of the matrix along a specified axis.\n",
    "        \n",
    "        Args:\n",
    "            axis (Optional[int]): Axis along which to compute the minimum. If None, computes across all dimensions.\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: A new instance containing the computed minimum values.\n",
    "        \n",
    "        Raises:\n",
    "            AttributeError: If no data attribute exists in the instance.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            if axis is None:\n",
    "                result = np.min(self.data)\n",
    "            else:\n",
    "                result = np.min(self.data, axis=axis)\n",
    "        else:\n",
    "            if axis is None:\n",
    "                result = torch.min(self.data)\n",
    "            else:\n",
    "                result, indices = torch.min(self.data, dim=axis)\n",
    "        return Matrix(result, backend=self._backend)\n",
    "    \n",
    "    def max(self, axis = None):\n",
    "        \"\"\"\n",
    "        Computes the maximum of the matrix along a specified axis.\n",
    "        \n",
    "        Args:\n",
    "            axis (Optional[int]): Axis along which to compute the maximum. If None, computes across all dimensions.\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: A new instance containing the computed maximum values.\n",
    "        \n",
    "        Raises:\n",
    "            AttributeError: If no data attribute exists in the instance.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            if axis is None:\n",
    "                result = np.max(self.data)\n",
    "            else:\n",
    "                result = np.max(self.data, axis=axis)\n",
    "        else:\n",
    "            if axis is None:\n",
    "                result = torch.max(self.data)\n",
    "            else:\n",
    "                result, indices = torch.max(self.data, dim=axis)\n",
    "        return Matrix(result, backend=self._backend)\n",
    "    \n",
    "    def clip(self, a_min=None, a_max=None):\n",
    "        \"\"\"\n",
    "        Clips the values of the matrix to a specified range.\n",
    "    \n",
    "        Args: \n",
    "            a_min (float or None): Minimum value for clipping. If None, no minimum is applied.\n",
    "            a_max (float or None): Maximum value for clipping. If None, no maximum is applied.\n",
    "    \n",
    "        Returns:\n",
    "            Matrix: A new instance containing the clipped values of the original data within the specified range.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.clip(self.data, a_min=a_min, a_max=a_max)\n",
    "        else:\n",
    "            result = torch.clip(self.data, min=a_min, max=a_max)\n",
    "        return Matrix(result, backend=self._backend)\n",
    "    \n",
    "    def sum(self, axis = None, keepdims = False):\n",
    "        \"\"\"\n",
    "        Computes the sum of the matrix along a specified axis.\n",
    "        \n",
    "        Args:\n",
    "            axis (Optional[int]): Axis along which to compute the sum. If None, computes across all dimensions.\n",
    "            keepdims (bool): If keeps the dimension or not.\n",
    "            \n",
    "        Returns:\n",
    "            Matrix: A new instance containing the computed sum values.\n",
    "        \n",
    "        Raises:\n",
    "            AttributeError: If no data attribute exists in the instance.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.sum(self.data, axis=axis, keepdims=keepdims)\n",
    "        else:\n",
    "            result = torch.sum(self.data, dim=axis, keepdim=keepdims)\n",
    "        return Matrix(result, backend=self._backend)\n",
    "    \n",
    "    def cumsum(self, axis = None):\n",
    "        \"\"\"\n",
    "        Computes the cumulative sum of the matrix along a specified axis.\n",
    "        \n",
    "        Args:\n",
    "            axis (Optional[int]): Axis along which to compute the cumulative sum. If None, computes across all dimensions.\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: A new instance containing the computed cumulative sum values.\n",
    "        \n",
    "        Raises:\n",
    "            AttributeError: If no data attribute exists in the instance.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.cumsum(self.data, axis=axis)\n",
    "        else:\n",
    "            result = torch.cumsum(self.data, dim=axis)\n",
    "        return Matrix(result, backend=self._backend)\n",
    "    \n",
    "    def prod(self, axis = None, keepdims = False):\n",
    "        \"\"\"\n",
    "        Computes the product of the matrix along a specified axis.\n",
    "        \n",
    "        Args:\n",
    "            axis (Optional[int]): Axis along which to compute the product. If None, computes across all dimensions.\n",
    "            keepdims (bool): If keeps the dimension or not.\n",
    "            \n",
    "        Returns:\n",
    "            Matrix: A new instance containing the computed product values.\n",
    "        \n",
    "        Raises:\n",
    "            AttributeError: If no data attribute exists in the instance.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.prod(self.data, axis=axis, keepdims=keepdims)\n",
    "        else:\n",
    "            result = torch.prod(self.data, dim=axis, keepdim=keepdims)\n",
    "        return Matrix(result, backend=self._backend)\n",
    "    \n",
    "    def cumprod(self, axis = None):\n",
    "        \"\"\"\n",
    "        Computes the cumulative product of the matrix along a specified axis.\n",
    "        \n",
    "        Args:\n",
    "            axis (Optional[int]): Axis along which to compute the cumulative product. If None, computes across all dimensions.\n",
    "    \n",
    "        Returns:\n",
    "            Matrix: A new instance containing the computed cumulative product values.\n",
    "        \n",
    "        Raises:\n",
    "            AttributeError: If no data attribute exists in the instance.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.cumprod(self.data, axis=axis)\n",
    "        else:\n",
    "            result = torch.cumprod(self.data, dim=axis)\n",
    "        return Matrix(result, backend=self._backend)\n",
    "    \n",
    "    def exp(self):\n",
    "        \"\"\"\n",
    "        Computes the element-wise exponential.\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: New matrix with exponential applied.\n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.exp(self.data)\n",
    "        else:\n",
    "            result = torch.exp(self.data)\n",
    "        return Matrix(result, backend=self._backend)\n",
    "    \n",
    "    def sin(self):\n",
    "        \"\"\"\n",
    "        Computes the element-wise sine.\n",
    "        \n",
    "        Args: \n",
    "            None\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: A new matrix containing the sine values of the original data.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.sin(self.data)\n",
    "        else:\n",
    "            result = torch.sin(self.data)\n",
    "        return Matrix(result, backend=self._backend)\n",
    "    \n",
    "    def cos(self):\n",
    "        \"\"\"\n",
    "        Computes the element-wise cosine.\n",
    "        \n",
    "        Args: \n",
    "            None\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: A new matrix containing the cosine values of the original data.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.cos(self.data)\n",
    "        else:\n",
    "            result = torch.cos(self.data)\n",
    "        return Matrix(result, backend=self._backend)\n",
    "    \n",
    "    def tan(self):\n",
    "        \"\"\"\n",
    "        Computes the element-wise tangent.\n",
    "        \n",
    "        Args: \n",
    "            None\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: A new matrix containing the tangent values of the original data.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.tan(self.data)\n",
    "        else:\n",
    "            result = torch.tan(self.data)\n",
    "        return Matrix(result, backend=self._backend)\n",
    "    \n",
    "    def sinh(self):\n",
    "        \"\"\"\n",
    "        Computes the element-wise hyperbolic sine.\n",
    "        \n",
    "        Args: \n",
    "            None\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: A new matrix containing the hyperbolic sine values of the original data.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.sinh(self.data)\n",
    "        else:\n",
    "            result = torch.sinh(self.data)\n",
    "        return Matrix(result, backend=self._backend)\n",
    "    \n",
    "    def cosh(self):\n",
    "        \"\"\"\n",
    "        Computes the element-wise hyperbolic cosine.\n",
    "        \n",
    "        Args: \n",
    "            None\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: A new matrix containing the hyperbolic cosine values of the original data.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.cosh(self.data)\n",
    "        else:\n",
    "            result = torch.cosh(self.data)\n",
    "        return Matrix(result, backend=self._backend)\n",
    "    \n",
    "    def tanh(self):\n",
    "        \"\"\"\n",
    "        Computes the element-wise hyperbolic tangent.\n",
    "        \n",
    "        Args: \n",
    "            None\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: A new matrix containing the hyperbolic tangent values of the original data.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.tanh(self.data)\n",
    "        else:\n",
    "            result = torch.tanh(self.data)\n",
    "        return Matrix(result, backend=self._backend)\n",
    "    \n",
    "    def abs(self):\n",
    "        \"\"\"\n",
    "        Computes the element-wise absolute values.\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: New matrix with absolute values applied.\n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.abs(self.data)\n",
    "        else:\n",
    "            result = torch.abs(self.data)\n",
    "        return Matrix(result, backend=self._backend)\n",
    "    \n",
    "    def relu(self):\n",
    "        \"\"\"\n",
    "        Computes the element-wise ReLU function.\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: New matrix with ReLU applied.\n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.clip(self.data, 0.0)\n",
    "        else:\n",
    "            result = torch.relu(self.data)\n",
    "        return Matrix(result, backend=self._backend)\n",
    "    \n",
    "    def log(self):\n",
    "        \"\"\"\n",
    "        Computes the element-wise natural logarithm.\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: New matrix with logarithm applied.\n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.log(self.data)\n",
    "        else:\n",
    "            result = torch.log(self.data)\n",
    "        return Matrix(result, backend=self._backend)\n",
    "    \n",
    "    def gamma(self):\n",
    "        \"\"\"\n",
    "        Computes the element-wise Gamma function.\n",
    "        \n",
    "        Args: \n",
    "            None\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: A new matrix containing the Gamma function values for each element in the original data.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = sp.special.gamma(self.data)\n",
    "        else:\n",
    "            def torch_gamma(x):\n",
    "                pos = torch.exp(torch.lgamma(x))\n",
    "                neg = torch.pi / (torch.sin(torch.pi * x) * torch.exp(torch.lgamma(1 - x)))\n",
    "                return torch.where(x > 0, pos, neg)\n",
    "            result = torch_gamma(self.data)\n",
    "        return Matrix(result, backend=self._backend) \n",
    "    \n",
    "    def loggamma(self):\n",
    "        \"\"\"\n",
    "        Computes the element-wise natural logarithm of the Gamma function.\n",
    "        \n",
    "        Args: \n",
    "            None\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: A new matrix containing the natural logarithm of the Gamma function values for each element in the original data.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = sp.special.gammaln(self.data)\n",
    "        else:\n",
    "            result = torch.special.gammaln(self.data)\n",
    "        return Matrix(result, backend=self._backend) \n",
    "    \n",
    "    def sigmoid(self):\n",
    "        \"\"\"\n",
    "        Applies the standard sigmoid function element-wise on the input Matrix.\n",
    "        \n",
    "        f(x) = L / (1 + exp(-1*x))\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: A new Matrix with the sigmoid function applied element-wise.\n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = 1.0 / (1.0 + np.exp(-1.0 * self.data ))\n",
    "        else:\n",
    "            result = torch.sigmoid(self.data)\n",
    "        return Matrix(result, backend=self._backend)\n",
    "    \n",
    "    def logistic(self, L=1.0, k=1.0, x0=0.0):\n",
    "        \"\"\"\n",
    "        Applies the logistic (sigmoid) function element-wise on the input Matrix.\n",
    "        \n",
    "        f(x) = L / (1 + exp(-k*(x - x0)))\n",
    "        \n",
    "        Args:\n",
    "            L (float): The curve's maximum value.\n",
    "            k (float): The steepness of the curve.\n",
    "            x0 (float): The x-value of the sigmoid's midpoint.\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: A new Matrix with the logistic function applied element-wise.\n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = L / (1.0 + np.exp(-k * (self.data - x0)))\n",
    "        else:\n",
    "            result = L / (1.0 + torch.exp(-k * (self.data - x0)))\n",
    "        return Matrix(result, backend=self._backend)\n",
    "    \n",
    "    def logistic_inv(self, L=1.0, k=1.0, x0=0.0):\n",
    "        \"\"\"\n",
    "        Applies the inverse of the logistic (sigmoid) function element-wise on the input Matrix.\n",
    "        \n",
    "        f⁻¹(y) = x0 - (1/k)*ln((L - y)/y)\n",
    "        \n",
    "        Args:\n",
    "            L (float): The curve's maximum value used in the logistic function.\n",
    "            k (float): The steepness of the curve used in the logistic function.\n",
    "            x0 (float): The sigmoid's midpoint used in the logistic function.\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: A new Matrix with the inverse logistic function applied element-wise.\n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = x0 - (1/k) * np.log((L - self.data) / self.data)\n",
    "        else:\n",
    "            result = x0 - (1/k) * torch.log((L - self.data) / self.data)\n",
    "        return Matrix(result, backend=self._backend)\n",
    "    \n",
    "    def cholesky(self, upper = False):\n",
    "        \"\"\"\n",
    "        Computes the Cholesky decomposition of a symmetric positive-definite matrix.\n",
    "        L @ U = self.data\n",
    "        returns L if upper = False else U\n",
    "        \n",
    "        Args:\n",
    "            upper (bool): If True, compute the upper triangular factor. Default is False.\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: A new matrix containing the lower triangular factor of the original data if `upper` is False,\n",
    "                    or its transpose if `upper` is True.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.linalg.cholesky(self.data)\n",
    "            if upper == True:\n",
    "                result = result.T\n",
    "        else:\n",
    "            result = torch.cholesky(self.data, upper = upper)\n",
    "        return Matrix(result, backend=self._backend)\n",
    "    \n",
    "    def softmax(self, axis = 1, keepdims:bool | None = True):\n",
    "        \"\"\"\n",
    "        Applies the softmax function along a specified axis.\n",
    "    \n",
    "        Args:\n",
    "            axis (int): Axis along which to apply the softmax. Default is 1.\n",
    "            keepdims (bool or None): Whether to keep the reduced dimensions as axes with size one. \n",
    "                                    If `True`, the shape of the result will be the same as input; otherwise, it will not have these dimensions.\n",
    "    \n",
    "        Returns:\n",
    "            Matrix: A new matrix containing the softmax values along the specified axis.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            if keepdims is not None and axis is not None:\n",
    "                e_x = np.exp(self.data - np.max(self.data, axis=axis, keepdims=keepdims))\n",
    "                result = e_x / e_x.sum(axis=axis, keepdims=keepdims)\n",
    "            else:\n",
    "                e_x = np.exp(self.data - np.max(self.data, axis=axis))\n",
    "                result = e_x / e_x.sum(axis=axis)\n",
    "        else:\n",
    "            result = torch.nn.functional.softmax(self.data, dim=axis)\n",
    "        return Matrix(result, backend=self._backend)\n",
    "\n",
    "    def argmax(self, axis = 1):\n",
    "        \"\"\"\n",
    "         Computes the indices of the maximum values along a specified axis.\n",
    "         Reminder -> Shapre will be shrinked by 1 so that you may need to reshape() it.\n",
    "        \n",
    "         Args:\n",
    "             axis (int | None): Axis along which to compute the argmax. Default is 1.\n",
    "         \n",
    "         Returns:\n",
    "             Matrix: A new matrix containing the indices of the maximum values along the specified axis.\n",
    "         \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.argmax(self.data, axis=axis)\n",
    "        else:\n",
    "            result = torch.argmax(self.data, dim=axis)\n",
    "        return Matrix(result, backend=self._backend)\n",
    "    \n",
    "    def argmin(self, axis = 1):\n",
    "        \"\"\"\n",
    "        Computes the indices of the minimum values along a specified axis.\n",
    "        Reminder -> Shapre will be shrinked by 1 so that you may need to reshape() it.\n",
    "    \n",
    "        Args:\n",
    "            axis (int | None): Axis along which to compute the argmin. Default is 1.\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: A new matrix containing the indices of the minimum values along the specified axis.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.argmin(self.data, axis=axis)\n",
    "        else:\n",
    "            result = torch.argmin(self.data, dim=axis)\n",
    "        return Matrix(result, backend=self._backend)    \n",
    "    \n",
    "    def flatten(self, major = \"row\"):\n",
    "        \"\"\"\n",
    "        Returns the flattened matrix.\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: The flattened matrix.\n",
    "        \"\"\"\n",
    "        if major == \"col\":\n",
    "            x = self.transpose()\n",
    "            return Matrix(x.data.flatten(), backend=self._backend)\n",
    "        elif major == \"row\":\n",
    "            return Matrix(self.data.flatten(), backend=self._backend)\n",
    "        else:\n",
    "            raise ValueError(\"major must be either 'row' or 'column'!\")\n",
    "            \n",
    "    def reverse(self, axis = 0):\n",
    "        \"\"\"\n",
    "        Reverse the flattened matrix.\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: The reversed matrix.\n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            return Matrix(np.flip(self.data, axis=axis), backend=self._backend)\n",
    "        else:\n",
    "            return Matrix(torch.flip(self.data, axis=axis), backend=self._backend)\n",
    "            \n",
    "    def stack(self, *wargs, axis = 0):\n",
    "        \"\"\"\n",
    "        Stack data in sequence on an axis.\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: The stacked matrix.\n",
    "        \"\"\"\n",
    "        data_list = [self.data]\n",
    "        for arg in wargs:\n",
    "            data_list.append(arg.data)\n",
    "        if self._is_numpy:\n",
    "            result = np.stack(data_list, axis=axis)\n",
    "        else:\n",
    "            result = torch.stack(data_list, dim=axis)\n",
    "        return Matrix(result, backend=self._backend)\n",
    "        \n",
    "    def vstack(self, *wargs):\n",
    "        \"\"\"\n",
    "        Stack data in sequence vertically (row wise).\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: The vstacked matrix.\n",
    "        \"\"\"\n",
    "        data_list = [self.data]\n",
    "        for arg in wargs:\n",
    "            data_list.append(arg.data)\n",
    "        if self._is_numpy:\n",
    "            result = np.vstack(data_list)\n",
    "        else:\n",
    "            result = torch.vstack(data_list)\n",
    "        return Matrix(result, backend=self._backend)\n",
    "    \n",
    "    def hstack(self, *wargs):\n",
    "        \"\"\"\n",
    "        Stack data in sequence horizontally (col wise).\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: The hstacked matrix.\n",
    "        \"\"\"\n",
    "        data_list = [self.data]\n",
    "        for arg in wargs:\n",
    "            data_list.append(arg.data)\n",
    "        if self._is_numpy:\n",
    "            result = np.hstack(data_list)\n",
    "        else:\n",
    "            result = torch.hstack(data_list)\n",
    "        return Matrix(result, backend=self._backend)\n",
    "    \n",
    "    def sign(self):\n",
    "        \"\"\"\n",
    "        Computes the element-wise sign of the data. Returns with the same type.\n",
    "    \n",
    "        Args: \n",
    "            None\n",
    "    \n",
    "        Returns:\n",
    "            Matrix: A new matrix containing the sign values (1 for positive, -1 for negative, 0 for zero) of each element in the original data.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.sign(self.data)\n",
    "        else:\n",
    "            result = torch.sign(self.data)\n",
    "        return Matrix(result, backend=self._backend)\n",
    "    \n",
    "    def repeat(self, repeats, axis=None):\n",
    "        \"\"\"\n",
    "        Repeats the matrix elements along a specified axis.\n",
    "    \n",
    "        Args:\n",
    "            repeats (int or tuple[int]): The number of times to repeat each element.\n",
    "            axis (int): Axis along which to repeat the elements. If `None`, repeats over all dimensions.\n",
    "    \n",
    "        Returns:\n",
    "            Matrix: A new matrix with repeated elements.\n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = self.data.repeat(repeats, axis=axis)\n",
    "        else:\n",
    "            if axis is None:\n",
    "                result = torch.repeat_interleave(self.data, repeats)\n",
    "            else:\n",
    "                result = torch.repeat_interleave(self.data, repeats, dim=axis)\n",
    "        return Matrix(result, backend=self._backend)\n",
    "    \n",
    "    def bincount(self, *, weights = None, inttype: type = int):\n",
    "        \"\"\"\n",
    "        Counts the number of occurrences of each value in `data` and optionally returns a weighted count.\n",
    "    \n",
    "        Args:\n",
    "            weights (array_like | Matrix): An array-like object containing weights corresponding to each element in `data`. Default is None.\n",
    "            inttype (type): A type that the data is going to be casted to.\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: A new matrix with the bin counts or weighted bin counts.\n",
    "    \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.bincount(self.astype(inttype).data, weights = weights)\n",
    "        else:\n",
    "            result = torch.bincount(self.astype(inttype).data, weights = weights)\n",
    "        return Matrix(result, backend=self._backend)\n",
    "    \n",
    "    def transpose(self, *axes):\n",
    "        \"\"\"\n",
    "        Returns the transpose of the matrix.\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: The transposed matrix.\n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = self.data.transpose(axes) if axes else self.data.T\n",
    "        else:\n",
    "            result = self.data.permute(*axes) if axes else self.data.permute(*reversed(range(self.data.dim())))\n",
    "        if len(self.shape) > 1:\n",
    "            return Matrix(result, backend=self._backend)\n",
    "        else:\n",
    "            # From a row vector to a column vector\n",
    "            return Matrix(result.reshape([self.shape[0], 1]), backend=self._backend)\n",
    "        \n",
    "    def quantile(self, q: float, axis = None, keepdims = False):\n",
    "        \"\"\"\n",
    "        Computes the specified quantiles along a given axis.\n",
    "    \n",
    "        Args:\n",
    "            q (float): The quantile to compute. Should be between 0 and 1.\n",
    "            axis (Optional[int]): Axis along which to compute the quantile. Default is None, which computes over all dimensions.\n",
    "            keepdims (bool): Whether to keep the reduced axes in the result as singleton dimensions. Default is False.\n",
    "    \n",
    "        Returns:\n",
    "            Matrix: A new matrix containing the computed quantiles.\n",
    "    \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.quantile(self.data, q, axis = axis, keepdims = keepdims)\n",
    "        else:\n",
    "            if axis is None:\n",
    "                result = torch.quantile(self.data, q, keepdims = keepdims)\n",
    "            else:\n",
    "                result = torch.quantile(self.data, q, dim = axis, keepdims = keepdims)\n",
    "        return Matrix(result, backend=self._backend)\n",
    "        \n",
    "    def sort(self, axis: int | None = None):\n",
    "        \"\"\"\n",
    "        Sorts the matrix elements along (the first column of) a specified axis.\n",
    "        If you intend to sort on only one array, use `sort_along` instead,\n",
    "        or if you want to sort along each column of each base dimension, use `sort_along_each_column` instead.\n",
    "        \n",
    "        Args:\n",
    "            axis (int or None): Axis to sort along. If `None`, sorts the entire matrix.\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: A new matrix with sorted elements.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.sort(self.data, axis=axis)\n",
    "        else:\n",
    "            if axis is None:\n",
    "                result, idx = self.data.sort()\n",
    "            else:\n",
    "                result, idx = self.data.sort(dim=axis)\n",
    "        return Matrix(result, backend=self._backend)\n",
    "    \n",
    "    def sort_along(self, axis: tuple = (None, 0)):\n",
    "        \"\"\"\n",
    "        Sort the N-dimensional data along the 1d values on `axis`.\n",
    "        If you intend to sort on the first column of axis `axis`, use `sort` to speed up,\n",
    "        or if you want to sort along each column of each base dimension, use `sort_along_each_column` instead.\n",
    "       \n",
    "        Detail:\n",
    "        Sort the input array x. The axis parameter is a tuple of t\n",
    "        he same length as x.ndim, where each position can be None or an integer. \n",
    "        It is required that exactly one position d in axis_vec is not None,\n",
    "        and the reference position of this dimension d is fixed = axis_vec[d], \n",
    "        but when taking this reference, the index 0 is selected for each dimension \n",
    "        in the global uniform way (i.e., only x[(0,)*d + (fixed,)] is used as the \n",
    "        reference). \n",
    "        The 1D sorting permutation is calculated (using argsort, in ascending order), \n",
    "        and then the global permutation is applied to dimension d+1 \n",
    "        (the subsequent dimension) of x, acting on all data without sorting each\n",
    "        preceding block separately.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "            axis : tuple\n",
    "                The indicator indicating sort which data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            Matrix, sorted copy.\n",
    "\n",
    "        \"\"\"\n",
    "        if len(self.data.shape) != len(axis):\n",
    "            raise ValueError(\"The length of axis must be equal to the number of dimensions of the input array.\")\n",
    "        non_none = [(d, val) for d, val in enumerate(axis) if val is not None]\n",
    "        if len(non_none) != 1:\n",
    "            raise ValueError(\"There must be exactly one non-None element in axis\")\n",
    "        d, fixed = non_none[0]\n",
    "        if d > len(self.data.shape) - 1:\n",
    "            raise ValueError(\"The non-None dimension cannot be greater than dimension.\")\n",
    "        if fixed < 0 or fixed >= self.data.shape[d]:\n",
    "            raise IndexError(f\"Fixed index {fixed} is out of range for dimension {d} (0 to {self.data.shape[d]-1})\")\n",
    "        \n",
    "        # Last dim sort\n",
    "        if d == len(self.data.shape) - 1 and len(self.data.shape) == 2:\n",
    "            # Transpose, and sort the row\n",
    "            return self.transpose().sort_along(axis=(axis[1], axis[0])).transpose()\n",
    "        elif d == len(self.data.shape) - 1 and len(self.data.shape) > 2:\n",
    "            # Transpose the inner two dimensions\n",
    "            tr_axe = list(range(len(self.data.shape))); tmp = tr_axe[-1]; tr_axe[-1] = tr_axe[-2]; tr_axe[-2] = tmp;\n",
    "            axis_new = list(np.repeat(None, len(self.data.shape))); axis_new[-2] = fixed\n",
    "            return self.transpose(*tr_axe).sort_along(axis=axis_new).transpose(*tr_axe)\n",
    "        \n",
    "        # Extract global reference: fixed on dimension d, but all dimensions before d are indexed as 0.\n",
    "        # Construct index tuple: fixed to 0 for dimensions < d, fixed to the dth dimension, and use slice(None) to eliminate the remaining axes.\n",
    "        idx = (0,) * d + (fixed,)\n",
    "        \n",
    "        if self._is_numpy:\n",
    "            # Extract the reference key, which is expected to be 1D and have a length equal to self.data.shape[d+1]\n",
    "            key = np.asarray(self.data[idx])\n",
    "            if key.ndim != 1 or key.shape[0] != self.data.shape[d+1]:\n",
    "                raise ValueError(\"The reference key must be one-dimensional and its length must be the same as the length of the sorting axis.\")\n",
    "            order = np.argsort(key)\n",
    "           \n",
    "            # Construct a global index array for np.take_along_axis: needs to have the same shape as x,\n",
    "            # but order along sorting axis d+1, other dimensions are copied via broadcasting.\n",
    "            order_shape = [1] * len(self.data.shape)\n",
    "            order_shape[d+1] = self.data.shape[d+1]\n",
    "            order_global = order.reshape(order_shape)\n",
    "            order_global = np.broadcast_to(order_global, self.data.shape)\n",
    "            sorted_ = np.take_along_axis(self.data, order_global, axis=d+1)\n",
    "            return Matrix(sorted_, backend=self._backend, dtype=self.dtype, device=self.device)\n",
    "\n",
    "        else:\n",
    "            key = self.data[idx]\n",
    "            # key should be 1D, and its length should be equal to self.data.shape[d+1]\n",
    "            if key.dim() != 1 or key.size(0) != self.data.shape[d+1]:\n",
    "                raise ValueError(\"The reference key must be one-dimensional and its length must be the same as the length of the sorting axis.\")\n",
    "            # Calculate the sort order (ascending)\n",
    "            order = torch.argsort(key, dim=0)\n",
    "            \n",
    "            # Construct a global index tensor with the same shape as self.data, but with order on the sorting axis d+1\n",
    "            order_shape = [1] * len(self.data.shape)\n",
    "            order_shape[d+1] = self.data.shape[d+1]\n",
    "            order_global = order.view(*order_shape).expand(self.data.shape)\n",
    "            \n",
    "            # Use torch.gather to rearrange self.data according to the global index tensor on dim=d+1\n",
    "            sorted_ = torch.gather(self.data, dim=d+1, index=order_global)\n",
    "            return Matrix(sorted_, backend=self._backend, dtype=self.dtype, device=self.device)\n",
    "        \n",
    "    def sort_along_each_column(self, axis: int = 1, on_col: int = 0):\n",
    "        \"\"\"\n",
    "        Sort the N-dimensional data along values on column `on_col` of the axis `axis`.\n",
    "        Note, it will sort EACH `on_col` of the exterior axises.\n",
    "        If you intend to sort on the first column of axis `axis`, use `sort` to speed up.\n",
    "       \n",
    "        Detail:\n",
    "        Instead of sorting itself in d dimensions, use the reference sequence obtained by taking index=i on the d axis of x, and apply the same rearrangement to the d+1 axis (next axis) of x.\n",
    "\n",
    "        For example, for a 2D array, when d=0, i=1,\n",
    "        take the reference sequence = x[1, :], calculate its argsort to get the sorted arrangement, and then rearrange the columns of each row of the entire array according to this arrangement;\n",
    "        For a 3D array, when d=1, i=0,\n",
    "        for each subarray with fixed axis=0, take the reference sequence = subarray[0, :] (that is, the row of axis=1 index 0),\n",
    "        calculate argsort (sort the elements in the reference sequence), and then rearrange all rows in the subarray (all slices of axis=1) on axis=2 according to this arrangement.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "            axis : int\n",
    "                The axis of the column is on. The default is 1.\n",
    "            on_col : int\n",
    "                The index of the column is on. The default is 0.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            Matrix, sorted copy.\n",
    "\n",
    "        \"\"\"\n",
    "        if len(self.data.shape) < 2:\n",
    "            raise ValueError(\"The input data must at least have 2 dimensions. Use `sort` if it is a 1d array.\")\n",
    "        if axis < 0 or axis > len(self.data.shape) - 1:\n",
    "            raise ValueError(\"The parameter d must be positive and smaller than ndim.\")\n",
    "        if axis == len(self.data.shape) - 1 and len(self.data.shape) == 2:\n",
    "            # Transpose, and sort the row\n",
    "            return self.transpose().sort_along_each_column(axis=0, on_col=on_col).transpose()\n",
    "        elif axis == len(self.data.shape) - 1 and len(self.data.shape) > 2:\n",
    "            # Transpose the inner two dimensions\n",
    "            tr_axe = list(range(len(self.data.shape))); tmp = tr_axe[-1]; tr_axe[-1] = tr_axe[-2]; tr_axe[-2] = tmp;\n",
    "            return self.transpose(*tr_axe).sort_along_each_column(axis=axis-1, on_col=on_col).transpose(*tr_axe)\n",
    "            \n",
    "        sorted_axis = axis + 1\n",
    "\n",
    "        if self._is_numpy:\n",
    "            key = np.take(self.data, indices=on_col, axis=axis)\n",
    "            order = np.argsort(key, axis=axis)\n",
    "            order_expanded = np.expand_dims(order, axis=axis)\n",
    "            sorted_ = np.take_along_axis(self.data, order_expanded, axis=sorted_axis)\n",
    "            return Matrix(sorted_, backend=self._backend, dtype=self.dtype, device=self.device)\n",
    "\n",
    "        else:\n",
    "            key = self.data.select(dim=axis, index=on_col)\n",
    "            order = torch.argsort(key, dim=axis)\n",
    "            order_expanded = order.unsqueeze(dim=axis)\n",
    "            expand_shape = list(self.data.shape)\n",
    "            index = order_expanded.expand(*expand_shape)\n",
    "            sorted_ = torch.gather(self.data, dim=sorted_axis, index=index)\n",
    "            return Matrix(sorted_, backend=self._backend, dtype=self.dtype, device=self.device)        \n",
    "    \n",
    "    def _eigen_kernel(self):\n",
    "        \"\"\"\n",
    "        Calculates eigenvalues and eigenvectors using internal libraries (numpy or torch).\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: eigen_values, eigen_vectors: The computed eigenvalues and eigenvectors.\n",
    "        \"\"\"\n",
    "        if self.shape[0] != self.shape[1]:\n",
    "            raise ValueError(\"Eigen decomposition requires a square matrix.\")\n",
    "        if self._is_numpy:\n",
    "            eigen_values, eigen_vectors = np.linalg.eig(self.data)\n",
    "        else:\n",
    "            # Using torch.linalg.eig (available in newer versions of PyTorch)\n",
    "            eigen_values, eigen_vectors = torch.linalg.eig(self.data)\n",
    "        return Matrix(eigen_values, backend=self._backend), Matrix(eigen_vectors, backend=self._backend)\n",
    "\n",
    "    def _jacobi_eigen(self, tol=1e-10, max_iterations=1000):\n",
    "        \"\"\"\n",
    "        Eigen decomposition using the Jacobi rotation method for symmetric matrices.\n",
    "        \n",
    "        Returns:\n",
    "            eigen_values, eigen_vectors: Sorted in descending order.\n",
    "        \"\"\"\n",
    "        A = self.data.astype(float).copy()\n",
    "        n = A.shape[0]\n",
    "        V = np.eye(n)\n",
    "        for iteration in range(max_iterations):\n",
    "            p, q = 0, 1\n",
    "            max_val = 0\n",
    "            for i in range(n):\n",
    "                for j in range(i+1, n):\n",
    "                    if abs(A[i, j]) > max_val:\n",
    "                        max_val = abs(A[i, j])\n",
    "                        p, q = i, j\n",
    "            if max_val < tol:\n",
    "                break\n",
    "            if abs(A[p, p] - A[q, q]) < tol:\n",
    "                theta = np.pi / 4\n",
    "            else:\n",
    "                theta = 0.5 * np.arctan2(2 * A[p, q], A[q, q] - A[p, p])\n",
    "            c = np.cos(theta)\n",
    "            s = np.sin(theta)\n",
    "            app, aqq, apq = A[p, p], A[q, q], A[p, q]\n",
    "            A[p, p] = c**2 * app - 2 * s * c * apq + s**2 * aqq\n",
    "            A[q, q] = s**2 * app + 2 * s * c * apq + c**2 * aqq\n",
    "            A[p, q] = 0.0\n",
    "            A[q, p] = 0.0\n",
    "            for i in range(n):\n",
    "                if i != p and i != q:\n",
    "                    aip, aiq = A[i, p], A[i, q]\n",
    "                    A[i, p] = c * aip - s * aiq\n",
    "                    A[p, i] = A[i, p]\n",
    "                    A[i, q] = s * aip + c * aiq\n",
    "                    A[q, i] = A[i, q]\n",
    "            for i in range(n):\n",
    "                vip, viq = V[i, p], V[i, q]\n",
    "                V[i, p] = c * vip - s * viq\n",
    "                V[i, q] = s * vip + c * viq\n",
    "        eigen_values = np.diag(A)\n",
    "        idx = np.argsort(eigen_values)[::-1]\n",
    "        eigen_values = eigen_values[idx]\n",
    "        eigen_vectors = V[:, idx]\n",
    "        return Matrix(eigen_values, backend=self._backend), Matrix(eigen_vectors, backend=self._backend)\n",
    "\n",
    "    def _qr_eigen_nonsymmetric(self, tol=1e-10, max_iterations=1000):\n",
    "        \"\"\"\n",
    "        Eigen decomposition for non-symmetric matrices using a basic QR algorithm\n",
    "        for eigenvalues and SVD-based extraction for eigenvectors.\n",
    "        \n",
    "        Returns:\n",
    "            eigen_values, eigen_vectors: Sorted in descending order by modulus.\n",
    "        \"\"\"\n",
    "        # Work in complex to capture possible complex eigenvalues.\n",
    "        A_orig = np.array(self.data, dtype=complex)\n",
    "        n = A_orig.shape[0]\n",
    "        A = A_orig.copy()\n",
    "        for _ in range(max_iterations):\n",
    "            Q, R = np.linalg.qr(A)\n",
    "            A = R @ Q\n",
    "            off_diag = A - np.diag(np.diag(A))\n",
    "            if np.linalg.norm(off_diag) < tol:\n",
    "                break\n",
    "        eigen_values = np.diag(A)\n",
    "        # Compute eigenvectors by solving (A_orig - lambda I)v = 0 via SVD.\n",
    "        eigen_vectors = np.empty((n, n), dtype=complex)\n",
    "        for j, lam in enumerate(eigen_values):\n",
    "            B = A_orig - lam * np.eye(n, dtype=complex)\n",
    "            U, S, Vh = np.linalg.svd(B)\n",
    "            v = Vh.conj().T[:, -1]\n",
    "            v = v / np.linalg.norm(v)\n",
    "            eigen_vectors[:, j] = v\n",
    "        # Sort eigenpairs by descending modulus of eigenvalues.\n",
    "        idx = np.argsort(np.abs(eigen_values))[::-1]\n",
    "        eigen_values = eigen_values[idx]\n",
    "        eigen_vectors = eigen_vectors[:, idx]\n",
    "        return Matrix(eigen_values, backend = self._backend), Matrix(eigen_vectors, backend = self._backend)\n",
    "\n",
    "    def eigen(self, method=\"kernel\", symmetric=None, tol=1e-10, max_iterations=1000):\n",
    "        \"\"\"\n",
    "        Unified eigen decomposition method.\n",
    "        \n",
    "        Args:\n",
    "            method (str): \"kernel\" to use internal libraries; \"selfimpl\" to use the self-implemented solver.\n",
    "            symmetric (bool, optional): If known symmetric; if None, determined automatically.\n",
    "            tol (float): Tolerance for convergence (used in self-implementation).\n",
    "            max_iterations (int): Maximum iterations (used in self-implementation).\n",
    "        \n",
    "        Returns:\n",
    "            eigen_values, eigen_vectors: eigenvalues and eigenvectors.\n",
    "        \"\"\"\n",
    "        if self.shape[0] != self.shape[1]:\n",
    "            raise ValueError(\"Eigen decomposition requires a square matrix.\")\n",
    "        # Determine symmetry if not explicitly provided.\n",
    "        if symmetric is None:\n",
    "            if self._is_numpy:\n",
    "                symmetric = np.allclose(self.data, self.data.T, atol=tol)\n",
    "            else:\n",
    "                symmetric = torch.allclose(self.data, self.data.T)\n",
    "                \n",
    "        if method == \"kernel\":\n",
    "            return self._eigen_kernel()\n",
    "        \n",
    "        elif method == \"selfimpl\":\n",
    "            if self._is_numpy:\n",
    "                if symmetric:\n",
    "                    return self._jacobi_eigen(tol, max_iterations)\n",
    "                else:\n",
    "                    return self._qr_eigen_nonsymmetric(tol, max_iterations)\n",
    "            else:\n",
    "                # Use kernel instead\n",
    "                return self._eigen_kernel()\n",
    "        else:\n",
    "            raise ValueError(\"Unknown eigen method. Use 'kernel' or 'selfimpl'.\")\n",
    "\n",
    "    def _check_square(self):\n",
    "        \"\"\"\n",
    "        Internal helper to ensure the matrix is square (required for determinant, inverse, and trace).\n",
    "        \n",
    "        Raises:\n",
    "            ValueError: If the matrix is not square.\n",
    "        \"\"\"\n",
    "        if len(self.shape) != 2 or self.shape[0] != self.shape[1]:\n",
    "            raise ValueError(\"This operation requires a square matrix.\")\n",
    "    \n",
    "    def determinant(self):\n",
    "        \"\"\"\n",
    "        Computes the determinant of a square matrix.\n",
    "        \n",
    "        Returns:\n",
    "            Scalar: The determinant value.\n",
    "            \n",
    "        Raises:\n",
    "            ValueError: If the matrix is not square.\n",
    "        \"\"\"\n",
    "        self._check_square()\n",
    "        if self._is_numpy:\n",
    "            return np.linalg.det(self.data)\n",
    "        else:\n",
    "            return torch.det(self.data)\n",
    "    \n",
    "    def inverse(self):\n",
    "        \"\"\"\n",
    "        Computes the inverse of a square matrix.\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: The inverse matrix.\n",
    "            \n",
    "        Raises:\n",
    "            ValueError: If the matrix is not square.\n",
    "        \"\"\"\n",
    "        self._check_square()\n",
    "        if self._is_numpy:\n",
    "            inv_data = np.linalg.inv(self.data)\n",
    "        else:\n",
    "            inv_data = torch.inverse(self.data)\n",
    "        return Matrix(inv_data, backend=self._backend)\n",
    "    \n",
    "    def trace(self):\n",
    "        \"\"\"\n",
    "        Computes the trace of a square matrix (sum of diagonal elements).\n",
    "        \n",
    "        Returns:\n",
    "            Scalar: The trace value.\n",
    "            \n",
    "        Raises:\n",
    "            ValueError: If the matrix is not square.\n",
    "        \"\"\"\n",
    "        self._check_square()\n",
    "        if self._is_numpy:\n",
    "            return np.trace(self.data)\n",
    "        else:\n",
    "            return torch.trace(self.data)\n",
    "    \n",
    "    def diag(self):\n",
    "        \"\"\"\n",
    "        Computes the diagonal vector of a square matrix.\n",
    "        Or create a diagonal matrix if 1D matrix.\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: The diagonal vector.\n",
    "            \n",
    "        Raises:\n",
    "            ValueError: If the matrix is not square.\n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            return Matrix(np.diag(self.data), backend=self._backend)\n",
    "        else:\n",
    "            return Matrix(torch.diag(self.data), backend=self._backend)\n",
    "    \n",
    "    def dot(self, other):\n",
    "        \"\"\"\n",
    "        Computes the dot product of this matrix with another Matrix.\n",
    "        \n",
    "        Args:\n",
    "            other (Matrix): The other matrix to multiply.\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: A new Matrix instance with the result of the dot product.\n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            return Matrix(np.dot(self.data, other.data), backend=\"numpy\")\n",
    "        else:\n",
    "            return Matrix(torch.matmul(self.data, other.data), backend=\"torch\")\n",
    "    \n",
    "    def inner(self, other):\n",
    "        \"\"\"\n",
    "        Computes the inner product of this matrix with another Matrix.\n",
    "        \n",
    "        Args:\n",
    "            other (Matrix): The other matrix to multiply.\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: A new Matrix instance with the result of the inner product.\n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            return Matrix(np.inner(self.data, other.data), backend=\"numpy\")\n",
    "        else:\n",
    "            return Matrix(torch.inner(self.data, other.data), backend=\"torch\")\n",
    "    \n",
    "    def outer(self, other):\n",
    "        \"\"\"\n",
    "        Computes the outer product of this matrix with another Matrix.\n",
    "        \n",
    "        Args:\n",
    "            other (Matrix): The other matrix to multiply.\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: A new Matrix instance with the result of the outer product.\n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            return Matrix(np.outer(self.data, other.data), backend=\"numpy\")\n",
    "        else:\n",
    "            return Matrix(torch.outer(self.data, other.data), backend=\"torch\")\n",
    "    \n",
    "    def svd(self, full_matrices=True):\n",
    "        \"\"\"\n",
    "        Computes the Singular Value Decomposition (SVD) of the matrix.\n",
    "        \n",
    "        Args:\n",
    "            full_matrices (bool): If True, compute the full SVD; otherwise, compute the reduced SVD.\n",
    "            \n",
    "        Returns:\n",
    "            Tuple[Matrix, Matrix, Matrix]: A tuple containing U, S, and V^T as Matrix objects.\n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            U, S, Vh = np.linalg.svd(self.data, full_matrices=full_matrices)\n",
    "            return Matrix(U, backend=\"numpy\"), Matrix(S, backend=\"numpy\"), Matrix(Vh, backend=\"numpy\")\n",
    "        else:\n",
    "            if hasattr(torch.linalg, 'svd'):\n",
    "                U, S, Vh = torch.linalg.svd(self.data, full_matrices=full_matrices)\n",
    "            else:\n",
    "                U, S, V = torch.svd(self.data, some=not full_matrices)\n",
    "                Vh = V.t()\n",
    "            return Matrix(U, backend=\"torch\"), Matrix(S, backend=\"torch\"), Matrix(Vh, backend=\"torch\")\n",
    "          \n",
    "    def to_zeros(self):\n",
    "        \"\"\"\n",
    "        Converts the Matrix data into a same shape Matrix with 0s.\n",
    "        \n",
    "        Args: \n",
    "            None\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: a same shape Matrix with 0s.\n",
    "        \n",
    "        \"\"\"\n",
    "        x = self.copy()\n",
    "        x[...] = 0\n",
    "        return x\n",
    "    \n",
    "    def to_ones(self):\n",
    "        \"\"\"\n",
    "        Converts the Matrix data into a same shape Matrix with 1s.\n",
    "        \n",
    "        Args: \n",
    "            None\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: a same shape Matrix with 1s.\n",
    "        \n",
    "        \"\"\"\n",
    "        x = self.copy()\n",
    "        x[...] = 1\n",
    "        return x\n",
    "    \n",
    "    def to_ks(self, k: float | int = 0):\n",
    "        \"\"\"\n",
    "        Converts the Matrix data into a same shape Matrix with ks.\n",
    "        \n",
    "        Args: \n",
    "            None\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: a same shape Matrix with 0s.\n",
    "        \n",
    "        \"\"\"\n",
    "        x = self.copy()\n",
    "        x[...] = k\n",
    "        return x        \n",
    "\n",
    "    def to_rands(self):\n",
    "        \"\"\"\n",
    "        Converts the Matrix data into a same shape Matrix with uniform random numbers.\n",
    "        \n",
    "        Args: \n",
    "            None\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: a same shape Matrix with uniform random numbers.\n",
    "        \n",
    "        \"\"\"\n",
    "        return self.rand(self.shape, backend=self._backend, dtype=self.dtype, device=self.device)\n",
    "\n",
    "    def to_list(self):\n",
    "        \"\"\"\n",
    "        Converts the matrix data into a Python list.\n",
    "        \n",
    "        Args: \n",
    "            None\n",
    "        \n",
    "        Returns:\n",
    "            list: A Python list containing the same elements as `self.data`.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            return self.data.tolist()\n",
    "        else:\n",
    "            return self.data.cpu().tolist()\n",
    "        \n",
    "    def to_numpy_array(self):\n",
    "        \"\"\"\n",
    "        Converts the matrix data into a NumPy array.\n",
    "        \n",
    "        Returns: \n",
    "            np.ndarray: The underlying NumPy array of the matrix.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            return self.data\n",
    "        else:\n",
    "            return self.data.detach().cpu().numpy()\n",
    "        \n",
    "    def to_torch_tensor(self, *, dtype=None, device=None):\n",
    "        \"\"\"\n",
    "        Converts the matrix data into a PyTorch tensor.\n",
    "        \n",
    "        Args: \n",
    "            dtype (torch.dtype or None): The desired data type for the resulting tensor. If not provided,\n",
    "                                         uses the current data type of `self.data`.\n",
    "            device (torch.device or None): The target device to which the tensor should be moved.\n",
    "                                           If not provided, it will use the default device.\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: A PyTorch tensor containing the same data as `self.data`.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            return torch.tensor(self.data, dtype=dtype, device=device)\n",
    "        else:\n",
    "            return self.data\n",
    "        \n",
    "    @staticmethod\n",
    "    def equal(x, other, *, equal_nan=False):\n",
    "        \"\"\"\n",
    "        Compare if two Matrix objects have the same shape and elements.\n",
    "        \n",
    "        Args:\n",
    "            x (Matrix): The one matrix to compare.\n",
    "            other (Matrix): The other matrix to compare.\n",
    "        \n",
    "        Returns:\n",
    "           ``True`` if two matrices have the same size and elements, \n",
    "           ``False`` otherwise.\n",
    "        \"\"\"\n",
    "        if x._is_numpy == True and other._is_numpy == True:\n",
    "            return np.array_equal(x, other, equal_nan=equal_nan)\n",
    "        elif  x._is_numpy == False and other._is_numpy == False:\n",
    "            return torch.equal(x, other)\n",
    "        else:\n",
    "            raise ValueError(\"Input `x` and `other` for comparison must have to have the same backend!\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def gather_along(data, axis, index):\n",
    "        \"\"\"\n",
    "        Gather values on an axis with specified index.\n",
    "        \n",
    "        Parameters:\n",
    "            axis: int, the axis number to gather values on.\n",
    "            index: list | array | Matrix, the indices for each row/column/.. to gather values on.\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: gathered elements.\n",
    "        \"\"\"\n",
    "        if data._is_numpy:\n",
    "            result = np.take_along_axis(data.data, indices=index, axis=axis)\n",
    "        else:\n",
    "            result = torch.gather(data.data, dim=axis, index=index.data)\n",
    "        return Matrix(result, backend=data._backend, dtype=data.dtype, device=data.device)\n",
    "    \n",
    "    @staticmethod\n",
    "    def where(condition, backend=\"numpy\", dtype=None, device=None):\n",
    "        \"\"\"\n",
    "        Returns elements depending on `condition`.\n",
    "        \n",
    "        Parameters:\n",
    "            condition : Internal Type (array_like); bool Where True\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: chosen elements.\n",
    "        \"\"\"\n",
    "        if backend == \"numpy\":\n",
    "            result = np.where(condition)\n",
    "        else:\n",
    "            result = torch.where(condition)         \n",
    "            if isinstance(result, tuple):\n",
    "                result = result[0]\n",
    "        return Matrix(result, backend=backend, dtype=dtype, device=device)\n",
    "    \n",
    "    @staticmethod\n",
    "    def where_as(condition, then, other, backend=\"numpy\", dtype=None, device=None):\n",
    "        \"\"\"\n",
    "        Returns elements depending on `condition`.\n",
    "        \n",
    "        Parameters:\n",
    "            condition : Internal Type (array_like); bool Where True\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: if true then applied then to true elements; other to fales elements.\n",
    "        \"\"\"\n",
    "        if backend == \"numpy\":\n",
    "            result = np.where(condition, then, other)\n",
    "        else:\n",
    "            result = torch.where(condition, then, other)         \n",
    "            if isinstance(result, tuple):\n",
    "                result = result[0]\n",
    "        return Matrix(result, backend=backend, dtype=dtype, device=device)\n",
    "    \n",
    "    @staticmethod\n",
    "    def zeros(shape, backend=\"numpy\", dtype=None, device=None):\n",
    "        \"\"\"\n",
    "        Creates a matrix filled with zeros.\n",
    "        \n",
    "        Args:\n",
    "            shape (tuple): The shape of the matrix.\n",
    "            backend (str): The backend (\"numpy\" or \"torch\").\n",
    "            dtype: Desired data type.\n",
    "            device: Data device, \"cpu\" or \"cuda\".\n",
    "            \n",
    "        Returns:\n",
    "            Matrix: A new matrix of zeros.\n",
    "        \"\"\"\n",
    "        if backend == \"numpy\":\n",
    "            data = np.zeros(shape, dtype=dtype)\n",
    "        elif backend == \"torch\":\n",
    "            data = torch.zeros(shape, dtype=dtype, device=device) if dtype else torch.zeros(shape, device=device)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported backend. Choose 'numpy' or 'torch'.\")\n",
    "        return Matrix(data, backend=backend)\n",
    "    \n",
    "    @staticmethod\n",
    "    def ones(shape, backend=\"numpy\", dtype=None, device=None):\n",
    "        \"\"\"\n",
    "        Creates a matrix filled with ones.\n",
    "        \n",
    "        Args:\n",
    "            shape (tuple): The shape of the matrix.\n",
    "            backend (str): The backend (\"numpy\" or \"torch\").\n",
    "            dtype: Desired data type.\n",
    "            device: Data device, \"cpu\" or \"cuda\".\n",
    "            \n",
    "        Returns:\n",
    "            Matrix: A new matrix of ones.\n",
    "        \"\"\"\n",
    "        if backend == \"numpy\":\n",
    "            data = np.ones(shape, dtype=dtype)\n",
    "        elif backend == \"torch\":\n",
    "            data = torch.ones(shape, dtype=dtype, device=device) if dtype else torch.ones(shape, device=device)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported backend. Choose 'numpy' or 'torch'.\")\n",
    "        return Matrix(data, backend=backend)\n",
    "    \n",
    "    @staticmethod\n",
    "    def zeros_like(x, backend=\"numpy\", dtype=None, device=None):\n",
    "        \"\"\"\n",
    "        Creates a matrix of zeros with the same shape and data type as another matrix.\n",
    "        \n",
    "        Args:\n",
    "            x (Matrix): The input matrix.\n",
    "            backend (str): The backend for computation (\"numpy\" or \"torch\"). Default is \"numpy\".\n",
    "            dtype: Desired data type for the result. If not specified, uses the data type from `x`.\n",
    "            device: Data device, \"cpu\" or \"cuda\".\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: A new matrix containing zeros with the same shape and type as `x`.\n",
    "        \n",
    "        Raises:\n",
    "            ValueError: If an unsupported backend is provided.\n",
    "        \n",
    "        \"\"\"\n",
    "        if backend == \"numpy\":\n",
    "            data = np.zeros_like(x.data, dtype=dtype)\n",
    "        elif backend == \"torch\":\n",
    "            data = torch.zeros_like(x.data, dtype=dtype, device=device) if dtype else torch.zeros_like(x.data, device=device)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported backend. Choose 'numpy' or 'torch'.\")\n",
    "        return Matrix(data, backend=backend)\n",
    "    \n",
    "    @staticmethod\n",
    "    def ones_like(x, backend=\"numpy\", dtype=None, device=None):\n",
    "        \"\"\"\n",
    "        Creates a matrix of ones with the same shape and data type as another matrix.\n",
    "        \n",
    "        Args:\n",
    "            x (Matrix): The input matrix.\n",
    "            backend (str): The backend for computation (\"numpy\" or \"torch\"). Default is \"numpy\".\n",
    "            dtype: Desired data type for the result. If not specified, uses the data type from `x`.\n",
    "            device: Data device, \"cpu\" or \"cuda\".\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: A new matrix containing ones with the same shape and type as `x`.\n",
    "        \n",
    "        Raises:\n",
    "            ValueError: If an unsupported backend is provided.\n",
    "        \n",
    "        \"\"\"\n",
    "        if backend == \"numpy\":\n",
    "            data = np.ones_like(x.data, dtype=dtype)\n",
    "        elif backend == \"torch\":\n",
    "            data = torch.ones_like(x.data, dtype=dtype, device=device) if dtype else torch.ones_like(x.data, device=device)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported backend. Choose 'numpy' or 'torch'.\")\n",
    "        return Matrix(data, backend=backend)\n",
    "    \n",
    "    @staticmethod\n",
    "    def identity(n, backend=\"numpy\", dtype=None):\n",
    "        \"\"\"\n",
    "        Creates an identity matrix of size n x n.\n",
    "        \n",
    "        Args:\n",
    "            n (int): The number of rows and columns.\n",
    "            backend (str): The backend (\"numpy\" or \"torch\").\n",
    "            dtype: Desired data type.\n",
    "            \n",
    "        Returns:\n",
    "            Matrix: An identity matrix.\n",
    "        \"\"\"\n",
    "        bk = backend.lower()\n",
    "        if bk == \"numpy\":\n",
    "            data = np.eye(n, dtype=dtype)\n",
    "        elif bk == \"torch\":\n",
    "            if torch is None:\n",
    "                raise ImportError(\"PyTorch is not installed.\")\n",
    "            data = torch.eye(n, dtype=dtype)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported backend. Choose 'numpy' or 'torch'.\")\n",
    "        return Matrix(data, backend=bk)\n",
    "    \n",
    "    @staticmethod\n",
    "    def rand(shape, backend=\"numpy\", dtype=None, device=None):\n",
    "        \"\"\"\n",
    "        Creates a matrix with random values uniformly distributed in [0, 1).\n",
    "        \n",
    "        Args:\n",
    "            shape (tuple): The shape of the matrix.\n",
    "            backend (str): The backend (\"numpy\" or \"torch\").\n",
    "            dtype: Desired data type.\n",
    "            device: Data device, \"cpu\" or \"cuda\".\n",
    "            \n",
    "        Returns:\n",
    "            Matrix: A new matrix with random values.\n",
    "        \"\"\"\n",
    "        bk = backend.lower()\n",
    "        if bk == \"numpy\":\n",
    "            data = np.random.rand(*shape)\n",
    "            if dtype:\n",
    "                data = data.astype(dtype)\n",
    "        elif bk == \"torch\":\n",
    "            if torch is None:\n",
    "                raise ImportError(\"PyTorch is not installed.\")\n",
    "            data = torch.rand(shape, dtype=dtype, device=device) if dtype else torch.rand(shape, device=device)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported backend. Choose 'numpy' or 'torch'.\")\n",
    "        return Matrix(data, backend=bk)\n",
    "\n",
    "    @staticmethod\n",
    "    def least_square(X, Y, backend=\"numpy\", dtype=None):\n",
    "        \"\"\"\n",
    "        Solves the linear system bX = Y using the normal equation approach.\n",
    "        \n",
    "        Args:\n",
    "            X (Matrix): The matrix of features or independent variables.\n",
    "            Y (Matrix): The matrix of observations or dependent variables.\n",
    "            backend (str): The backend (\"numpy\" or \"torch\").\n",
    "            dtype: Desired data type for the result.\n",
    "            \n",
    "        Returns:\n",
    "            Matrix: The least-squares solution b satisfying bX = Y\n",
    "        \"\"\"    \n",
    "        # Check if the matrix Y is with one column or not\n",
    "        if len(Y.shape) == 1:\n",
    "            Y = Y.reshape([Y.shape[0], 1])\n",
    "        if Y.shape[1] != 1:\n",
    "            raise ValueError(\"The input matrix Y must be of 1 column!\")\n",
    "        \n",
    "        # Compute the least-squares solution (X^T@X)^-1@X^T@Y\n",
    "        X_transpose = X.transpose()\n",
    "        b = (X_transpose @ X).inverse() @ X_transpose @ Y\n",
    "        if dtype:\n",
    "            b = b.astype(dtype)\n",
    "            \n",
    "        return b\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`P. Tensor Wrapper Library (self-implemented)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This tensor class is self implemented and open-sourced\n",
    "# Available at https://github.com/dof-studio/MML/\n",
    "# By Nathmath Huang (bh2821)\n",
    "# License: Apache License Version 2.0\n",
    "\n",
    "class Tensor(Object):\n",
    "    \"\"\"\n",
    "    A production-level Tensor class providing a unified interface for common machine learning operations.\n",
    "    This class supports either a numpy.ndarray or a torch.Tensor as its underlying backend. To optimize performance,\n",
    "    the backend string is processed once at initialization, and boolean flags (_is_numpy and _is_torch) are used to \n",
    "    avoid repeated string comparisons.\n",
    "    \n",
    "    The implemented operations include element-wise arithmetic, matrix multiplication, reshaping, reductions (sum, mean, \n",
    "    max, min), and element-wise exponential and logarithmic functions.\n",
    "    \n",
    "    Attributes:\n",
    "        data (np.ndarray or torch.Tensor): Underlying storage for tensor data.\n",
    "        _backend (str): Lowercase string for the backend (\"numpy\" or \"torch\").\n",
    "        _is_numpy (bool): True if using numpy as the backend.\n",
    "        _is_torch (bool): True if using torch as the backend.\n",
    "    \"\"\"\n",
    "    \n",
    "    __attr__ = \"MML.Tensor\"    \n",
    "    \n",
    "    def e(self):\n",
    "        \"\"\"\n",
    "        Returns natural exponent value as a single-value Matrix.\n",
    "        \n",
    "        Returns:\n",
    "            -------\n",
    "            Tensor with 0 shape. exp value.\n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            return Tensor(np.e, backend = self._backend, dtype = self.dtype, device = self.device)\n",
    "        else:\n",
    "            return Tensor(torch.e, backend = self._backend, dtype = self.dtype, device = self.device)\n",
    "    \n",
    "    def pi(self):\n",
    "        \"\"\"\n",
    "        Returns pi value as a single-value Matrix.\n",
    "        \n",
    "        Returns:\n",
    "            -------\n",
    "            Tensor with 0 shape. pi value.\n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            return Tensor(np.pi, backend = self._backend, dtype = self.dtype, device = self.device)\n",
    "        else:\n",
    "            return Tensor(torch.pi, backend = self._backend, dtype = self.dtype, device = self.device)\n",
    "    \n",
    "    def __init__(self, data, backend=\"numpy\", *, dtype=None, device=None):\n",
    "        \"\"\"\n",
    "        Initializes a Tensor instance with the specified backend.\n",
    "        \n",
    "        Args:\n",
    "            data (array-like): Input data to be converted into a tensor.\n",
    "            backend (str): Backend to use (\"numpy\" or \"torch\").\n",
    "            dtype(str): The type of data to be stored in (any type or None).\n",
    "            device (str): Device where the data is stored on (\"cpu\" or \"cuda\", or None).\n",
    "            \n",
    "        Raises:\n",
    "            ValueError: If an unsupported backend is specified.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self._backend = backend.lower()\n",
    "        if self._backend not in (\"numpy\", \"torch\"):\n",
    "            raise ValueError(\"Unsupported backend. Please choose 'numpy' or 'torch'.\")\n",
    "        self._is_numpy = (self._backend == \"numpy\")\n",
    "        self._is_torch = (self._backend == \"torch\")\n",
    "        \n",
    "        # Convert input data to the appropriate tensor type.\n",
    "        if self._is_numpy:\n",
    "            self.data = np.array(data, dtype=dtype)\n",
    "        else:\n",
    "            if torch is None:\n",
    "                raise ImportError(\"PyTorch is not installed but backend 'torch' was requested.\")\n",
    "            self.data = data.to(device, dtype=dtype) if isinstance(data, torch.Tensor) else torch.tensor(data, device=device, dtype=dtype)\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        Returns a string representation showing backend, shape, and data.\n",
    "        \"\"\"\n",
    "        return f\"Tensor(backend={self._backend}, shape={self.shape}, data=\\n{self.data})\"\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "       \"\"\"\n",
    "       Allows subscription using a[i, j]. \n",
    "       If the result is an array, it returns a new Tensor; otherwise, the scalar value.\n",
    "       \"\"\"\n",
    "       result = self.data[key]\n",
    "       if (self._is_numpy and isinstance(result, np.ndarray)) or (self._is_torch and torch is not None and isinstance(result, torch.Tensor)):\n",
    "           return Tensor(result, backend=self._backend)\n",
    "       return result\n",
    "   \n",
    "    def __setitem__(self, key, value):\n",
    "        \"\"\"\n",
    "        Allows assignment using a[i, j] = value.\n",
    "        If the value is a Tensor instance, its underlying data is used.\n",
    "        \"\"\"\n",
    "        if isinstance(value, Tensor):\n",
    "            value = value.data\n",
    "        self.data[key] = value\n",
    "\n",
    "    @property\n",
    "    def shape(self):\n",
    "        \"\"\"\n",
    "        Retrieves the shape of the tensor.\n",
    "        \"\"\"\n",
    "        return self.data.shape\n",
    "\n",
    "    @property\n",
    "    def dtype(self):\n",
    "        \"\"\"\n",
    "        Retrieves the data type of the tensor elements.\n",
    "        \"\"\"\n",
    "        return self.data.dtype\n",
    "        \n",
    "    @property\n",
    "    def device(self):\n",
    "        \"\"\"\n",
    "        Returns the data device of the tensor elements.\n",
    "        \"\"\"\n",
    "        if self._backend == \"numpy\":\n",
    "            return \"cpu\"\n",
    "        else:\n",
    "            return self.data.device.type\n",
    "        \n",
    "    @property\n",
    "    def requires_grad(self):\n",
    "        \"\"\"\n",
    "        Retrieve whether it requires gradients or not\n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            raise NotImplementedError(\"Autograd is not implemented in numpy backend.\")\n",
    "        else:\n",
    "            return self.data.requires_grad\n",
    "        \n",
    "    def zero_grad(self):\n",
    "        \"\"\"\n",
    "        Clear the Pytorch Gradient if any.\n",
    "        \"\"\"\n",
    "        if self._is_numpy == False:\n",
    "            if self.data.grad is not None:\n",
    "                self.data.grad.zero_()\n",
    "        return self\n",
    "    \n",
    "    def reshape(self, shape):\n",
    "        \"\"\"\n",
    "        Converts the tensor into a new shape.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: A new Tensor object with the specified shape.\n",
    "        \"\"\"\n",
    "        return Tensor(self.data.reshape(shape), backend=self._backend)\n",
    "    \n",
    "    def astype(self, dtype):\n",
    "        \"\"\"\n",
    "        Converts the underlying data to the specified type.\n",
    "        \n",
    "        For the numpy backend, it uses np.ndarray.astype.\n",
    "        For the torch backend, it maps the input (which can be a torch.dtype, a string, or a numpy type)\n",
    "        to the corresponding torch dtype and uses tensor.to(dtype=...).\n",
    "        \n",
    "        Args:\n",
    "            dtype: The desired data type. For numpy, any valid numpy dtype is accepted.\n",
    "                   For torch, this can be a torch.dtype, a string (e.g., \"float32\", \"int64\"),\n",
    "                   or a numpy dtype.\n",
    "                   \n",
    "        Returns:\n",
    "            A new Matrix instance with the data converted to the specified type.\n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            new_data = self.data.astype(dtype)\n",
    "            return Tensor(new_data, backend=\"numpy\")\n",
    "        else:\n",
    "            # Map the input dtype to a torch dtype.\n",
    "            torch_dtype = None\n",
    "            if isinstance(dtype, torch.dtype):\n",
    "                torch_dtype = dtype\n",
    "            elif isinstance(dtype, str):\n",
    "                mapping = {\n",
    "                    \"float32\": torch.float32,\n",
    "                    \"float\": torch.float32,\n",
    "                    \"float64\": torch.float64,\n",
    "                    \"double\": torch.float64,\n",
    "                    \"int32\": torch.int32,\n",
    "                    \"int\": torch.int32,\n",
    "                    \"int64\": torch.int64,\n",
    "                    \"long\": torch.int64,\n",
    "                    \"bool\": torch.bool,\n",
    "                    \"complex64\": torch.complex64,\n",
    "                    \"complex128\": torch.complex128\n",
    "                }\n",
    "                if dtype in mapping:\n",
    "                    torch_dtype = mapping[dtype]\n",
    "                else:\n",
    "                    raise ValueError(f\"Unsupported dtype string: {dtype}\")\n",
    "            elif isinstance(dtype, (np.dtype, type)):\n",
    "                np_dtype = np.dtype(dtype)\n",
    "                mapping = {\n",
    "                    np.dtype(\"float32\"): torch.float32,\n",
    "                    np.dtype(\"float64\"): torch.float64,\n",
    "                    np.dtype(\"int32\"): torch.int32,\n",
    "                    np.dtype(\"int64\"): torch.int64,\n",
    "                    np.dtype(\"bool\"): torch.bool,\n",
    "                    np.dtype(\"complex64\"): torch.complex64,\n",
    "                    np.dtype(\"complex128\"): torch.complex128,\n",
    "                }\n",
    "                if np_dtype in mapping:\n",
    "                    torch_dtype = mapping[np_dtype]\n",
    "                else:\n",
    "                    raise ValueError(f\"Unsupported numpy dtype: {np_dtype}\")\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported dtype argument: {dtype}\")\n",
    "            new_data = self.data.to(dtype=torch_dtype)\n",
    "            return Tensor(new_data, backend=\"torch\")\n",
    "    \n",
    "    def to(self, backend, *, dtype = None, device=None):\n",
    "        \"\"\"\n",
    "        Converts the tensor to the specified backend and moves it to the specified device.\n",
    "        \n",
    "        Args:\n",
    "            backend (str): The target backend (\"numpy\" or \"torch\").\n",
    "            dtype (str, optional): The target type (any numpy or torch type nor None for auto inferenence).\n",
    "            device (str, optional): The target device (\"cpu\" or \"cuda\"). This parameter is only applicable when the target or source is torch.\n",
    "            \n",
    "        Returns:\n",
    "            Tensor: A new Tensor object with data in the target backend and on the specified device.\n",
    "        \"\"\"\n",
    "        target = backend.lower()\n",
    "        # If the target backend is the same as the current one.\n",
    "        if target == self._backend:\n",
    "            if self._is_torch:\n",
    "                # If already torch tensor, just move it to the desired device.\n",
    "                return Tensor(self.data.to(device, dtype = dtype), backend=\"torch\")\n",
    "            return Tensor(self.data, backend=self._backend, device=device, dtype=dtype)\n",
    "        \n",
    "        # Convert to numpy if requested.\n",
    "        if target == \"numpy\":\n",
    "            if self._is_torch:\n",
    "                # Move to CPU first (numpy only works on CPU) then convert to numpy.\n",
    "                return Tensor(self.data.cpu().to(dtype = dtype).numpy(), backend=\"numpy\")\n",
    "        \n",
    "        # Convert to torch if requested.\n",
    "        elif target == \"torch\":\n",
    "            if torch is None:\n",
    "                raise ImportError(\"PyTorch is not installed.\")\n",
    "            if self._is_numpy:\n",
    "                # Create a torch tensor from numpy array.\n",
    "                tensor = torch.tensor(self.data, dtype = dtype, device = device)\n",
    "            else:\n",
    "                tensor = self.data\n",
    "            return Tensor(tensor, backend=\"torch\")\n",
    "        \n",
    "        raise ValueError(\"Unsupported backend conversion.\")\n",
    "\n",
    "    def _apply_op(self, other, op):\n",
    "        \"\"\"\n",
    "        Helper method to apply an element-wise binary operation.\n",
    "        \n",
    "        Args:\n",
    "            other (Tensor or scalar): Other operand.\n",
    "            op (callable): Function applying the desired operation element-wise.\n",
    "            \n",
    "        Returns:\n",
    "            Tensor: New Tensor resulting from the operation.\n",
    "        \"\"\"\n",
    "        other_val = other.data if isinstance(other, Tensor) else other\n",
    "        result = op(self.data, other_val)\n",
    "        return Tensor(result, backend=self._backend)\n",
    "    \n",
    "    def copy(self, *, backend=None, dtype=None, device=None):\n",
    "        \"\"\"\n",
    "        Creates a deep copy of the current tensor with the specified backend and data type.\n",
    "        \n",
    "        Args:\n",
    "            backend (str): The backend for the copied matrix. Default is None.\n",
    "            dtype: Desired data type for the result. Default is None.\n",
    "            device: Device to which the tensor should be moved if applicable. Default is None.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: A deep copy of the current matrix with the specified parameters.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            if backend is None:\n",
    "                return Tensor(self.data.copy(), backend=self._backend, dtype=dtype, device=device)\n",
    "            else:\n",
    "                return Tensor(self.data.copy(), backend=backend, dtype=dtype, device=device)\n",
    "        else:\n",
    "            if backend is None:\n",
    "                return Tensor(self.data.clone().detach(), backend=self._backend, dtype=dtype, device=device)\n",
    "            else:\n",
    "                return Tensor(self.data.clone().detach(), backend=backend, dtype=dtype, device=device)\n",
    "    \n",
    "    def clone(self, *, backend=None, dtype=None, device=None):\n",
    "        \"\"\"\n",
    "        Creates a deep copy of the current tensor with the specified backend and data type without detach.\n",
    "        \n",
    "        Args:\n",
    "            backend (str): The backend for the copied matrix. Default is None.\n",
    "            dtype: Desired data type for the result. Default is None.\n",
    "            device: Device to which the tensor should be moved if applicable. Default is None.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: A deep copy of the current matrix with the specified parameters.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            if backend is None:\n",
    "                return Tensor(self.data.copy(), backend=self._backend, dtype=dtype, device=device)\n",
    "            else:\n",
    "                return Tensor(self.data.copy(), backend=backend, dtype=dtype, device=device)\n",
    "        else:\n",
    "            if backend is None:\n",
    "                return Tensor(self.data.clone(), backend=self._backend, dtype=dtype, device=device)\n",
    "            else:\n",
    "                return Tensor(self.data.clone(), backend=backend, dtype=dtype, device=device)\n",
    "            \n",
    "    def clone_detach(self, *, backend=None, dtype=None, device=None):\n",
    "        \"\"\"\n",
    "        Creates a deep copy of the current tensor with the specified backend and data type with detach.\n",
    "        \n",
    "        Args:\n",
    "            backend (str): The backend for the copied matrix. Default is None.\n",
    "            dtype: Desired data type for the result. Default is None.\n",
    "            device: Device to which the tensor should be moved if applicable. Default is None.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: A deep copy of the current matrix with the specified parameters.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            if backend is None:\n",
    "                return Tensor(self.data.copy(), backend=self._backend, dtype=dtype, device=device)\n",
    "            else:\n",
    "                return Tensor(self.data.copy(), backend=backend, dtype=dtype, device=device)\n",
    "        else:\n",
    "            if backend is None:\n",
    "                return Tensor(self.data.clone().detach(), backend=self._backend, dtype=dtype, device=device)\n",
    "            else:\n",
    "                return Tensor(self.data.clone().detach(), backend=backend, dtype=dtype, device=device)\n",
    "    \n",
    "    def append(self, to_append, axis=0):\n",
    "        \"\"\"\n",
    "        Append a scalar (broadcasted) or an array to the matrix along the specified axis.\n",
    "        The tensor is a general n-dimensional tensor, so the appended data must have the same \n",
    "        shape as the original tensor on all axes except the specified axis.\n",
    "    \n",
    "        Args:\n",
    "            to_append: A scalar or an array-like object (or Tensor instance) to append.\n",
    "            axis (int): Axis along which to append. Negative values are supported.\n",
    "    \n",
    "        Returns:\n",
    "            Tensor: A new Tensor instance with the appended data.\n",
    "        \"\"\"\n",
    "        # Get number of dimensions and normalize the axis.\n",
    "        n_dim = len(self.data.shape)\n",
    "        if axis < 0:\n",
    "            axis = axis % n_dim\n",
    "        if axis >= n_dim:\n",
    "            raise ValueError(f\"Axis {axis} out of bounds for array with {n_dim} dimensions.\")\n",
    "    \n",
    "        orig_shape = self.data.shape\n",
    "    \n",
    "        # If to_append is a Tensor instance, extract its data.\n",
    "        if isinstance(to_append, type(self)):\n",
    "            appended_data = to_append.data\n",
    "        else:\n",
    "            # If to_append is a scalar, create an array/tensor with shape matching the original\n",
    "            # on every axis except the specified axis (which will be 1).\n",
    "            if np.isscalar(to_append):\n",
    "                new_shape = tuple(1 if i == axis else orig_shape[i] for i in range(n_dim))\n",
    "                if self._is_numpy:\n",
    "                    appended_data = np.full(new_shape, to_append, dtype=self.data.dtype)\n",
    "                else:\n",
    "                    appended_data = torch.full(new_shape, to_append, dtype=self.data.dtype, device=self.data.device)\n",
    "            elif isinstance(to_append, Tensor):\n",
    "                # Convert to array/tensor using the appropriate backend.\n",
    "                if self._is_numpy:\n",
    "                    appended_data = np.array(to_append.data.copy(), dtype=self.data.dtype)\n",
    "                else:\n",
    "                    appended_data = torch.tensor(to_append.data.clone().detach(), dtype=self.data.dtype, device=self.data.device)\n",
    "            else:\n",
    "                # Convert to array/tensor using the appropriate backend.\n",
    "                if self._is_numpy:\n",
    "                    appended_data = np.array(to_append.copy(), dtype=self.data.dtype)\n",
    "                else:\n",
    "                    appended_data = torch.tensor(to_append.clone().detach(), dtype=self.data.dtype, device=self.data.device)\n",
    "        \n",
    "        # If appended_data has one less dimension, expand it along the specified axis.\n",
    "        if len(appended_data.shape) == n_dim - 1:\n",
    "            if self._is_numpy:\n",
    "                appended_data = np.expand_dims(appended_data, axis=axis)\n",
    "            else:\n",
    "                appended_data = torch.unsqueeze(appended_data, dim=axis)\n",
    "        elif len(appended_data.shape) != n_dim:\n",
    "            raise ValueError(\"Appended data must have either the same number of dimensions as the original Tensor or one less.\")\n",
    "        \n",
    "        # Validate shape compatibility: for all dimensions except the specified axis, sizes must match.\n",
    "        for i in range(n_dim):\n",
    "            if i != axis and appended_data.shape[i] != orig_shape[i]:\n",
    "                raise ValueError(f\"Shape mismatch at dimension {i}: expected {orig_shape[i]}, got {appended_data.shape[i]}.\")\n",
    "        \n",
    "        # Concatenate along the specified axis.\n",
    "        if self._is_numpy:\n",
    "            new_data = np.concatenate((self.data, appended_data), axis=axis)\n",
    "        else:\n",
    "            new_data = torch.cat((self.data, appended_data), dim=axis)\n",
    "        \n",
    "            # Return a new Matrix instance with the updated data.\n",
    "        return Tensor(new_data, backend=self._backend)\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        \"\"\" Equals operator. \"\"\"\n",
    "        if isinstance(other, Object):\n",
    "            if np.prod(self.data.shape) == 1 and np.prod(other.data.shape) == 1:\n",
    "                # Scalar to scalar, output a scalar\n",
    "                return bool(self.flatten().data == other.flatten().data)\n",
    "            else:\n",
    "                return Tensor(self.data == other.data, backend=self._backend)\n",
    "                \n",
    "        elif np.prod(self.data.shape) == 1 and hasattr(other, \"__len__\") == False:\n",
    "            # Scalar to scalar, output a scalar\n",
    "            return bool(self.data == other)\n",
    "        else:\n",
    "            return Tensor(self.data == other, backend=self._backend)\n",
    "\n",
    "    def __add__(self, other):\n",
    "        \"\"\"Element-wise addition.\"\"\"\n",
    "        other_val = other.data if isinstance(other, Object) else other\n",
    "        return Tensor(self.data + other_val, backend=self._backend)\n",
    "\n",
    "    def __radd__(self, other):\n",
    "        \"\"\"Right-hand element-wise addition.\"\"\"\n",
    "        other_val = other.data if isinstance(other, Object) else other\n",
    "        return Tensor(other_val + self.data, backend=self._backend)\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        \"\"\"Element-wise subtraction.\"\"\"\n",
    "        other_val = other.data if isinstance(other, Object) else other\n",
    "        return Tensor(self.data - other_val, backend=self._backend)\n",
    "\n",
    "    def __rsub__(self, other):\n",
    "        \"\"\"Right-hand element-wise subtraction.\"\"\"\n",
    "        other_val = other.data if isinstance(other, Object) else other\n",
    "        return Tensor(other_val - self.data, backend=self._backend)\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        \"\"\"Element-wise multiplication.\"\"\"\n",
    "        other_val = other.data if isinstance(other, Object) else other\n",
    "        return Tensor(self.data * other_val, backend=self._backend)\n",
    "\n",
    "    def __rmul__(self, other):\n",
    "        \"\"\"Right-hand element-wise multiplication.\"\"\"\n",
    "        other_val = other.data if isinstance(other, Object) else other\n",
    "        return Tensor(other_val * self.data, backend=self._backend)\n",
    "\n",
    "    def __truediv__(self, other):\n",
    "        \"\"\"Element-wise true division.\"\"\"\n",
    "        other_val = other.data if isinstance(other, Object) else other\n",
    "        return Tensor(self.data / other_val, backend=self._backend)\n",
    "\n",
    "    def __rtruediv__(self, other):\n",
    "        \"\"\"Right-hand element-wise true division.\"\"\"\n",
    "        other_val = other.data if isinstance(other, Object) else other\n",
    "        return Tensor(other_val / self.data, backend=self._backend)\n",
    "    \n",
    "    def __pow__(self, to_power):\n",
    "        \"\"\"Element-wise power.\"\"\"\n",
    "        return self._apply_op(to_power, lambda a, b: a ** b)\n",
    "\n",
    "    def __rpow__(self, other):\n",
    "        \"\"\"Right-hand element-wise power.\"\"\"\n",
    "        return self.__pow__(other)\n",
    "\n",
    "    def __matmul__(self, other):\n",
    "        \"\"\"\n",
    "        Matrix multiplication using the @ operator.\n",
    "        \n",
    "        Uses the backend's built-in matmul operator.\n",
    "        \"\"\"\n",
    "        other_val = other.data if isinstance(other, Tensor) else other\n",
    "        result = self.data @ other_val\n",
    "        return Tensor(result, backend=self._backend)\n",
    "\n",
    "    def __neg__(self):\n",
    "        \"\"\"Negates the tensor element-wise.\"\"\"\n",
    "        return Tensor(-self.data, backend=self._backend)\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the length of the tensor.\n",
    "        \"\"\"\n",
    "        return len(self.data)\n",
    "    \n",
    "    def requires_grad_(self, requires_grad: bool = False):\n",
    "        \"\"\"\n",
    "        Require gradients to be computed or not. Only support torch.tensor.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: A revised self tensor with gradients opt-in or opt-out.\n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            raise NotImplementedError(\"Autograd is not implemented by numpy backend\")\n",
    "        else:\n",
    "            self.data = self.data.requires_grad_(requires_grad)\n",
    "        return self\n",
    "    \n",
    "    def unique(self):\n",
    "        \"\"\"\n",
    "        Returns the unique values of the elements that are non-zero.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: The unique value tensor.\n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.unique(self.data)\n",
    "        else:\n",
    "            result = torch.unique(self.data)\n",
    "        return Tensor(result, backend=self._backend)\n",
    "    \n",
    "    def nonzero(self):\n",
    "        \"\"\"\n",
    "        Returns the indices of the elements that are non-zero.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: The indices tensor.\n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = self.data.nonzero()\n",
    "        else:\n",
    "            result = self.data.nonzero()\n",
    "        return Tensor(result, backend=self._backend)\n",
    "    \n",
    "    def any(self, axis = None):\n",
    "        \"\"\"\n",
    "        Computes the element-wise logical OR along a specified axis.\n",
    "        \n",
    "        Args: \n",
    "            axis (int or None): Axis along which to apply the `any` operation. If not provided,\n",
    "                                it applies over all elements of the matrix.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: A new Tensor containing the result of the logical OR operation along the specified axis.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.any(self.data, axis=axis)\n",
    "        else:\n",
    "            result = self.data.any(dim=axis)\n",
    "        return Tensor(result, backend=self._backend)\n",
    "    \n",
    "    def all(self, axis = None):\n",
    "        \"\"\"\n",
    "        Computes the element-wise logical AND along a specified axis.\n",
    "    \n",
    "        Args: \n",
    "            axis (int or None): Axis along which to apply the `all` operation. If not provided,\n",
    "                                it applies over all elements of the matrix.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: A new Tensor containing the result of the logical AND operation along the specified axis.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.all(self.data, axis=axis)\n",
    "        else:\n",
    "            result = self.data.all(dim=axis)\n",
    "        return Tensor(result, backend=self._backend)\n",
    "    \n",
    "    def round(self, digits = 0):\n",
    "        \"\"\"\n",
    "        Rounds the data to a specified number of decimal places.\n",
    "    \n",
    "        Args:\n",
    "            digits (int): The number of decimal places to round the data. Default is 0.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: A new Tensor containing the rounded values of the original data.\n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.round(self.data, decimals = digits)\n",
    "        else:\n",
    "            result = torch.round(self.data, decimals = digits)\n",
    "        return Tensor(result, backend=self._backend)\n",
    "    \n",
    "    def mean(self, axis = None):\n",
    "        \"\"\"\n",
    "        Computes the mean of the Tensor along a specified axis.\n",
    "        \n",
    "        Args:\n",
    "            axis (Optional[int]): Axis along which to compute the mean. If None, computes the mean over all dimensions.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: A new instance containing the computed mean values.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.mean(self.data, axis=axis)\n",
    "        else:\n",
    "            result = torch.mean(self.data, dim=axis)\n",
    "        return Tensor(result, backend=self._backend)\n",
    "    \n",
    "    def median(self, axis = None):\n",
    "        \"\"\"\n",
    "        Computes the median along a given axis.\n",
    "    \n",
    "        Args:\n",
    "            axis (Optional[int]): Axis along which to compute the median. Default is None, which computes over all dimensions.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: A new Tensor containing the computed medians.\n",
    "    \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.median(self.data, axis=axis)\n",
    "        else:\n",
    "            if axis is None:\n",
    "                result = torch.median(self.data)\n",
    "            else:\n",
    "                result, _ = torch.median(self.data, dim=axis)\n",
    "        return Tensor(result, backend=self._backend)\n",
    "    \n",
    "    def std(self, axis = None):\n",
    "        \"\"\"\n",
    "        Computes the standard deviation of the Tensor along a specified axis.\n",
    "        \n",
    "        Args:\n",
    "            axis (Optional[int]): Axis along which to compute the standard deviation. If None, computes the mean over all dimensions.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: A new instance containing the computed standard deviation values.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.std(self.data, axis=axis)\n",
    "        else:\n",
    "            result = torch.std(self.data, dim=axis)\n",
    "        return Tensor(result, backend=self._backend)\n",
    "    \n",
    "    def var(self, axis = None):\n",
    "        \"\"\"\n",
    "        Computes the variance of the Tensor along a specified axis.\n",
    "    \n",
    "        Args:\n",
    "            axis (Optional[int]): Axis along which to compute the variance. If None, computes the mean over all dimensions.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: A new instance containing the computed variance values.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.var(self.data, axis=axis)\n",
    "        else:\n",
    "            result = torch.var(self.data, dim=axis)\n",
    "        return Tensor(result, backend=self._backend)\n",
    "    \n",
    "    def min(self, axis = None):\n",
    "        \"\"\"\n",
    "        Computes the minimum of the Tensor along a specified axis.\n",
    "        \n",
    "        Args:\n",
    "            axis (Optional[int]): Axis along which to compute the minimum. If None, computes the mean over all dimensions.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: A new instance containing the computed minimum values.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            if axis is None:\n",
    "                result = np.min(self.data)\n",
    "            else:\n",
    "                result = np.min(self.data, axis=axis)\n",
    "        else:\n",
    "            if axis is None:\n",
    "                result = torch.min(self.data)\n",
    "            else:\n",
    "                result, indices = torch.min(self.data, dim=axis)\n",
    "        return Tensor(result, backend=self._backend)\n",
    "    \n",
    "    def max(self, axis = None):\n",
    "        \"\"\"\n",
    "        Computes the maximum of the Tensor along a specified axis.\n",
    "    \n",
    "        Args:\n",
    "            axis (Optional[int]): Axis along which to compute the maximum. If None, computes the maximum over all dimensions.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: A new instance containing the computed maximum values.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            if axis is None:\n",
    "                result = np.max(self.data)\n",
    "            else:\n",
    "                result = np.max(self.data, axis=axis)\n",
    "        else:\n",
    "            if axis is None:\n",
    "                result = torch.max(self.data)\n",
    "            else:\n",
    "                result, indices = torch.max(self.data, dim=axis)\n",
    "        return Tensor(result, backend=self._backend)\n",
    "    \n",
    "    def clip(self, a_min=None, a_max=None):\n",
    "        \"\"\"\n",
    "        Clips the values of the matrix to a specified range.\n",
    "    \n",
    "        Args: \n",
    "            a_min (float or None): Minimum value for clipping. If None, no minimum is applied.\n",
    "            a_max (float or None): Maximum value for clipping. If None, no maximum is applied.\n",
    "    \n",
    "        Returns:\n",
    "            Tensor: A new instance containing the clipped values of the original data within the specified range.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.clip(self.data, a_min=a_min, a_max=a_max)\n",
    "        else:\n",
    "            result = torch.clip(self.data, min=a_min, max=a_max)\n",
    "        return Tensor(result, backend=self._backend)\n",
    "\n",
    "    def sum(self, axis = None, keepdims = False):\n",
    "        \"\"\"\n",
    "        Computes the sum of the Tensor along a specified axis.\n",
    "        \n",
    "        Args:\n",
    "            axis (Optional[int]): Axis along which to compute the sum. If None, computes across all dimensions.\n",
    "            keepdims (bool): If keeps the dimension or not.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: A new instance containing the computed sum values.\n",
    "        \n",
    "        Raises:\n",
    "            AttributeError: If no data attribute exists in the instance.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.sum(self.data, axis=axis, keepdims=keepdims)\n",
    "        else:\n",
    "            result = torch.sum(self.data, dim=axis, keepdim=keepdims)\n",
    "        return Tensor(result, backend=self._backend)\n",
    "    \n",
    "    def cumsum(self, axis = None):\n",
    "        \"\"\"\n",
    "        Computes the cumulative sum of the Tensor along a specified axis.\n",
    "        \n",
    "        Args:\n",
    "            axis (Optional[int]): Axis along which to compute the cumulative sum. If None, computes across all dimensions.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: A new instance containing the computed cumulative sum values.\n",
    "        \n",
    "        Raises:\n",
    "            AttributeError: If no data attribute exists in the instance.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.cumsum(self.data, axis=axis)\n",
    "        else:\n",
    "            result = torch.cumsum(self.data, dim=axis)\n",
    "        return Tensor(result, backend=self._backend)\n",
    "    \n",
    "    def prod(self, axis = None, keepdims = False):\n",
    "        \"\"\"\n",
    "        Computes the product of the Tensor along a specified axis.\n",
    "        \n",
    "        Args:\n",
    "            axis (Optional[int]): Axis along which to compute the product. If None, computes across all dimensions.\n",
    "            keepdims (bool): If keeps the dimension or not.\n",
    "    \n",
    "        Returns:\n",
    "            Tensor: A new instance containing the computed product values.\n",
    "        \n",
    "        Raises:\n",
    "            AttributeError: If no data attribute exists in the instance.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.prod(self.data, axis=axis, keepdims=keepdims)\n",
    "        else:\n",
    "            result = torch.prod(self.data, dim=axis, keepdim=keepdims)\n",
    "        return Tensor(result, backend=self._backend)\n",
    "    \n",
    "    def cumprod(self, axis = None):\n",
    "        \"\"\"\n",
    "        Computes the cumulative product of the Tensor along a specified axis.\n",
    "        \n",
    "        Args:\n",
    "            axis (Optional[int]): Axis along which to compute the cumulative product. If None, computes across all dimensions.\n",
    "    \n",
    "        Returns:\n",
    "            Tensor: A new instance containing the computed cumulative product values.\n",
    "        \n",
    "        Raises:\n",
    "            AttributeError: If no data attribute exists in the instance.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.cumprod(self.data, axis=axis)\n",
    "        else:\n",
    "            result = torch.cumprod(self.data, dim=axis)\n",
    "        return Tensor(result, backend=self._backend)\n",
    "    \n",
    "    def gamma(self):\n",
    "        \"\"\"\n",
    "        Computes the element-wise Gamma function.\n",
    "        \n",
    "        Args: \n",
    "            None\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: A new Tensor containing the Gamma function values for each element in the original data.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = sp.special.gamma(self.data)\n",
    "        else:\n",
    "            def torch_gamma(x):\n",
    "                pos = torch.exp(torch.lgamma(x))\n",
    "                neg = torch.pi / (torch.sin(torch.pi * x) * torch.exp(torch.lgamma(1 - x)))\n",
    "                return torch.where(x > 0, pos, neg)\n",
    "            result = torch_gamma(self.data)\n",
    "        return Tensor(result, backend=self._backend) \n",
    "    \n",
    "    def loggamma(self):\n",
    "        \"\"\"\n",
    "        Computes the element-wise natural logarithm of the Gamma function.\n",
    "        \n",
    "        Args: \n",
    "            None\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: A new Tensor containing the natural logarithm of the Gamma function values for each element in the original data.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = sp.special.gammaln(self.data)\n",
    "        else:\n",
    "            result = torch.special.gammaln(self.data)\n",
    "        return Tensor(result, backend=self._backend) \n",
    "    \n",
    "    def sigmoid(self):\n",
    "        \"\"\"\n",
    "        Applies the standard sigmoid function element-wise on the input Matrix.\n",
    "        \n",
    "        f(x) = L / (1 + exp(-1*x))\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: A new Tensor with the sigmoid function applied element-wise.\n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = 1.0 / (1.0 + np.exp(-1.0 * self.data ))\n",
    "        else:\n",
    "            result = torch.sigmoid(self.data)\n",
    "        return Tensor(result, backend=self._backend)\n",
    "    \n",
    "    def logistic(self, L=1.0, k=1.0, x0=0.0):\n",
    "        \"\"\"\n",
    "        Applies the logistic (sigmoid) function element-wise on the input Tensor.\n",
    "        \n",
    "        f(x) = L / (1 + exp(-k*(x - x0)))\n",
    "        \n",
    "        Args:\n",
    "            L (float): The curve's maximum value.\n",
    "            k (float): The steepness of the curve.\n",
    "            x0 (float): The x-value of the sigmoid's midpoint.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: A new Tensor with the logistic function applied element-wise.\n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = L / (1.0 + np.exp(-k * (self.data - x0)))\n",
    "        else:\n",
    "            result = L / (1.0 + torch.exp(-k * (self.data - x0)))\n",
    "        return Tensor(result, backend=self._backend)\n",
    "    \n",
    "    def logistic_inv(self, L=1.0, k=1.0, x0=0.0):\n",
    "        \"\"\"\n",
    "        Applies the inverse of the logistic (sigmoid) function element-wise on the input Tensor.\n",
    "        \n",
    "        f⁻¹(y) = x0 - (1/k)*ln((L - y)/y)\n",
    "        \n",
    "        Args:\n",
    "            L (float): The curve's maximum value used in the logistic function.\n",
    "            k (float): The steepness of the curve used in the logistic function.\n",
    "            x0 (float): The sigmoid's midpoint used in the logistic function.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: A new Tensor with the inverse logistic function applied element-wise.\n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = x0 - (1/k) * np.log((L - self.data) / self.data)\n",
    "        else:\n",
    "            result = x0 - (1/k) * torch.log((L - self.data) / self.data)\n",
    "        return Tensor(result, backend=self._backend)\n",
    "    \n",
    "    def cholesky(self, upper = False):\n",
    "        \"\"\"\n",
    "        Computes the Cholesky decomposition of a symmetric positive-definite matrix.\n",
    "        L @ U = self.data\n",
    "        returns L if upper = False else U\n",
    "        \n",
    "        Args:\n",
    "            upper (bool): If True, compute the upper triangular factor. Default is False.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: A new Tensor containing the lower triangular factor of the original data if `upper` is False,\n",
    "                    or its transpose if `upper` is True.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.linalg.cholesky(self.data)\n",
    "            if upper == True:\n",
    "                result = result.T\n",
    "        else:\n",
    "            result = torch.cholesky(self.data, upper = upper)\n",
    "        return Tensor(result, backend=self._backend)\n",
    "    \n",
    "    def exp(self):\n",
    "        \"\"\"\n",
    "        Computes the element-wise exponential.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: New tensor with exponential applied.\n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.exp(self.data)\n",
    "        else:\n",
    "            result = torch.exp(self.data)\n",
    "        return Tensor(result, backend=self._backend)\n",
    "    \n",
    "    def sin(self):\n",
    "        \"\"\"\n",
    "        Computes the element-wise sine.\n",
    "        \n",
    "        Args: \n",
    "            None\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: A new Tensor containing the sine values of the original data.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.sin(self.data)\n",
    "        else:\n",
    "            result = torch.sin(self.data)\n",
    "        return Tensor(result, backend=self._backend)\n",
    "    \n",
    "    def cos(self):\n",
    "        \"\"\"\n",
    "        Computes the element-wise cosine.\n",
    "        \n",
    "        Args: \n",
    "            None\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: A new Tensor containing the cosine values of the original data.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.cos(self.data)\n",
    "        else:\n",
    "            result = torch.cos(self.data)\n",
    "        return Tensor(result, backend=self._backend)\n",
    "    \n",
    "    def tan(self):\n",
    "        \"\"\"\n",
    "        Computes the element-wise tangent.\n",
    "        \n",
    "        Args: \n",
    "            None\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: A new Tensor containing the tangent values of the original data.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.tan(self.data)\n",
    "        else:\n",
    "            result = torch.tan(self.data)\n",
    "        return Tensor(result, backend=self._backend)\n",
    "    \n",
    "    def sinh(self):\n",
    "        \"\"\"\n",
    "        Computes the element-wise hyperbolic sine.\n",
    "        \n",
    "        Args: \n",
    "            None\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: A new Tensor containing the hyperbolic sine values of the original data.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.sinh(self.data)\n",
    "        else:\n",
    "            result = torch.sinh(self.data)\n",
    "        return Tensor(result, backend=self._backend)\n",
    "    \n",
    "    def cosh(self):\n",
    "        \"\"\"\n",
    "        Computes the element-wise hyperbolic cosine.\n",
    "        \n",
    "        Args: \n",
    "            None\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: A new Tensor containing the hyperbolic cosine values of the original data.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.cosh(self.data)\n",
    "        else:\n",
    "            result = torch.cosh(self.data)\n",
    "        return Tensor(result, backend=self._backend)\n",
    "    \n",
    "    def tanh(self):\n",
    "        \"\"\"\n",
    "        Computes the element-wise hyperbolic tangent.\n",
    "        \n",
    "        Args: \n",
    "            None\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: A new Tensor containing the hyperbolic tangent values of the original data.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.tanh(self.data)\n",
    "        else:\n",
    "            result = torch.tanh(self.data)\n",
    "        return Tensor(result, backend=self._backend)\n",
    "    \n",
    "    def abs(self):\n",
    "        \"\"\"\n",
    "        Computes the element-wise absolute values.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: New tensor with absolute values applied.\n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.abs(self.data)\n",
    "        else:\n",
    "            result = torch.abs(self.data)\n",
    "        return Tensor(result, backend=self._backend)\n",
    "    \n",
    "    def log(self):\n",
    "        \"\"\"\n",
    "        Computes the element-wise natural logarithm.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: New tensor with logarithm applied.\n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.log(self.data)\n",
    "        else:\n",
    "            result = torch.log(self.data)\n",
    "        return Tensor(result, backend=self._backend)\n",
    "    \n",
    "    def relu(self):\n",
    "        \"\"\"\n",
    "        Computes the element-wise ReLU function.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: New Tensor with ReLU applied.\n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.clip(self.data, 0.0)\n",
    "        else:\n",
    "            result = torch.relu(self.data)\n",
    "        return Tensor(result, backend=self._backend)\n",
    "    \n",
    "    def softmax(self, axis = 1, keepdims:bool | None = True):\n",
    "        \"\"\"\n",
    "        Applies the softmax function along a specified axis.\n",
    "        Reminder -> Shapre will be shrinked by 1 so that you may need to reshape() it.\n",
    "        \n",
    "        Args:\n",
    "            axis (int): Axis along which to apply the softmax. Default is 1.\n",
    "            keepdims (bool or None): Whether to keep the reduced dimensions as axes with size one. \n",
    "                                    If `True`, the shape of the result will be the same as input; otherwise, it will not have these dimensions.\n",
    "    \n",
    "        Returns:\n",
    "            Tensor: A new Tensor containing the softmax values along the specified axis.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            if keepdims is not None and axis is not None:\n",
    "                e_x = np.exp(self.data - np.max(self.data, axis=axis, keepdims=keepdims))\n",
    "                result = e_x / e_x.sum(axis=axis, keepdims=keepdims)\n",
    "            else:\n",
    "                e_x = np.exp(self.data - np.max(self.data, axis=axis))\n",
    "                result = e_x / e_x.sum(axis=axis)\n",
    "        else:\n",
    "            result = torch.nn.functional.softmax(self.data, dim=axis)\n",
    "        return Tensor(result, backend=self._backend)\n",
    "\n",
    "    def argmax(self, axis = 1):\n",
    "        \"\"\"\n",
    "         Computes the indices of the maximum values along a specified axis.\n",
    "         Reminder -> Shapre will be shrinked by 1 so that you may need to reshape() it.\n",
    "        \n",
    "         Args:\n",
    "             axis (int | None): Axis along which to compute the argmax. Default is 1.\n",
    "         \n",
    "         Returns:\n",
    "             Tensor: A new Tensor containing the indices of the maximum values along the specified axis.\n",
    "         \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.argmax(self.data, axis=axis)\n",
    "        else:\n",
    "            result = torch.argmax(self.data, dim=axis)\n",
    "        return Tensor(result, backend=self._backend)\n",
    "    \n",
    "    def argmin(self, axis = 1):\n",
    "        \"\"\"\n",
    "        Computes the indices of the minimum values along a specified axis.\n",
    "    \n",
    "        Args:\n",
    "            axis (int | None): Axis along which to compute the argmin. Default is 1.\n",
    "        \n",
    "        Returns:\n",
    "           Tensor: A new Tensor containing the indices of the minimum values along the specified axis.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.argmin(self.data, axis=axis)\n",
    "        else:\n",
    "            result = torch.argmin(self.data, dim=axis)\n",
    "        return Tensor(result, backend=self._backend)    \n",
    "    \n",
    "    def flatten(self):\n",
    "        \"\"\"\n",
    "        Returns the flattened tensor.\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: The flattened tensor.\n",
    "        \"\"\"\n",
    "        return Tensor(self.data.flatten(), backend=self._backend)\n",
    "    \n",
    "    def diag(self):\n",
    "        \"\"\"\n",
    "        Computes the diagonal vector of a square matrix.\n",
    "        Or create a diagonal matrix if 1D matrix.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: The diagonal vector.\n",
    "            \n",
    "        Raises:\n",
    "            ValueError: If the Tensor is not square.\n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            return Tensor(np.diag(self.data), backend=self._backend)\n",
    "        else:\n",
    "            return Tensor(torch.diag(self.data), backend=self._backend)\n",
    "    \n",
    "    def reverse(self, axis = 0):\n",
    "        \"\"\"\n",
    "        Reverse the flattened tensor.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: The reversed tensor.\n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            return Tensor(np.flip(self.data, axis=axis), backend=self._backend)\n",
    "        else:\n",
    "            return Tensor(torch.flip(self.data, axis=axis), backend=self._backend)\n",
    "\n",
    "    def stack(self, *wargs, axis = 0):\n",
    "        \"\"\"\n",
    "        Stack data in sequence on an axis.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: The stacked Tensor.\n",
    "        \"\"\"\n",
    "        data_list = [self.data]\n",
    "        for arg in wargs:\n",
    "            data_list.append(arg.data)\n",
    "        if self._is_numpy:\n",
    "            result = np.stack(data_list, axis=axis)\n",
    "        else:\n",
    "            result = torch.stack(data_list, dim=axis)\n",
    "        return Tensor(result, backend=self._backend)\n",
    "\n",
    "    def vstack(self, *wargs):\n",
    "        \"\"\"\n",
    "        Stack data in sequence vertically (row wise).\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: The vstacked tensor.\n",
    "        \"\"\"\n",
    "        data_list = [self.data]\n",
    "        for arg in wargs:\n",
    "            data_list.append(arg.data)\n",
    "        if self._is_numpy:\n",
    "            result = np.vstack(data_list)\n",
    "        else:\n",
    "            result = torch.vstack(data_list)\n",
    "        return Tensor(result, backend=self._backend)\n",
    "    \n",
    "    def hstack(self, *wargs):\n",
    "        \"\"\"\n",
    "        Stack data in sequence horizontally (col wise).\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: The hstacked tensor.\n",
    "        \"\"\"\n",
    "        data_list = [self.data]\n",
    "        for arg in wargs:\n",
    "            data_list.append(arg.data)\n",
    "        if self._is_numpy:\n",
    "            result = np.hstack(data_list)\n",
    "        else:\n",
    "            result = torch.hstack(data_list)\n",
    "        return Tensor(result, backend=self._backend)\n",
    "\n",
    "    def split(self, split_size_or_sections, axis=0):\n",
    "        \"\"\"\n",
    "        Splits the data into chunks. Each chunk is a copy of the original data.\n",
    "        \n",
    "        Returns:\n",
    "            A tuple of Tensors: The splitted tensors.\n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.split(self.data, split_size_or_sections, axis=axis)\n",
    "        else:\n",
    "            result = torch.split(self.data, split_size_or_sections, dim=axis)\n",
    "        result = list(result)\n",
    "        for i, r in enumerate(result):\n",
    "            result[i] = Tensor(r, backend=self._backend) \n",
    "        return tuple(result)\n",
    "\n",
    "    def sign(self):\n",
    "        \"\"\"\n",
    "        Computes the element-wise sign of the data. Returns with the same type.\n",
    "    \n",
    "        Args: \n",
    "            None\n",
    "    \n",
    "        Returns:\n",
    "            Tensor: A new Tensor containing the sign values (1 for positive, -1 for negative, 0 for zero) of each element in the original data.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.sign(self.data)\n",
    "        else:\n",
    "            result = torch.sign(self.data)\n",
    "        return Tensor(result, backend=self._backend)\n",
    "\n",
    "    def repeat(self, repeats, axis=None):\n",
    "        \"\"\"\n",
    "        Repeats the Tensor elements along a specified axis.\n",
    "    \n",
    "        Args:\n",
    "            repeats (int or tuple[int]): The number of times to repeat each element.\n",
    "            axis (int): Axis along which to repeat the elements. If `None`, repeats over all dimensions.\n",
    "    \n",
    "        Returns:\n",
    "            Tensor: A new Tensor with repeated elements.\n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = self.data.repeat(repeats, axis=axis)\n",
    "        else:\n",
    "            if axis is None:\n",
    "                result = torch.repeat_interleave(self.data, repeats)\n",
    "            else:\n",
    "                result = torch.repeat_interleave(self.data, repeats, dim=axis)\n",
    "        return Tensor(result, backend=self._backend)\n",
    "    \n",
    "    def bincount(self, *, weights = None, inttype: type = int):\n",
    "        \"\"\"\n",
    "        Counts the number of occurrences of each value in `data` and optionally returns a weighted count.\n",
    "        Values will be forcefully casted to `inttype`\n",
    "    \n",
    "        Args:\n",
    "            weights (array_like | Tensor): An array-like object containing weights corresponding to each element in `data`. Default is None.\n",
    "            inttype (type): A type that the data is going to be casted to.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: A new Tensor with the bin counts or weighted bin counts.\n",
    "    \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.bincount(self.astype(inttype).data, weights = weights)\n",
    "        else:\n",
    "            result = torch.bincount(self.astype(inttype).data, weights = weights)\n",
    "        return Tensor(result, backend=self._backend)\n",
    "    \n",
    "    def transpose(self, *axes):\n",
    "        \"\"\"\n",
    "        Transposes the tensor dimensions. If axes are provided, permutes accordingly; otherwise, reverses dimensions.\n",
    "        \n",
    "        Args:\n",
    "            *axes: Optional permutation of dimensions.\n",
    "            \n",
    "        Returns:\n",
    "            Tensor: New transposed tensor.\n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = self.data.transpose(axes) if axes else self.data.T\n",
    "        else:\n",
    "            result = self.data.permute(*axes) if axes else self.data.permute(*reversed(range(self.data.dim())))\n",
    "        if len(self.shape) > 1:\n",
    "            return Tensor(result, backend=self._backend)\n",
    "        else:\n",
    "            # From a row vector to a column vector\n",
    "            return Tensor(result.reshape([self.shape[0], 1]), backend=self._backend)\n",
    "    \n",
    "    def quantile(self, q: float, axis = None, keepdims = False):\n",
    "        \"\"\"\n",
    "        Computes the specified quantiles along a given axis.\n",
    "    \n",
    "        Args:\n",
    "            q (float): The quantile to compute. Should be between 0 and 1.\n",
    "            axis (Optional[int]): Axis along which to compute the quantile. Default is None, which computes over all dimensions.\n",
    "            keepdims (bool): Whether to keep the reduced axes in the result as singleton dimensions. Default is False.\n",
    "    \n",
    "        Returns:\n",
    "            Tensor: A new Tensor containing the computed quantiles.\n",
    "    \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.quantile(self.data, q, axis = axis, keepdims = keepdims)\n",
    "        else:\n",
    "            if axis is None:\n",
    "                result = torch.quantile(self.data, q, keepdims = keepdims)\n",
    "            else:\n",
    "                result = torch.quantile(self.data, q, dim = axis, keepdims = keepdims)\n",
    "        return Tensor(result, backend=self._backend)\n",
    "    \n",
    "    def sort(self, axis: int | None = None):\n",
    "        \"\"\"\n",
    "        Sorts the tensor elements along (the first column of) a specified axis.\n",
    "        If you intend to sort on only one array, use `sort_along` instead,\n",
    "        or if you want to sort along each column of each base dimension, use `sort_along_each_column` instead.\n",
    "        \n",
    "        Args:\n",
    "            axis (int or None): Axis to sort along. If `None`, sorts the entire matrix.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: A new Tensor with sorted elements.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.sort(self.data, axis=axis)\n",
    "        else:\n",
    "            if axis is None:\n",
    "                result, idx = self.data.sort()\n",
    "            else:\n",
    "                result, idx = self.data.sort(dim=axis)\n",
    "        return Tensor(result, backend=self._backend)\n",
    "    \n",
    "    def sort_along(self, axis: tuple = (None, 0)):\n",
    "        \"\"\"\n",
    "        Sort the N-dimensional data along the 1d values on `axis`.\n",
    "        If you intend to sort on the first column of axis `axis`, use `sort` to speed up,\n",
    "        or if you want to sort along each column of each base dimension, use `sort_along_each_column` instead.\n",
    "       \n",
    "        Detail:\n",
    "        Sort the input array x. The axis parameter is a tuple of t\n",
    "        he same length as x.ndim, where each position can be None or an integer. \n",
    "        It is required that exactly one position d in axis_vec is not None,\n",
    "        and the reference position of this dimension d is fixed = axis_vec[d], \n",
    "        but when taking this reference, the index 0 is selected for each dimension \n",
    "        in the global uniform way (i.e., only x[(0,)*d + (fixed,)] is used as the \n",
    "        reference). \n",
    "        The 1D sorting permutation is calculated (using argsort, in ascending order), \n",
    "        and then the global permutation is applied to dimension d+1 \n",
    "        (the subsequent dimension) of x, acting on all data without sorting each\n",
    "        preceding block separately.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "            axis : tuple\n",
    "                The indicator indicating sort which data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            Tensor, sorted copy.\n",
    "\n",
    "        \"\"\"\n",
    "        if len(self.data.shape) != len(axis):\n",
    "            raise ValueError(\"The length of axis must be equal to the number of dimensions of the input array.\")\n",
    "        non_none = [(d, val) for d, val in enumerate(axis) if val is not None]\n",
    "        if len(non_none) != 1:\n",
    "            raise ValueError(\"There must be exactly one non-None element in axis\")\n",
    "        d, fixed = non_none[0]\n",
    "        if d > len(self.data.shape) - 1:\n",
    "            raise ValueError(\"The non-None dimension cannot be greater than dimension.\")\n",
    "        if fixed < 0 or fixed >= self.data.shape[d]:\n",
    "            raise IndexError(f\"Fixed index {fixed} is out of range for dimension {d} (0 to {self.data.shape[d]-1})\")\n",
    "        \n",
    "        # Last dim sort\n",
    "        if d == len(self.data.shape) - 1 and len(self.data.shape) == 2:\n",
    "            # Transpose, and sort the row\n",
    "            return self.transpose().sort_along(axis=(axis[1], axis[0])).transpose()\n",
    "        elif d == len(self.data.shape) - 1 and len(self.data.shape) > 2:\n",
    "            # Transpose the inner two dimensions\n",
    "            tr_axe = list(range(len(self.data.shape))); tmp = tr_axe[-1]; tr_axe[-1] = tr_axe[-2]; tr_axe[-2] = tmp;\n",
    "            axis_new = list(np.repeat(None, len(self.data.shape))); axis_new[-2] = fixed\n",
    "            return self.transpose(*tr_axe).sort_along(axis=axis_new).transpose(*tr_axe)\n",
    "        \n",
    "        # Extract global reference: fixed on dimension d, but all dimensions before d are indexed as 0.\n",
    "        # Construct index tuple: fixed to 0 for dimensions < d, fixed to the dth dimension, and use slice(None) to eliminate the remaining axes.\n",
    "        idx = (0,) * d + (fixed,)\n",
    "        \n",
    "        if self._is_numpy:\n",
    "            # Extract the reference key, which is expected to be 1D and have a length equal to self.data.shape[d+1]\n",
    "            key = np.asarray(self.data[idx])\n",
    "            if key.ndim != 1 or key.shape[0] != self.data.shape[d+1]:\n",
    "                raise ValueError(\"The reference key must be one-dimensional and its length must be the same as the length of the sorting axis.\")\n",
    "            order = np.argsort(key)\n",
    "           \n",
    "            # Construct a global index array for np.take_along_axis: needs to have the same shape as x,\n",
    "            # but order along sorting axis d+1, other dimensions are copied via broadcasting.\n",
    "            order_shape = [1] * len(self.data.shape)\n",
    "            order_shape[d+1] = self.data.shape[d+1]\n",
    "            order_global = order.reshape(order_shape)\n",
    "            order_global = np.broadcast_to(order_global, self.data.shape)\n",
    "            sorted_ = np.take_along_axis(self.data, order_global, axis=d+1)\n",
    "            return Tensor(sorted_, backend=self._backend, dtype=self.dtype, device=self.device)\n",
    "\n",
    "        else:\n",
    "            key = self.data[idx]\n",
    "            # key should be 1D, and its length should be equal to self.data.shape[d+1]\n",
    "            if key.dim() != 1 or key.size(0) != self.data.shape[d+1]:\n",
    "                raise ValueError(\"The reference key must be one-dimensional and its length must be the same as the length of the sorting axis.\")\n",
    "            # Calculate the sort order (ascending)\n",
    "            order = torch.argsort(key, dim=0)\n",
    "            \n",
    "            # Construct a global index tensor with the same shape as self.data, but with order on the sorting axis d+1\n",
    "            order_shape = [1] * len(self.data.shape)\n",
    "            order_shape[d+1] = self.data.shape[d+1]\n",
    "            order_global = order.view(*order_shape).expand(self.data.shape)\n",
    "            \n",
    "            # Use torch.gather to rearrange self.data according to the global index tensor on dim=d+1\n",
    "            sorted_ = torch.gather(self.data, dim=d+1, index=order_global)\n",
    "            return Tensor(sorted_, backend=self._backend, dtype=self.dtype, device=self.device)\n",
    "        \n",
    "    def sort_along_each_column(self, axis: int = 1, on_col: int = 0):\n",
    "        \"\"\"\n",
    "        Sort the N-dimensional data along values on column `on_col` of the axis `axis`.\n",
    "        Note, it will sort EACH `on_col` of the exterior axises.\n",
    "        If you intend to sort on the first column of axis `axis`, use `sort` to speed up.\n",
    "       \n",
    "        Detail:\n",
    "        Instead of sorting itself in d dimensions, use the reference sequence obtained by taking index=i on the d axis of x, and apply the same rearrangement to the d+1 axis (next axis) of x.\n",
    "\n",
    "        For example, for a 2D array, when d=0, i=1,\n",
    "        take the reference sequence = x[1, :], calculate its argsort to get the sorted arrangement, and then rearrange the columns of each row of the entire array according to this arrangement;\n",
    "        For a 3D array, when d=1, i=0,\n",
    "        for each subarray with fixed axis=0, take the reference sequence = subarray[0, :] (that is, the row of axis=1 index 0),\n",
    "        calculate argsort (sort the elements in the reference sequence), and then rearrange all rows in the subarray (all slices of axis=1) on axis=2 according to this arrangement.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "            axis : int\n",
    "                The axis of the column is on. The default is 1.\n",
    "            on_col : int\n",
    "                The index of the column is on. The default is 0.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            Tensor, sorted copy.\n",
    "\n",
    "        \"\"\"\n",
    "        if len(self.data.shape) < 2:\n",
    "            raise ValueError(\"The input data must at least have 2 dimensions. Use `sort` if it is a 1d array.\")\n",
    "        if axis < 0 or axis > len(self.data.shape) - 1:\n",
    "            raise ValueError(\"The parameter d must be positive and smaller than ndim.\")\n",
    "        if axis == len(self.data.shape) - 1 and len(self.data.shape) == 2:\n",
    "            # Transpose, and sort the row\n",
    "            return self.transpose().sort_along_each_column(axis=0, on_col=on_col).transpose()\n",
    "        elif axis == len(self.data.shape) - 1 and len(self.data.shape) > 2:\n",
    "            # Transpose the inner two dimensions\n",
    "            tr_axe = list(range(len(self.data.shape))); tmp = tr_axe[-1]; tr_axe[-1] = tr_axe[-2]; tr_axe[-2] = tmp;\n",
    "            return self.transpose(*tr_axe).sort_along_each_column(axis=axis-1, on_col=on_col).transpose(*tr_axe)\n",
    "            \n",
    "        sorted_axis = axis + 1\n",
    "\n",
    "        if self._is_numpy:\n",
    "            key = np.take(self.data, indices=on_col, axis=axis)\n",
    "            order = np.argsort(key, axis=axis)\n",
    "            order_expanded = np.expand_dims(order, axis=axis)\n",
    "            sorted_ = np.take_along_axis(self.data, order_expanded, axis=sorted_axis)\n",
    "            return Tensor(sorted_, backend=self._backend, dtype=self.dtype, device=self.device)\n",
    "\n",
    "        else:\n",
    "            key = self.data.select(dim=axis, index=on_col)\n",
    "            order = torch.argsort(key, dim=axis)\n",
    "            order_expanded = order.unsqueeze(dim=axis)\n",
    "            expand_shape = list(self.data.shape)\n",
    "            index = order_expanded.expand(*expand_shape)\n",
    "            sorted_ = torch.gather(self.data, dim=sorted_axis, index=index)\n",
    "            return Tensor(sorted_, backend=self._backend, dtype=self.dtype, device=self.device)        \n",
    "    \n",
    "    def determinant(self):\n",
    "        \"\"\"\n",
    "        Computes the determinant if the tensor is a matrix.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: New tensor of determinant.\n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            return np.linalg.det(self.data)\n",
    "        else:\n",
    "            if hasattr(torch, 'linalg') and hasattr(torch.linalg, 'det'):\n",
    "                return torch.linalg.det(self.data)\n",
    "            else:\n",
    "                return torch.det(self.data)\n",
    "            \n",
    "    def inverse(self):\n",
    "        \"\"\"\n",
    "        Computes the inverse of a tensor.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: New tensor with its inversed stored.\n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.linalg.inv(self.data)\n",
    "        else:\n",
    "            if hasattr(torch, 'linalg') and hasattr(torch.linalg, 'inv'):\n",
    "                result = torch.linalg.inv(self.data)\n",
    "            else:\n",
    "                result = torch.inverse(self.data)\n",
    "        return Tensor(result, backend=self._backend)\n",
    "\n",
    "    def trace(self):\n",
    "        \"\"\"\n",
    "        Computes the trace of a tensor.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: New tensor with trace stored.\n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            return np.trace(self.data)\n",
    "        else:\n",
    "            return torch.trace(self.data)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def dot(self, other):\n",
    "        \"\"\"\n",
    "        Computes the dot product of this matrix with another Matrix.\n",
    "        \n",
    "        Args:\n",
    "            other (Matrix): The other matrix to multiply.\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: A new Matrix instance with the result of the dot product.\n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            return Tensor(np.dot(self.data, other.data), backend=\"numpy\")\n",
    "        else:\n",
    "            return Tensor(torch.matmul(self.data, other.data), backend=\"torch\")\n",
    "        \n",
    "    def inner(self, other):\n",
    "        \"\"\"\n",
    "        Computes the inner product of this Tensor with another Tensor.\n",
    "        \n",
    "        Args:\n",
    "            other (Tensor): The other Tensor to multiply.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: A new Tensor instance with the result of the inner product.\n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            return Tensor(np.inner(self.data, other.data), backend=\"numpy\")\n",
    "        else:\n",
    "            return Tensor(torch.inner(self.data, other.data), backend=\"torch\")\n",
    "    \n",
    "    def outer(self, other):\n",
    "        \"\"\"\n",
    "        Computes the outer product of this Tensor with another Tensor.\n",
    "        \n",
    "        Args:\n",
    "            other (Tensor): The other Tensor to multiply.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: A new Tensor instance with the result of the outer product.\n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            return Tensor(np.outer(self.data, other.data), backend=\"numpy\")\n",
    "        else:\n",
    "            return Tensor(torch.outer(self.data, other.data), backend=\"torch\")\n",
    "\n",
    "    def svd(self, full_matrices=True):\n",
    "        \"\"\"\n",
    "        Computes the singular value decomposition for a general tensor.\n",
    "        \n",
    "        Returns:\n",
    "            Tensors: New s,v,d tensors in a tuple.\n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            U, s, Vh = np.linalg.svd(self.data, full_matrices=full_matrices)\n",
    "            return Tensor(U, backend=\"numpy\"), Tensor(s, backend=\"numpy\"), Tensor(Vh, backend=\"numpy\")\n",
    "        else:\n",
    "            if hasattr(torch, 'linalg') and hasattr(torch.linalg, 'svd'):\n",
    "                U, s, Vh = torch.linalg.svd(self.data, full_matrices=full_matrices)\n",
    "            else:\n",
    "                U, s, V = torch.svd(self.data, some=not full_matrices)\n",
    "                Vh = V.t()\n",
    "            return Tensor(U, backend=\"torch\"), Tensor(s, backend=\"torch\"), Tensor(Vh, backend=\"torch\")\n",
    " \n",
    "    def to_zeros(self):\n",
    "        \"\"\"\n",
    "        Converts the Tensor data into a same shape Tensor with 0s.\n",
    "        \n",
    "        Args: \n",
    "            None\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: a same shape Tensor with 0s.\n",
    "        \n",
    "        \"\"\"\n",
    "        x = self.copy()\n",
    "        x[...] = 0\n",
    "        return x\n",
    "    \n",
    "    def to_ones(self):\n",
    "        \"\"\"\n",
    "        Converts the Tensor data into a same shape Tensor with 1s.\n",
    "        \n",
    "        Args: \n",
    "            None\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: a same shape Tensor with 0s.\n",
    "        \n",
    "        \"\"\"\n",
    "        x = self.copy()\n",
    "        x[...] = 1\n",
    "        return x\n",
    "    \n",
    "    def to_ks(self, k: float | int = 0):\n",
    "        \"\"\"\n",
    "        Converts the Tensor data into a same shape Tensor with ks.\n",
    "        \n",
    "        Args: \n",
    "            None\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: a same shape Tensor with 1s.\n",
    "        \n",
    "        \"\"\"\n",
    "        x = self.copy()\n",
    "        x[...] = k\n",
    "        return x\n",
    "    \n",
    "    def to_rands(self):\n",
    "        \"\"\"\n",
    "        Converts the Tensor data into a same shape Tensor with uniform random numbers.\n",
    "        \n",
    "        Args: \n",
    "            None\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: a same shape Tensor with uniform random numbers.\n",
    "        \n",
    "        \"\"\"\n",
    "        return self.rand(self.shape, backend=self._backend, dtype=self.dtype, device=self.device)\n",
    "\n",
    "    def to_list(self):\n",
    "        \"\"\"\n",
    "        Converts the Tensor data into a Python list.\n",
    "        \n",
    "        Args: \n",
    "            None\n",
    "        \n",
    "        Returns:\n",
    "            list: A Python list containing the same elements as `self.data`.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            return self.data.tolist()\n",
    "        else:\n",
    "            return self.data.cpu().tolist()\n",
    "        \n",
    "    def to_numpy_array(self):\n",
    "        \"\"\"\n",
    "        Converts the Tensor data into a NumPy array.\n",
    "        \n",
    "        Returns: \n",
    "            np.ndarray: The underlying NumPy array of the matrix.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            return self.data\n",
    "        else:\n",
    "            return self.data.detach().cpu().numpy()\n",
    "        \n",
    "    def to_torch_tensor(self, *, dtype=None, device=None):\n",
    "        \"\"\"\n",
    "        Converts the Tensor data into a PyTorch tensor.\n",
    "        \n",
    "        Args: \n",
    "            dtype (torch.dtype or None): The desired data type for the resulting tensor. If not provided,\n",
    "                                         uses the current data type of `self.data`.\n",
    "            device (torch.device or None): The target device to which the tensor should be moved.\n",
    "                                           If not provided, it will use the default device.\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: A PyTorch tensor containing the same data as `self.data`.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            return torch.tensor(self.data, dtype=dtype, device=device)\n",
    "        else:\n",
    "            return self.data\n",
    "        \n",
    "    @staticmethod\n",
    "    def equal(x, other, *, equal_nan=False):\n",
    "        \"\"\"\n",
    "        Compare if two Tensor objects have the same shape and elements.\n",
    "        \n",
    "        Args:\n",
    "            x (Tensor): The one tensor to compare.\n",
    "            other (Tensor): The other tensor to compare.\n",
    "        \n",
    "        Returns:\n",
    "           ``True`` if two tensors have the same size and elements, \n",
    "           ``False`` otherwise.\n",
    "        \"\"\"\n",
    "        if x._is_numpy == True and other._is_numpy == True:\n",
    "            return np.array_equal(x, other, equal_nan=equal_nan)\n",
    "        elif  x._is_numpy == False and other._is_numpy == False:\n",
    "            return torch.equal(x, other)\n",
    "        else:\n",
    "            raise ValueError(\"Input `x` and `other` for comparison must have to have the same backend!\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def gather_along(data, axis, index):\n",
    "        \"\"\"\n",
    "        Gather values on an axis with specified index.\n",
    "        \n",
    "        Parameters:\n",
    "            axis: int, the axis number to gather values on.\n",
    "            index: list | array | Matrix, the indices for each row/column/.. to gather values on.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: gathered elements.\n",
    "        \"\"\"\n",
    "        if data._is_numpy:\n",
    "            result = np.take_along_axis(data.data, indices=index, axis=axis)\n",
    "        else:\n",
    "            result = torch.gather(data.data, dim=axis, index=index.data)\n",
    "        return Tensor(result, backend=data._backend, dtype=data.dtype, device=data.device)\n",
    "    \n",
    "    @staticmethod\n",
    "    def where(condition, backend=\"numpy\", dtype=None, device=None):\n",
    "        \"\"\"\n",
    "        Returns elements depending on `condition`.\n",
    "        \n",
    "        Parameters:\n",
    "            condition : Internal Type (array_like); bool Where True\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: chosen elements.\n",
    "        \"\"\"\n",
    "        if backend == \"numpy\":\n",
    "            result = np.where(condition)\n",
    "        else:\n",
    "            result, = torch.where(condition)\n",
    "            if isinstance(result, tuple):\n",
    "                result = result[0]\n",
    "        return Tensor(result, backend=backend, dtype=dtype, device=device)\n",
    "    \n",
    "    @staticmethod\n",
    "    def where_as(condition, then, other, backend=\"numpy\", dtype=None, device=None):\n",
    "        \"\"\"\n",
    "        Returns elements depending on `condition`.\n",
    "        \n",
    "        Parameters:\n",
    "            condition : Internal Type (array_like); bool Where True\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: if true then applied then to true elements; other to fales elements.\n",
    "        \"\"\"\n",
    "        if backend == \"numpy\":\n",
    "            result = np.where(condition, then, other)\n",
    "        else:\n",
    "            result = torch.where(condition, then, other)         \n",
    "            if isinstance(result, tuple):\n",
    "                result = result[0]\n",
    "        return Tensor(result, backend=backend, dtype=dtype, device=device)\n",
    "        \n",
    "    @staticmethod\n",
    "    def zeros(shape, backend=\"numpy\", dtype=None, device=None):\n",
    "        \"\"\"\n",
    "        Creates a tensor filled with zeros.\n",
    "        \n",
    "        Args:\n",
    "            shape (tuple): Desired shape.\n",
    "            backend (str): Backend (\"numpy\" or \"torch\").\n",
    "            dtype: Desired data type.\n",
    "            device: Data device, \"cpu\" or \"cuda\".\n",
    "            \n",
    "        Returns:\n",
    "            Tensor: New tensor of zeros.\n",
    "        \"\"\"\n",
    "        if backend == \"numpy\":\n",
    "            data = np.zeros(shape, dtype=dtype)\n",
    "        elif backend == \"torch\":\n",
    "            data = torch.zeros(shape, dtype=dtype, device=device) if dtype else torch.zeros(shape, device=device)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported backend. Choose 'numpy' or 'torch'.\")\n",
    "        return Tensor(data, backend=backend)\n",
    "\n",
    "    @staticmethod\n",
    "    def ones(shape, backend=\"numpy\", dtype=None, device=None):\n",
    "        \"\"\"\n",
    "        Creates a tensor filled with ones.\n",
    "        \n",
    "        Args:\n",
    "            shape (tuple): Desired shape.\n",
    "            backend (str): Backend (\"numpy\" or \"torch\").\n",
    "            dtype: Desired data type.\n",
    "            device: Data device, \"cpu\" or \"cuda\".\n",
    "            \n",
    "        Returns:\n",
    "            Tensor: New tensor of ones.\n",
    "        \"\"\"\n",
    "        if backend == \"numpy\":\n",
    "            data = np.ones(shape, dtype=dtype)\n",
    "        elif backend == \"torch\":\n",
    "            data = torch.ones(shape, dtype=dtype, device=device) if dtype else torch.ones(shape, device=device)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported backend. Choose 'numpy' or 'torch'.\")\n",
    "        return Tensor(data, backend=backend)\n",
    "    \n",
    "    @staticmethod\n",
    "    def zeros_like(x, backend=\"numpy\", dtype=None, device=None):\n",
    "        \"\"\"\n",
    "        Creates a matrix of zeros with the same shape and data type as another matrix.\n",
    "        \n",
    "        Args:\n",
    "            x (Tensor): The input matrix.\n",
    "            backend (str): The backend for computation (\"numpy\" or \"torch\"). Default is \"numpy\".\n",
    "            dtype: Desired data type for the result. If not specified, uses the data type from `x`.\n",
    "            device: Data device, \"cpu\" or \"cuda\".\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: A new Tensor containing zeros with the same shape and type as `x`.\n",
    "        \n",
    "        Raises:\n",
    "            ValueError: If an unsupported backend is provided.\n",
    "        \n",
    "        \"\"\"\n",
    "        if backend == \"numpy\":\n",
    "            data = np.zeros_like(x.data, dtype=dtype)\n",
    "        elif backend == \"torch\":\n",
    "            data = torch.zeros_like(x.data, dtype=dtype, device=device) if dtype else torch.zeros_like(x.data, device=device)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported backend. Choose 'numpy' or 'torch'.\")\n",
    "        return Tensor(data, backend=backend)\n",
    "    \n",
    "    @staticmethod\n",
    "    def ones_like(x, backend=\"numpy\", dtype=None, device=None):\n",
    "        \"\"\"\n",
    "        Creates a matrix of ones with the same shape and data type as another matrix.\n",
    "        \n",
    "        Args:\n",
    "            x (Tensor): The input matrix.\n",
    "            backend (str): The backend for computation (\"numpy\" or \"torch\"). Default is \"numpy\".\n",
    "            dtype: Desired data type for the result. If not specified, uses the data type from `x`.\n",
    "            device: Data device, \"cpu\" or \"cuda\".\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: A new Tensor containing ones with the same shape and type as `x`.\n",
    "        \n",
    "        Raises:\n",
    "            ValueError: If an unsupported backend is provided.\n",
    "        \n",
    "        \"\"\"\n",
    "        if backend == \"numpy\":\n",
    "            data = np.ones_like(x.data, dtype=dtype)\n",
    "        elif backend == \"torch\":\n",
    "            data = torch.ones_like(x.data, dtype=dtype, device=device) if dtype else torch.ones_like(x.data, device=device)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported backend. Choose 'numpy' or 'torch'.\")\n",
    "        return Tensor(data, backend=backend)\n",
    "    \n",
    "    @staticmethod\n",
    "    def rand(shape, backend=\"numpy\", dtype=None, device=None):\n",
    "        \"\"\"\n",
    "        Creates a tensor with random values uniformly distributed in [0, 1).\n",
    "        \n",
    "        Args:\n",
    "            shape (tuple): Desired shape.\n",
    "            backend (str): Backend (\"numpy\" or \"torch\").\n",
    "            dtype: Desired data type.\n",
    "            device: Data device, \"cpu\" or \"cuda\".\n",
    "            \n",
    "        Returns:\n",
    "            Tensor: New tensor with random values.\n",
    "        \"\"\"\n",
    "        bk = backend.lower()\n",
    "        if bk == \"numpy\":\n",
    "            data = np.random.rand(*shape)\n",
    "            if dtype:\n",
    "                data = data.astype(dtype)\n",
    "        elif bk == \"torch\":\n",
    "            if torch is None:\n",
    "                raise ImportError(\"PyTorch is not installed.\")\n",
    "            data = torch.rand(shape, dtype=dtype, device=device) if dtype else torch.rand(shape, device=device)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported backend. Choose 'numpy' or 'torch'.\")\n",
    "        return Tensor(data, backend=bk)\n",
    "    \n",
    "    @staticmethod\n",
    "    def identity(n, backend=\"numpy\", dtype=None):\n",
    "        \"\"\"\n",
    "        Creates a tensor with identity property.\n",
    "        \n",
    "        Args:\n",
    "            backend (str): Backend (\"numpy\" or \"torch\").\n",
    "            dtype: Desired data type.\n",
    "            \n",
    "        Returns:\n",
    "            Tensor: New identity tensor.\n",
    "        \"\"\"\n",
    "        bk = backend.lower()\n",
    "        if bk == \"numpy\":\n",
    "            data = np.eye(n, dtype=dtype)\n",
    "        elif bk == \"torch\":\n",
    "            if torch is None:\n",
    "                raise ImportError(\"PyTorch is not installed.\")\n",
    "            data = torch.eye(n, dtype=dtype)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported backend. Choose 'numpy' or 'torch'.\")\n",
    "        return Tensor(data, backend=bk)\n",
    "\n",
    "    def reshape_(self, *shape):\n",
    "        \"\"\"\n",
    "        In-place reshape of the tensor.\n",
    "        \n",
    "        Args:\n",
    "            *shape: New shape dimensions.\n",
    "            \n",
    "        Returns:\n",
    "            self: The reshaped tensor.\n",
    "        \"\"\"\n",
    "        self.data = self.data.reshape(*shape)\n",
    "        return self\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`P. Metrics for Binary and Multi-Class Classification Implementation (self-implemented)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These metrics classes are self implemented and open-sourced\n",
    "# Available at https://github.com/dof-studio/MML/\n",
    "# By Nathmath Huang (bh2821)\n",
    "# License: Apache License Version 2.0\n",
    "\n",
    "# Metrics Base Class\n",
    "class BaseMetrics:\n",
    "    \n",
    "    __attr__ = \"MML.BaseMetrics\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def compute(self):\n",
    "        \"\"\"\n",
    "        Compute the specified metric for the predictions given true data.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Compute is NOT implemented in the base class.\")\n",
    "        \n",
    "    def deriv_1(self):\n",
    "        \"\"\"\n",
    "        Compute the sample-wise 1st order derivatives of metric for the predictions given true data.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Deriv_1 is NOT implemented in the base class.\")\n",
    "        \n",
    "    def deriv_2(self):\n",
    "        \"\"\"\n",
    "        Compute the sample-wise 2nd order derivatives of metric for the predictions given true data.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Deriv_2 is NOT implemented in the base class.\")\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"BaseMetrics(Abstract Class).\"\n",
    "\n",
    "\n",
    "# Metrics for regression\n",
    "class RegressionMetrics(BaseMetrics):\n",
    "    \"\"\"\n",
    "    A class to compute common regression metrics between predicted results and target values.\n",
    "    \n",
    "    Supported metrics:\n",
    "        - MSE (Mean Squared Error)\n",
    "        - RMSE (Root Mean Squared Error) \n",
    "        - MAE (Mean Absolute Error)\n",
    "        - MAPE (Mean Absolute Percentage Error)\n",
    "        - Huber Loss\n",
    "        - Quantile Loss\n",
    "        - WMSE (Weighted Mean Squared Error)\n",
    "        - WRMSE (Weighted Root Mean Squared Error)\n",
    "        - R^2 (R Square)\n",
    "        - Adjusted R^2 (Adjusted R Square)\n",
    "        \n",
    "    Special metrics:\n",
    "        - Negative R^2 (R Square)\n",
    "        - Negative Adjusted R^2 (Adjusted R Square)\n",
    "        \n",
    "    The computations are performed using the underlying tensor operations, maintaining\n",
    "    compatibility with both numpy and torch backends.\n",
    "    \n",
    "    Attributes:\n",
    "        result: Predicted results tensor\n",
    "        target: Target values tensor\n",
    "        metric_type: String specifying which metric to compute ('mse', 'rmse', 'mae', 'mape', 'r2', 'adjusted r2')\n",
    "    \"\"\"\n",
    "    \n",
    "    __attr__ = \"MML.RegressionMetrics\"\n",
    "    \n",
    "    def __init__(self, result: Tensor | Matrix, target: Tensor | Matrix, metric_type: str, k: int | None = None, **kwargs):\n",
    "        \"\"\"\n",
    "        Initializes the RegressionMetrics instance with result and target tensors.\n",
    "        \n",
    "        Args:\n",
    "            result (Tensor | Matrix): Predicted results tensor\n",
    "            target (Tensor | Matrix): Target values tensor\n",
    "            metric_type (str): Metric type to compute ('mse', 'rmse', 'mae', 'mape', 'huber_loss', 'quantile_loss', \n",
    "                              'wmse', 'wrmse', 'r2', 'adjusted r2', 'nr2', 'nadjusted r2')\n",
    "            k (int): Number of predictors (parameters) in the model, only used in Adjusted R2.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Different instances or different backends.\n",
    "        if isinstance(result,  Object) == False or isinstance(target, Object) == False:\n",
    "            raise ValueError(\"Predicted `result` and real `target` should be either `Matrix` or `Tensor` type!\")\n",
    "        if type(result) != type(target):\n",
    "            raise ValueError(\"Predicted `result` and real `target` should have the same type, either Tensor or Matrix!\")\n",
    "        if result._backend != target._backend:\n",
    "            raise ValueError(\"Predicted `result` and real `target` should have the same backend, either numpy or torch!\")\n",
    "        \n",
    "        # Member variables.\n",
    "        self.k = k\n",
    "        self.result = result\n",
    "        self.target = target\n",
    "        self.typeclass = type(result)\n",
    "        self.metric_type = metric_type.lower()\n",
    "        \n",
    "        if not self.result.shape == self.target.shape:\n",
    "            raise ValueError(\"Result and target tensors must have the same shape.\")\n",
    "            \n",
    "    def compute(self, **kwargs) -> Tensor | Matrix:\n",
    "        \"\"\"\n",
    "        Computes the specified regression metric between result and target.\n",
    "        \n",
    "        Args:\n",
    "            **kwargs: Other arguments supported by metrics.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor | Matrix: The computed metric value as a tensor\n",
    "        \"\"\"\n",
    "        if self.metric_type == 'mse':\n",
    "            return self._compute_mse(**kwargs)\n",
    "        elif self.metric_type == 'rmse':\n",
    "            return self._compute_rmse(**kwargs)\n",
    "        elif self.metric_type == 'mae':\n",
    "            return self._compute_mae(**kwargs)\n",
    "        elif self.metric_type == 'mape':\n",
    "            return self._compute_mape(**kwargs)\n",
    "        elif self.metric_type == 'huber_loss':\n",
    "            return self._compute_huber_loss(**kwargs)\n",
    "        elif self.metric_type == 'quantile_loss':\n",
    "            return self._compute_quantile_loss(**kwargs)\n",
    "        elif self.metric_type == 'wmse':\n",
    "            return self._compute_wmse(**kwargs)\n",
    "        elif self.metric_type == 'wrmse':\n",
    "            return self._compute_wrmse(**kwargs)\n",
    "        elif self.metric_type == 'r2':\n",
    "            return self._compute_r2(**kwargs)\n",
    "        elif self.metric_type == 'adjusted r2':\n",
    "            return self._compute_adjusted_r2(**kwargs)\n",
    "        # Special Metrics\n",
    "        elif self.metric_type == 'nr2':\n",
    "            return - self._compute_r2(**kwargs)\n",
    "        elif self.metric_type == 'nadjusted r2':\n",
    "            return - self._compute_adjusted_r2(**kwargs)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported metric type: {self.metric_type}\")\n",
    "    \n",
    "    def deriv_1(self, **kwargs) -> Tensor | Matrix:\n",
    "        \"\"\"\n",
    "        Computes the sample-wise 1st order derivative of the specified regression metric between result and target.\n",
    "        \n",
    "        Args:\n",
    "            **kwargs: Other arguments supported by metrics.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor | Matrix: The computed gradient vector as a matrix or a tensor\n",
    "        \"\"\"\n",
    "        if self.metric_type == 'mse':\n",
    "            return self._deriv_1_mse(**kwargs)\n",
    "        elif self.metric_type == 'rmse':\n",
    "            return self._deriv_1_rmse(**kwargs)\n",
    "        elif self.metric_type == 'mae':\n",
    "            return self._deriv_1_mae(**kwargs)\n",
    "        elif self.metric_type == 'mape':\n",
    "            return self._deriv_1_mape(**kwargs)\n",
    "        elif self.metric_type == 'huber_loss':\n",
    "            return self._deriv_1_huber_loss(**kwargs)\n",
    "        elif self.metric_type == 'quantile_loss':\n",
    "            return self._deriv_1_quantile_loss(**kwargs)\n",
    "        elif self.metric_type == 'wmse':\n",
    "            return self._deriv_1_wmse(**kwargs)\n",
    "        elif self.metric_type == 'wrmse':\n",
    "            return self._deriv_1_wrmse(**kwargs)\n",
    "        elif self.metric_type == 'r2':\n",
    "            raise  ValueError(f\"Metric type: {self.metric_type} cannot compute derivatives.\")\n",
    "        elif self.metric_type == 'adjusted r2':\n",
    "            raise  ValueError(f\"Metric type: {self.metric_type} cannot compute derivatives.\")\n",
    "        # Special Metrics\n",
    "        elif self.metric_type == 'nr2':\n",
    "            raise  ValueError(f\"Metric type: {self.metric_type} cannot compute derivatives.\")\n",
    "        elif self.metric_type == 'nadjusted r2':\n",
    "            raise  ValueError(f\"Metric type: {self.metric_type} cannot compute derivatives.\")\n",
    "        else:\n",
    "            raise  ValueError(f\"Unsupported metric type: {self.metric_type}\")\n",
    "    \n",
    "    def deriv_2(self, **kwargs) -> Tensor | Matrix:\n",
    "        \"\"\"\n",
    "        Computes the sample-wise 2nd order derivative of the specified regression metric between result and target.\n",
    "        \n",
    "        Args:\n",
    "            **kwargs: Other arguments supported by metrics.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor | Matrix: The computed hessian matrix (without cross terms) as a matrix or a tensor\n",
    "        \"\"\"\n",
    "        if self.metric_type == 'mse':\n",
    "            return self._deriv_2_mse(**kwargs)\n",
    "        elif self.metric_type == 'rmse':\n",
    "            return self._deriv_2_rmse(**kwargs)\n",
    "        elif self.metric_type == 'mae':\n",
    "            return self._deriv_2_mae(**kwargs)\n",
    "        elif self.metric_type == 'mape':\n",
    "            return self._deriv_2_mape(**kwargs)\n",
    "        elif self.metric_type == 'huber_loss':\n",
    "            return self._deriv_2_huber_loss(**kwargs)\n",
    "        elif self.metric_type == 'quantile_loss':\n",
    "            return self._deriv_2_quantile_loss(**kwargs)\n",
    "        elif self.metric_type == 'wmse':\n",
    "            return self._deriv_2_wmse(**kwargs)\n",
    "        elif self.metric_type == 'wrmse':\n",
    "            return self._deriv_2_wrmse(**kwargs)\n",
    "        elif self.metric_type == 'r2':\n",
    "            raise  ValueError(f\"Metric type: {self.metric_type} cannot compute derivatives.\")\n",
    "        elif self.metric_type == 'adjusted r2':\n",
    "            raise  ValueError(f\"Metric type: {self.metric_type} cannot compute derivatives.\")\n",
    "        # Special Metrics\n",
    "        elif self.metric_type == 'nr2':\n",
    "            raise  ValueError(f\"Metric type: {self.metric_type} cannot compute derivatives.\")\n",
    "        elif self.metric_type == 'nadjusted r2':\n",
    "            raise  ValueError(f\"Metric type: {self.metric_type} cannot compute derivatives.\")\n",
    "        else:\n",
    "            raise  ValueError(f\"Unsupported metric type: {self.metric_type}\")\n",
    "        \n",
    "    def _compute_mse(self, axis: int | None = None, **kwargs) -> Tensor | Matrix:\n",
    "        \"\"\"\n",
    "        Computes the Mean Squared Error between result and target.\n",
    "        \n",
    "        Args:\n",
    "            axis: None or int, if you intend to get the per-output metrics/derivs, set axis = 0. Else None.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor | Matrix: MSE tensor or matrix\n",
    "        \"\"\"\n",
    "        error = (self.result - self.target)\n",
    "        squared_error = error ** 2\n",
    "        if axis is None:\n",
    "            mean_squared_error = squared_error.sum(axis = axis) / np.array(squared_error.shape).prod()\n",
    "        else:\n",
    "            mean_squared_error = squared_error.sum(axis = axis) / squared_error.shape[axis]\n",
    "        return mean_squared_error\n",
    "    \n",
    "    def _deriv_1_mse(self, axis: int | None = None, **kwargs) -> Tensor | Matrix:\n",
    "        \"\"\"\n",
    "        Computes the first-order derivative of the Mean Squared Error between result and target.\n",
    "        \n",
    "        Args:\n",
    "            axis: None or int, if you intend to get the per-output metrics/derivs, set axis = 0. Else None.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor | Matrix: Derivative of MSE tensor or matrix with respect to result.\n",
    "        \"\"\"\n",
    "        error = self.result - self.target\n",
    "        grad = 2 * error / error.shape[0]\n",
    "        if axis is None:\n",
    "            grad = 2 * error / np.array(error.shape).prod()\n",
    "        else:\n",
    "            grad = 2 * error / error.shape[axis]\n",
    "        return grad\n",
    "    \n",
    "    def _deriv_2_mse(self, axis: int | None = None, **kwargs) -> Tensor | Matrix:\n",
    "        \"\"\"\n",
    "        Computes the second-order derivative (Hessian diagonal) of the Mean Squared Error between result and target.\n",
    "    \n",
    "        Args:\n",
    "            only_diag: bool, if True, only calculate the diagonal vector and return,\n",
    "                             else, return the full hessian matrix.\n",
    "    \n",
    "        Returns:\n",
    "            Tensor | Matrix: Constant Hessian of MSE (2/N) with the same shape as result.\n",
    "        \"\"\"\n",
    "        ones = self.result.copy(); ones[...] = 1;\n",
    "        if axis is None:\n",
    "            return ones * (2.0 / np.array(self.result.shape).prod())\n",
    "        else:\n",
    "            return ones * (2.0 / self.result.shape[axis])\n",
    "    \n",
    "    def _compute_rmse(self, axis: int | None = None, **kwargs) -> Tensor | Matrix:\n",
    "        \"\"\"\n",
    "        Computes the Root Mean Squared Error between result and target.\n",
    "        \n",
    "        Args:\n",
    "            axis: None or int, if you intend to get the per-output metrics/derivs, set axis = 0. Else None.\n",
    "                \n",
    "        Returns:\n",
    "            Tensor | Matrix: RMSE tensor or matrix\n",
    "        \"\"\"\n",
    "        error = (self.result - self.target)\n",
    "        squared_error = error ** 2\n",
    "        if axis is None:\n",
    "            mean_squared_error = squared_error.sum(axis = axis) / np.array(squared_error.shape).prod()\n",
    "        else:\n",
    "            mean_squared_error = squared_error.sum(axis = axis) / squared_error.shape[axis]\n",
    "        rmse = mean_squared_error ** 0.5\n",
    "        return rmse\n",
    "    \n",
    "    def _deriv_1_rmse(self, axis: int | None = None, **kwargs) -> Tensor | Matrix:\n",
    "        \"\"\"\n",
    "        Computes the first-order derivative of the Root Mean Squared Error between result and target.\n",
    "        \n",
    "        Args:\n",
    "            axis: None or int, if you intend to get the per-output metrics/derivs, set axis = 0. Else None.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor | Matrix: Derivative of RMSE tensor or matrix with respect to result.\n",
    "        \"\"\"\n",
    "        error = self.result - self.target\n",
    "        squared_error = error ** 2\n",
    "        if axis is None:\n",
    "            mean_squared_error = squared_error.sum(axis = axis) / np.array(squared_error.shape).prod()\n",
    "            rmse = mean_squared_error ** 0.5\n",
    "            grad = error / (np.array(squared_error.shape).prod() * rmse)\n",
    "        else:\n",
    "            mean_squared_error = squared_error.sum(axis = axis) / squared_error.shape[axis]\n",
    "            rmse = mean_squared_error ** 0.5\n",
    "            grad = error / (error.shape[axis] * rmse)\n",
    "\n",
    "        return grad\n",
    "    \n",
    "    def _deriv_2_rmse(self, axis: int | None = None, **kwargs) -> Tensor | Matrix:\n",
    "        \"\"\"\n",
    "        Computes the second-order derivative of the Root Mean Squared Error between result and target.\n",
    "        \n",
    "        Args:\n",
    "            axis: None or int, if you intend to get the per-output metrics/derivs, set axis = 0. Else None.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor | Matrix: Second-order derivative (Hessian diagonal) of RMSE with respect to result.\n",
    "        \"\"\"\n",
    "        error = self.result - self.target\n",
    "        squared_error = error ** 2\n",
    "        if axis is None:\n",
    "            sum_squared = squared_error.sum(axis = axis)\n",
    "            mean_squared_error = sum_squared / np.array(squared_error.shape).prod()\n",
    "            rmse = mean_squared_error ** 0.5\n",
    "            hessian = (sum_squared - squared_error) / ((np.array(squared_error.shape).prod() ** 2) * (rmse ** 3))\n",
    "        else:\n",
    "            sum_squared = squared_error.sum(axis = axis)\n",
    "            mean_squared_error = sum_squared / squared_error.shape[axis]\n",
    "            rmse = mean_squared_error ** 0.5\n",
    "            hessian = (sum_squared - squared_error) / ((error.shape[axis] ** 2) * (rmse ** 3))\n",
    "        \n",
    "        return hessian\n",
    "    \n",
    "    def _compute_mae(self, axis: int | None = None, **kwargs) -> Tensor | Matrix:\n",
    "        \"\"\"\n",
    "        Computes the Mean Absolute Error between result and target.\n",
    "        \n",
    "        Args:\n",
    "            axis: None or int, if you intend to get the per-output metrics/derivs, set axis = 0. Else None.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor | Matrix: MAE tensor or matrix\n",
    "        \"\"\"\n",
    "        error = (self.result - self.target)\n",
    "        absolute_error = error.abs()\n",
    "        if axis is None:\n",
    "            mean_absolute_error = absolute_error.sum(axis = axis) / np.array(absolute_error.shape).prod()\n",
    "        else:\n",
    "            mean_absolute_error = absolute_error.sum(axis = axis) / absolute_error.shape[axis]\n",
    "        return mean_absolute_error\n",
    "\n",
    "    def _deriv_1_mae(self, axis: int | None = None, **kwargs) -> Tensor | Matrix:\n",
    "        \"\"\"\n",
    "        Computes the first-order derivative of the Mean Absolute Error between result and target.\n",
    "        \n",
    "        Args:\n",
    "            axis: None or int, if you intend to get the per-output metrics/derivs, set axis = 0. Else None.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor | Matrix: Derivative of MAE tensor or matrix with respect to result.\n",
    "        \"\"\"\n",
    "        error = self.result - self.target\n",
    "        grad = error.sign() / error.shape[0]\n",
    "        if axis is None:\n",
    "            grad = error.sign() / np.array(error.shape).prod()\n",
    "        else:\n",
    "            grad = error.sign() / error.shape[axis]\n",
    "        return grad\n",
    "    \n",
    "    def _deriv_2_mae(self, axis: int | None = None, **kwargs) -> Tensor | Matrix:\n",
    "        \"\"\"\n",
    "        Computes the second-order derivative of per-output MAE between result and target.\n",
    "        \n",
    "        Args:\n",
    "            axis: None or int, if you intend to get the per-output metrics/derivs, set axis = 0. Else None.\n",
    "                \n",
    "        Returns:\n",
    "            Tensor | Matrix: Hessian diagonal of MAE (zero), shape (N, D).\n",
    "        \"\"\"\n",
    "        zeros = self.result.copy(); zeros[...] = 0;\n",
    "        return zeros\n",
    "    \n",
    "    def _compute_mape(self, axis: int | None = None, **kwargs) -> Tensor | Matrix:\n",
    "        \"\"\"\n",
    "        Computes the Mean Absolute Percentage Error between result and target.\n",
    "        \n",
    "        Note: Division by zero occurs if target contains zeros. This is handled\n",
    "        gracefully by the underlying tensor operations, but users should ensure \n",
    "        target values are non-zero when using MAPE.\n",
    "        \n",
    "        Args:\n",
    "            axis: None or int, if you intend to get the per-output metrics/derivs, set axis = 0. Else None.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor | Matrix: MAPE tensor or matrix\n",
    "        \"\"\"\n",
    "        error = (self.result - self.target) / self.target\n",
    "        absolute_percentage_error = error.abs()\n",
    "        if axis is None:\n",
    "            mean_absolute_percentage_error = absolute_percentage_error.sum(axis = axis) / np.array(absolute_percentage_error.shape).prod()\n",
    "        else:\n",
    "            mean_absolute_percentage_error = absolute_percentage_error.sum(axis = axis) / absolute_percentage_error.shape[axis]\n",
    "        return mean_absolute_percentage_error\n",
    "\n",
    "    def _deriv_1_mape(self, axis: int | None = None, **kwargs) -> Tensor | Matrix:\n",
    "        \"\"\"\n",
    "        Computes the first-order derivative of the Mean Absolute Percentage Error between result and target.\n",
    "        \n",
    "        Args:\n",
    "            axis: None or int, if you intend to get the per-output metrics/derivs, set axis = 0. Else None.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor | Matrix: Derivative of MAPE tensor or matrix with respect to result.\n",
    "        \"\"\"\n",
    "        # Derivative: sign(ratio) * (1/target) / N\n",
    "        ratio = (self.result - self.target) / self.target\n",
    "        if axis is None:\n",
    "            grad = ratio.sign() / (self.target * np.array(ratio.shape).prod())\n",
    "        else:\n",
    "            grad = ratio.sign() / (self.target * ratio.shape[axis])\n",
    "        return grad\n",
    "    \n",
    "    def _deriv_2_mape(self, axis: int | None = None, **kwargs) -> Tensor | Matrix:\n",
    "        \"\"\"\n",
    "        Computes the second-order derivative of per-output MAPE between result and target.\n",
    "        \n",
    "        Args:\n",
    "            axis: None or int, if you intend to get the per-output metrics/derivs, set axis = 0. Else None.\n",
    "                \n",
    "        Returns:\n",
    "            Tensor | Matrix: Hessian diagonal of MAPE (zero), shape (N, D).\n",
    "        \"\"\"\n",
    "        zeros = self.result.copy(); zeros[...] = 0;\n",
    "        return zeros\n",
    "    \n",
    "    def _compute_huber_loss(self, axis: int | None = None, delta: float = 1.0, **kwargs) -> Tensor | Matrix:\n",
    "        \"\"\"\n",
    "        Computes the Huber Loss between result and target.\n",
    "        \n",
    "        Args:\n",
    "            axis: None or int, if you intend to get the per-output metrics/derivs, set axis = 0. Else None.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor | Matrix: Huber Loss tensor or matrix\n",
    "        \"\"\"\n",
    "        error = self.result - self.target\n",
    "        abs_error = error.abs()\n",
    "        # The mask is in an INTERNAL format (np/torch)\n",
    "        small_mask = abs_error.data <= delta\n",
    "        \n",
    "        # The squared region: 0.5 * e^2\n",
    "        sq_loss = 0.5 * (error ** 2)\n",
    "        # The linear region: delta * (|e| - 0.5 * delta)\n",
    "        lin_loss = delta * (abs_error - 0.5 * delta)\n",
    "        \n",
    "        # Huber loss is a combinition of mse and mae\n",
    "        if self.result._is_numpy:\n",
    "            huber = np.where(small_mask, sq_loss.data, lin_loss.data)\n",
    "        else:\n",
    "            huber = torch.where(small_mask, sq_loss.data, lin_loss.data)\n",
    "        \n",
    "        if axis is None:\n",
    "            return type(self.result)(huber, backend=self.result._backend, dtype=self.result.dtype, device=self.result.device).sum(axis = axis) / np.array(error.shape).prod()\n",
    "        else:\n",
    "            return type(self.result)(huber, backend=self.result._backend, dtype=self.result.dtype, device=self.result.device).sum(axis = axis) / error.shape[axis]\n",
    "    \n",
    "    def _deriv_1_huber_loss(self, axis: int | None = None, delta: float = 1.0, **kwargs) -> Tensor | Matrix:\n",
    "        \"\"\"\n",
    "        Computes the first-order derivative of the Huber Loss between result and target.\n",
    "        \n",
    "        Args:\n",
    "            axis: None or int, if you intend to get the per-output metrics/derivs, set axis = 0. Else None.\n",
    "            delta: float, the Huber threshold parameter.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor | Matrix: Derivative of Huber Loss tensor or matrix with respect to result.\n",
    "        \"\"\"\n",
    "        error = self.result - self.target\n",
    "        abs_error = error.abs()\n",
    "        # The mask is in an INTERNAL format (np/torch)\n",
    "        small_mask = abs_error.data <= delta\n",
    "        \n",
    "        if self.result._is_numpy:\n",
    "            grad_elt = np.where(small_mask, error.data, delta * error.sign().data)\n",
    "        else:\n",
    "            grad_elt = torch.where(small_mask, error.data, delta * error.sign().data)\n",
    "        \n",
    "        if axis is None:\n",
    "            return type(self.result)(grad_elt, backend=self.result._backend, dtype=self.result.dtype, device=self.result.device) / np.array(error.shape).prod()\n",
    "        else:\n",
    "            return type(self.result)(grad_elt, backend=self.result._backend, dtype=self.result.dtype, device=self.result.device) / error.shape[axis]\n",
    "\n",
    "    def _deriv_2_huber_loss(self, axis: int | None = None, delta: float = 1.0, **kwargs) -> Tensor | Matrix:\n",
    "        \"\"\"\n",
    "        Computes the second-order derivative (Hessian diagonal) of the Huber Loss between result and target.\n",
    "\n",
    "        Args:\n",
    "            axis: None or int, if you intend to get per-output metrics/derivs, set axis = 0. Else None.\n",
    "            delta: float, the Huber threshold parameter.\n",
    "            \n",
    "        Returns:\n",
    "            Tensor | Matrix: Second-order derivative of Huber Loss wrt result, shape like result.\n",
    "        \"\"\"\n",
    "        error = self.result - self.target\n",
    "        abs_error = error.abs()\n",
    "        # The mask is in an INTERNAL format (np/torch)\n",
    "        small_mask = abs_error.data <= delta\n",
    "\n",
    "        if self.result._is_numpy:\n",
    "            hess_elt = small_mask.astype(float)\n",
    "        else:\n",
    "            # torch.where on a boolean mask: 1.0 where small, else 0.0\n",
    "            one = error.ones(error.shape, backend=self.result._backend).to(backend=self.result._backend, dtype=self.result.dtype, device=self.result.device)\n",
    "            zero = error.zeros(error.shape, backend=self.result._backend).to(backend=self.result._backend, dtype=self.result.dtype, device=self.result.device)\n",
    "            hess_elt = torch.where(small_mask, one.data, zero.data)\n",
    "\n",
    "        if axis is None:\n",
    "            return type(self.result)(hess_elt, backend=self.result._backend, dtype=self.result.dtype, device=self.result.device) / np.array(error.shape).prod()\n",
    "        else:\n",
    "            return type(self.result)(hess_elt, backend=self.result._backend, dtype=self.result.dtype, device=self.result.device) / error.shape[axis]\n",
    "\n",
    "    def _compute_quantile_loss(self, axis: int | None = None, q: float = 0.5, **kwargs) -> Tensor | Matrix:\n",
    "        \"\"\"\n",
    "        Computes the Quantile Loss between result and target.\n",
    "        \n",
    "        Args:\n",
    "            axis: None or int, if you intend to get the per-output metrics/derivs, set axis = 0. Else None.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor | Matrix: Quantile Loss tensor or matrix\n",
    "        \"\"\"\n",
    "        error = self.result - self.target\n",
    "        if self.result._is_numpy:\n",
    "            loss = np.where(error.data >= 0, q * error.data, (q - 1) * error.data)\n",
    "        else:\n",
    "            loss = torch.where(error.data >= 0, q * error.data, (q - 1) * error.data)\n",
    "        \n",
    "        if axis is None:\n",
    "            return type(self.result)(loss, backend=self.result._backend, dtype=self.result.dtype, device=self.result.device).sum(axis = axis) / np.array(error.shape).prod()\n",
    "        else:\n",
    "            return type(self.result)(loss, backend=self.result._backend, dtype=self.result.dtype, device=self.result.device).sum(axis = axis) / error.shape[axis]\n",
    "\n",
    "    def _deriv_1_quantile_loss(self, axis: int | None = None, q: float = 0.5, **kwargs) -> Tensor | Matrix:\n",
    "        \"\"\"\n",
    "        Computes the first-order derivative of the Quantile Loss between result and target.\n",
    "        \n",
    "        Args:\n",
    "            axis: None or int, if you intend to get per-output metrics/derivs, set axis = 0. Else None.\n",
    "            q: float in (0,1), the quantile level.\n",
    "            \n",
    "        Returns:\n",
    "            Tensor | Matrix: Derivative of Quantile Loss tensor or matrix with respect to result.\n",
    "        \"\"\"\n",
    "        error = self.result - self.target\n",
    "        if self.result._is_numpy:\n",
    "            grad_elt = np.where(error.data >= 0, q, q - 1)\n",
    "        else:\n",
    "            grad_elt = torch.where(error.data >= 0, q, q - 1)\n",
    "\n",
    "        if axis is None:\n",
    "            return type(self.result)(grad_elt, backend=self.result._backend, dtype=self.result.dtype, device=self.result.device) / np.array(error.shape).prod()\n",
    "        else:\n",
    "            return type(self.result)(grad_elt, backend=self.result._backend, dtype=self.result.dtype, device=self.result.device) / error.shape[axis]\n",
    "\n",
    "    def _deriv_2_quantile_loss(self, axis: int | None = None, q: float = 0.5, **kwargs) -> Tensor | Matrix:\n",
    "        \"\"\"\n",
    "        Computes the second-order derivative (Hessian diagonal) of the Quantile Loss between result and target.\n",
    "\n",
    "        Args:\n",
    "            axis: None or int, if you intend to get per-output metrics/derivs, set axis = 0. Else None.\n",
    "            q: float in (0,1), the quantile level.\n",
    "            \n",
    "        Returns:\n",
    "            Tensor | Matrix: Second-order derivative of Quantile Loss wrt result, shape like result (all zeros).\n",
    "        \"\"\"\n",
    "        error = self.result - self.target\n",
    "        \n",
    "        if axis is None:\n",
    "            return error.zeros_like(error).to(backend=self.result._backend, dtype=self.result.dtype, device=self.result.device) / np.array(error.shape).prod()\n",
    "        else:\n",
    "            return error.zeros_like(error).to(backend=self.result._backend, dtype=self.result.dtype, device=self.result.device) / error.shape[axis]\n",
    "\n",
    "    def _compute_wmse(self, axis: int | None = None, weights: Matrix | Tensor | None = None, **kwargs) -> Tensor | Matrix:\n",
    "        \"\"\"\n",
    "        Computes the Weighted Mean Squared Error between result and target.\n",
    "        \n",
    "        Args:\n",
    "            axis: None or int, if you intend to get the per-output metrics/derivs, set axis = 0. Else None.\n",
    "            weights: Matrix or Tensor or None, if None, fail to normal mse.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor | Matrix: MSE tensor or matrix\n",
    "        \"\"\"\n",
    "        if weights is None:\n",
    "            return self._compute_mse(axis = axis, **kwargs)\n",
    "        \n",
    "        error = (self.result - self.target)\n",
    "        squared_error = error ** 2\n",
    "        if axis is None:\n",
    "            mean_squared_error = (weights * squared_error).sum(axis = axis) / np.array(squared_error.shape).prod()\n",
    "        else:\n",
    "            mean_squared_error = (weights * squared_error).sum(axis = axis) / squared_error.shape[axis]\n",
    "        return mean_squared_error\n",
    "    \n",
    "    def _deriv_1_wmse(self, axis: int | None = None, weights: Matrix | Tensor | None = None, **kwargs) -> Tensor | Matrix:\n",
    "        \"\"\"\n",
    "        Computes the first-order derivative of the Weighted Mean Squared Error between result and target.\n",
    "        \n",
    "        Args:\n",
    "            axis: None or int, if you intend to get the per-output metrics/derivs, set axis = 0. Else None.\n",
    "            weights: Matrix or Tensor or None, if None, fail to normal mse.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor | Matrix: Derivative of MSE tensor or matrix with respect to result.\n",
    "        \"\"\"\n",
    "        if weights is None:\n",
    "            return self._deriv_1_mse(axis = axis, **kwargs)\n",
    "        \n",
    "        error = self.result - self.target\n",
    "        grad = 2 * error / error.shape[0]\n",
    "        if axis is None:\n",
    "            grad = 2 * weights * error / np.array(error.shape).prod()\n",
    "        else:\n",
    "            grad = 2 * weights * error / error.shape[axis]\n",
    "        return grad\n",
    "    \n",
    "    def _deriv_2_wmse(self, axis: int | None = None, weights: Matrix | Tensor | None = None, **kwargs) -> Tensor | Matrix:\n",
    "        \"\"\"\n",
    "        Computes the second-order derivative (Hessian diagonal) of the Weighted Mean Squared Error between result and target.\n",
    "    \n",
    "        Args:\n",
    "            axis: None or int, if you intend to get the per-output metrics/derivs, set axis = 0. Else None.\n",
    "            weights: Matrix or Tensor or None, if None, fail to normal mse.\n",
    "            \n",
    "        Returns:\n",
    "            Tensor | Matrix: Constant Hessian of MSE (2/N) with the same shape as result.\n",
    "        \"\"\"\n",
    "        if weights is None:\n",
    "            return self._deriv_2_mse(axis = axis, **kwargs)\n",
    "        \n",
    "        ones = self.result.copy(); ones[...] = 1;\n",
    "        if axis is None:\n",
    "            return ones * (2.0 * weights / np.array(self.result.shape).prod())\n",
    "        else:\n",
    "            return ones * (2.0 * weights / self.result.shape[axis])\n",
    "    \n",
    "    def _compute_wrmse(self, axis: int | None = None, weights: Matrix | Tensor | None = None, **kwargs) -> Tensor | Matrix:\n",
    "        \"\"\"\n",
    "        Computes the Weighted Root Mean Squared Error between result and target.\n",
    "        \n",
    "        Args:\n",
    "            axis: None or int, if you intend to get the per-output metrics/derivs, set axis = 0. Else None.\n",
    "            weights: Matrix or Tensor or None, if None, fail to normal rmse.\n",
    "            \n",
    "        Returns:\n",
    "            Tensor | Matrix: RMSE tensor or matrix\n",
    "        \"\"\"\n",
    "        if weights is None:\n",
    "            return self._compute_rmse(axis = axis, **kwargs)\n",
    "        \n",
    "        error = (self.result - self.target)\n",
    "        squared_error = weights * error ** 2\n",
    "        if axis is None:\n",
    "            mean_squared_error = squared_error.sum(axis = axis) / np.array(squared_error.shape).prod()\n",
    "        else:\n",
    "            mean_squared_error = squared_error.sum(axis = axis) / squared_error.shape[axis]\n",
    "        rmse = mean_squared_error ** 0.5\n",
    "        return rmse\n",
    "    \n",
    "    def _deriv_1_wrmse(self, axis: int | None = None, weights: Matrix | Tensor | None = None, **kwargs) -> Tensor | Matrix:\n",
    "        \"\"\"\n",
    "        Computes the first-order derivative of the Weighted Root Mean Squared Error between result and target.\n",
    "        \n",
    "        Args:\n",
    "            axis: None or int, if you intend to get the per-output metrics/derivs, set axis = 0. Else None.\n",
    "            weights: Matrix or Tensor or None, if None, fail to normal rmse.\n",
    "            \n",
    "        Returns:\n",
    "            Tensor | Matrix: Derivative of RMSE tensor or matrix with respect to result.\n",
    "        \"\"\"\n",
    "        if weights is None:\n",
    "            return self._deriv_1_rmse(axis = axis, **kwargs)\n",
    "        \n",
    "        error = self.result - self.target\n",
    "        squared_error = weights * error ** 2\n",
    "        if axis is None:\n",
    "            mean_squared_error = squared_error.sum(axis = axis) / np.array(squared_error.shape).prod()\n",
    "            rmse = mean_squared_error ** 0.5\n",
    "            grad = weights * error / (np.array(squared_error.shape).prod() * rmse)\n",
    "        else:\n",
    "            mean_squared_error = squared_error.sum(axis = axis) / squared_error.shape[axis]\n",
    "            rmse = mean_squared_error ** 0.5\n",
    "            grad = weights * error / (error.shape[axis] * rmse)\n",
    "\n",
    "        return grad\n",
    "    \n",
    "    def _deriv_2_wrmse(self, axis: int | None = None, weights: Matrix | Tensor | None = None, **kwargs) -> Tensor | Matrix:\n",
    "        \"\"\"\n",
    "        Computes the second-order derivative of the Weighted Root Mean Squared Error between result and target.\n",
    "        \n",
    "        Args:\n",
    "            axis: None or int, if you intend to get the per-output metrics/derivs, set axis = 0. Else None.\n",
    "            weights: Matrix or Tensor or None, if None, fail to normal rmse.\n",
    "            \n",
    "        Returns:\n",
    "            Tensor | Matrix: Second-order derivative (Hessian diagonal) of RMSE with respect to result.\n",
    "        \"\"\"\n",
    "        if weights is None:\n",
    "            return self._deriv_2_rmse(axis = axis, **kwargs)\n",
    "        \n",
    "        error = self.result - self.target\n",
    "        squared_error = weights * error ** 2\n",
    "        if axis is None:\n",
    "            sum_squared = squared_error.sum(axis = axis)\n",
    "            mean_squared_error = sum_squared / np.array(squared_error.shape).prod()\n",
    "            rmse = mean_squared_error ** 0.5\n",
    "            N = np.array(squared_error.shape).prod()\n",
    "            hessian = weights / (N * rmse) - (weights **2 * error ** 2) / ((N ** 2) * (rmse ** 3))\n",
    "        else:\n",
    "            sum_squared = squared_error.sum(axis = axis)\n",
    "            mean_squared_error = sum_squared / squared_error.shape[axis]\n",
    "            rmse = mean_squared_error ** 0.5\n",
    "            N = error.shape[axis] \n",
    "            hessian = weights / (N * rmse) - (weights **2 * error ** 2) / ((N ** 2) * (rmse ** 3))\n",
    "        \n",
    "        return hessian\n",
    "    \n",
    "    def _compute_r2(self, **kwargs) -> Tensor | Matrix:\n",
    "        \"\"\"\n",
    "        Computes the coefficient of determination R^2 between result and target.\n",
    "        \n",
    "        Args:\n",
    "            axis: None or int, if you intend to get the per-output metrics/derivs, set axis = 0. Else None.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor | Matrix: R^2 value.\n",
    "        \"\"\"\n",
    "        # Compute the residual sum of squares (SS_res)\n",
    "        error = self.result - self.target\n",
    "        ss_res = (error ** 2.0).sum()\n",
    "        \n",
    "        # Compute the total sum of squares (SS_tot)\n",
    "        target_mean = self.target.sum() / self.target.shape[0]\n",
    "        total_error = self.target - target_mean\n",
    "        ss_tot = (total_error ** 2.0).sum()\n",
    "        \n",
    "        # Calculate R^2 = 1 - (SS_res / SS_tot)\n",
    "        r2 = 1 - (ss_res / ss_tot)\n",
    "        return r2\n",
    "    \n",
    "    def _compute_adjusted_r2(self, **kwargs) -> Tensor | Matrix:\n",
    "        \"\"\"\n",
    "        Computes the adjusted R^2 value.\n",
    "        \n",
    "        Args:\n",
    "            axis: None or int, if you intend to get the per-output metrics/derivs, set axis = 0. Else None.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor | Matrix: Adjusted R^2 value.\n",
    "        \"\"\"\n",
    "        # If self.k is None, badly initialized.\n",
    "        if self.k is None or isinstance(self.k, int) == False:\n",
    "            raise ValueError(\"You must specify a valid `k` as the number of parameters in the model before calculating Adjusted R^2.\")\n",
    "        \n",
    "        # Compute R^2 using the previously defined method.\n",
    "        r2 = self._compute_r2()\n",
    "        \n",
    "        # Determine the number of observations (be the size along the first dimension)\n",
    "        n = self.target.shape[0]\n",
    "        \n",
    "        # Calculate adjusted R^2 using: 1 - (1-R^2)*((n-1)/(n-p-1))\n",
    "        adjusted_r2 = 1 - ((1 - r2) * ((n - 1) /  (n - self.k - 1)))\n",
    "        return adjusted_r2\n",
    "    \n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        String representation of the RegressionMetrics instance.\n",
    "        \"\"\"\n",
    "        return f\"RegressionMetrics(metric_type={self.metric_type}, shape={self.result.shape})\"\n",
    "\n",
    "\n",
    "# Metrics for classfication (base)\n",
    "class ClassificationMetrics(BaseMetrics):\n",
    "\n",
    "    __attr__ = \"MML.ClassificationMetrics\"    \n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"ClassificationMetrics(Abstract Class).\"\n",
    "\n",
    "\n",
    "# Metrics for binary classification\n",
    "class BinaryClassificationMetrics(ClassificationMetrics):\n",
    "    \"\"\"\n",
    "    A class to compute common binary classification metrics between predicted results and target values.\n",
    "    \n",
    "    Supported metrics include:\n",
    "        - accuracy\n",
    "        - precision\n",
    "        - recall (sensitivity) [TPR]\n",
    "        - f1 score\n",
    "        - specificity [TNR]\n",
    "        - auc_roc\n",
    "        - confusion_matrix\n",
    "        - tpr (True Positive Rate)\n",
    "        - tnr (True Negative Rate)\n",
    "        - fpr (False Positive Rate)\n",
    "        - fnr (False Negative Rate)\n",
    "        - logloss (continuous)\n",
    "    \n",
    "    The computations are performed using the underlying tensor operations. It is assumed that both \n",
    "    the result and target are of the same type (Tensor or Matrix) and support similar operations.\n",
    "    \n",
    "    Attributes:\n",
    "        result: Predicted results tensor or matrix (can be continuous scores or binary labels).\n",
    "        target: Target binary values tensor or matrix.\n",
    "        metric_type: A string specifying which metric to compute ('accuracy', 'precision', 'recall',\n",
    "                     'f1', 'specificity', 'auc_roc', 'confusion_matrix').\n",
    "        threshold: A float value used to convert continuous scores into binary predictions (default 0.5).\n",
    "    \"\"\"\n",
    "    \n",
    "    __attr__ = \"MML.BinaryClassificationMetrics\"   \n",
    "    \n",
    "    def __init__(self, result: Tensor | Matrix, target: Tensor | Matrix, metric_type: str = \"accuracy\", threshold: float = 0.5, **kwargs):\n",
    "        \"\"\"\n",
    "        Initializes the BinaryClassificationMetrics instance with result and target tensors.\n",
    "        \n",
    "        Args:\n",
    "            result (Tensor | Matrix): Predicted results tensor\n",
    "            target (Tensor | Matrix): Target values tensor\n",
    "            metric_type (str): Metric type to compute ('accuracy', 'precision', 'recall', 'f1', 'specificity',\n",
    "                               'auc_roc', 'confusion_matrix', 'tpr', 'tnr', 'fpr', 'fnr', 'logloss')\n",
    "            threshold (float): a threshold for considering which one to be the positive samples and negative samples.\n",
    "                               In normal tasks, it is recommended to be 0.5. But adjusting this may change the metrics.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Different instances or different backends.\n",
    "        if isinstance(result, Object) == False or isinstance(target, Object) == False:\n",
    "            raise ValueError(\"Predicted `result` and real `target` should be either `Matrix` or `Tensor` type!\")\n",
    "        if type(result) != type(target):\n",
    "            raise ValueError(\"Predicted `result` and real `target` should have the same type, either Tensor or Matrix!\")\n",
    "        if result._backend != target._backend:\n",
    "            raise ValueError(\"Predicted `result` and real `target` should have the same backend, either numpy or torch!\")\n",
    "        \n",
    "        # Data Members.\n",
    "        self.result = result\n",
    "        self.target = target\n",
    "        self.metric_type = metric_type.lower()\n",
    "        self.threshold = threshold\n",
    "        \n",
    "        # Use the type of result as the typeclass.\n",
    "        self.typeclass = type(result)\n",
    "        \n",
    "        if not self.result.shape == self.target.shape:\n",
    "            raise ValueError(\"Result and target tensors must have the same shape.\")\n",
    "\n",
    "    def compute(self, **kwargs) -> Matrix | Tensor:\n",
    "        \"\"\"\n",
    "        Computes the specified metric for a given model or data.\n",
    "    \n",
    "        Args:\n",
    "            **kwargs: Other arguments supported by metrics.\n",
    "            \n",
    "        Returns:\n",
    "            Matrix | Tensor: The computed metric value. The result is always returned as a Matrix or Tensor object,\n",
    "                                      even if the computation yields a scalar.                            \n",
    "        \n",
    "        Raises:\n",
    "            ValueError: If an unsupported metric type is provided.\n",
    "            \n",
    "        \"\"\"\n",
    "        # Note, all results are stored in a Matrix | Tensor even it is a scalar.\n",
    "        if self.metric_type == 'accuracy':\n",
    "            return self._compute_accuracy(**kwargs)\n",
    "        elif self.metric_type == 'precision':\n",
    "            return self._compute_precision(**kwargs)\n",
    "        elif self.metric_type in ('recall', 'sensitivity', 'tpr'):\n",
    "            return self._compute_recall(**kwargs)\n",
    "        elif self.metric_type in ('f1', 'f1 score'):\n",
    "            return self._compute_f1(**kwargs)\n",
    "        elif self.metric_type in ('specificity', 'tnr'):\n",
    "            return self._compute_specificity(**kwargs)\n",
    "        elif self.metric_type == 'fpr':\n",
    "            return self._compute_fpr(**kwargs)\n",
    "        elif self.metric_type == 'fnr':\n",
    "            return self._compute_fnr(**kwargs)\n",
    "        elif self.metric_type == 'auc_roc':\n",
    "            return self._compute_auc_roc(**kwargs)\n",
    "        elif self.metric_type == 'confusion_matrix':\n",
    "            return self._compute_confusion_matrix(**kwargs)\n",
    "        elif self.metric_type in ('logloss', 'log-loss', 'entropy', 'cross-entropy'):\n",
    "            return self._compute_logloss(**kwargs)\n",
    "        # Implemented by Nathmath Huang.\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported metric type: {self.metric_type}\")\n",
    "    \n",
    "    def deriv_1(self, **kwargs) -> Tensor | Matrix:\n",
    "        \"\"\"\n",
    "        Computes the sample-wise 1st order derivative for a given model or data.\n",
    "        \n",
    "        Args:\n",
    "            **kwargs: Other arguments supported by metrics.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor | Matrix: The computed metric value as a tensor\n",
    "        \"\"\"\n",
    "        # Note, all results are stored in a Matrix | Tensor even it is a scalar.\n",
    "        if self.metric_type in ('logloss', 'log-loss', 'entropy', 'cross-entropy'):\n",
    "            return self._deriv_1_logloss(**kwargs)\n",
    "        elif self.metric_type == 'accuracy':\n",
    "            raise  ValueError(f\"Metric type: {self.metric_type} cannot compute derivatives.\")\n",
    "        elif self.metric_type == 'precision':\n",
    "            raise  ValueError(f\"Metric type: {self.metric_type} cannot compute derivatives.\")\n",
    "        elif self.metric_type in ('recall', 'sensitivity', 'tpr'):\n",
    "            raise  ValueError(f\"Metric type: {self.metric_type} cannot compute derivatives.\")\n",
    "        elif self.metric_type in ('f1', 'f1 score'):\n",
    "            raise  ValueError(f\"Metric type: {self.metric_type} cannot compute derivatives.\")\n",
    "        elif self.metric_type in ('specificity', 'tnr'):\n",
    "            raise  ValueError(f\"Metric type: {self.metric_type} cannot compute derivatives.\")\n",
    "        elif self.metric_type == 'fpr':\n",
    "            raise  ValueError(f\"Metric type: {self.metric_type} cannot compute derivatives.\")\n",
    "        elif self.metric_type == 'fnr':\n",
    "            raise  ValueError(f\"Metric type: {self.metric_type} cannot compute derivatives.\")\n",
    "        elif self.metric_type == 'auc_roc':\n",
    "            raise  ValueError(f\"Metric type: {self.metric_type} cannot compute derivatives.\")\n",
    "        elif self.metric_type == 'confusion_matrix':\n",
    "            raise  ValueError(f\"Metric type: {self.metric_type} cannot compute derivatives.\")\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported metric type: {self.metric_type}\")\n",
    "    \n",
    "    def deriv_2(self, **kwargs) -> Tensor | Matrix:\n",
    "        \"\"\"\n",
    "        Computes the sample-wise 2nd order derivative for a given model or data.\n",
    "        \n",
    "        Args:\n",
    "            **kwargs: Other arguments supported by metrics.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor | Matrix: The computed metric value as a tensor\n",
    "        \"\"\"\n",
    "        # Note, all results are stored in a Matrix | Tensor even it is a scalar.\n",
    "        if self.metric_type in ('logloss', 'log-loss', 'entropy', 'cross-entropy'):\n",
    "            return self._deriv_2_logloss(**kwargs)\n",
    "        elif self.metric_type == 'accuracy':\n",
    "            raise  ValueError(f\"Metric type: {self.metric_type} cannot compute derivatives.\")\n",
    "        elif self.metric_type == 'precision':\n",
    "            raise  ValueError(f\"Metric type: {self.metric_type} cannot compute derivatives.\")\n",
    "        elif self.metric_type in ('recall', 'sensitivity', 'tpr'):\n",
    "            raise  ValueError(f\"Metric type: {self.metric_type} cannot compute derivatives.\")\n",
    "        elif self.metric_type in ('f1', 'f1 score'):\n",
    "            raise  ValueError(f\"Metric type: {self.metric_type} cannot compute derivatives.\")\n",
    "        elif self.metric_type in ('specificity', 'tnr'):\n",
    "            raise  ValueError(f\"Metric type: {self.metric_type} cannot compute derivatives.\")\n",
    "        elif self.metric_type == 'fpr':\n",
    "            raise  ValueError(f\"Metric type: {self.metric_type} cannot compute derivatives.\")\n",
    "        elif self.metric_type == 'fnr':\n",
    "            raise  ValueError(f\"Metric type: {self.metric_type} cannot compute derivatives.\")\n",
    "        elif self.metric_type == 'auc_roc':\n",
    "            raise  ValueError(f\"Metric type: {self.metric_type} cannot compute derivatives.\")\n",
    "        elif self.metric_type == 'confusion_matrix':\n",
    "            raise  ValueError(f\"Metric type: {self.metric_type} cannot compute derivatives.\")\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported metric type: {self.metric_type}\")\n",
    "            \n",
    "    def _binarize(self, y_real_or_pred: Matrix | Tensor) -> Matrix | Tensor:\n",
    "        \"\"\"\n",
    "        Binarizes continuous prediction scores by applying a threshold.\n",
    "    \n",
    "        Args:\n",
    "            y_real_or_pred (Matrix | Tensor): the y values to be binarized.\n",
    "    \n",
    "        Returns:\n",
    "            Matrix | Tensor: A matrix or tensor containing binary predictions (True/False values).\n",
    "    \n",
    "        \"\"\"\n",
    "        # Return the results in a Matrix or Tensor of Booleans\n",
    "        return self.typeclass(y_real_or_pred.data >= self.threshold, backend = y_real_or_pred._backend, device = y_real_or_pred.device)\n",
    "    \n",
    "    def _compute_confusion_counts(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Computes the counts of true positives (TP), true negatives (TN), \n",
    "                     false positives (FP) and false negatives (FN) using binarized predictions.\n",
    "    \n",
    "        Args:\n",
    "            None\n",
    "    \n",
    "        Returns:\n",
    "            tuple: A tuple containing four elements, each representing TP, TN, FP, and FN respectively.\n",
    "                  Each element is a matrix or tensor of the same type as self.target.\n",
    "    \n",
    "        \"\"\"\n",
    "        pred = self._binarize(self.result)  # Full of Booleans.\n",
    "        real = self._binarize(self.target)  # Full of Booleans.\n",
    "\n",
    "        TP = ((pred.data == True) & (real.data == True)).sum()\n",
    "        TN = ((pred.data == False) & (real.data == False)).sum()\n",
    "        FP = ((pred.data == True) & (real.data == False)).sum()\n",
    "        FN = ((pred.data == False) & (real.data == True)).sum()\n",
    "        return (self.typeclass(TP, backend=self.target._backend, dtype=self.target.dtype, device=self.target.device), \n",
    "                self.typeclass(TN, backend=self.target._backend, dtype=self.target.dtype, device=self.target.device),\n",
    "                self.typeclass(FP, backend=self.target._backend, dtype=self.target.dtype, device=self.target.device),\n",
    "                self.typeclass(FN, backend=self.target._backend, dtype=self.target.dtype, device=self.target.device)\n",
    "                )\n",
    "\n",
    "    def _compute_accuracy(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Computes accuracy = (TP + TN) / total.\n",
    "        \n",
    "        Args:\n",
    "            None\n",
    "    \n",
    "        Returns:\n",
    "            Matrix | Tensor: The computed accuracy value.\n",
    "        \n",
    "        \"\"\"\n",
    "        # Always return a Matrix | Tensor as the class input.\n",
    "        TP, TN, FP, FN = self._compute_confusion_counts()\n",
    "        total = TP + TN + FP + FN\n",
    "        return (TP + TN) / total\n",
    "\n",
    "    def _compute_precision(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Computes precision = TP / (TP + FP).\n",
    "    \n",
    "        Args:\n",
    "            None\n",
    "    \n",
    "        Returns:\n",
    "            Matrix | Tensor: The computed precision value.\n",
    "        \n",
    "        \"\"\"\n",
    "        # Always return a Matrix | Tensor as the class input.\n",
    "        TP, _, FP, _ = self._compute_confusion_counts()\n",
    "        denom = TP + FP\n",
    "        if bool(denom.data == 0) == True:\n",
    "            return self.typeclass(0, backend=self.target._backend, dtype=self.target.dtype, device=self.target.device)\n",
    "        return TP / denom\n",
    "\n",
    "    def _compute_recall(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Computes recall (sensitivity) = TP / (TP + FN).\n",
    "        \n",
    "        Args:\n",
    "            None\n",
    "    \n",
    "        Returns:\n",
    "            Matrix | Tensor: The computed recall value.\n",
    "        \n",
    "        \"\"\"\n",
    "        # Always return a Matrix | Tensor as the class input.\n",
    "        TP, _, _, FN = self._compute_confusion_counts()\n",
    "        denom = TP + FN\n",
    "        if bool(denom.data == 0) == True:\n",
    "            return self.typeclass(0, backend=self.target._backend, dtype=self.target.dtype, device=self.target.device)\n",
    "        return TP / denom\n",
    "\n",
    "    def _compute_f1(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Computes the F1 score as the harmonic mean of precision and recall.\n",
    "        \n",
    "        Args:\n",
    "            None\n",
    "    \n",
    "        Returns:\n",
    "            Matrix | Tensor: The computed f1 score value.\n",
    "        \n",
    "        \"\"\"\n",
    "        # Always return a Matrix | Tensor as the class input.\n",
    "        TP, TN, FP, FN = self._compute_confusion_counts()\n",
    "        denom = 2 * TP + FP + FN\n",
    "        if bool(denom.data == 0) == True:\n",
    "            return self.typeclass(0, backend=self.target._backend, dtype=self.target.dtype, device=self.target.device)\n",
    "        return 2 * TP / denom\n",
    "\n",
    "    def _compute_specificity(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Computes specificity = TN / (TN + FP).\n",
    "        \n",
    "        Args:\n",
    "            None\n",
    "    \n",
    "        Returns:\n",
    "            Matrix | Tensor: The computed specificity value.\n",
    "        \"\"\"\n",
    "        # Always return a Matrix | Tensor as the class input.\n",
    "        _, TN, FP, _ = self._compute_confusion_counts()\n",
    "        denom = TN + FP\n",
    "        if bool(denom.data == 0) == True:\n",
    "            return self.typeclass(0, backend=self.target._backend, dtype=self.target.dtype, device=self.target.device)\n",
    "        return TN / denom\n",
    "\n",
    "    def _compute_tpr(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Computes recall (TPR) = TP / (TP + FN).\n",
    "        \n",
    "        Args:\n",
    "            None\n",
    "    \n",
    "        Returns:\n",
    "            Matrix | Tensor: The computed TPR value.\n",
    "        \n",
    "        \"\"\"\n",
    "        # Always return a Matrix | Tensor as the class input.\n",
    "        TP, _, _, FN = self._compute_confusion_counts()\n",
    "        denom = TP + FN\n",
    "        if bool(denom.data == 0) == True:\n",
    "            return self.typeclass(0, backend=self.target._backend, dtype=self.target.dtype, device=self.target.device)\n",
    "        return TP / denom\n",
    "\n",
    "    def _compute_tnr(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Computes specificity (TNR) = TN / (TN + FP).\n",
    "        \n",
    "        Args:\n",
    "            None\n",
    "    \n",
    "        Returns:\n",
    "            Matrix | Tensor: The computed TNR value.\n",
    "        \"\"\"\n",
    "        # Always return a Matrix | Tensor as the class input.\n",
    "        _, TN, FP, _ = self._compute_confusion_counts()\n",
    "        denom = TN + FP\n",
    "        if bool(denom.data == 0) == True:\n",
    "            return self.typeclass(0, backend=self.target._backend, dtype=self.target.dtype, device=self.target.device)\n",
    "        return TN / denom\n",
    "\n",
    "    def _compute_fpr(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Computes FPR = FP / (FP + TN).\n",
    "        \n",
    "        Args:\n",
    "            None\n",
    "    \n",
    "        Returns:\n",
    "            Matrix | Tensor: The computed TPR value.\n",
    "        \n",
    "        \"\"\"\n",
    "        # Always return a Matrix | Tensor as the class input.\n",
    "        _, TN, FP, _ = self._compute_confusion_counts()\n",
    "        denom = FP + TN\n",
    "        if bool(denom.data == 0) == True:\n",
    "            return self.typeclass(0, backend=self.target._backend, dtype=self.target.dtype, device=self.target.device)\n",
    "        return FP / denom\n",
    "\n",
    "    def _compute_fnr(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Computes FNR = FN / (TP + FN).\n",
    "        \n",
    "        Args:\n",
    "            None\n",
    "    \n",
    "        Returns:\n",
    "            Matrix | Tensor: The computed FNR value.\n",
    "        \n",
    "        \"\"\"\n",
    "        TP, _, _, FN = self._compute_confusion_counts()\n",
    "        denom = TP + FN\n",
    "        if bool(denom.data == 0) == True:\n",
    "            return self.typeclass(0, backend=self.target._backend, dtype=self.target.dtype, device=self.target.device)\n",
    "        return FN / denom\n",
    "\n",
    "    def _compute_auc_roc(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Computes the area under the ROC curve (AUC-ROC) using the trapezoidal rule.\n",
    "        This method assumes that self.result contains continuous prediction scores.\n",
    "        \n",
    "        Args:\n",
    "            None\n",
    "    \n",
    "        Returns:\n",
    "            Matrix | Tensor: The computed auc_roc area.\n",
    "        \"\"\"\n",
    "        # Always return a Matrix | Tensor as the class input.\n",
    "        scores = self.result.data\n",
    "        labels = self.target.data\n",
    "        \n",
    "        # Sort indices based on scores in descending order.\n",
    "        sorted_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)\n",
    "        sorted_labels = [labels[i] for i in sorted_indices]\n",
    "        P = sum(labels)\n",
    "        N = len(labels) - P\n",
    "        if P == 0 or N == 0:\n",
    "            return self.typeclass(0.0, backend=self.target._backend, dtype=self.target.dtype, device=self.target.device)\n",
    "\n",
    "        tpr, fpr = [], []\n",
    "        tp = 0\n",
    "        fp = 0\n",
    "        for label in sorted_labels:\n",
    "            if label == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fp += 1\n",
    "            tpr.append(tp / P)\n",
    "            fpr.append(fp / N)\n",
    "        \n",
    "        auc = 0.0\n",
    "        prev_fpr = 0.0\n",
    "        prev_tpr = 0.0\n",
    "        for current_fpr, current_tpr in zip(fpr, tpr):\n",
    "            auc += (current_fpr - prev_fpr) * (current_tpr + prev_tpr) / 2.0\n",
    "            prev_fpr = current_fpr\n",
    "            prev_tpr = current_tpr\n",
    "        return self.typeclass(auc, backend=self.target._backend, dtype=self.target.dtype, device=self.target.device)\n",
    "\n",
    "    def _compute_logloss(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Computes the binary classification log loss between predicted and actual values.\n",
    "    \n",
    "        Args: \n",
    "            None\n",
    "            \n",
    "        Returns:\n",
    "            Matrix | Tensor: The computed logloss using this formula:\n",
    "                logloss = - (y_true * log(y_pred) + (1 - y_true) * log(1 - y_pred))\n",
    "        \"\"\"\n",
    "        epsilon = 1e-15\n",
    "        preds = self.result.to(self.result._backend, dtype=float)\n",
    "        labels = self.target.to(self.result._backend, dtype=float)\n",
    "        clipped_preds = preds.clip(epsilon, 1 - epsilon)\n",
    "        losses = -(labels * clipped_preds.log() + (1 - labels) * (1 - clipped_preds).log())\n",
    "        return losses.mean()\n",
    "\n",
    "    def _deriv_1_logloss(self, **kwargs) -> Tensor | Matrix:\n",
    "        \"\"\"\n",
    "        Computes the first-order derivative of the binary classification log loss between predicted and actual values.\n",
    "\n",
    "        Args: \n",
    "            None\n",
    "            \n",
    "        Returns:\n",
    "            Matrix | Tensor: Derivative of logloss with respect to the predictions.\n",
    "        \"\"\"\n",
    "        epsilon = 1e-15\n",
    "        preds = self.result.to(self.result._backend, dtype=float)\n",
    "        labels = self.target.to(self.result._backend, dtype=float)\n",
    "        clipped = preds.clip(epsilon, 1 - epsilon)\n",
    "        grad = ((1 - labels) / (1 - clipped) - labels / clipped) / clipped.shape[0]\n",
    "        return grad\n",
    "    \n",
    "    def _deriv_2_logloss(self, **kwargs) -> Tensor | Matrix:\n",
    "        \"\"\"\n",
    "        Computes the second-order derivative of the binary classification log loss between predicted and actual values.\n",
    "\n",
    "        Args: \n",
    "            None\n",
    "            \n",
    "        Returns:\n",
    "            Matrix | Tensor: Second-order derivative (Hessian diagonal) of logloss with respect to the predictions.\n",
    "        \"\"\"\n",
    "        epsilon = 1e-15\n",
    "        preds = self.result.to(self.result._backend, dtype=float)\n",
    "        labels = self.target.to(self.result._backend, dtype=float)\n",
    "        clipped = preds.clip(epsilon, 1 - epsilon)\n",
    "        hess = ((1 - labels) / (1 - clipped) ** 2 + labels / clipped ** 2) / clipped.shape[0]\n",
    "        return hess\n",
    "\n",
    "    def _compute_confusion_matrix(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Computes the confusion matrix as a 2x2 tensor or matrix with the format:\n",
    "          [[TP, FP],\n",
    "           [FN, TN]]\n",
    "            \n",
    "        Args:\n",
    "            None\n",
    "    \n",
    "        Returns:\n",
    "            Matrix | Tensor: The computed confusion matrix, with shape 2,2.\n",
    "        \"\"\"\n",
    "        TP, TN, FP, FN = self._compute_confusion_counts()\n",
    "        return self.typeclass(\n",
    "            [TP.data, FP.data,\n",
    "             FN.data, TN.data], \n",
    "            backend=self.target._backend, dtype=self.target.dtype, device=self.target.device).reshape([2,2])\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"BinaryClassificationMetrics(metric_type={self.metric_type}, shape={self.result.shape})\"\n",
    "\n",
    "\n",
    "# Metrics for multi-class classification\n",
    "class MultiClassificationMetrics(ClassificationMetrics):\n",
    "    \"\"\"\n",
    "    A class to compute common multi-class classification metrics between predicted results and target values.\n",
    "    \n",
    "    Supported metrics include:\n",
    "        - accuracy\n",
    "        - precision        (macro-average computed either in one-vs-rest (OVR) or one-vs-one (OVO) mode)\n",
    "        - recall           (macro-average computed either in OVR or OVO mode)\n",
    "        - f1 score         (macro-average computed either in OVR or OVO mode)\n",
    "        - logloss          (cross-entropy loss, continuous)\n",
    "        - confusion_matrix (of shape [n_classes, n_classes])\n",
    "        \n",
    "    The class is designed to support two scenarios:\n",
    "        1. Multi-target: where predictions are provided as a 1D vector of labels\n",
    "           (e.g. 0, 1, 2, 3, ...) and the target is also a vector.\n",
    "        2. One-hot: where the target (and optionally predictions) are provided as a\n",
    "           one-hot encoded matrix of shape [n_samples, n_classes].\n",
    "    \n",
    "    When computing precision, recall, and f1-score, the user can specify\n",
    "    whether the aggregation should be based on one-vs-rest (default) or one-vs-one.\n",
    "    \n",
    "    Attributes:\n",
    "        result (Tensor | Matrix): Predicted results. Can be either a 1D vector (labels) \n",
    "                                   or a 2D matrix (probabilities / one-hot scores). \n",
    "        target (Tensor | Matrix): True labels. Must be in a format compatible with result\n",
    "                                   (either both 1D or both 2D, or convertible between them).\n",
    "        metric_type (str): Which metric to compute (\"accuracy\", \"precision\", \"recall\",\n",
    "                           \"f1\", \"logloss\", \"confusion_matrix\").\n",
    "        mode (str): For metrics that require binary decomposition (\"precision\",\n",
    "                    \"recall\", \"f1\"), the aggregation mode: either \"ovr\" (one-vs-rest) or \"ovo\" (one-vs-one).\n",
    "    \"\"\"\n",
    "    \n",
    "    __attr__ = \"MML.MultiClassificationMetrics\"   \n",
    "    \n",
    "    def __init__(self, result: Tensor | Matrix, target: Tensor | Matrix, metric_type: str = \"accuracy\", n_classes: int = None, mode: str = \"ovr\", **kwargs):\n",
    "        \"\"\"\n",
    "        Initializes the MultiClassificationMetrics instance with result and target tensors.\n",
    "        \n",
    "        Args:\n",
    "            result (Tensor | Matrix): Predicted results tensor\n",
    "            target (Tensor | Matrix): Target values tensor\n",
    "            metric_type (str): Metric type to compute ('accuracy', 'precision', 'recall', 'f1', 'confusion_matrix', 'logloss')\n",
    "            n_classes (int): Number of Classes\n",
    "            mode (str): `ovr` or `ovo`, one versus remaining or one versus one.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Check type compatibility\n",
    "        if isinstance(result, Object) == False or isinstance(target,  Object) == False:\n",
    "            raise ValueError(\"Predicted `result` and real `target` should be either `Matrix` or `Tensor` type!\")\n",
    "        if type(result) != type(target):\n",
    "            raise ValueError(\"Predicted `result` and real `target` should have the same type, either Tensor or Matrix!\")\n",
    "        if result._backend != target._backend:\n",
    "            raise ValueError(\"Predicted `result` and real `target` should have the same backend, either numpy or torch!\")\n",
    "\n",
    "        # Data Members.        \n",
    "        self.result = result\n",
    "        self.target = target\n",
    "        self.metric_type = metric_type.lower()\n",
    "        self.mode = mode.lower()\n",
    "        self.typeclass = type(result)\n",
    "        \n",
    "        # Determine classification format and number of classes.\n",
    "        # If given, then okay, or infer.\n",
    "        # If one of the inputs is two-dimensional, we assume the second dimension is the number of classes.\n",
    "        if n_classes is not None:\n",
    "            self.n_classes = n_classes\n",
    "        else:\n",
    "            if len(result.shape) == 2:\n",
    "                if result.shape[1] == 1:\n",
    "                    # Check if it is indeed a binary problem\n",
    "                    if len(result.flatten().bincount().unique()) == 2:\n",
    "                        self.n_classes = 2\n",
    "                    else:\n",
    "                        self.n_classes = len(result.flatten().bincount().unique())\n",
    "                        # Not safe, but okay.\n",
    "                else:\n",
    "                    self.n_classes = result.shape[1]\n",
    "            elif len(target.shape) == 2:\n",
    "                if target.shape[1] == 1:\n",
    "                    # Check if it is indeed a binary problem\n",
    "                    if len(target.flatten().unique()) == 2:\n",
    "                        self.n_classes = 2\n",
    "                    else:\n",
    "                        self.n_classes = len(target.flatten().unique())\n",
    "                        # Not safe, but okay.\n",
    "                else:\n",
    "                    self.n_classes = target.shape[1]\n",
    "            else:\n",
    "                # Error. The result dimension is not 2?!!\n",
    "                raise ValueError(\"The input `result` and `target` do not have a 2-dimension shape. Make sure it is a multi-classification problem. Set n_classes or resize the Matrix | Tensor if you only have one row.\")\n",
    "        \n",
    "    def compute(self, eps: float = 1e-15, floattype: type = float, **kwargs) -> Matrix | Tensor:\n",
    "        \"\"\"\n",
    "        Computes the specified multi-class metric.\n",
    "        \n",
    "        Supported metric_type values (case insensitive):\n",
    "            - 'accuracy'\n",
    "            - 'precision'\n",
    "            - 'recall'\n",
    "            - 'f1'\n",
    "            - 'logloss'\n",
    "            - 'confusion_matrix'\n",
    "        \n",
    "        For precision, recall and f1, the results are computed according to the specified mode (ovr or ovo).\n",
    "        \n",
    "        Args:\n",
    "            eps: float, clip value to ensure 0/0 cases.\n",
    "            floattype: type, the internal precision of calculation.\n",
    "            **kwargs: Other arguments supported by metrics.\n",
    "    \n",
    "        Returns:\n",
    "            Matrix | Tensor: The computed metric value. The result is always returned as a Matrix or Tensor object,\n",
    "                                      even if the computation yields a scalar.\n",
    "        \n",
    "        Raises:\n",
    "            ValueError: If an unsupported metric type or mode type is provided.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Note, all results are stored in a Matrix | Tensor even it is a scalar.\n",
    "        # Accuracy\n",
    "        if self.metric_type == 'accuracy':\n",
    "            return self._compute_accuracy(floattype=floattype, **kwargs)\n",
    "        # Precision\n",
    "        elif self.metric_type == 'precision':\n",
    "            if self.mode == 'ovr':\n",
    "                return self._compute_precision_ovr(eps=eps, floattype=floattype, **kwargs)\n",
    "            elif self.mode == 'ovo':\n",
    "                return self._compute_precision_ovo(eps=eps, floattype=floattype, **kwargs)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported mode for precision: {self.mode}. Use `ovo` or `ovr`.\")\n",
    "        # Recall\n",
    "        elif self.metric_type in ('recall', 'sensitivity', 'tpr'):\n",
    "            if self.mode == 'ovr':\n",
    "                return self._compute_recall_ovr(eps=eps, floattype=floattype, **kwargs)\n",
    "            elif self.mode == 'ovo':\n",
    "                return self._compute_recall_ovo(eps=eps, floattype=floattype, **kwargs)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported mode for recall: {self.mode}. Use `ovo` or `ovr`.\")\n",
    "        # F1 Score\n",
    "        elif self.metric_type in ('f1', 'f1 score'):\n",
    "            if self.mode == 'ovr':\n",
    "                return self._compute_f1_ovr(eps=eps, floattype=floattype, **kwargs)\n",
    "            elif self.mode == 'ovo':\n",
    "                return self._compute_f1_ovo(eps=eps, floattype=floattype, **kwargs)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported mode for f1: {self.mode}. Use `ovo` or `ovr`.\")\n",
    "        # Cross entropy/logloss\n",
    "        elif self.metric_type in ('logloss', 'log-loss', 'entropy', 'cross-entropy'):\n",
    "            return self._compute_logloss(eps=eps, floattype=floattype, **kwargs)\n",
    "        # Confusion matrix\n",
    "        elif self.metric_type == 'confusion_matrix':\n",
    "            return self._compute_confusion_matrix(**kwargs)\n",
    "        # Implemented by Nathmath Huang.\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported metric type: {self.metric_type}\")\n",
    "    \n",
    "    def deriv_1(self, eps: float = 1e-15, floattype: type = float, **kwargs) -> Matrix | Tensor:\n",
    "        \"\"\"\n",
    "        Computes the specified multi-class element-wise 1st order derivative.\n",
    "        \n",
    "        Supported metric_type values (case insensitive):\n",
    "            - 'logloss'\n",
    "        \n",
    "        Args:\n",
    "            eps: float, clip value to ensure 0/0 cases.\n",
    "            floattype: type, the internal precision of calculation.\n",
    "            **kwargs: Other arguments supported by metrics.\n",
    "    \n",
    "        Returns:\n",
    "            Matrix | Tensor: The computed metric value. The result is always returned as a Matrix or Tensor object,\n",
    "                                      even if the computation yields a scalar.\n",
    "        \n",
    "        Raises:\n",
    "            ValueError: If an unsupported metric type or mode type is provided.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Note, only \"logloss\" supports derivatives.\n",
    "        # Cross entropy/logloss\n",
    "        if self.metric_type in ('logloss', 'log-loss', 'entropy', 'cross-entropy'):\n",
    "            return self._deriv_1_logloss(eps=eps, floattype=floattype, **kwargs)\n",
    "        # Accuracy\n",
    "        elif self.metric_type == 'accuracy':\n",
    "            raise  ValueError(f\"Metric type: {self.metric_type} cannot compute derivatives.\")\n",
    "        # Precision\n",
    "        elif self.metric_type == 'precision':\n",
    "            raise  ValueError(f\"Metric type: {self.metric_type} cannot compute derivatives.\")\n",
    "        # Recall\n",
    "        elif self.metric_type in ('recall', 'sensitivity', 'tpr'):\n",
    "            raise  ValueError(f\"Metric type: {self.metric_type} cannot compute derivatives.\")\n",
    "        # F1 Score\n",
    "        elif self.metric_type in ('f1', 'f1 score'):\n",
    "            raise  ValueError(f\"Metric type: {self.metric_type} cannot compute derivatives.\")\n",
    "\n",
    "        # Confusion matrix\n",
    "        elif self.metric_type == 'confusion_matrix':\n",
    "            raise  ValueError(f\"Metric type: {self.metric_type} cannot compute derivatives.\")\n",
    "        \n",
    "        # Implemented by Nathmath Huang.\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported metric type: {self.metric_type}\")\n",
    "    \n",
    "    def deriv_2(self, eps: float = 1e-15, floattype: type = float, **kwargs) -> Tensor | Matrix:\n",
    "        \"\"\"\n",
    "        Computes the sample-wise 2nd order derivative for a given model or data.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor | Matrix: The computed metric value as a tensor\n",
    "        \"\"\"\n",
    "        # Note, only \"logloss\" supports derivatives.\n",
    "        # Cross entropy/logloss\n",
    "        if self.metric_type in ('logloss', 'log-loss', 'entropy', 'cross-entropy'):\n",
    "            return self._deriv_2_logloss(eps=eps, floattype=floattype, **kwargs)\n",
    "        # Accuracy\n",
    "        elif self.metric_type == 'accuracy':\n",
    "            raise  ValueError(f\"Metric type: {self.metric_type} cannot compute derivatives.\")\n",
    "        # Precision\n",
    "        elif self.metric_type == 'precision':\n",
    "            raise  ValueError(f\"Metric type: {self.metric_type} cannot compute derivatives.\")\n",
    "        # Recall\n",
    "        elif self.metric_type in ('recall', 'sensitivity', 'tpr'):\n",
    "            raise  ValueError(f\"Metric type: {self.metric_type} cannot compute derivatives.\")\n",
    "        # F1 Score\n",
    "        elif self.metric_type in ('f1', 'f1 score'):\n",
    "            raise  ValueError(f\"Metric type: {self.metric_type} cannot compute derivatives.\")\n",
    "\n",
    "        # Confusion matrix\n",
    "        elif self.metric_type == 'confusion_matrix':\n",
    "            raise  ValueError(f\"Metric type: {self.metric_type} cannot compute derivatives.\")\n",
    "        \n",
    "        # Implemented by Nathmath Huang.\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported metric type: {self.metric_type}\")\n",
    "            \n",
    "    def _to_labels(self, x: Tensor | Matrix, *, apply_softmax:bool = False) -> Tensor | Matrix:\n",
    "        \"\"\"\n",
    "        Converts predictions or targets into label vectors.\n",
    "        \n",
    "        If x has more than one column (i.e. one-hot or probability matrix), it returns\n",
    "        the index of the maximum value along axis 1. Otherwise, x is assumed already to be a vector.\n",
    "             \n",
    "        Args:\n",
    "            x: Matrix | Tensor: The one-hot or probability matrix.\n",
    "            apply_softmax: bool, whether to apply softmax before calculating argmax or not.\n",
    "    \n",
    "        Returns:\n",
    "            Matrix | Tensor: The converted Tensor or Matrix in (n_samples, 1) shape.\n",
    "        \"\"\"\n",
    "        # Wide-table: prob or one-hot\n",
    "        if len(x.shape) > 1 and x.shape[1] > 1:\n",
    "            # Always keep the dim.\n",
    "            return x.argmax(axis=1).reshape([-1, 1]) if apply_softmax == False else x.softmax(axis=1).argmax(axis=1).reshape([-1, 1])\n",
    "        # Narrow table\n",
    "        else:    \n",
    "            return x\n",
    "\n",
    "    def _to_onehot(self, x: Tensor | Matrix, *, binarize = False) -> Tensor | Matrix:\n",
    "        \"\"\"\n",
    "        Converts a label vector into a one-hot encoded matrix of shape [n_samples, n_classes].\n",
    "        If x is already a matrix with the correct number of columns, it is returned unaltered.\n",
    "        If x is binary probability input and binarize is False, then will return the probablistic one-hot.\n",
    "                     \n",
    "        Args:\n",
    "            x: Matrix | Tensor: The label-encoded matrix.\n",
    "    \n",
    "        Returns:\n",
    "            Matrix | Tensor: The converted one-hot Tensor or Matrix in (n_samples, n_classes) shape.\n",
    "        \"\"\"\n",
    "        if len(x.shape) == 2 and x.shape[1] == self.n_classes:\n",
    "            return x\n",
    "        \n",
    "        # If binary case, then create a probabilistic one-hot to reduce information loss\n",
    "        if self.n_classes == 2 and binarize == False:\n",
    "            onehot_data = type(x).zeros([x.shape[0], 2], backend=x._backend)\n",
    "            onehot_data[:, 1] = x.flatten()\n",
    "            onehot_data[:, 0] = 1.0 - onehot_data[:, 1]\n",
    "            return onehot_data.to(backend=x._backend, device=x.device, dtype=x.dtype)\n",
    "        \n",
    "        # Else, do the round\n",
    "        else:\n",
    "            # Create one-hot by comparing each element with a range vector.\n",
    "            range_vec = self.typeclass(np.arange(self.n_classes), backend=x._backend, device=x.device)\n",
    "            # Reshape x to [n_samples, 1] if necessary\n",
    "            x_reshaped = x.reshape([x.shape[0], 1])\n",
    "        \n",
    "            # Broadcast the comparison: each entry becomes True if equal to the class index.\n",
    "            onehot_data = x_reshaped.astype(self.result.dtype).round() == range_vec\n",
    "            # The above one produces a boolean array -> like True, False, True, ...\n",
    "            #                                                False, True, False, ...\n",
    "            return onehot_data.to(backend=x._backend, device=x.device, dtype=float)\n",
    "\n",
    "    def _compute_accuracy(self, *, floattype: type = float, **kwargs):\n",
    "        \"\"\"\n",
    "        Computes accuracy = (# correct predictions) / (# total samples).\n",
    "                             \n",
    "        Args:\n",
    "            floattype: type, the internal precision of calculation\n",
    "    \n",
    "        Returns:\n",
    "            Matrix | Tensor: The computed accuracy value.\n",
    "        \"\"\"\n",
    "        pred_labels = self._to_labels(self.result)\n",
    "        true_labels = self._to_labels(self.target)\n",
    "        correct = pred_labels == true_labels\n",
    "        total = true_labels.shape[0]\n",
    "        return correct.sum().to(correct._backend, dtype=floattype, device=correct.device) / total\n",
    "    \n",
    "    def _compute_confusion_matrix(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Computes the multi-class confusion matrix of shape [n_classes, n_classes],\n",
    "        where rows correspond to true labels and columns to predicted labels.\n",
    "        \n",
    "        Args:\n",
    "            None\n",
    "    \n",
    "        Returns:\n",
    "            Matrix | Tensor: The computed confusion matrix with shape [n_classes, n_classes].\n",
    "        \"\"\"\n",
    "        # Convert both predictions and targets to label vectors\n",
    "        pred_labels = self._to_labels(self.result)\n",
    "        true_labels = self._to_labels(self.target)\n",
    "        \n",
    "        # Convert them into one-hot matrices of shape [n_samples, n_classes]\n",
    "        pred_onehot = self._to_onehot(pred_labels)\n",
    "        true_onehot = self._to_onehot(true_labels)\n",
    "        \n",
    "        # Compute the confusion matrix as: (true_onehot)^T dot (pred_onehot)\n",
    "        conf_matrix = true_onehot.transpose().dot(pred_onehot)\n",
    "        return conf_matrix\n",
    "\n",
    "    def _compute_logloss(self, *, eps: float = 1e-15, floattype: type = float, **kwargs):\n",
    "        \"\"\"\n",
    "        Computes the cross-entropy loss (logloss) for multi-class classification.\n",
    "        \n",
    "        Assumes that `result` is a probability matrix of shape [n_samples, n_classes].\n",
    "        The loss is computed as:\n",
    "            logloss = - 1/N * sum_over_samples [ sum_over_classes (y_true * log(y_pred)) ]\n",
    "            \n",
    "        Args:\n",
    "            eps: float, clip value to ensure predictions are not going to be log(0)\n",
    "            floattype: type, the internal precision of calculation\n",
    "    \n",
    "        Returns:\n",
    "            Matrix | Tensor: The computed log loss value.\n",
    "        \"\"\"\n",
    "        if len(self.result.shape) != 2:\n",
    "            raise ValueError(\"Logloss metric requires probability predictions with shape [n_samples, n_classes].\")\n",
    "        \n",
    "        # Ensure predictions are floating point and clip values to avoid log(0)\n",
    "        preds = self.result.to(self.result._backend, dtype=floattype, device=self.result.device).clip(eps, 1 - eps)\n",
    "\n",
    "        # Compute elementwise: y_true * log(y_pred), then sum over classes (axis=1) then average over samples.\n",
    "        true_onehot = self._to_onehot(self._to_labels(self.target))\n",
    "        losses = -(true_onehot * preds.log()).sum(axis=1)\n",
    "        return losses.mean()\n",
    "    \n",
    "    def _deriv_1_logloss(self, *, eps: float = 1e-15, floattype: type = float, **kwargs) -> Tensor | Matrix:\n",
    "        \"\"\"\n",
    "        Computes the first-order derivative of the cross-entropy loss (logloss) for multi-class classification.\n",
    "        \n",
    "        Assumes that `result` is a probability matrix of shape [n_samples, n_classes].\n",
    "        \n",
    "        Args:\n",
    "            eps: float, clip value to ensure predictions are not going to be log(0)\n",
    "            floattype: type, the internal precision of calculation\n",
    "    \n",
    "        Returns:\n",
    "            Matrix | Tensor: Derivative of the logloss with respect to the predictions.\n",
    "        \"\"\"\n",
    "        if len(self.result.shape) != 2:\n",
    "            raise ValueError(\"Logloss metric requires probability predictions with shape [n_samples, n_classes].\")\n",
    "        \n",
    "        # Ensure predictions are floating point and clip values to avoid log(0)\n",
    "        preds = self.result.to(self.result._backend, dtype=floattype, device=self.result.device).clip(eps, 1 - eps)\n",
    "\n",
    "        # Convert to one-hot labels\n",
    "        true_onehot = self._to_onehot(self._to_labels(self.target))\n",
    "\n",
    "        # ∂L/∂p = -y/p \n",
    "        grad = -(true_onehot / preds) / preds.shape[0]\n",
    "        return grad\n",
    "    \n",
    "    def _deriv_2_logloss(self, *, eps: float = 1e-15, floattype: type = float, **kwargs) -> Tensor | Matrix:\n",
    "        \"\"\"\n",
    "        Computes the second-order derivative of the cross-entropy loss (logloss) for multi-class classification.\n",
    "        \n",
    "        Assumes that `result` is a probability matrix of shape [n_samples, n_classes].\n",
    "        \n",
    "        Args:\n",
    "            eps: float, clip value to ensure predictions are not going to be log(0)\n",
    "            floattype: type, the internal precision of calculation\n",
    "    \n",
    "        Returns:\n",
    "            Matrix | Tensor: Second-order derivative (Hessian diagonal) of the logloss with respect to the predictions.\n",
    "        \"\"\"\n",
    "        if len(self.result.shape) != 2:\n",
    "            raise ValueError(\"Logloss metric requires probability predictions with shape [n_samples, n_classes].\")\n",
    "        \n",
    "        # Ensure predictions are floating point and clip values to avoid log(0)\n",
    "        preds = self.result.to(self.result._backend, dtype=floattype, device=self.result.device).clip(eps, 1 - eps)\n",
    "\n",
    "        # Convert to one-hot labels\n",
    "        true_onehot = self._to_onehot(self._to_labels(self.target))\n",
    "        \n",
    "        hess = (true_onehot / preds ** 2) / preds.shape[0]\n",
    "        return hess\n",
    "\n",
    "    # OVR (One-Vs-Remaining) implementations for precision, recall and f1\n",
    "\n",
    "    def _compute_precision_ovr(self, *, eps: float = 1e-15, floattype: type = float, **kwargs):\n",
    "        \"\"\"\n",
    "        Computes macro-average precision using a one-vs-rest approach.\n",
    "        \n",
    "        For each class c:\n",
    "            precision[c] = TP[c] / (TP[c] + FP[c])\n",
    "        and the final metric is the mean over classes.\n",
    "        \n",
    "        Args:\n",
    "            eps: float, clip value to ensure 0/0 cases.\n",
    "            floattype: type, the internal precision of calculation.\n",
    "    \n",
    "        Returns:\n",
    "            Matrix | Tensor: The computed precision value.\n",
    "        \"\"\"\n",
    "        pred_labels = self._to_labels(self.result)\n",
    "        true_labels = self._to_labels(self.target)\n",
    "        pred_onehot = self._to_onehot(pred_labels.astype(self.result.dtype))\n",
    "        true_onehot = self._to_onehot(true_labels.astype(self.target.dtype))\n",
    "        \n",
    "        # True positives: elementwise multiplication then sum over samples (axis=0)\n",
    "        TP = (true_onehot * pred_onehot).sum(axis=0)\n",
    "       \n",
    "        # False positives: predicted positive but not truly positive.\n",
    "        FP = ((self.typeclass.ones_like(true_onehot, backend=true_onehot._backend) - true_onehot) * pred_onehot).sum(axis=0)\n",
    "        precision_per_class = TP / (TP + FP + floattype(eps))\n",
    "        return precision_per_class.mean()\n",
    "\n",
    "    def _compute_recall_ovr(self, *, eps: float = 1e-15, floattype: type = float, **kwargs):\n",
    "        \"\"\"\n",
    "        Computes macro-average recall (sensitivity) using a one-vs-rest approach.\n",
    "        \n",
    "        For each class c:\n",
    "            recall[c] = TP[c] / (TP[c] + FN[c])\n",
    "        and the final metric is the mean over classes.\n",
    "        \n",
    "        Args:\n",
    "            eps: float, clip value to ensure 0/0 cases.\n",
    "            floattype: type, the internal precision of calculation.\n",
    "    \n",
    "        Returns:\n",
    "            Matrix | Tensor: The computed recall value.\n",
    "        \"\"\"\n",
    "        pred_labels = self._to_labels(self.result)\n",
    "        true_labels = self._to_labels(self.target)\n",
    "        pred_onehot = self._to_onehot(pred_labels.astype(self.result.dtype))\n",
    "        true_onehot = self._to_onehot(true_labels.astype(self.target.dtype))\n",
    "        \n",
    "        # True positives: elementwise multiplication then sum over samples (axis=0)\n",
    "        TP = (true_onehot * pred_onehot).sum(axis=0)\n",
    "        \n",
    "        # False negatives: predicted negative but not trully negative.\n",
    "        FN = (true_onehot * (self.typeclass.ones_like(pred_onehot, backend=pred_onehot._backend) - pred_onehot)).sum(axis=0)\n",
    "        recall_per_class = TP / (TP + FN + floattype(eps))\n",
    "        return recall_per_class.mean()\n",
    "\n",
    "    def _compute_f1_ovr(self, *, eps: float = 1e-15, floattype: type = float, **kwargs):\n",
    "        \"\"\"\n",
    "        Computes macro-average F1-score in one-vs-rest mode.\n",
    "        F1 per class is computed as:\n",
    "            F1[c] = 2 * precision[c] * recall[c] / (precision[c] + recall[c])\n",
    "        The final score is the average over classes.\n",
    "        \n",
    "        Args:\n",
    "            eps: float, clip value to ensure 0/0 cases.\n",
    "            floattype: type, the internal precision of calculation.\n",
    "    \n",
    "        Returns:\n",
    "            Matrix | Tensor: The computed recall value.\n",
    "        \"\"\"\n",
    "        # Compute per-class precision and recall in OVR mode.\n",
    "        pred_labels = self._to_labels(self.result)\n",
    "        true_labels = self._to_labels(self.target)\n",
    "        pred_onehot = self._to_onehot(pred_labels.astype(self.result.dtype))\n",
    "        true_onehot = self._to_onehot(true_labels.astype(self.target.dtype))\n",
    "        \n",
    "        TP = (true_onehot * pred_onehot).sum(axis=0)\n",
    "        FP = ((self.typeclass.ones_like(true_onehot, backend=true_onehot._backend) - true_onehot) * pred_onehot).sum(axis=0)\n",
    "        FN = (true_onehot * (self.typeclass.ones_like(pred_onehot, backend=pred_onehot._backend) - pred_onehot)).sum(axis=0)\n",
    "        \n",
    "        precision_per_class = TP / (TP + FP + floattype(eps))\n",
    "        recall_per_class = TP / (TP + FN + floattype(eps))\n",
    "        f1_per_class = (2 * precision_per_class * recall_per_class) / (precision_per_class + recall_per_class + floattype(eps))\n",
    "        return f1_per_class.mean()\n",
    "\n",
    "    # OVO (One-Vs-One) implementations for precision, recall and f1\n",
    "    #\n",
    "    # These computations use the full confusion matrix. For every pair of different classes\n",
    "    # (i, j), we define binary precision and recall:\n",
    "    #   For class i in pair (i,j):\n",
    "    #       precision_i = M[i,i] / (M[i,i] + M[j,i] + eps)\n",
    "    #       recall_i = M[i,i] / (M[i,i] + M[i,j] + eps)\n",
    "    #   Similarly for class j.\n",
    "    # The final OVO metric is computed as the average over all the binary evaluations.\n",
    "    \n",
    "    def _compute_precision_ovo(self, *, eps: float = 1e-15, floattype: type = float, **kwargs):\n",
    "        \"\"\"\n",
    "        Computes macro-average precision using a ovo approach.\n",
    "        \n",
    "        Args:\n",
    "            eps: float, clip value to ensure 0/0 cases.\n",
    "            floattype: type, the internal precision of calculation.\n",
    "    \n",
    "        Returns:\n",
    "            Matrix | Tensor: The computed precision value.\n",
    "        \"\"\"\n",
    "        conf_matrix = self._compute_confusion_matrix()  # shape: [n_classes, n_classes]\n",
    "        \n",
    "        # Create index matrices using broadcasting.\n",
    "        idx = self.typeclass(np.arange(self.n_classes), backend=conf_matrix._backend, device=conf_matrix.device)\n",
    "        I = idx.reshape([self.n_classes, 1]).repeat(self.n_classes, axis=1)\n",
    "        J = idx.reshape([1, self.n_classes]).repeat(self.n_classes, axis=0)\n",
    "        mask = I.data < J.data  # boolean mask selecting one instance per unordered pair\n",
    "                                # Internal type\n",
    "        \n",
    "        # Extract diagonal elements as a vector.\n",
    "        diag = conf_matrix.diag()  # shape [n_classes]\n",
    "        \n",
    "        # Expand diagonals for broadcasting.\n",
    "        diag_i = diag.reshape([self.n_classes, 1]).repeat(self.n_classes, axis=1)  # each row: diag[i]\n",
    "        diag_j = diag.reshape([1, self.n_classes]).repeat(self.n_classes, axis=0)  # each column: diag[j]\n",
    "        \n",
    "        # For a given pair (i, j):\n",
    "        # precision for class i:\n",
    "        p_i_matrix = diag_i / (diag_i + conf_matrix.transpose())  \n",
    "        \n",
    "        # We need M[j, i] for p_i. In our matrix, conf_matrix[j,i] is given by\n",
    "        # conf_matrix.transpose()[i,j]. Thus, we use:\n",
    "        p_i_matrix = diag_i / (diag_i + conf_matrix.transpose() + floattype(eps))\n",
    "        \n",
    "        # And precision for class j:\n",
    "        p_j_matrix = diag_j / (diag_j + conf_matrix + floattype(eps))\n",
    "        \n",
    "        # Now select only entries for pairs where I < J.\n",
    "        p_i_vals = p_i_matrix[mask]\n",
    "        p_j_vals = p_j_matrix[mask]\n",
    "        \n",
    "        # Concatenate and compute the mean.\n",
    "        all_precisions = p_i_vals.append(p_j_vals, axis=0)\n",
    "        return all_precisions.mean()\n",
    "\n",
    "    def _compute_recall_ovo(self, *, eps: float = 1e-15, floattype: type = float, **kwargs):\n",
    "        \"\"\"\n",
    "        Computes macro-average recall using a ovo approach.\n",
    "        \n",
    "        Args:\n",
    "            eps: float, clip value to ensure 0/0 cases.\n",
    "            floattype: type, the internal precision of calculation.\n",
    "    \n",
    "        Returns:\n",
    "            Matrix | Tensor: The computed recall value.\n",
    "        \"\"\"\n",
    "        conf_matrix = self._compute_confusion_matrix()  # shape: [n_classes, n_classes]\n",
    "        \n",
    "        # Create index matrices using broadcasting.\n",
    "        idx = self.typeclass(np.arange(self.n_classes), backend=conf_matrix._backend, device=conf_matrix.device)\n",
    "        I = idx.reshape([self.n_classes, 1]).repeat(self.n_classes, axis=1)\n",
    "        J = idx.reshape([1, self.n_classes]).repeat(self.n_classes, axis=0)\n",
    "        mask = I.data < J.data  # boolean mask selecting one instance per unordered pair\n",
    "                                # Internal type\n",
    "        \n",
    "        # Extract diagonal elements as a vector.\n",
    "        diag = conf_matrix.diag()\n",
    "        diag_i = diag.reshape([self.n_classes, 1]).repeat(self.n_classes, axis=1)\n",
    "        diag_j = diag.reshape([1, self.n_classes]).repeat(self.n_classes, axis=0)\n",
    "\n",
    "        # For recall in a pair (i, j):\n",
    "        # recall for class i:\n",
    "        r_i_matrix = diag_i / (diag_i + conf_matrix +  floattype(eps))\n",
    "        # and recall for class j:\n",
    "        r_j_matrix = diag_j / (diag_j + conf_matrix.transpose() + floattype(eps))\n",
    "        \n",
    "        # Now select only entries for pairs where I < J.\n",
    "        r_i_vals = r_i_matrix[mask]\n",
    "        r_j_vals = r_j_matrix[mask]\n",
    "        \n",
    "        # Concatenate and compute the mean.\n",
    "        all_recalls = r_i_vals.append(r_j_vals, axis=0)\n",
    "        return all_recalls.mean()\n",
    "\n",
    "    def _compute_f1_ovo(self, *, eps: float = 1e-15, floattype: type = float, **kwargs):\n",
    "        \"\"\"\n",
    "        Computes macro-average f1-score using a ovo approach.\n",
    "        \n",
    "        Args:\n",
    "            eps: float, clip value to ensure 0/0 cases.\n",
    "            floattype: type, the internal precision of calculation.\n",
    "    \n",
    "        Returns:\n",
    "            Matrix | Tensor: The computed f1-score.\n",
    "        \"\"\"\n",
    "        \n",
    "        # First compute the binary precisions and recalls from OVO.\n",
    "        precision_ovo = self._compute_precision_ovo(eps=eps, floattype=floattype)\n",
    "        recall_ovo = self._compute_recall_ovo(eps=eps, floattype=floattype)\n",
    "        f1_ovo = (2 * precision_ovo * recall_ovo) / (precision_ovo + recall_ovo + floattype(eps))\n",
    "        return f1_ovo\n",
    "\n",
    "    def __repr__(self):\n",
    "        return (f\"MultiClassificationMetrics(metric_type={self.metric_type}, mode={self.mode}, \"\n",
    "                f\"n_classes={self.n_classes}, result_shape={self.result.shape})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`P. BaseML Classes for all algorithms (self-implemented)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These base classes are self implemented and open-sourced\n",
    "# Available at https://github.com/dof-studio/MML/\n",
    "# By Nathmath Huang (bh2821)\n",
    "# License: Apache License Version 2.0\n",
    "\n",
    "# Machine Learning Algorithm Base Class\n",
    "class MLBase:\n",
    "    \"\"\"\n",
    "    Base class that provides common traits for machine learning tasks,\n",
    "    including data splitting methods.\n",
    "    \"\"\"\n",
    "    \n",
    "    __attr__ = \"MML.MLBase\"\n",
    "    \n",
    "    def _random_state_next(self, attr: str = \"random_state\") -> int | None:\n",
    "        \"\"\"\n",
    "        Advances the random state for a given attribute and returns it.\n",
    "        If assigned as None, then return None without doing anything.\n",
    "        \n",
    "        Args:\n",
    "            attr (str): The name of the attribute to retrieve and advance. Default is 'random_state'.\n",
    "        \n",
    "        Returns:\n",
    "            int | None: The next value of the random state or None if no such state exists.\n",
    "        \n",
    "        Raises:\n",
    "            AttributeError: If the specified attribute does not exist in the object.\n",
    "        \n",
    "        \"\"\"\n",
    "        # Retrieve the random state atrribute\n",
    "        if getattr(self, attr) is None:\n",
    "            return None  # Nonetype cannot be advanced\n",
    "        else:\n",
    "            random_state = getattr(self, attr)\n",
    "        \n",
    "        # If existing random_state_count, retrieve the count, else create it\n",
    "        try:\n",
    "            if getattr(self, attr + \"_count\") is None:\n",
    "                setattr(self, attr + \"_count\", 0)\n",
    "        except AttributeError as e:\n",
    "            setattr(self, attr + \"_count\", 0)\n",
    "        \n",
    "        # If existing random_state_offset, retrieve the offset, else create it\n",
    "        try:\n",
    "            if getattr(self, attr + \"_offset\") is None:\n",
    "                setattr(self, attr + \"_offset\", 57119)\n",
    "        except AttributeError as e:\n",
    "            setattr(self, attr + \"_offset\", 57119)\n",
    "        random_state_offset = getattr(self, attr + \"_offset\")\n",
    "        \n",
    "        # Next the random state and return it\n",
    "        random_state += random_state_offset\n",
    "        setattr(self, attr + \"_count\", getattr(self, attr + \"_count\") + 1)\n",
    "        \n",
    "        return random_state\n",
    "    \n",
    "    @staticmethod\n",
    "    def train_test_split(X: Matrix | Tensor, y: Matrix | Tensor, test_size=0.2, random_state=None):\n",
    "        \"\"\"\n",
    "        Splits the input data into training and testing sets.\n",
    "        \n",
    "        Args:\n",
    "            X (Matrix | Tensor): The feature matrix.\n",
    "            y (Matrix | Tensor): The target vector.\n",
    "            test_size (float): Proportion of samples to include in the test split.\n",
    "            random_state (int or None): Seed for reproducible random number generation. Default is None.\n",
    "        \n",
    "        Returns:\n",
    "            tuple[Matrix | Tensor, Matrix | Tensor]: A tuple containing four elements: \n",
    "                - X_train: Training feature matrix.\n",
    "                - X_test: Testing feature matrix.\n",
    "                - y_train: Training target vector.\n",
    "                - y_test: Testing target vector.\n",
    "        \n",
    "        Raises:\n",
    "            TypeError: If 'X' and 'y' are not of the same type (Matrix or Tensor).\n",
    "        \n",
    "        \"\"\"\n",
    "        if X.__attr__ != y.__attr__:\n",
    "            raise TypeError(\"Input 'X' and 'y' should have the same type Matrix or Tensor!\")\n",
    "        if random_state is not None:\n",
    "            np.random.seed(random_state)\n",
    "        n_samples = X.shape[0]\n",
    "        indices = np.random.permutation(n_samples)\n",
    "        test_count = int(round(n_samples * test_size))\n",
    "        train_idx = indices[test_count:]\n",
    "        test_idx = indices[:test_count]\n",
    "        return X[train_idx], X[test_idx], y[train_idx], y[test_idx]\n",
    "\n",
    "    @staticmethod\n",
    "    def train_test_split_for_timeseries(X: Matrix | Tensor, y: Matrix | Tensor, test_size=0.2):\n",
    "        \"\"\"\n",
    "        Splits time series data into training and testing sets.\n",
    "        \n",
    "        Args:\n",
    "            X (Matrix | Tensor): The feature matrix of the time series.\n",
    "            y (Matrix | Tensor): The target vector or dependent variable of the time series.\n",
    "            test_size (float): Proportion of the dataset to include in the test split. Default is 0.2.\n",
    "        \n",
    "        Returns:\n",
    "            tuple[Matrix, Matrix]: A tuple containing two matrices: \n",
    "                                   - X_train: Training feature matrix\n",
    "                                   - X_test: Testing feature matrix\n",
    "                                   - y_train: Training target vector\n",
    "                                   - y_test: Testing target vector\n",
    "        \n",
    "        Raises:\n",
    "            TypeError: If 'X' and 'y' are not of the same type (Matrix or Tensor).\n",
    "        \n",
    "        \"\"\"\n",
    "        if X.__attr__ != y.__attr__:\n",
    "            raise TypeError(\"Input 'X' and 'y' should have the same type Matrix or Tensor!\")\n",
    "        n_samples = X.shape[0]\n",
    "        test_count = int(round(n_samples * test_size))\n",
    "        # For time series the split is sequential: training data comes first.\n",
    "        train_idx = slice(0, n_samples - test_count)\n",
    "        test_idx = slice(n_samples - test_count, n_samples)\n",
    "        return X[train_idx], X[test_idx], y[train_idx], y[test_idx]\n",
    "\n",
    "    @staticmethod\n",
    "    def train_test_split_binarydata_siid(X: Matrix | Tensor, y: Matrix | Tensor, test_size=0.2, random_state=None):\n",
    "        \"\"\"\n",
    "        Splits the input data into training and testing sets ensuring that the percentage of \n",
    "        positives and negatives in the target vector y are similar in both sets, as if they\n",
    "        are similar to iid distributed in the train and the test set.\n",
    "        \n",
    "        Args:\n",
    "            X (Matrix | Tensor): The feature matrix.\n",
    "            y (Matrix | Tensor): The binary target vector.\n",
    "            test_size (float): Proportion of samples to include in the test split.\n",
    "            random_state (int or None): Seed for reproducible random number generation. Default is None.\n",
    "            \n",
    "        Returns:\n",
    "            tuple: A tuple containing four elements:\n",
    "                - X_train: Training feature matrix.\n",
    "                - X_test: Testing feature matrix.\n",
    "                - y_train: Training target vector.\n",
    "                - y_test: Testing target vector.\n",
    "        \n",
    "        Raises:\n",
    "            TypeError: If 'X' and 'y' are not of the same type (Matrix or Tensor).\n",
    "            ValueError: If y does not contain binary labels (0 and 1).\n",
    "        \"\"\"\n",
    "        # Ensure both X and y are of the same type.\n",
    "        if X.__attr__ != y.__attr__:\n",
    "            raise TypeError(\"Input 'X' and 'y' should have the same type Matrix or Tensor!\")\n",
    "        \n",
    "        if random_state is not None:\n",
    "            np.random.seed(random_state)\n",
    "        \n",
    "        # Verify y is binary.\n",
    "        unique_labels = y.unique().to(\"numpy\")\n",
    "        if len(unique_labels.data) != 2:\n",
    "            raise ValueError(f\"Target vector y must be binary (contain 2 kinds of labels) while it contains {len(unique_labels)} kinds.\")\n",
    "        \n",
    "        # Get indices for each class.\n",
    "        idx0 = np.where(y.flatten().to(\"numpy\").data == unique_labels.data[0])[0]\n",
    "        idx1 = np.where(y.flatten().to(\"numpy\").data == unique_labels.data[1])[0]\n",
    "        \n",
    "        # Shuffle indices for each class.\n",
    "        idx0 = np.random.permutation(idx0)\n",
    "        idx1 = np.random.permutation(idx1)\n",
    "        \n",
    "        # Determine the number of test samples per class.\n",
    "        n_test_0 = int(round(len(idx0) * test_size))\n",
    "        n_test_1 = int(round(len(idx1) * test_size))\n",
    "        \n",
    "        # Split indices for each class.\n",
    "        test_idx = np.concatenate((idx0[:n_test_0], idx1[:n_test_1]))\n",
    "        train_idx = np.concatenate((idx0[n_test_0:], idx1[n_test_1:]))\n",
    "        \n",
    "        # Shuffle the final indices.\n",
    "        train_idx = np.random.permutation(train_idx)\n",
    "        test_idx = np.random.permutation(test_idx)\n",
    "        \n",
    "        return X[train_idx], X[test_idx], y[train_idx], y[test_idx]\n",
    "\n",
    "    @staticmethod\n",
    "    def k_fold(X: Matrix | Tensor, y: Matrix | Tensor, n_splits=5, random_state=None) -> List:\n",
    "        \"\"\"\n",
    "        Splits the data into `n_splits` folds for cross-validation.\n",
    "        \n",
    "        Args:\n",
    "            X (Matrix | Tensor): The feature matrix.\n",
    "            y (Matrix | Tensor): The target vector.\n",
    "            n_splits (int): Number of splits to make. Default is 5.\n",
    "            random_state (Optional[int]): Seed value for reproducible randomness. Default is None.\n",
    "        \n",
    "        Returns:\n",
    "            List: A list where each element contains a tuple with the training and test indices for `X` and `y`.\n",
    "                  List[ (X[train_idx], X[test_idx], y[train_idx], y[test_idx]) ]\n",
    "        \n",
    "        Raises:\n",
    "            TypeError: If 'X' and 'y' are not of the same type, either Matrix or Tensor.\n",
    "        \n",
    "        \"\"\"        \n",
    "        if X.__attr__ != y.__attr__:\n",
    "            raise TypeError(\"Input 'X' and 'y' should have the same type Matrix or Tensor!\")\n",
    "        if random_state is not None:\n",
    "            np.random.seed(random_state)\n",
    "        n_samples = X.shape[0]\n",
    "        indices = np.random.permutation(n_samples)\n",
    "        fold_size = n_samples // n_splits\n",
    "        folds = []\n",
    "        for i in range(n_splits):\n",
    "            start = i * fold_size\n",
    "            # Make sure the last fold takes all remaining samples\n",
    "            end = (i + 1) * fold_size if i < n_splits - 1 else n_samples\n",
    "            test_idx = indices[start:end]\n",
    "            train_idx = np.concatenate((indices[:start], indices[end:]))\n",
    "            folds.append((X[train_idx], X[test_idx], y[train_idx], y[test_idx]))\n",
    "        return folds\n",
    "\n",
    "    @staticmethod\n",
    "    def make_rolling_window(X: Matrix | Tensor, y: Matrix | Tensor, window_size=10) -> List[Matrix | Tensor]:\n",
    "        \"\"\"\n",
    "        Make the data into rolling window 3D data and make the 1st dimension as saples.\n",
    "        \n",
    "        Args:\n",
    "            X (Matrix | Tensor): The feature matrix.\n",
    "            y (Matrix | Tensor): The target vector.\n",
    "            window_size (int): The number of historical window size (axis = 0)\n",
    "\n",
    "        Returns:\n",
    "            List: A list where each element is for processed `X` and `y`.\n",
    "                  List[X, y], where y is using the LAST row of target in the sliced piece.\n",
    "        \n",
    "        Raises:\n",
    "            TypeError: If 'X' and 'y' are not of the same type, either Matrix or Tensor.\n",
    "        \n",
    "        \"\"\"   \n",
    "        if X.__attr__ != y.__attr__:\n",
    "            raise TypeError(\"Input 'X' and 'y' should have the same type Matrix or Tensor!\")\n",
    "            \n",
    "        T, D = X.shape\n",
    "        if window_size > T:\n",
    "            raise ValueError(\"Window size cannot exceed number of 1st dimensions\")\n",
    "        \n",
    "        # build each window by slicing and then stack into a 3D tensor\n",
    "        X_list = [X[i : i + window_size] for i in range(T - window_size + 1)]\n",
    "        # shape: (T - window_size + 1, window_size, D)\n",
    "        X_new = X_list[0].stack(*X_list[1:], axis = 0)\n",
    "        \n",
    "        # Create renewed y\n",
    "        y_new = y[(window_size-1):]\n",
    "        \n",
    "        return X_new, y_new\n",
    "                \n",
    "    @staticmethod\n",
    "    def save(instance, filepath:str):\n",
    "        \"\"\"\n",
    "        Save the model object into a file to your disk.\n",
    "        \n",
    "        Args:\n",
    "            instance: a MLBase derived object\n",
    "            filepath: str, the destination file path to save.\n",
    "        \"\"\"\n",
    "        save({\"__attr__\" : instance.__attr__, \"data\": instance}, filepath, kompress=lzma, protocol=5)\n",
    "        \n",
    "    def load(self, filepath:str):\n",
    "        \"\"\"\n",
    "        Load the model object from a file from your disk.\n",
    "        Return the loaded model instead of evaluating to self.\n",
    "        \n",
    "        Args:\n",
    "            filepath: str, the destination file path to load.\n",
    "        \"\"\"\n",
    "        rawobj = load(filepath, kompress=lzma)\n",
    "        if isinstance(rawobj, dict) == False:\n",
    "            raise ValueError(f\"The file input is NOT a valid {self.__attr__} model.\")\n",
    "        if rawobj.get(\"__attr__\", \"\") != self.__attr__:\n",
    "            raise ValueError(f\"The file input is NOT a valid {self.__attr__} model.\")\n",
    "        return rawobj[\"data\"]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"MLBase(Machine Learning Abstract Base Class).\"\n",
    "\n",
    "\n",
    "# Base Class for Regression Models\n",
    "class Regression(MLBase):\n",
    "    \"\"\"\n",
    "    Base regression model that provides common traits for regression tasks.\n",
    "    \"\"\"\n",
    "    \n",
    "    __attr__ = \"MML.Regression\"\n",
    "    \n",
    "    def fit(self, X: Matrix | Tensor, y: Matrix | Tensor):\n",
    "        \"\"\"\n",
    "        Fits a regression model to the given data.\n",
    "        \n",
    "        Args:\n",
    "            X (Matrix | Tensor): The feature matrix.\n",
    "            y (Matrix | Tensor): The target vector.\n",
    "        \n",
    "        Raises:\n",
    "            TypeError: If 'X' and 'y' are not of the same type, either Matrix or Tensor.\n",
    "            NotImplementedError: If the specific regression model does not implement a fit method.\n",
    "        \n",
    "        \"\"\"\n",
    "        if X.__attr__ != y.__attr__:\n",
    "            raise TypeError(\"Input 'X' and 'y' should have the same type Matrix or Tensor!\")\n",
    "        raise NotImplementedError(\"Regression model must implement fit method.\")\n",
    "\n",
    "    def predict(self, X: Matrix | Tensor):\n",
    "        \"\"\"\n",
    "        Predicts target values for the given feature matrix `X`.\n",
    "        \n",
    "        Args:\n",
    "            X (Matrix | Tensor): The feature matrix.\n",
    "        \n",
    "        Raises:\n",
    "            NotImplementedError: If the specific regression model does not implement a predict method.\n",
    "        \n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Regression model must implement predict method.\")\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"Regression(Regression Abstract Base Class).\"\n",
    "\n",
    "\n",
    "# Base Class for Classification Models\n",
    "class Classification(MLBase):\n",
    "    \"\"\"\n",
    "    Base classification model that provides common traits for classification tasks.\n",
    "    \"\"\"\n",
    "    \n",
    "    __attr__ = \"MML.Classification\"\n",
    "    \n",
    "    def fit(self, X: Matrix | Tensor, y: Matrix | Tensor):\n",
    "        \"\"\"\n",
    "        Fits a classification model to the provided feature matrix `X` and target vector `y`.\n",
    "        \n",
    "        Args:\n",
    "            X (Matrix | Tensor): The feature matrix.\n",
    "            y (Matrix | Tensor): The target vector.\n",
    "        \n",
    "        Raises:\n",
    "            TypeError: If 'X' and 'y' are not of the same type, either Matrix or Tensor.\n",
    "            NotImplementedError: If a derived class has not implemented the `fit` method for classification models.\n",
    "        \n",
    "        \"\"\"\n",
    "        if X.__attr__ != y.__attr__:\n",
    "            raise TypeError(\"Input 'X' and 'y' should have the same type Matrix or Tensor!\")\n",
    "        raise NotImplementedError(\"Classification model must implement fit method.\")\n",
    "\n",
    "    def predict(self, X: Matrix | Tensor):\n",
    "        \"\"\"\n",
    "        Predicts the target values for a given set of features.\n",
    "        \n",
    "        Args:\n",
    "            X (Matrix | Tensor): The feature matrix or tensor to make predictions on.\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: A matrix containing the predicted target values.\n",
    "        \n",
    "        Raises:\n",
    "            NotImplementedError: This method should be implemented by subclasses as it is abstract in the current model class.\n",
    "        \n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Classification model must implement predict method.\")\n",
    "\n",
    "    @staticmethod\n",
    "    def _to_binary_prob(x: Tensor | Matrix) -> Tensor | Matrix:\n",
    "        \"\"\"\n",
    "        Converts one-hot predictions or targets into binary probability.\n",
    "        \n",
    "        If x has more than one column (two, must be), it returns\n",
    "        probabilities of entries to be 1.\n",
    "             \n",
    "        Args:\n",
    "            x: Matrix | Tensor: The one-hot or probability matrix.\n",
    "\n",
    "        Returns:\n",
    "            Matrix | Tensor: The converted Tensor or Matrix in (n_samples, 1) shape.\n",
    "        \"\"\"\n",
    "        # Wide-table: prob or one-hot\n",
    "        if len(x.shape) > 1 and x.shape[1] == 2:\n",
    "            # Always keep the dim.\n",
    "            return x[:,1].reshape([-1, 1])\n",
    "        # Already only one column\n",
    "        elif len(x.shape) > 1 and x.shape[1] == 1:\n",
    "            return x\n",
    "        # Unknown cases\n",
    "        else:\n",
    "            raise ValueError(\"When converting to binary_probability from one-hot probabilities, the input dimension must be (n_samples, 2) or aleady been (n_samples, 1)\")\n",
    "\n",
    "    @staticmethod\n",
    "    def _to_labels(x: Tensor | Matrix, *, apply_softmax:bool = False) -> Tensor | Matrix:\n",
    "        \"\"\"\n",
    "        Converts predictions or targets into label vectors.\n",
    "        \n",
    "        If x has more than one column (i.e. one-hot or probability matrix), it returns\n",
    "        the index of the maximum value along axis 1. Otherwise, x is assumed already to be a vector.\n",
    "             \n",
    "        Args:\n",
    "            x: Matrix | Tensor: The one-hot or probability matrix.\n",
    "            apply_softmax: bool, whether to apply softmax before calculating argmax or not.\n",
    "    \n",
    "        Returns:\n",
    "            Matrix | Tensor: The converted Tensor or Matrix in (n_samples, 1) shape.\n",
    "        \"\"\"\n",
    "        # Wide-table: prob or one-hot\n",
    "        if len(x.shape) > 1 and x.shape[1] > 1:\n",
    "            # Always keep the dim.\n",
    "            return x.argmax(axis=1).reshape([-1, 1]) if apply_softmax == False else x.softmax(axis=1).argmax(axis=1).reshape([-1, 1])\n",
    "        # Narrow table\n",
    "        else:    \n",
    "            return x\n",
    "\n",
    "    @staticmethod\n",
    "    def _to_onehot(x: Tensor | Matrix, n_classes: int, *, binarize = False) -> Tensor | Matrix:\n",
    "        \"\"\"\n",
    "        Converts a label vector into a one-hot encoded matrix of shape [n_samples, n_classes].\n",
    "        If x is already a matrix with the correct number of columns, it is returned unaltered.\n",
    "        If x is binary probability input and binarize is False, then will return the probablistic one-hot.\n",
    "                     \n",
    "        Args:\n",
    "            x: Matrix | Tensor: The label-encoded matrix.\n",
    "    \n",
    "        Returns:\n",
    "            Matrix | Tensor: The converted one-hot Tensor or Matrix in (n_samples, n_classes) shape.\n",
    "        \"\"\"\n",
    "        if len(x.shape) == 2 and x.shape[1] == n_classes:\n",
    "            return x\n",
    "        \n",
    "        # If binary case, then create a probabilistic one-hot to reduce information loss\n",
    "        if n_classes == 2 and binarize == False:\n",
    "            onehot_data = type(x).zeros([x.shape[0], 2], backend=x._backend)\n",
    "            onehot_data[:, 1] = x.flatten()\n",
    "            onehot_data[:, 0] = 1.0 - onehot_data[:, 1]\n",
    "            return onehot_data.to(backend=x._backend, device=x.device, dtype=x.dtype)\n",
    "        \n",
    "        # Else, do the round\n",
    "        else:\n",
    "            # Create one-hot by comparing each element with a range vector.\n",
    "            range_vec = type(x)(np.arange(n_classes), backend=x._backend, device=x.device)\n",
    "            # Reshape x to [n_samples, 1] if necessary\n",
    "            x_reshaped = x.reshape([x.shape[0], 1])\n",
    "            \n",
    "            # Broadcast the comparison: each entry becomes True if equal to the class index.\n",
    "            onehot_data = x_reshaped.astype(float).round() == range_vec\n",
    "            # The above one produces a boolean array -> like True, False, True, ...\n",
    "            #                                                False, True, False, ...\n",
    "            return onehot_data.to(backend=x._backend, device=x.device, dtype=float)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"Classification(Regression Abstract Base Class).\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`P. Data Scaler (self-implemented)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This scaling class is self implemented and open-sourced\n",
    "# Available at https://github.com/dof-studio/MML/\n",
    "# By Nathmath Huang (bh2821)\n",
    "# License: Apache License Version 2.0\n",
    "\n",
    "# Implementation of Data Scaler\n",
    "class Scaling:\n",
    "    \"\"\"\n",
    "    Scale class that fits on a Matrix and can perform either centralization (subtracting the mean)\n",
    "    or min-max scaling (scaling features to the [0, 1] range), with the ability to reverse the operation.\n",
    "    \"\"\"\n",
    "    \n",
    "    __attr__ = \"MML.Scaling\"\n",
    "    \n",
    "    def __init__(self, method=\"centralize\", *, robust_p = 0.25):\n",
    "        '''\n",
    "        Args:\n",
    "            `method` can be:\n",
    "                \"centralize\": only subtract the mean\n",
    "                \"normalize\": subtract the mean and standardize the variance to 1\n",
    "                \"minmax\": keep the data with in the range of [0, 1]\n",
    "                \"robust\": compute median and interquartile range to reduce the effect of outliers.\n",
    "            `robust_p` the lower percentile [0,1] of the percentile estimate. 0.25 means 25% and 75%\n",
    "        '''\n",
    "        self.method = method\n",
    "        self.params = {}\n",
    "        \n",
    "        # Method specific parameters\n",
    "        self.robust_p = robust_p if robust_p < 0.5 else 1 - robust_p\n",
    "\n",
    "    def fit(self, X: Matrix | Tensor, axis = 0):\n",
    "        \"\"\"\n",
    "        Fits the scaling parameters to the data.\n",
    "    \n",
    "        Args:\n",
    "            X (Matrix | Tensor): The input matrix or tensor for fitting.\n",
    "            axis (int): Axis along which to compute the mean and standard deviation. Default is 0.\n",
    "    \n",
    "        Returns:\n",
    "            self: The fitted instance of the class, allowing method chaining.\n",
    "    \n",
    "        Raises:\n",
    "            ValueError: If an unsupported scaling method is provided.\n",
    "    \n",
    "        \"\"\"\n",
    "        type_X = type(X)\n",
    "        if self.method == \"centralize\":\n",
    "            # Just demean the data to 0 mean\n",
    "            if X._is_numpy:\n",
    "                mean_val = np.mean(X.data, axis=axis)\n",
    "            else:\n",
    "                mean_val = torch.mean(X.data, dim=axis)\n",
    "            self.params['mean'] = type_X(mean_val, backend=X._backend, device=X.device, dtype=X.dtype)\n",
    "        \n",
    "        elif self.method == \"normalize\":\n",
    "            # Normalize the data with 0 mean and std of 1\n",
    "            if X._is_numpy:\n",
    "                mean_val = np.mean(X.data, axis=axis)\n",
    "                stdev_val = np.std(X.data, axis=axis)\n",
    "            else:\n",
    "                mean_val = torch.mean(X.data, dim=axis)\n",
    "                stdev_val = torch.std(X.data, dim=axis)\n",
    "            self.params['mean'] = type_X(mean_val, backend=X._backend, device=X.device, dtype=X.dtype)\n",
    "            self.params['std'] = type_X(stdev_val, backend=X._backend, device=X.device, dtype=X.dtype)\n",
    "        \n",
    "        elif self.method == \"minmax\":\n",
    "            # Minmax to make data in a range of [0,1]\n",
    "            if X._is_numpy:\n",
    "                min_val = np.min(X.data, axis=axis)\n",
    "                max_val = np.max(X.data, axis=axis)\n",
    "            else:\n",
    "                min_val = torch.min(X.data, dim=axis).values\n",
    "                max_val = torch.max(X.data, dim=axis).values\n",
    "            self.params['min'] = type_X(min_val, backend=X._backend, device=X.device, dtype=X.dtype)\n",
    "            self.params['max'] = type_X(max_val, backend=X._backend, device=X.device, dtype=X.dtype)\n",
    "        \n",
    "        elif self.method == \"robust\":\n",
    "            # Compute median and interquartile range to reduce the effect of outliers.\n",
    "            if X._is_numpy:\n",
    "                median_val = np.median(X.data, axis=axis)\n",
    "                q1 = np.percentile(X.data, int(self.robust_p * 100), axis=axis)\n",
    "                q3 = np.percentile(X.data, 100 - int(self.robust_p * 100), axis=axis)\n",
    "                iqr_val = q3 - q1\n",
    "            else:\n",
    "                median_val = torch.median(X.data, dim=axis).values\n",
    "                q1 = torch.quantile(X.data, self.robust_p, dim=axis)\n",
    "                q3 = torch.quantile(X.data, 1 - self.robust_p, dim=axis)\n",
    "                iqr_val = q3 - q1\n",
    "            self.params['p'] = self.robust_p\n",
    "            self.params['median'] = type_X(median_val, backend=X._backend, device=X.device, dtype=X.dtype)\n",
    "            self.params['iqr'] = type_X(iqr_val, backend=X._backend, device=X.device, dtype=X.dtype)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported scaling method. Choose 'centralize', 'normalize', 'minmax', or 'robust'.\")\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: Matrix | Tensor):\n",
    "        \"\"\"\n",
    "        Transforms the input matrix using the fitted parameters.\n",
    "        \n",
    "        Args:\n",
    "            X (Matrix | Tensor): The input matrix for transformation.\n",
    "        \n",
    "        Returns:\n",
    "            Matrix | Tensor: The transformed matrix or tensor.\n",
    "        \n",
    "        Raises:\n",
    "            InterruptedError: If no scaling parameters have been fitted yet.\n",
    "            ValueError: If an unsupported scaling method is provided.\n",
    "        \n",
    "        \"\"\"\n",
    "        if len(self.params) == 0:\n",
    "            raise InterruptedError(\"You should call `fit` before doing any transformation\")\n",
    "        if self.method == \"centralize\":\n",
    "            return (X - self.params['mean'])\n",
    "        elif self.method == \"normalize\":\n",
    "            return (X - self.params['mean']) / self.params['std']\n",
    "        elif self.method == \"minmax\":\n",
    "            range_matrix = self.params['max'] - self.params['min']\n",
    "            return (X - self.params['min']) / range_matrix\n",
    "        elif self.method == \"robust\":\n",
    "            return (X - self.params['median']) / self.params['iqr']\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported scaling method. Choose 'centralize', 'normalize', 'minmax', or 'robust'.\")\n",
    "\n",
    "    def inverse_transform(self, X: Matrix | Tensor):\n",
    "        \"\"\"\n",
    "        Inverses the transformation applied during fitting.\n",
    "        \n",
    "        Args:\n",
    "            X (Matrix | Tensor): The transformed matrix for inversion.\n",
    "        \n",
    "        Returns:\n",
    "            Matrix | Tensor: The original matrix or tensor before scaling.\n",
    "        \n",
    "        Raises:\n",
    "            InterruptedError: If no scaling parameters have been fitted yet.\n",
    "            ValueError: If an unsupported scaling method is provided. \n",
    "        \n",
    "        \"\"\"\n",
    "        if len(self.params) == 0:\n",
    "            raise InterruptedError(\"You should call `fit` before doing any transformation\")\n",
    "        if self.method == \"centralize\":\n",
    "            # Inverse centralization: add the mean back.\n",
    "            return X + self.params['mean']\n",
    "        if self.method == \"normalize\":\n",
    "            # Inverse centralization: multiply the std and add the mean back.\n",
    "            return X * self.params['std'] + self.params['mean']\n",
    "        elif self.method == \"minmax\":\n",
    "            # Inverse minmax scaling: X*(max - min) + min\n",
    "            range_matrix = self.params['max'] - self.params['min']\n",
    "            return X * range_matrix + self.params['min']\n",
    "        elif self.method == \"robust\":\n",
    "            # Inverse robust scaling: X*(iqr) + median\n",
    "            return X * self.params['iqr'] + self.params['median']\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported scaling method. Choose 'centralize', 'normalize', 'minmax', or 'robust'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`P. Threadpool and Mutex Wrapper (self-implemented, for future purpose, what if one day GIL is deprecated)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These threading interface is self implemented and open-sourced\n",
    "# Available at https://github.com/dof-studio/MML/\n",
    "# By Nathmath Huang (bh2821)\n",
    "# License: Apache License Version 2.0\n",
    "\n",
    "import concurrent.futures\n",
    "import uuid\n",
    "import threading\n",
    "from typing import Any\n",
    "\n",
    "# A threadpool worker class\n",
    "class ThreadPool:\n",
    "    \"\"\"\n",
    "    A simple thread pool for executing functions in separate threads.\n",
    "    Each submitted task returns a unique id, and you can wait until a task finishes or stop all tasks.\n",
    "    \"\"\"\n",
    "    def __init__(self, max_workers=4):\n",
    "        \"\"\"\n",
    "        Initialize the thread pool.\n",
    "        \n",
    "        Parameters:\n",
    "            max_workers (int): Maximum number of worker threads (default: system default).\n",
    "        \"\"\"\n",
    "        self.executor = concurrent.futures.ThreadPoolExecutor(max_workers=max_workers)\n",
    "        self.tasks = {}  # Mapping from task id to Future\n",
    "        self.lock = threading.Lock()\n",
    "\n",
    "    # Execute something with an assigned task number returned\n",
    "    def execute(self, func, *args, **kwargs) -> Any:\n",
    "        \"\"\"\n",
    "        Submit a function to be executed in a separate thread.\n",
    "        \n",
    "        Parameters:\n",
    "            func (callable): The function to execute.\n",
    "            *args: Positional arguments for the function.\n",
    "            **kwargs: Keyword arguments for the function.\n",
    "            \n",
    "        Returns:\n",
    "            str: A unique task id representing the submitted task.\n",
    "        \"\"\"\n",
    "        task_id = str(uuid.uuid4())\n",
    "        future = self.executor.submit(func, *args, **kwargs)\n",
    "        with self.lock:\n",
    "            self.tasks[task_id] = future\n",
    "        return task_id\n",
    "\n",
    "    # Coresively stop all tasks\n",
    "    def stopall(self):\n",
    "        \"\"\"\n",
    "        Attempt to cancel all tasks that haven't started.\n",
    "        Note that tasks already running may not be cancelled.\n",
    "        Clears the internal task registry.\n",
    "        \"\"\"\n",
    "        with self.lock:\n",
    "            for task_id, future in list(self.tasks.items()):\n",
    "                future.cancel()\n",
    "            self.tasks.clear()\n",
    "            \n",
    "    # Wait for a certain task\n",
    "    def waituntil(self, task_id: Any):\n",
    "        \"\"\"\n",
    "        Block until the task corresponding to the given id has finished.\n",
    "        \n",
    "        Parameters:\n",
    "            task_id (str): The unique id of the task.\n",
    "        \n",
    "        Returns:\n",
    "            The result of the task, if it completed successfully.\n",
    "        \n",
    "        Raises:\n",
    "            ValueError: If the task id is not found.\n",
    "        \"\"\"\n",
    "        with self.lock:\n",
    "            future = self.tasks.get(task_id)\n",
    "        if future is None:\n",
    "            raise ValueError(f\"Task with id {task_id} not found.\")\n",
    "        return future.result()  # Blocks until the task completes\n",
    "    \n",
    "    # Normally shut down\n",
    "    def shutdown(self, wait=True):\n",
    "        \"\"\"\n",
    "        Shutdown the thread pool.\n",
    "        \n",
    "        Parameters:\n",
    "            wait (bool): If True, block until all running tasks are finished.\n",
    "        \"\"\"\n",
    "        self.executor.shutdown(wait=wait)\n",
    "\n",
    "\n",
    "# A Pythonic/STL mutex comptible wrapper\n",
    "class Mutex:\n",
    "    \"\"\"\n",
    "    A thin wrapper around :class:`threading.Lock` that mimics the interface\n",
    "    of C++ `std::mutex` while feeling Pythonic.\n",
    "\n",
    "    It supports the three canonical methods—``lock``, ``try_lock``, and\n",
    "    ``unlock``—plus context‑manager helpers so you can use the ``with``‑statement\n",
    "    for automatic acquisition / release.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> m = Mutex()\n",
    "    >>> m.lock()          # block until the mutex is free\n",
    "    >>> m.unlock()        # release it again\n",
    "    >>> m.try_lock()      # returns True or False\n",
    "    >>> with m:           # RAII style\n",
    "    ...     critical()\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"\n",
    "        Create an unlocked mutex.\n",
    "        \"\"\"\n",
    "        self._lock: threading.Lock = threading.Lock()\n",
    "\n",
    "    # C++ std::mutex::lock()\n",
    "    def lock(self) -> None:\n",
    "        \"\"\"\n",
    "        Block the calling thread until the mutex is acquired.\n",
    "        \"\"\"\n",
    "        self._lock.acquire()\n",
    "\n",
    "    # C++ std::mutex::try_lock()\n",
    "    def try_lock(self) -> bool:\n",
    "        \"\"\"\n",
    "        Attempt to acquire the mutex without blocking.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        bool\n",
    "            ``True`` if the lock was acquired, ``False`` otherwise.\n",
    "        \"\"\"\n",
    "        return self._lock.acquire(blocking=False)\n",
    "\n",
    "    # C++ std::mutex::unlock()\n",
    "    def unlock(self) -> None:\n",
    "        \"\"\"\n",
    "        Release the mutex.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        Only the thread that currently owns the lock may call this.\n",
    "        \"\"\"\n",
    "        self._lock.release()\n",
    "\n",
    "    def __enter__(self) -> \"Mutex\":\n",
    "        \"\"\"\n",
    "        Enter a ``with``‑block by locking the mutex.\n",
    "        \"\"\"\n",
    "        self.lock()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type: Any, exc: Any, tb: Any) -> None:\n",
    "        \"\"\"\n",
    "        Exit a ``with``‑block by unlocking the mutex—even if an exception\n",
    "        was raised inside the block.\n",
    "        \"\"\"\n",
    "        self.unlock()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`P. Save and Load Interface for Saving a Model (self-implemented, using pickle and lzma)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These save/load interface is self implemented and open-sourced\n",
    "# Available at https://github.com/dof-studio/MML/\n",
    "# By Nathmath Huang (bh2821)\n",
    "# License: Apache License Version 2.0\n",
    "\n",
    "import pickle\n",
    "from typing import Any\n",
    "\n",
    "# Kompress can be:\n",
    "#  gzip\n",
    "#  lzma\n",
    "\n",
    "def save(obj: Any, filename: str, *, kompress: Any = None, protocol: int | None = None, **kwargs):\n",
    "    \"\"\"\n",
    "    Save a Python object to a file using pickle.\n",
    "    Directly save without wrapping.\n",
    "    \"\"\"\n",
    "    # Uncompress\n",
    "    if kompress is None:\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(obj, f, protocol = protocol)\n",
    "    # Compress\n",
    "    else:\n",
    "        with kompress.open(filename, 'wb', **kwargs) as f:\n",
    "            pickle.dump(obj, f, protocol = protocol)\n",
    "\n",
    "def load(filename: str, *, kompress: Any = None) -> Any:\n",
    "    \"\"\"\n",
    "    Load a Python object from a pickle file.\n",
    "    Generally loading. Try to unwrap if possible\n",
    "    \n",
    "    Exception:\n",
    "        Throw a ValueError when in the dumping mode and failed to\n",
    "        pass the hash test.\n",
    "    \"\"\"\n",
    "    # Uncompress\n",
    "    if kompress is None:\n",
    "        with open(filename, 'rb') as f:\n",
    "            obj = pickle.load(f)\n",
    "    else:\n",
    "        with kompress.open(filename, 'rb') as f:\n",
    "            obj = pickle.load(f)\n",
    "    \n",
    "    # No need to unwrap\n",
    "    if isinstance(obj, dict) == False:\n",
    "        return obj\n",
    "    elif isinstance(obj, dict) == True and obj.get(\"~attr~\", None) is None:\n",
    "        return obj\n",
    "    \n",
    "    # Need to unwrap\n",
    "    if isinstance(obj, dict) == True and obj.get(\"~attr~\", None) == \"~dump~\":\n",
    "        if obj.get(\"~hash~\", None) is None:\n",
    "            raise ValueError(\"Corrupted dumpped file. Hash attribute has Nonetype.\")\n",
    "        elif isinstance(obj.get(\"~hash~\", None), str) == False:\n",
    "            raise ValueError(\"Corrupted dumpped file. Hash attribute has Non-string type.\")\n",
    "        if obj.get(\"data\", None) is None:\n",
    "            raise ValueError(\"Corrupted dumpped file. Data attribute is Nonetype.\")\n",
    "        if obj.get(\"~hash~\", None) != str(hash(obj.get(\"data\"))):\n",
    "            raise ValueError(\"Corrupted dumpped file. Data hash mismatched.\")\n",
    "        return obj[\"data\"] \n",
    "    \n",
    "    else:\n",
    "        return obj\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`P. Basic Neural Network Components (self-implemented)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These basic Neural Network Components are self implemented and open-sourced\n",
    "# Available at https://github.com/dof-studio/MML/\n",
    "# By Nathmath Huang (bh2821)\n",
    "# License: Apache License Version 2.0\n",
    "\n",
    "# Deep Neural Network Abstract Module Base Class\n",
    "class nn_Base(Regression, Classification):\n",
    "    \n",
    "    __attr__ = \"MML.nn_Base\"    \n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def zero_grad(self):\n",
    "        \"\"\"\n",
    "        Set all gradients of all parameters to zero.\n",
    "        It will clear the gradients accumulated and restore when a new batch starts.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"zero_grad() is not implemented in nn_Base\")\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Set module to training mode.\n",
    "        It will affect dropouts and enable gradients calculation.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"train() is not implemented in nn_Base\")\n",
    "\n",
    "    def eval(self):\n",
    "        \"\"\"\n",
    "        Set module to evaluation mode.\n",
    "        It will disable dropouts and disable gradients calculation.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"eval() is not implemented in nn_Base\")\n",
    "\n",
    "    def parameters(self):\n",
    "        \"\"\"\n",
    "        Return an iterator of all Parameters in this module (includes children)\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"parameters() is not implemented in nn_Base\")\n",
    "        \n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Perform a forward propagation to calculate the loss.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"forward() is not implemented in nn_Base\")\n",
    "        \n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Perform a backward propagation to compute gradients for updating weights.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"backward() is not implemented in nn_Base\")\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return \"nn_Base(Deep Neural Network Abstract Module Base Class).\"\n",
    "\n",
    "\n",
    "# A Deep Neural Network Trainable Parameter Class\n",
    "class nn_Parameter(MLBase):\n",
    "    \"\"\"\n",
    "    A trainable parameter base data structure with gradient storage.\n",
    "    Contains data and manually implemented gradients, while you can use pytorch autograd techniques.\n",
    "    Optionally, you may set `autograd` = True to enable torch Autograd functionality instead of manual grads computation.\n",
    "    \"\"\"\n",
    "    \n",
    "    __attr__ = \"MML.nn_Parameter\"    \n",
    "    \n",
    "    def __init__(self, \n",
    "                 data: Tensor, \n",
    "                 requires_grad: bool = True, \n",
    "                 *, \n",
    "                 device: str | None = None, \n",
    "                 dtype: str | None = None, \n",
    "                 autograd: bool = False, \n",
    "                 **kwargs): \n",
    "        \"\"\"\n",
    "        Create a wrapped Neural Network Parameter Container, including gradients.\n",
    "        \n",
    "        Parameters:\n",
    "            --------\n",
    "            data: Tensor, The initial value for the parameter as a Tensor object.\n",
    "            requires_grad: bool, A flag indicating whether gradients should be tracked for this parameter. Defaults to True.\n",
    "            Optional:\n",
    "                device: str | None, The device where the tensor should reside (e.g., \"cpu\", \"cuda\"). If None, uses the default device. Defaults to None.\n",
    "                dtype: str | None, The data type of the tensor (e.g., \"float32\", \"float64\", or type like torch.float32). If None, uses the data type of the input `data`. Defaults to None.\n",
    "                autograd: bool, A flag indicating whether to use PyTorch's autograd functionality for gradient computation.  If True, manual gradient tracking is disabled. Defaults to None.\n",
    "        \n",
    "        Raises:\n",
    "            --------\n",
    "            ValueError: If the input `data` is not a Tensor object.\n",
    "    \n",
    "        Attributes:\n",
    "            --------\n",
    "            self.autograd: bool, Indicates whether PyTorch's autograd is enabled.\n",
    "            self.requires_grad: bool, A flag indicating whether gradients are tracked for this parameter.\n",
    "            self.data: Tensor, The parameter data as a Tensor object, cloned and potentially moved to the specified device/dtype.\n",
    "            self.grad: Tensor | None, The manually accumulated gradient (reserved for future evaluation); set to None initially.\n",
    "        \"\"\"\n",
    "        \n",
    "        # MLBase is for save/load purposes.\n",
    "        super().__init__()\n",
    "        \n",
    "        # Record if it uses pytorch's autograd\n",
    "        self.autograd = autograd\n",
    "        \n",
    "        # Record if it uses gradients (either autograd or manually calculated)\n",
    "        self.requires_grad = requires_grad\n",
    "        \n",
    "        # Initialize the parameter with a tensor (ensure float dtype and device placement)\n",
    "        if not isinstance(data, Tensor):\n",
    "            raise ValueError(\"Input data MUST be a MML.Tensor! Please convert by calling Tensor(data, backend='torch')\")\n",
    "        # Parameter Data - a Tensor Object\n",
    "        self.data = data if dtype is None and device is None else data.to(backend=data._backend, dtype=dtype, device=device)\n",
    "        # Gradient manually accumulated during backprop\n",
    "        # If uses autograd, then self.data.grad will record it\n",
    "        self.grad = self.data.to_zeros() if requires_grad == True and autograd == False else None \n",
    "        \n",
    "        # If uses pytorch autograd, then enable if requires grad\n",
    "        if autograd == True and requires_grad == True:\n",
    "            self.data.requires_grad_(True)\n",
    "       \n",
    "    def zero_grad(self):\n",
    "        \"\"\"\n",
    "        Reset the gradient to zero.\n",
    "        \n",
    "        Returns:\n",
    "            -------\n",
    "            self\n",
    "        \"\"\"\n",
    "        if self.requires_grad == True:\n",
    "            # Use pytorch's autograd, then directly set to 0\n",
    "            if self.autograd == True:\n",
    "                self.data.data.grad.zero_()\n",
    "            # Use manually calculated grads, then manually set to 0\n",
    "            else:\n",
    "                if self.grad is not None:\n",
    "                    self.grad[...] = 0\n",
    "                else:\n",
    "                    self.grad = self.data.to_zeros()\n",
    "        return self\n",
    "\n",
    "    def requires_grad_(self, requires_grad: bool = True):\n",
    "        \"\"\"\n",
    "        Set the attributes of `requires_grid` and enable/disable autograd if used.\n",
    "        \n",
    "        Returns:\n",
    "            -------\n",
    "            self\n",
    "        \"\"\"\n",
    "        \n",
    "        # If status conflicts, then create/disable grad\n",
    "        if self.requires_grad != requires_grad:\n",
    "            \n",
    "            # Set the attribute of requiring grads or not\n",
    "            self.requires_grad = requires_grad\n",
    "            \n",
    "            # If uses pytorch autograd, then enable if requires grad\n",
    "            if self.autograd == True:\n",
    "                self.data.requires_grad_(requires_grad)\n",
    "            else:\n",
    "                if requires_grad == True:\n",
    "                    self.zero_grad()\n",
    "                else:\n",
    "                    self.grad = None\n",
    "\n",
    "    def to(self, device: str | None = None):\n",
    "        \"\"\"\n",
    "        Move the parameters and gradients to the specified device.\n",
    "        \n",
    "        Parameters:\n",
    "            --------\n",
    "            device: str | None, The device where the tensor should reside (e.g., \"cpu\", \"cuda\"). If None, do nothing.\n",
    "        \"\"\"\n",
    "        if device is not None:\n",
    "            self.data = self.data.to(backend = self.data._backend, device = device)\n",
    "            if self.grad is not None:\n",
    "                self.grad = self.grad.to(backend = self.grad._backend, device = device)\n",
    "        return self\n",
    "\n",
    "    def copy(self):\n",
    "        \"\"\"\n",
    "        Create a deepcopy of the parameters and gradiets.\n",
    "        \"\"\"\n",
    "        return deepcopy(self)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"nn_Parameter(Deep Neural Network Trainable Parameter Class).\" + \"\\nData: \" + self.data.__repr__() + \"\\nGrad: \" + (self.grad.__repr__() if self.grad is not None else \"\")\n",
    "\n",
    "\n",
    "# A Deep Neural Network Interface Module Base Class\n",
    "class nn_BaseModule(nn_Base):\n",
    "    \"\"\"\n",
    "    A even base class for Neural Network Modules.\n",
    "    \"\"\"\n",
    "    \n",
    "    __attr__ = \"MML.nn_BaseModule\"    \n",
    "    \n",
    "    def __setattr__(self, name: str, value: Any):\n",
    "        \"\"\"\n",
    "        Override setattr to register Parameters and Modules.\n",
    "        \n",
    "        Registers parameters (nn_Parameter) and modules (nn_Module) under the `._parameters` and `._modules` dictionaries \n",
    "        when appropriate. Delegates attribute assignment to the base class's `__setattr__` method for standard attributes.\n",
    "        \n",
    "        Args:\n",
    "            name: str, The name of the attribute being set.\n",
    "            value: Any, The value to assign to the attribute.\n",
    "            \n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        # Register a parameter if is nn_Parameter\n",
    "        if isinstance(value, nn_Parameter):\n",
    "            self._parameters[name] = value\n",
    "            \n",
    "        # Register a submodule if nn_Module\n",
    "        elif isinstance(value, nn_BaseModule):\n",
    "            self._modules[name] = value\n",
    "            \n",
    "        # Copy if it is a Object based variable\n",
    "        elif isinstance(value, Object):\n",
    "            object.__setattr__(self, name, value)\n",
    "        \n",
    "        # In all cases, set the attribute normally as an attribute\n",
    "        object.__setattr__(self, name, value)\n",
    "    \n",
    "    def __call__(self, *inputs):\n",
    "        \"\"\"\n",
    "        Delegates to the forward method to perform the forward pass of the module.\n",
    "        \n",
    "        This method allows a Module instance to be called like a function, passing inputs to the forward() method.\n",
    "        It delegates attribute assignment and functionality to the base class's `__setattr__` and `forward()` methods.\n",
    "        \n",
    "        Args:\n",
    "            *inputs: list, Variable number of input tensors or values to pass to the forward method.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: The output tensor resulting from the forward pass.\n",
    "        \n",
    "        Raises:\n",
    "            NotImplementedError: If the forward() method is not implemented in the subclass.\n",
    "        \"\"\"\n",
    "        # Allows Module instance to be called like a function to perform forward pass\n",
    "        return self.forward(*inputs)\n",
    "    \n",
    "    def __init__(self, module_name: str = \"Default_Module_Container\"):\n",
    "        \"\"\"\n",
    "        Initializes a base neural network module with basic structural components.\n",
    "\n",
    "        This constructor sets up essential properties for a module, including its name,\n",
    "        training mode flag, and containers for parameters and submodules. It follows a\n",
    "        convention similar to PyTorch modules, where modules are organized hierarchically\n",
    "        with parameter tracking and submodule management.\n",
    "\n",
    "        Parameters:\n",
    "            --------\n",
    "            module_name: str, The name of the module instance. Defaults to \"Default_Module_Container\".\n",
    "\n",
    "        Attributes:\n",
    "            self.name: The name of the module instance, set dynamically via __setattr__.\n",
    "            self.training: A flag indicating whether the module is in training mode. Defaults to True.\n",
    "            self._parameter: A dictionary container for all parameters of this module.\n",
    "            self._modules: A dictionary container for all submodule instances nested within this module.\n",
    "        \"\"\"\n",
    "        \n",
    "        # A default module at least contains:\n",
    "        # 1. self.name, str, the name of this module instance\n",
    "        # 2. self.training, bool, whether the module is in training or evaluation mode\n",
    "        # 3. self.accumulate, bool, whether the module is accumulating gradients\n",
    "        # 4. self._parameters, container, all parameters of THIS module\n",
    "        # 5. self._modules, container, all instances of SUB modules\n",
    "        \n",
    "        # Module name, indicating the name of the module, a string\n",
    "        self.__setattr__(\"name\", module_name)\n",
    "        \n",
    "        # Training flag, indicating the model is training or not\n",
    "        self.__setattr__(\"training\", False)\n",
    "        \n",
    "        # Accumulating flag, indicating the model is accumulating gradients or not\n",
    "        self.__setattr__(\"accumulate\", False)\n",
    "        # You can only set this to True by calling accumulate_grad before doing forward\n",
    "        # to accumulate gradients when training.\n",
    "        \n",
    "        # Initialize internal containers for parameters\n",
    "        self.__setattr__(\"_parameters\", {})\n",
    "        self.__setattr__(\"_modules\", {})\n",
    "\n",
    "    def forward(self, *inputs):\n",
    "        \"\"\"\n",
    "        Override this method in subclasses to define forward pass.\n",
    "        A forward pass is the way that the neural network passes the inputs\n",
    "        through parameters and generates the output.\n",
    "        \n",
    "        Raises:\n",
    "            NotImplementedError: You should implement your own forward pass. Calling the base will raise this error.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"forward() is not implemented in the base module. You should define the architecture of your neural network manually.\")\n",
    "\n",
    "    def parameters(self):\n",
    "        \"\"\"\n",
    "        Returns an iterator (or list) of all Parameters in this module, including those from child modules.\n",
    "\n",
    "        This method recursively collects all `Parameter` instances from the current module and its submodules,\n",
    "        following a pattern similar to PyTorch's `parameters()` method. It aggregates parameters from both direct\n",
    "        parameters (`self._parameters`) and nested modules (`self._modules`).\n",
    "\n",
    "        Returns:\n",
    "            list: An iterator of all `Parameter` objects in this module and its submodules.\n",
    "        \"\"\"\n",
    "        params = []\n",
    "        \n",
    "        # Own parameters\n",
    "        for param in self._parameters.values():\n",
    "            params.append(param)\n",
    "            \n",
    "        # Parameters of submodules\n",
    "        for module in self._modules.values():\n",
    "            params.extend(module.parameters())\n",
    "        return params\n",
    "\n",
    "    def zero_grad(self):\n",
    "        \"\"\"\n",
    "        Resets the gradients of all parameters in this module and its submodules to zero.\n",
    "\n",
    "        This method iterates through all `Parameter` objects in the module (including those from child modules)\n",
    "        and calls `zero_grad()` on each, effectively clearing the gradient buffers. It follows the same pattern\n",
    "        as PyTorch's `zero_grad()` method for efficiency and consistency with standard neural network training workflows.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        for param in self.parameters():\n",
    "            param.zero_grad()\n",
    "\n",
    "    def accumulate_grad(self):\n",
    "        \"\"\"\n",
    "        Set module to gradient accumulate mode. \n",
    "\n",
    "        This method sets the `accumulate` flag to True, ensures gradients are accumulated in backward passing instead of being set to 0.\n",
    "        You should first call `train` to turn the module into training model. Otherwise it will raise a RuntimeError.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "            \n",
    "        Raises:\n",
    "            RuntimeError: if calling accumulate_grad in non-training mode.\n",
    "        \"\"\"\n",
    "        # If called in non-training mode, raise RuntimeError\n",
    "        if self.training == False:\n",
    "            raise RuntimeError(f\"Calling accumulate_grad() on Module {self.name} to accumulate gradients, but without turning training mode on. Please call .train() first.\")\n",
    "        \n",
    "        # Set the status to accumulating\n",
    "        self.accumulate = True\n",
    "        \n",
    "        # For other modules, set to accumulate mode\n",
    "        for module in self._modules.values():\n",
    "            module.accumulate_grad()\n",
    "        \n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Set module to training mode (affects dropout, and enables gradients).\n",
    "\n",
    "        This method sets the `training` flag to True, ensures all direct parameters require gradients, and recursively applies \n",
    "        the training mode to all submodules. It is typically used at the beginning of a training loop to activate behaviors \n",
    "        specific to training, such as dropout or batch normalization.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        # Set the status to training\n",
    "        self.training = True\n",
    "        \n",
    "        # For this module, set all parameters to requires_grad = True\n",
    "        for param in self._parameters.values():\n",
    "            param.requires_grad_(True)\n",
    "        \n",
    "        # For other modules, set to train modes\n",
    "        for module in self._modules.values():\n",
    "            module.train()\n",
    "\n",
    "    def eval(self):\n",
    "        \"\"\"\n",
    "        Set module to evaluation mode (disable dropout and gradients).\n",
    "        \n",
    "        This method sets the `training` flag to False, which disables behaviors specific to training (e.g., dropout). \n",
    "        It also explicitly sets all direct parameters to require gradients (`requires_grad=True`) and recursively applies \n",
    "        evaluation mode to all submodules.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        # Set the status to evalutating\n",
    "        self.training = False\n",
    "        \n",
    "        # For this module, set all parameters to requires_grad = False\n",
    "        for param in self._parameters.values():\n",
    "            param.requires_grad_(False)\n",
    "        \n",
    "        # For other modules, set to evaluation mode\n",
    "        for module in self._modules.values():\n",
    "            module.eval()\n",
    "\n",
    "    def to(self, device: str | None = None):\n",
    "        \"\"\"\n",
    "        Moves all parameters and submodules to the specified device.\n",
    "\n",
    "        This method relocates the module's parameters and nested submodules to the given device (e.g., 'cpu', 'cuda'). \n",
    "        If `device` is None, it uses the default device. This is essential for moving models between devices during training or inference.\n",
    "\n",
    "        Args:\n",
    "            device (str | None): The target device (e.g., \"cpu\", \"cuda\"). If None, the default device is used.\n",
    "\n",
    "        Returns:\n",
    "            self\n",
    "        \"\"\"\n",
    "        # Set device directly\n",
    "        if device == self.device:\n",
    "            return self\n",
    "        else:\n",
    "            self.device = device\n",
    "        \n",
    "        # For this module, set every parameters to device\n",
    "        for param in self._parameters.values():\n",
    "            param.to(device)\n",
    "       \n",
    "        # For other modules, set every parameters to device\n",
    "        for module in self._modules.values():\n",
    "            module.to(device)\n",
    "        return self\n",
    "\n",
    "    def backward(self, grad_output: Tensor | None):\n",
    "        \"\"\"\n",
    "        Backpropagate through the module (feedforward).\n",
    "        \n",
    "        If you need gradients with respect to internal states like GRU or LSTM states,\n",
    "        please manually override this method to capture the gradients.\n",
    "        By default, we only capture the 1st gradient which is gradient with respect to inputs.\n",
    "\n",
    "        This method performs gradient propagation through the module's submodules in reverse order of their addition.\n",
    "        ** By default, if the module contains submodules, propagate grad through them in reverse order.\n",
    "        ** Leaf modules (layers) should override this to implement their own backward logic.\n",
    "        ** Leaf modules (layers) should also be compatible to pytorch's autograd and manual calculation.\n",
    "\n",
    "        Args:\n",
    "            grad_output (Tensor): The gradient tensor resulting from the output of the module, used as input for backpropagation..\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The propagated gradient after processing through all submodules.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If `grad_output` is not a valid MML.Tensor object.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Note. This will be override by Layers to implement the actual logic of \n",
    "        #       gradient calculation and backpropagation.\n",
    "        # For a normal non-leaf module, we assume it is a container containing NO\n",
    "        #       parameter and just sub-modules. So we invoke backward of submodules.\n",
    "        \n",
    "        # If autograd, then the true backward is performed by LOSS function\n",
    "        # Although it gets soemthing here into the module, it just for compatibility.\n",
    "        # We don't need to processs it anymore.\n",
    "        if self.autograd == True:\n",
    "            return None\n",
    "        \n",
    "        # Type check, it must be a Tensor object if non-autograd mode\n",
    "        if isinstance(grad_output, Tensor) == False:\n",
    "            raise ValueError(f\"Output gradient must be in a MML `Tensor` format but you have {type(grad_output)}\")\n",
    "            \n",
    "        # Status check, if non-training, RuntimeErrror\n",
    "        if self.training == False:\n",
    "            raise RuntimeError(f\"Backward can only be called in training mode but your current module {self.name} is not in the training mode.\")\n",
    "            \n",
    "        # Propagate gradient through submodules in `reverse` order of addition\n",
    "        for module in reversed(list(self._modules.values())):\n",
    "            # For GNU or layers with hidden spaces, backward may\n",
    "            # return more than 1 element, but the 1st is ensured to be \n",
    "            # grad wrt to inputs.\n",
    "            # In the general feed-forward case, we only keep the 1st gradient.\n",
    "            # If you need other terms, override this method to achive it.\n",
    "            grad_output = module.backward(grad_output)\n",
    "            if isinstance(grad_output, tuple):\n",
    "                grad_output = grad_output[0]\n",
    "            \n",
    "        return grad_output\n",
    "    \n",
    "    def copy(self):\n",
    "        \"\"\"\n",
    "        Create a deepcopy of the parameters and gradiets.\n",
    "        \"\"\"\n",
    "        return deepcopy(self)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"nn_BaseModule(Deep Neural Network Base Module Class \\nConstruct Your Own Network by Creating Children of this Module.).\"\n",
    "\n",
    "\n",
    "# A Deep Neural Network Interface Module Class\n",
    "# Construct Your Own Network by Creating Children of this Module\n",
    "class nn_Module(nn_BaseModule):\n",
    "    \n",
    "    \"\"\"\n",
    "    Base interface for all neural network modules, with standard keyword arguments.\n",
    "    \n",
    "    All neural network modules implemented by users should inherit this class\n",
    "    and utilize the forward() to define the architecture of their networks.\n",
    "    By interacting with an optimizer and loss function, you can update\n",
    "    the weights stored in Parameters and train your neural network.\n",
    "    \"\"\"\n",
    "    \n",
    "    __attr__ = \"MML.nn_Module\"    \n",
    "\n",
    "    def __init__(self, \n",
    "                 *,\n",
    "                 module_name: str = \"Default_Module_Container\",\n",
    "                 backend: Literal[\"torch\", \"numpy\"] = \"torch\",\n",
    "                 dtype: type | str = None,\n",
    "                 device: str | None = None,\n",
    "                 autograd: bool = False,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        Initializes a neural network module interface with basic structural components.\n",
    "\n",
    "        This constructor sets up essential properties for a module, including its name,\n",
    "        training mode flag, and containers for parameters and submodules. It follows a\n",
    "        convention similar to PyTorch modules, where modules are organized hierarchically\n",
    "        with parameter tracking and submodule management.\n",
    "\n",
    "        Parameters:\n",
    "            --------\n",
    "            module_name: str, The name of the module instance. Defaults to \"Default_Module_Container\".\n",
    "            backend: Literal[\"torch\", \"numpy\"], The computational backend to use. Defaults to \"torch\".\n",
    "            dtype: type, The data type for the tensor values. Defaults to None (auto detection). \n",
    "                    For PyTorch, this corresponds to torch.dtype; for NumPy, it corresponds to np.dtype.\n",
    "            device: str | None, The target device (e.g., \"cpu\", \"cuda\") where the layer's parameters will be placed. \n",
    "                    If None, uses the default device. Defaults to None (auto detection).\n",
    "            autograd: bool, A flag indicating whether to use PyTorch's autograd for gradient computation. \n",
    "                    If True, manual gradient tracking is disabled. Defaults to False.\n",
    "\n",
    "        Attributes:\n",
    "            self.name: The name of the module instance, set dynamically via __setattr__.\n",
    "            self.training: A flag indicating whether the module is in training mode. Defaults to True.\n",
    "            self._parameter: A dictionary container for all parameters of this module.\n",
    "            self._modules: A dictionary container for all submodule instances nested within this module.\n",
    "            self.backend: Literal[\"torch\", \"numpy\"], The computational backend used by the layer.\n",
    "            self.dtype: type, The data type of the tensor values.\n",
    "            self.device: str | None, The device where the parameters are placed.\n",
    "            self.autograd: bool, Whether PyTorch's autograd is enabled for this layer.\n",
    "        \"\"\"\n",
    "        \n",
    "        # A default module at least contains:\n",
    "        # 1. self.name, str, the name of this module instance\n",
    "        # 2. self.training, bool, whether the module is in training or evaluation mode\n",
    "        # 3. self.accumulate, bool, whether the module is accumulating gradients\n",
    "        # 4. self._parameters, container, all parameters of THIS module\n",
    "        # 5. self._modules, container, all instances of SUB modules\n",
    "        # 6. self.backend, str, saying which backend Tensor will by default use\n",
    "        # 7. self.dtype, type, saying the type the data will be stored\n",
    "        # 8. self.device, str, saying the device where the data is stored on\n",
    "        # 9. self.autograd, bool, whether the modules use pytorch autograd or not\n",
    "        \n",
    "        # Module name, internal containers, calling base init to initialize\n",
    "        super().__init__(module_name = module_name)\n",
    "        \n",
    "        # Process the default types\n",
    "        if backend not in (\"numpy\", \"torch\"):\n",
    "            raise ValueError(f\"In creating a module {module_name}, an unsupported backend is passed in. Use 'numpy' or 'torch' only.\")\n",
    "        if backend == \"numpy\":\n",
    "            dtype = np.float32 if dtype is None else dtype\n",
    "            device = \"cpu\" if device is None else device\n",
    "        elif backend == \"torch\":\n",
    "            dtype = torch.float32 if dtype is None else dtype\n",
    "            device = \"cpu\" if device is None else device\n",
    "        \n",
    "        # Record the backend, dtype, device, autograd traits\n",
    "        self.__setattr__(\"backend\", backend)\n",
    "        self.__setattr__(\"dtype\", dtype)\n",
    "        self.__setattr__(\"device\", device)\n",
    "        self.__setattr__(\"autograd\", autograd)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"nn_Module(name = {self.name}).\"\n",
    "\n",
    "\n",
    "# Alias for nn_Module\n",
    "Module = nn_Module\n",
    "\n",
    "\n",
    "# Implementation of Dense Layer (Fully Connected Layer)\n",
    "class nn_Layer_Dense(nn_Module):\n",
    "    \"\"\"\n",
    "    Dense Layer (Fully-Connected Layer) Implementation\n",
    "    \n",
    "    This class serves as the foundation for implementing fully connected (dense)\n",
    "    neural network layers. It contains weight and bias in nn_Parameter containers\n",
    "    and ready to perform forward() and backward() pass to perform MLP tasks.    \n",
    "    \"\"\"\n",
    "    \n",
    "    __attr__ = \"MML.nn_Layer_Dense\"    \n",
    "    \n",
    "    def __init__(self, \n",
    "                 in_features: int = 1, \n",
    "                 out_features: int = 1, \n",
    "                 has_bias: str = True,\n",
    "                 init_scale: float = 0.01,\n",
    "                 *,\n",
    "                 module_name: str = \"nn_Layer_Dense\", \n",
    "                 backend: Literal[\"torch\", \"numpy\"] = \"torch\",\n",
    "                 dtype: type | str = None,\n",
    "                 device: str | None = None,\n",
    "                 autograd: bool = False,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        A fully connected layer: y = x @ W + b.\n",
    "        \n",
    "        This class implements a dense (fully connected) neural network layer, which performs a linear transformation\n",
    "        on input data followed by an optional bias addition. It is designed to be compatible with both PyTorch and NumPy backends,\n",
    "        supporting automatic gradient computation via autograd or manual gradient tracking.\n",
    "\n",
    "        Parameters:\n",
    "            in_features: int, The number of input features for this layer. Defaults to 1.\n",
    "            out_features: int, The number of output features for this layer. Defaults to 1.\n",
    "            has_bias: str, A flag indicating whether to include a bias term. Valid values are \"True\" or \"False\".\n",
    "                    If set to \"True\", the layer includes an additive bias term (b). Defaults to \"True\".\n",
    "            module_name: str, The name of the module instance. Defaults to \"nn_Layer_Dense\".\n",
    "            backend: Literal[\"torch\", \"numpy\"], The computational backend to use. Defaults to \"torch\".\n",
    "            dtype: type, The data type for the tensor values. Defaults to None (auto detection). \n",
    "                    For PyTorch, this corresponds to torch.dtype; for NumPy, it corresponds to np.dtype.\n",
    "            device: str | None, The target device (e.g., \"cpu\", \"cuda\") where the layer's parameters will be placed. \n",
    "                    If None, uses the default device. Defaults to None (auto detection).\n",
    "            autograd: bool, A flag indicating whether to use PyTorch's autograd for gradient computation. \n",
    "                    If True, manual gradient tracking is disabled. Defaults to False.\n",
    "\n",
    "        Attributes:\n",
    "            self.in_features: int, The number of input features for this layer.\n",
    "            self.out_features: int, The number of output features for this layer.\n",
    "            self.has_bias: str, A flag indicating whether a bias term is included (\"True\" or \"False\").\n",
    "            self.init_scale: float, A floatting number indicating the maximum value of initial random weights.\n",
    "            self.backend: Literal[\"torch\", \"numpy\"], The computational backend used by the layer.\n",
    "            self.dtype: type, The data type of the tensor values.\n",
    "            self.device: str | None, The device where the parameters are placed.\n",
    "            self.autograd: bool, Whether PyTorch's autograd is enabled for this layer.\n",
    "            self._parameters.weight: nn_Parameter, The weight parameter matrix (shape: [in_features, out_features]).\n",
    "            self._parameters.bias: nn_Parameter | optional, The optional bias vector (shape: [out_features]) if has_bias is True.\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__(module_name = module_name, backend = backend, dtype = dtype, device = device, autograd = autograd)\n",
    "        \n",
    "        # Shape Notation\n",
    "        # Input: (in_features)\n",
    "        # Output: (out_features)\n",
    "        # self._parameters[\"weight\"]: (in_features, out_features)\n",
    "        # self._parameters[\"bias\"]: (out_features)\n",
    "        \n",
    "        # Record shapes etc\n",
    "        self.__setattr__(\"in_features\", in_features)\n",
    "        self.__setattr__(\"out_features\", out_features)\n",
    "        self.__setattr__(\"has_bias\", has_bias)\n",
    "        self.__setattr__(\"init_scale\", init_scale)\n",
    "        \n",
    "        # Initialize weight and bias parameters\n",
    "        self.__setattr__(\"weight\", nn_Parameter(\n",
    "            Tensor.rand([in_features, out_features], backend=backend, dtype=dtype, device=device) * init_scale,\n",
    "            requires_grad = True,\n",
    "            dtype = None,\n",
    "            device = None,\n",
    "            autograd = autograd)\n",
    "            )\n",
    "        \n",
    "        if has_bias == True:\n",
    "            # If uses bias, then set the bias\n",
    "            self.__setattr__(\"bias\", nn_Parameter(\n",
    "                Tensor.zeros([out_features], backend=backend, dtype=dtype, device=device),\n",
    "                requires_grad = True,\n",
    "                dtype = None,\n",
    "                device = None,\n",
    "                autograd = autograd)\n",
    "                )\n",
    "            \n",
    "        return\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Compute the layer output: out = x @ W + b and return the out.\n",
    "\n",
    "        This method performs the forward computation for a dense neural network layer,\n",
    "        computing the linear transformation `out = x @ W + b`, where `W` is the weight matrix\n",
    "        and `b` is the bias vector. The input tensor `x` is processed through this operation,\n",
    "        and the result is returned as the output of the layer.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape (batch_size, in_features) to be transformed by the layer.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output tensor of shape (batch_size, out_features) after applying the dense transformation.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the input `x` is not a valid MML.Tensor object.\n",
    "\n",
    "        Attributes:\n",
    "            self.input (Tensor): The input tensor saved for use in backward propagation.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Type check, x must be an instance of Tensor\n",
    "        if isinstance(x, Tensor) == False:\n",
    "            raise ValueError(f\"In performing forward(), input `x` must be in a MML `Tensor` format but you have {type(x)}\")\n",
    "        \n",
    "        # Save input for backward\n",
    "        self.__setattr__(\"input\", x)\n",
    "        \n",
    "        # Perform forward pass x @ W + b\n",
    "        out = x @ self._parameters[\"weight\"].data\n",
    "        if self.has_bias == True:\n",
    "            out += self._parameters[\"bias\"].data\n",
    "        return out\n",
    "\n",
    "    def backward(self, grad_output: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Backward pass: compute gradients for weight, bias, and input.\n",
    "\n",
    "        This method performs the gradient computation for a dense layer during backpropagation. \n",
    "        It calculates the gradients of the loss with respect to the weights, biases, and input tensor,\n",
    "        based on the provided `grad_output` (gradient from the next layer). The implementation\n",
    "        supports both PyTorch autograd and manual gradient calculation modes.\n",
    "\n",
    "        Args:\n",
    "            grad_output (Tensor): Gradient tensor resulting from the output of the layer, used as input for backpropagation.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Gradient with respect to the input tensor, for recursive backward calculations in previous layers.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If `grad_output` is not a valid MML.Tensor object..\n",
    "        \"\"\"\n",
    "        \n",
    "        # If use autograd, pass; if manual mode, then calculate\n",
    "        if self.autograd == True:\n",
    "            return None\n",
    "        \n",
    "        # Type check, grad_output must be an instance of Tensor\n",
    "        if isinstance(grad_output, Tensor) == False:\n",
    "            raise ValueError(f\"In performing backward(), `grad_output` must be in a MML `Tensor` format but you have {type(grad_output)}\")\n",
    "        \n",
    "        # Gradient wrt. weight: X^T * grad_output\n",
    "        self._parameters[\"weight\"].grad = self.input.transpose() @ grad_output\n",
    "        \n",
    "        # Gradient wrt. bias: sum grad_output over batch dimension\n",
    "        if self.has_bias == True:\n",
    "            # Sum over the samples to get the gradients\n",
    "            self._parameters[\"bias\"].grad = grad_output.sum(axis = 0)\n",
    "        \n",
    "        # Gradient wrt. input: grad_output * W^T\n",
    "        grad_input = grad_output @ self._parameters[\"weight\"].data.transpose()\n",
    "        \n",
    "        # Return the gradient with respect to input for recursive backward calculation\n",
    "        return grad_input\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"nn_Layer_Dense(shape: ({self.in_features}, {self.out_features}) with{'out' if self.has_bias == False else ''} bias).\"\n",
    "    \n",
    "    \n",
    "# Alias for nn_Layer_Dense\n",
    "Dense = nn_Layer_Dense\n",
    "\n",
    "# Implementation of Dropout Layer (Masked Layer)\n",
    "class nn_Layer_Dropout(nn_Module):\n",
    "    \"\"\"\n",
    "    Dropout Layer (Masked Layer) Implementation\n",
    "    \n",
    "    Dropout layer that zeros out inputs with probability p during training, and \n",
    "    it will not mask anything in non-training mode.\n",
    "    Dropout layer can be used to improve anti-over-fitting capabilities of your model.\n",
    "    And it is compatible for any kind of Tensors with any shape (not only a 2D).\n",
    "    It does not have any learnable parameters.\n",
    "    \"\"\"\n",
    "    \n",
    "    __attr__ = \"MML.nn_Layer_Dropout\"  \n",
    "    \n",
    "    def __init__(self, \n",
    "                 p: float = 0.1,\n",
    "                 *,\n",
    "                 module_name: str = \"nn_Layer_Dropout\", \n",
    "                 backend: Literal[\"torch\", \"numpy\"] = \"torch\",\n",
    "                 dtype: type | str = None,\n",
    "                 device: str | None = None,\n",
    "                 autograd: bool = False,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        A training active dropout layer.\n",
    "        \n",
    "        This class implements a dropout neural network layer, which performs randomly dropout when training and \n",
    "        do nothing in evaluation process. Dropping out is controlled by a dropout rate which is typically ranging from\n",
    "        0 to 1 and common values are [0.1, 0.4].\n",
    "\n",
    "        Parameters:\n",
    "            p: float, The ratio of dropout when training the neural network. By default, it is 0.1.\n",
    "            module_name: str, The name of the module instance. Defaults to \"nn_Layer_Dense\".\n",
    "            backend: Literal[\"torch\", \"numpy\"], The computational backend to use. Defaults to \"torch\".\n",
    "            dtype: type, The data type for the tensor values. Defaults to None (auto detection). \n",
    "                    For PyTorch, this corresponds to torch.dtype; for NumPy, it corresponds to np.dtype.\n",
    "            device: str | None, The target device (e.g., \"cpu\", \"cuda\") where the layer's parameters will be placed. \n",
    "                    If None, uses the default device. Defaults to None (auto detection).\n",
    "            autograd: bool, A flag indicating whether to use PyTorch's autograd for gradient computation. \n",
    "                    If True, manual gradient tracking is disabled. Defaults to False.\n",
    "\n",
    "        Attributes:\n",
    "            self.dropout_p: float, the dropout rate specified and used in training.\n",
    "            self.dtype: type, The data type of the tensor values.\n",
    "            self.device: str | None, The device where the parameters are placed.\n",
    "            self.autograd: bool, Whether PyTorch's autograd is enabled for this layer.\n",
    "            self._parameters.weight: nn_Parameter, The weight parameter matrix (shape: [in_features, out_features]).\n",
    "            self._parameters.bias: nn_Parameter | optional, The optional bias vector (shape: [out_features]) if has_bias is True.\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__(module_name = module_name, backend = backend, dtype = dtype, device = device, autograd = autograd)\n",
    "        \n",
    "        # Record the dropout rate as a non-trainable parameter\n",
    "        self.__setattr__(\"dropout_p\", p)\n",
    "        \n",
    "        # Record an empty tuple showing the shape of the mask\n",
    "        self.__setattr__(\"shape\", ())\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Apply dropout during forward pass.\n",
    "\n",
    "        This method implements the forward computation of a dropout layer, which randomly sets elements of the input tensor to zero\n",
    "        with probability `p` during training. In evaluation mode, it returns the input unchanged. The dropout mask is stored for use\n",
    "        in backpropagation during training.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input tensor to apply dropout to. Shape should match the expected dimensions for the layer.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output tensor after applying dropout. During training, this tensor has elements randomly zeroed out and scaled.\n",
    "                   In evaluation mode, it returns the input tensor unchanged.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the input `x` is not a valid MML.Tensor object.\n",
    "        \"\"\"\n",
    "\n",
    "        # Type check, x must be an instance of Tensor\n",
    "        if isinstance(x, Tensor) == False:\n",
    "            raise ValueError(f\"In performing forward(), input `x` must be in a MML `Tensor` format but you have {type(x)}\")\n",
    "        \n",
    "        # Mask it in training mode\n",
    "        if self.training == True:\n",
    "            \n",
    "            # Save the shape of the mask\n",
    "            self.shape = x.shape\n",
    "            \n",
    "            # Create a dropout mask: 1 with probability (1-p), 0 with probability p\n",
    "            mask = Tensor.rand(x.shape, backend=self.backend, dtype=self.dtype, device=self.device)\n",
    "            mask.data = mask.data >= self.dropout_p\n",
    "            mask.astype(self.dtype)\n",
    "            \n",
    "            # Scale mask by 1 / (1-p) to keep expectation the same\n",
    "            mask = mask / (1 - self.dropout_p)\n",
    "            \n",
    "            # Save the mask as an attribute (non-parameter attribute)\n",
    "            self.__setattr__(\"mask\", mask)\n",
    "            \n",
    "            return x * mask\n",
    "        \n",
    "        # Do nothing in evaluation mode\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    def backward(self, grad_output: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Backward pass: compute gradients for weight, bias, and input.\n",
    "\n",
    "        This method performs the gradient computation for a dense layer during backpropagation. \n",
    "        It calculates the gradients of the loss with respect to the weights, biases, and input tensor,\n",
    "        based on the provided `grad_output` (gradient from the next layer). The implementation\n",
    "        supports both PyTorch autograd and manual gradient calculation modes.\n",
    "\n",
    "        Args:\n",
    "            grad_output (Tensor): Gradient tensor resulting from the output of the layer, used as input for backpropagation.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Gradient with respect to the input tensor, for recursive backward calculations in previous layers.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If `grad_output` is not a valid MML.Tensor object..\n",
    "        \"\"\"\n",
    "        \n",
    "        # If use autograd, pass; if manual mode, then calculate\n",
    "        if self.autograd == True:\n",
    "            return None\n",
    "        \n",
    "        # Type check, grad_output must be an instance of Tensor\n",
    "        if isinstance(grad_output, Tensor) == False:\n",
    "            raise ValueError(f\"In performing backward(), `grad_output` must be in a MML `Tensor` format but you have {type(grad_output)}\")\n",
    "        \n",
    "        # If training, apply the same mask to the gradient\n",
    "        if self.training:\n",
    "            return grad_output * self.mask\n",
    "        \n",
    "        # Else, return identity\n",
    "        else:\n",
    "            return grad_output\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"nn_Layer_Dropout(shape: {self.shape} with probability {round(self.dropout_p)}).\"\n",
    "    \n",
    "    \n",
    "# Alias for nn_Layer_Dropout\n",
    "Dropout = nn_Layer_Dropout\n",
    "\n",
    "# Implementation of Flatten Layer (Make Tensor Flatten)\n",
    "class nn_Layer_Flatten(nn_Module):\n",
    "    \"\"\"\n",
    "    Flatten Layer Implementation\n",
    "    \n",
    "    Flatten Layer just turn the input Tensor into a flatten 2D tensor (batch_size, features),\n",
    "    very suitable for transforming high dimensional data into low dimension ones and then apply\n",
    "    Dense and Dropout layers. It does not have any learnable parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    __attr__ = \"MML.nn_Layer_Flatten\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 *,\n",
    "                 module_name: str = \"nn_Layer_Flatten\", \n",
    "                 backend: Literal[\"torch\", \"numpy\"] = \"torch\",\n",
    "                 dtype: type | str = None,\n",
    "                 device: str | None = None,\n",
    "                 autograd: bool = False,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        A training active dropout layer.\n",
    "        \n",
    "        This class implements a dropout neural network layer, which performs randomly dropout when training and \n",
    "        do nothing in evaluation process. Dropping out is controlled by a dropout rate which is typically ranging from\n",
    "        0 to 1 and common values are [0.1, 0.4].\n",
    "\n",
    "        Parameters:\n",
    "            module_name: str, The name of the module instance. Defaults to \"nn_Layer_Dense\".\n",
    "            backend: Literal[\"torch\", \"numpy\"], The computational backend to use. Defaults to \"torch\".\n",
    "            dtype: type, The data type for the tensor values. Defaults to None (auto detection). \n",
    "                    For PyTorch, this corresponds to torch.dtype; for NumPy, it corresponds to np.dtype.\n",
    "            device: str | None, The target device (e.g., \"cpu\", \"cuda\") where the layer's parameters will be placed. \n",
    "                    If None, uses the default device. Defaults to None (auto detection).\n",
    "            autograd: bool, A flag indicating whether to use PyTorch's autograd for gradient computation. \n",
    "                    If True, manual gradient tracking is disabled. Defaults to False.\n",
    "\n",
    "        Attributes:\n",
    "            self.dtype: type, The data type of the tensor values.\n",
    "            self.device: str | None, The device where the parameters are placed.\n",
    "            self.autograd: bool, Whether PyTorch's autograd is enabled for this layer.\n",
    "            self._parameters.weight: nn_Parameter, The weight parameter matrix (shape: [in_features, out_features]).\n",
    "            self._parameters.bias: nn_Parameter | optional, The optional bias vector (shape: [out_features]) if has_bias is True.\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__(module_name = module_name, backend = backend, dtype = dtype, device = device, autograd = autograd)\n",
    "        \n",
    "        # Record an empty tuple showing the original shape of the input\n",
    "        self.__setattr__(\"original_shape\", ())\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Reshape the input tensor into a 2D format (batch_size, features).\n",
    "        If the input is high dimensional, except the 1st dimension, which is batch_size,\n",
    "        any other dimensions will be flatten into a flatten tensor.\n",
    "        The output of this Layer will always be a 2D Tensor.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input tensor with arbitrary dimensions. The first dimension represents the batch size.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output tensor reshaped into 2D format (batch_size, total_features).\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the input `x` is not a valid MML.Tensor object.\n",
    "        \"\"\"\n",
    "        # Type check, x must be an instance of Tensor\n",
    "        if isinstance(x, Tensor) == False:\n",
    "            raise ValueError(f\"In performing forward(), input `x` must be in a MML `Tensor` format but you have {type(x)}\")\n",
    "        \n",
    "        # Save the original input shape for backward pass\n",
    "        self.original_shape = x.shape\n",
    "\n",
    "        # Flatten all dimensions except the batch dimension (dim 0)\n",
    "        out = x.reshape([x.shape[0], -1])\n",
    "        # Reshape to (batch_size, total_features)\n",
    "        return out\n",
    "\n",
    "    def backward(self, grad_output: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Backward pass: compute gradients for weight, bias, and input.\n",
    "\n",
    "        This method performs the gradient computation for a dense layer during backpropagation. \n",
    "        It calculates the gradients of the loss with respect to the weights, biases, and input tensor,\n",
    "        based on the provided `grad_output` (gradient from the next layer). The implementation\n",
    "        supports both PyTorch autograd and manual gradient calculation modes.\n",
    "\n",
    "        Args:\n",
    "            grad_output (Tensor): Gradient tensor resulting from the output of the layer, used as input for backpropagation.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Gradient with respect to the input tensor, for recursive backward calculations in previous layers.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If `grad_output` is not a valid MML.Tensor object..\n",
    "        \"\"\"\n",
    "        # If use autograd, pass; if manual mode, then calculate\n",
    "        if self.autograd == True:\n",
    "            return None\n",
    "        \n",
    "        # Type check, grad_output must be an instance of Tensor\n",
    "        if isinstance(grad_output, Tensor) == False:\n",
    "            raise ValueError(f\"In performing backward(), `grad_output` must be in a MML `Tensor` format but you have {type(grad_output)}\")\n",
    "\n",
    "        # Reshape gradient back to original shape\n",
    "        return grad_output.reshape(self.original_shape)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"nn_Layer_Flatten(with original shape {self.original_shape}).\"\n",
    "\n",
    "\n",
    "# Alias for nn_Layer_Flatten\n",
    "Flatten = nn_Layer_Flatten\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These Activation Functions are self implemented and open-sourced\n",
    "# Available at https://github.com/dof-studio/MML/\n",
    "# By Nathmath Huang (bh2821)\n",
    "# License: Apache License Version 2.0\n",
    "\n",
    "\n",
    "# Implementation of ReLU Activation\n",
    "class nn_Activation_ReLU(nn_Module):\n",
    "    \"\"\"\n",
    "    ReLU activation function.\n",
    "    \n",
    "    The Rectified Linear Unit (ReLU) is a widely used activation function \n",
    "    defined by the formula: f(x) = \\max(0, x). This function outputs the \n",
    "    input value if it is positive, and zero otherwise. ReLU is celebrated for its \n",
    "    computational efficiency and ability to mitigate vanishing gradient problems \n",
    "    during backpropagation, making it a cornerstone in modern deep learning architectures.\n",
    "    \n",
    "    Formula: f(x) = max(0, x)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    __attr__ = \"MML.nn_Activation_ReLU\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 *,\n",
    "                 module_name: str = \"nn_Activation_ReLU\", \n",
    "                 backend: Literal[\"torch\", \"numpy\"] = \"torch\",\n",
    "                 dtype: type | str = None,\n",
    "                 device: str | None = None,\n",
    "                 autograd: bool = False,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        An ReLU activation function.\n",
    "\n",
    "        Parameters:\n",
    "            module_name: str, The name of the module instance.\n",
    "            backend: Literal[\"torch\", \"numpy\"], The computational backend to use. Defaults to \"torch\".\n",
    "            dtype: type, The data type for the tensor values. Defaults to None (auto detection). \n",
    "                    For PyTorch, this corresponds to torch.dtype; for NumPy, it corresponds to np.dtype.\n",
    "            device: str | None, The target device (e.g., \"cpu\", \"cuda\") where the layer's parameters will be placed. \n",
    "                    If None, uses the default device. Defaults to None (auto detection).\n",
    "            autograd: bool, A flag indicating whether to use PyTorch's autograd for gradient computation. \n",
    "                    If True, manual gradient tracking is disabled. Defaults to False.\n",
    "\n",
    "        Attributes:\n",
    "            self.dtype: type, The data type of the tensor values.\n",
    "            self.device: str | None, The device where the parameters are placed.\n",
    "            self.autograd: bool, Whether PyTorch's autograd is enabled for this layer.\n",
    "            self._parameters.weight: nn_Parameter, The weight parameter matrix (shape: [in_features, out_features]).\n",
    "            self._parameters.bias: nn_Parameter | optional, The optional bias vector (shape: [out_features]) if has_bias is True.\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__(module_name = module_name, backend = backend, dtype = dtype, device = device, autograd = autograd)\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Apply the Rectified Linear Unit (ReLU) activation function to the input tensor.\n",
    "\n",
    "        This method computes the element-wise ReLU activation, which outputs the input if it is positive,\n",
    "        and zero otherwise. It is a fundamental non-linearity in neural networks, enabling the model\n",
    "        to learn complex patterns by introducing non-linearities. The input is saved for backward propagation\n",
    "        to compute gradients during training.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of any shape. The ReLU operation is applied element-wise.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output tensor after applying the ReLU activation, with the same shape as the input.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the input `x` is not a valid MML.Tensor object.\n",
    "        \"\"\"\n",
    "                \n",
    "        # Type check, x must be an instance of Tensor\n",
    "        if isinstance(x, Tensor) == False:\n",
    "            raise ValueError(f\"In performing forward(), input `x` must be in a MML `Tensor` format but you have {type(x)}\")\n",
    "        \n",
    "        # Save input for backward\n",
    "        self.__setattr__(\"input\", x)\n",
    "\n",
    "        # Apply ReLU to the input data\n",
    "        return Tensor.where_as(x.data > 0, x.data, 0, backend=self.backend, dtype=self.dtype, device=self.device)\n",
    "\n",
    "    def backward(self, grad_output: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Backward pass: compute gradients for weight, bias, and input.\n",
    "\n",
    "        This method performs the gradient computation for a module during backpropagation. \n",
    "        It calculates the gradients of the loss with respect to the weights, biases, and input tensor,\n",
    "        based on the provided `grad_output` (gradient from the next layer). The implementation\n",
    "        supports both PyTorch autograd and manual gradient calculation modes.\n",
    "\n",
    "        Args:\n",
    "            grad_output (Tensor): Gradient tensor resulting from the output of the layer, used as input for backpropagation.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Gradient with respect to the input tensor, for recursive backward calculations in previous layers.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If `grad_output` is not a valid MML.Tensor object..\n",
    "        \"\"\"\n",
    "        \n",
    "        # If use autograd, pass; if manual mode, then calculate\n",
    "        if self.autograd == True:\n",
    "            return None\n",
    "        \n",
    "        # Type check, grad_output must be an instance of Tensor\n",
    "        if isinstance(grad_output, Tensor) == False:\n",
    "            raise ValueError(f\"In performing backward(), `grad_output` must be in a MML `Tensor` format but you have {type(grad_output)}\")\n",
    "        \n",
    "        # Pass gradient only where input was positive\n",
    "        grad_input = Tensor.where_as(self.input.data <= 0, 0, grad_output.data, backend=self.backend, dtype=self.dtype, device=self.device)\n",
    "        return grad_input\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"nn_Activation_ReLU(ReLU Activation Function).\"\n",
    "    \n",
    "\n",
    "# Alias for nn_Activation_ReLU\n",
    "ReLU = nn_Activation_ReLU\n",
    "\n",
    "\n",
    "# Implementation of Leaky ReLU Activation\n",
    "class nn_Activation_LeakyReLU(nn_Module):\n",
    "    \"\"\"\n",
    "    Leaky ReLU activation with a small slope for negative inputs.\n",
    "    \n",
    "    The Leaky Rectified Linear Unit (Leaky ReLU) is a variant of the ReLU \n",
    "    activation function that allows a small, non-zero gradient when the input \n",
    "    is negative. This helps mitigate the \"dying ReLU\" problem where neurons \n",
    "    become inactive and cease to learn. The function is defined as:\n",
    "    \n",
    "    Formula: f(x) = max(0, x, α*x), where α is a small positive slope (typically 0.01).\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    __attr__ = \"MML.nn_Activation_LeakyReLU\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 leaky_slope: float = 0.01,\n",
    "                 *,\n",
    "                 module_name: str = \"nn_Activation_ReLU\", \n",
    "                 backend: Literal[\"torch\", \"numpy\"] = \"torch\",\n",
    "                 dtype: type | str = None,\n",
    "                 device: str | None = None,\n",
    "                 autograd: bool = False,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        A Leaky ReLU activation function.\n",
    "\n",
    "        Parameters:\n",
    "            leaky_slope: float, The slope of negative values.\n",
    "            module_name: str, The name of the module instance.\n",
    "            backend: Literal[\"torch\", \"numpy\"], The computational backend to use. Defaults to \"torch\".\n",
    "            dtype: type, The data type for the tensor values. Defaults to None (auto detection). \n",
    "                    For PyTorch, this corresponds to torch.dtype; for NumPy, it corresponds to np.dtype.\n",
    "            device: str | None, The target device (e.g., \"cpu\", \"cuda\") where the layer's parameters will be placed. \n",
    "                    If None, uses the default device. Defaults to None (auto detection).\n",
    "            autograd: bool, A flag indicating whether to use PyTorch's autograd for gradient computation. \n",
    "                    If True, manual gradient tracking is disabled. Defaults to False.\n",
    "\n",
    "        Attributes:\n",
    "            self.leaky_slope: float, The slope applied to negative values.\n",
    "            self.dtype: type, The data type of the tensor values.\n",
    "            self.device: str | None, The device where the parameters are placed.\n",
    "            self.autograd: bool, Whether PyTorch's autograd is enabled for this layer.\n",
    "            self._parameters.weight: nn_Parameter, The weight parameter matrix (shape: [in_features, out_features]).\n",
    "            self._parameters.bias: nn_Parameter | optional, The optional bias vector (shape: [out_features]) if has_bias is True.\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__(module_name = module_name, backend = backend, dtype = dtype, device = device, autograd = autograd)\n",
    "    \n",
    "        # Record the leaky slope as a non-Parameter attribute\n",
    "        self.__setattr__(\"leaky_slope\", leaky_slope)\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Apply the Leaky Rectified Linear Unit (Leaky ReLU) activation function to the input tensor.\n",
    "\n",
    "        This method computes the element-wise Leaky ReLU activation, which outputs the input if it is positive,\n",
    "        and a small negative slope multiplied by the input otherwise. This variant of ReLU mitigates the \"dying ReLU\"\n",
    "        problem by allowing a controlled negative slope, improving gradient flow for negative inputs. The input is saved\n",
    "        for backward propagation to compute gradients during training.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of any shape. The Leaky ReLU operation is applied element-wise.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output tensor after applying the Leaky ReLU activation, with the same shape as the input.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the input `x` is not a valid MML.Tensor object.\n",
    "        \"\"\"\n",
    "                \n",
    "        # Type check, x must be an instance of Tensor\n",
    "        if isinstance(x, Tensor) == False:\n",
    "            raise ValueError(f\"In performing forward(), input `x` must be in a MML `Tensor` format but you have {type(x)}\")\n",
    "        \n",
    "        # Save input for backward\n",
    "        self.__setattr__(\"input\", x)\n",
    "\n",
    "        # Apply Leaky ReLU to the input data\n",
    "        return Tensor.where_as(x.data > 0, x.data, x.data * self.leaky_slope, backend=self.backend, dtype=self.dtype, device=self.device)\n",
    "\n",
    "    def backward(self, grad_output: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Backward pass: compute gradients for weight, bias, and input.\n",
    "\n",
    "        This method performs the gradient computation for a module during backpropagation. \n",
    "        It calculates the gradients of the loss with respect to the weights, biases, and input tensor,\n",
    "        based on the provided `grad_output` (gradient from the next layer). The implementation\n",
    "        supports both PyTorch autograd and manual gradient calculation modes.\n",
    "\n",
    "        Args:\n",
    "            grad_output (Tensor): Gradient tensor resulting from the output of the layer, used as input for backpropagation.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Gradient with respect to the input tensor, for recursive backward calculations in previous layers.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If `grad_output` is not a valid MML.Tensor object..\n",
    "        \"\"\"\n",
    "        \n",
    "        # If use autograd, pass; if manual mode, then calculate\n",
    "        if self.autograd == True:\n",
    "            return None\n",
    "        \n",
    "        # Type check, grad_output must be an instance of Tensor\n",
    "        if isinstance(grad_output, Tensor) == False:\n",
    "            raise ValueError(f\"In performing backward(), `grad_output` must be in a MML `Tensor` format but you have {type(grad_output)}\")\n",
    "        \n",
    "        # Pass gradient only where input was positive\n",
    "        grad_input = Tensor.where_as(self.input.data <= 0, grad_output.data * self.leaky_slope, grad_output.data, backend=self.backend, dtype=self.dtype, device=self.device)\n",
    "        return grad_input\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"nn_Activation_LeakyReLU(Leaky ReLU Activation Function with alpha = {self.leaky_slope}).\"\n",
    "    \n",
    "      \n",
    "# Alias for nn_Activation_LeakyReLU\n",
    "LeakyReLU = nn_Activation_LeakyReLU \n",
    "\n",
    "\n",
    "# Implementation of Sigmoid Activation\n",
    "class nn_Activation_Sigmoid(nn_Module):\n",
    "    \"\"\"\n",
    "    Sigmoid activation function.\n",
    "    \n",
    "    The Sigmoid function maps input values to a range between 0 and 1, \n",
    "    making it suitable for binary classification tasks. It is defined by the \n",
    "    formula: f(x) = 1 / (1 + e^(-x)). However, it suffers from vanishing gradient \n",
    "    issues in deep networks due to its saturation regions near ±1.\n",
    "    \n",
    "    Formula: f(x) = \\frac{1}{1 + e^{-x}}\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    __attr__ = \"MML.nn_Activation_Sigmoid\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 *,\n",
    "                 module_name: str = \"nn_Activation_Sigmoid\", \n",
    "                 backend: Literal[\"torch\", \"numpy\"] = \"torch\",\n",
    "                 dtype: type | str = None,\n",
    "                 device: str | None = None,\n",
    "                 autograd: bool = False,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        An Sigmoid activation function.\n",
    "\n",
    "        Parameters:\n",
    "            module_name: str, The name of the module instance.\n",
    "            backend: Literal[\"torch\", \"numpy\"], The computational backend to use. Defaults to \"torch\".\n",
    "            dtype: type, The data type for the tensor values. Defaults to None (auto detection). \n",
    "                    For PyTorch, this corresponds to torch.dtype; for NumPy, it corresponds to np.dtype.\n",
    "            device: str | None, The target device (e.g., \"cpu\", \"cuda\") where the layer's parameters will be placed. \n",
    "                    If None, uses the default device. Defaults to None (auto detection).\n",
    "            autograd: bool, A flag indicating whether to use PyTorch's autograd for gradient computation. \n",
    "                    If True, manual gradient tracking is disabled. Defaults to False.\n",
    "\n",
    "        Attributes:\n",
    "            self.dtype: type, The data type of the tensor values.\n",
    "            self.device: str | None, The device where the parameters are placed.\n",
    "            self.autograd: bool, Whether PyTorch's autograd is enabled for this layer.\n",
    "            self._parameters.weight: nn_Parameter, The weight parameter matrix (shape: [in_features, out_features]).\n",
    "            self._parameters.bias: nn_Parameter | optional, The optional bias vector (shape: [out_features]) if has_bias is True.\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__(module_name = module_name, backend = backend, dtype = dtype, device = device, autograd = autograd)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Apply the Sigmoid activation function to the input tensor.\n",
    "\n",
    "        This method computes the element-wise Sigmoid activation, which maps input values\n",
    "        to the range (0, 1). The Sigmoid function is widely used in neural networks for\n",
    "        binary classification tasks due to its smooth, differentiable nature. The output\n",
    "        is saved for use during backward propagation to compute gradients.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of any shape. The Sigmoid operation is applied element-wise.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output tensor after applying the Sigmoid activation, with the same shape as the input.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the input `x` is not a valid MML.Tensor object.\n",
    "\n",
    "        \"\"\"\n",
    "                \n",
    "        # Type check, x must be an instance of Tensor\n",
    "        if isinstance(x, Tensor) == False:\n",
    "            raise ValueError(f\"In performing forward(), input `x` must be in a MML `Tensor` format but you have {type(x)}\")\n",
    "        \n",
    "        # Perform a sigmoid function on the input\n",
    "        output = x.sigmoid()\n",
    "        \n",
    "        # Save output for backward\n",
    "        self.__setattr__(\"output\", output)\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def backward(self, grad_output: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Backward pass: compute gradients for weight, bias, and input.\n",
    "\n",
    "        This method performs the gradient computation for a module during backpropagation. \n",
    "        It calculates the gradients of the loss with respect to the weights, biases, and input tensor,\n",
    "        based on the provided `grad_output` (gradient from the next layer). The implementation\n",
    "        supports both PyTorch autograd and manual gradient calculation modes.\n",
    "\n",
    "        Args:\n",
    "            grad_output (Tensor): Gradient tensor resulting from the output of the layer, used as input for backpropagation.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Gradient with respect to the input tensor, for recursive backward calculations in previous layers.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If `grad_output` is not a valid MML.Tensor object..\n",
    "        \"\"\"\n",
    "        \n",
    "        # If use autograd, pass; if manual mode, then calculate\n",
    "        if self.autograd == True:\n",
    "            return None\n",
    "        \n",
    "        # Type check, grad_output must be an instance of Tensor\n",
    "        if isinstance(grad_output, Tensor) == False:\n",
    "            raise ValueError(f\"In performing backward(), `grad_output` must be in a MML `Tensor` format but you have {type(grad_output)}\")\n",
    "        \n",
    "        # grad = grad_output * sigmoid(x) * (1 - sigmoid(x))\n",
    "        grad_input = grad_output * self.output * (1 - self.output)\n",
    "        return grad_input\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"nn_Activation_Sigmoid(Sigmoid Activation Function).\"\n",
    "    \n",
    "    \n",
    "# Alias for nn_Activation_Sigmoid\n",
    "Sigmoid = nn_Activation_Sigmoid\n",
    "\n",
    "\n",
    "# Implementation of Tanh Activation\n",
    "class nn_Activation_Tanh(nn_Module):\n",
    "    \"\"\"\n",
    "    Tanh activation function.\n",
    "    \n",
    "    The hyperbolic tangent (tanh) function maps input values to a range between -1 and 1,\n",
    "    making it suitable for scenarios requiring symmetric output distribution. It is defined as:\n",
    "    \n",
    "    Formula: f(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}} = \\tanh(x)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    __attr__ = \"MML.nn_Activation_Tanh\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 *,\n",
    "                 module_name: str = \"nn_Activation_Sigmoid\", \n",
    "                 backend: Literal[\"torch\", \"numpy\"] = \"torch\",\n",
    "                 dtype: type | str = None,\n",
    "                 device: str | None = None,\n",
    "                 autograd: bool = False,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        An Sigmoid activation function.\n",
    "\n",
    "        Parameters:\n",
    "            module_name: str, The name of the module instance.\n",
    "            backend: Literal[\"torch\", \"numpy\"], The computational backend to use. Defaults to \"torch\".\n",
    "            dtype: type, The data type for the tensor values. Defaults to None (auto detection). \n",
    "                    For PyTorch, this corresponds to torch.dtype; for NumPy, it corresponds to np.dtype.\n",
    "            device: str | None, The target device (e.g., \"cpu\", \"cuda\") where the layer's parameters will be placed. \n",
    "                    If None, uses the default device. Defaults to None (auto detection).\n",
    "            autograd: bool, A flag indicating whether to use PyTorch's autograd for gradient computation. \n",
    "                    If True, manual gradient tracking is disabled. Defaults to False.\n",
    "\n",
    "        Attributes:\n",
    "            self.dtype: type, The data type of the tensor values.\n",
    "            self.device: str | None, The device where the parameters are placed.\n",
    "            self.autograd: bool, Whether PyTorch's autograd is enabled for this layer.\n",
    "            self._parameters.weight: nn_Parameter, The weight parameter matrix (shape: [in_features, out_features]).\n",
    "            self._parameters.bias: nn_Parameter | optional, The optional bias vector (shape: [out_features]) if has_bias is True.\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__(module_name = module_name, backend = backend, dtype = dtype, device = device, autograd = autograd)\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Apply the Tangent Hyperbolic activation function to the input tensor.\n",
    "\n",
    "        This method computes the element-wise hyperbolic tangent (tanh) activation,\n",
    "        which maps input values to the range (-1, 1). The tanh function is smooth and\n",
    "        differentiable everywhere, making it suitable for neural network layers that\n",
    "        require non-linear transformations. The output is saved for use in backward\n",
    "        propagation to compute gradients during training.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of any shape. The tanh operation is applied element-wise.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output tensor after applying the tanh activation, with the same shape as the input.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the input `x` is not a valid MML.Tensor object.\n",
    "        \"\"\"\n",
    "                \n",
    "        # Type check, x must be an instance of Tensor\n",
    "        if isinstance(x, Tensor) == False:\n",
    "            raise ValueError(f\"In performing forward(), input `x` must be in a MML `Tensor` format but you have {type(x)}\")\n",
    "        \n",
    "        # Perform a tanh function on the input\n",
    "        output = x.tanh()\n",
    "        \n",
    "        # Save output for backward\n",
    "        self.__setattr__(\"output\", output)\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def backward(self, grad_output: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Backward pass: compute gradients for weight, bias, and input.\n",
    "\n",
    "        This method performs the gradient computation for a module during backpropagation. \n",
    "        It calculates the gradients of the loss with respect to the weights, biases, and input tensor,\n",
    "        based on the provided `grad_output` (gradient from the next layer). The implementation\n",
    "        supports both PyTorch autograd and manual gradient calculation modes.\n",
    "\n",
    "        Args:\n",
    "            grad_output (Tensor): Gradient tensor resulting from the output of the layer, used as input for backpropagation.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Gradient with respect to the input tensor, for recursive backward calculations in previous layers.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If `grad_output` is not a valid MML.Tensor object..\n",
    "        \"\"\"\n",
    "        \n",
    "        # If use autograd, pass; if manual mode, then calculate\n",
    "        if self.autograd == True:\n",
    "            return None\n",
    "        \n",
    "        # Type check, grad_output must be an instance of Tensor\n",
    "        if isinstance(grad_output, Tensor) == False:\n",
    "            raise ValueError(f\"In performing backward(), `grad_output` must be in a MML `Tensor` format but you have {type(grad_output)}\")\n",
    "        \n",
    "        # grad = grad_output * (1 - tanh(x)^2)\n",
    "        grad_input = grad_output * (1 - self.output ** 2)\n",
    "        return grad_input\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"nn_Activation_Tanh(Tanh Activation Function).\"\n",
    "   \n",
    "    \n",
    "# Alias for nn_Activation_Tanh\n",
    "Tanh = nn_Activation_Tanh\n",
    "\n",
    "   \n",
    "# Implementation of Softmax Activation\n",
    "class nn_Activation_Softmax(nn_Module):\n",
    "    \"\"\"\n",
    "    Softmax activation function.\n",
    "    \n",
    "    The Softmax function converts raw scores (logits) into probabilities \n",
    "    that sum to 1, making it suitable for multi-class classification tasks. \n",
    "    It generalizes the sigmoid function to multiple classes by applying the \n",
    "    formula: f(x_i) = exp(x_i) / sum_j(exp(x_j)), where x_i is the input score \n",
    "    for class i. This ensures the output represents a probability distribution \n",
    "    over the classes.\n",
    "    \n",
    "    Formula: f(x_i) = \\frac{e^{x_i}}{\\sum_{j} e^{x_j}}\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    __attr__ = \"MML.nn_Activation_Softmax\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 dim: int = 1,\n",
    "                 *,\n",
    "                 module_name: str = \"nn_Activation_Sigmoid\", \n",
    "                 backend: Literal[\"torch\", \"numpy\"] = \"torch\",\n",
    "                 dtype: type | str = None,\n",
    "                 device: str | None = None,\n",
    "                 autograd: bool = False,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        An Sigmoid activation function.\n",
    "\n",
    "        Parameters:\n",
    "            dim: int, The dimension to apply softmax on. Defaults to 1.\n",
    "            module_name: str, The name of the module instance.\n",
    "            backend: Literal[\"torch\", \"numpy\"], The computational backend to use. Defaults to \"torch\".\n",
    "            dtype: type, The data type for the tensor values. Defaults to None (auto detection). \n",
    "                    For PyTorch, this corresponds to torch.dtype; for NumPy, it corresponds to np.dtype.\n",
    "            device: str | None, The target device (e.g., \"cpu\", \"cuda\") where the layer's parameters will be placed. \n",
    "                    If None, uses the default device. Defaults to None (auto detection).\n",
    "            autograd: bool, A flag indicating whether to use PyTorch's autograd for gradient computation. \n",
    "                    If True, manual gradient tracking is disabled. Defaults to False.\n",
    "\n",
    "        Attributes:\n",
    "            self.softmax_dim: int, The dimension to apply softmax on.\n",
    "            self.dtype: type, The data type of the tensor values.\n",
    "            self.device: str | None, The device where the parameters are placed.\n",
    "            self.autograd: bool, Whether PyTorch's autograd is enabled for this layer.\n",
    "            self._parameters.weight: nn_Parameter, The weight parameter matrix (shape: [in_features, out_features]).\n",
    "            self._parameters.bias: nn_Parameter | optional, The optional bias vector (shape: [out_features]) if has_bias is True.\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__(module_name = module_name, backend = backend, dtype = dtype, device = device, autograd = autograd)\n",
    "    \n",
    "        # Record the softmax dimension as a non-Parameter attribute\n",
    "        self.__setattr__(\"softmax_dim\", dim)\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Apply the Softmax activation function to the input tensor.\n",
    "\n",
    "        This method applies the softmax function along the specified axis (softmax_dim)\n",
    "        to convert logits into probabilities. The output is a Tensor with the same shape\n",
    "        as the input, but with values normalized to the range [0, 1] along the specified axis.\n",
    "        This operation is commonly used in classification tasks to produce probability distributions.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input tensor containing logits (unnormalized log probabilities).\n",
    "            softmax_dim (int): The axis along which to apply the softmax function. \n",
    "                              For example, for a batch of images, this could be the channel dimension.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output tensor with probabilities computed via softmax along the specified axis.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the input `x` is not a valid MML.Tensor object.\n",
    "\n",
    "        \"\"\"\n",
    "                \n",
    "        # Type check, x must be an instance of Tensor\n",
    "        if isinstance(x, Tensor) == False:\n",
    "            raise ValueError(f\"In performing forward(), input `x` must be in a MML `Tensor` format but you have {type(x)}\")\n",
    "        \n",
    "        # Perform a softmax function on the input\n",
    "        output = x.softmax(axis = self.softmax_dim, keepdims = True)\n",
    "        \n",
    "        # Save output for backward\n",
    "        self.__setattr__(\"output\", output)\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def backward(self, grad_output: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Backward pass: compute gradients for weight, bias, and input.\n",
    "\n",
    "        This method performs the gradient computation for a module during backpropagation. \n",
    "        It calculates the gradients of the loss with respect to the weights, biases, and input tensor,\n",
    "        based on the provided `grad_output` (gradient from the next layer). The implementation\n",
    "        supports both PyTorch autograd and manual gradient calculation modes.\n",
    "\n",
    "        Args:\n",
    "            grad_output (Tensor): Gradient tensor resulting from the output of the layer, used as input for backpropagation.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Gradient with respect to the input tensor, for recursive backward calculations in previous layers.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If `grad_output` is not a valid MML.Tensor object..\n",
    "        \"\"\"\n",
    "        \n",
    "        # If use autograd, pass; if manual mode, then calculate\n",
    "        if self.autograd == True:\n",
    "            return None\n",
    "        \n",
    "        # Type check, grad_output must be an instance of Tensor\n",
    "        if isinstance(grad_output, Tensor) == False:\n",
    "            raise ValueError(f\"In performing backward(), `grad_output` must be in a MML `Tensor` format but you have {type(grad_output)}\")\n",
    "        \n",
    "        # Compute gradient w.r.t input using Jacobian: grad_input = y * (grad_out - (grad_out * y).sum_along_dim)\n",
    "        grad_sum = (grad_output * self.output).sum(axis=self.softmax_dim, keepdims=True) \n",
    "        grad_input = self.output * (grad_output - grad_sum)\n",
    "        return grad_input\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"nn_Activation_Softmax(Softmax Activation Function).\"\n",
    "   \n",
    "    \n",
    "# Alias for nn_Activation_Softmax\n",
    "Softmax = nn_Activation_Softmax\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These Loss Functions are self implemented and open-sourced\n",
    "# Available at https://github.com/dof-studio/MML/\n",
    "# By Nathmath Huang (bh2821)\n",
    "# License: Apache License Version 2.0\n",
    "\n",
    "# Implementation of Base Lose Class\n",
    "class nn_Loss_BaseLoss(nn_Module):\n",
    "\n",
    "    __attr__ = \"MML.nn_Loss_BaseLoss\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 *,\n",
    "                 module_name: str = \"nn_Loss_Base\",\n",
    "                 backend: Literal[\"torch\", \"numpy\"] = \"torch\",\n",
    "                 dtype: type | str = None,\n",
    "                 device: str | None = None,\n",
    "                 autograd: bool = False,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        An abstract Loss Implemetation.\n",
    "\n",
    "        Parameters:\n",
    "            module_name: str, The name of the module instance.\n",
    "            backend: Literal[\"torch\", \"numpy\"], The computational backend to use. Defaults to \"torch\".\n",
    "            dtype: type, The data type for the tensor values. Defaults to None (auto detection). \n",
    "                    For PyTorch, this corresponds to torch.dtype; for NumPy, it corresponds to np.dtype.\n",
    "            device: str | None, The target device (e.g., \"cpu\", \"cuda\") where the layer's parameters will be placed. \n",
    "                    If None, uses the default device. Defaults to None (auto detection).\n",
    "            autograd: bool, A flag indicating whether to use PyTorch's autograd for gradient computation. \n",
    "                    If True, manual gradient tracking is disabled. Defaults to False.\n",
    "\n",
    "        Attributes:\n",
    "            self.dtype: type, The data type of the tensor values.\n",
    "            self.device: str | None, The device where the parameters are placed.\n",
    "            self.autograd: bool, Whether PyTorch's autograd is enabled for this layer.\n",
    "            self._parameters.weight: nn_Parameter, The weight parameter matrix (shape: [in_features, out_features]).\n",
    "            self._parameters.bias: nn_Parameter | optional, The optional bias vector (shape: [out_features]) if has_bias is True.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__(module_name=module_name, backend=backend, dtype=dtype, device=device, autograd=autograd)\n",
    "\n",
    "    def forward(self, pred: Tensor, target: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass, to calculate the loss.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"forward() is not implemented in the base loss class\")\n",
    "        \n",
    "    def backward(self, grad_output: Tensor | None = None) -> Tensor | None:\n",
    "        \"\"\"\n",
    "        Backward pass, to calculate the chained gradient with respect to parameters and return \n",
    "        the gradients with respect to inputs.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"backward() is not implemented in the base loss class\")\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return \"nn_Loss_BaseLoss(Abstract Loss Class).\"   \n",
    "\n",
    "\n",
    "# Implementation of Mean Square Error Loss\n",
    "class nn_Loss_MSE(nn_Loss_BaseLoss):\n",
    "    \"\"\"\n",
    "    Mean Squared Error Loss.\n",
    "\n",
    "    The Mean Squared Error (MSE) loss function quantifies the average squared difference \n",
    "    between predicted values and true values. It is widely used in regression tasks \n",
    "    and is defined as: \n",
    "\n",
    "    Formula: L = \\frac{1}{n} \\sum_{i=1}^{n} (y_{true,i} - y_{pred,i})^2\n",
    "\n",
    "    Where:\n",
    "        - $ n $ is the number of samples\n",
    "        - $ y_{true,i} $ is the true value for sample i\n",
    "        - $ y_{pred,i} $ is the predicted value for sample i\n",
    "\n",
    "    MSE penalizes larger errors more heavily due to squaring, making it sensitive \n",
    "    to outliers. It is differentiable and computationally efficient, but not suitable \n",
    "    for classification tasks where probabilistic outputs are required.\n",
    "\n",
    "    Formula: L = \\frac{1}{n} \\sum_{i=1}^{n} (y_{true,i} - y_{pred,i})^2\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    __attr__ = \"MML.nn_Loss_MSE\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 *,\n",
    "                 module_name: str = \"nn_Loss_MSE\",\n",
    "                 backend: Literal[\"torch\", \"numpy\"] = \"torch\",\n",
    "                 dtype: type | str = None,\n",
    "                 device: str | None = None,\n",
    "                 autograd: bool = False,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        A Mean Squared Error Loss Function.\n",
    "\n",
    "        Parameters:\n",
    "            module_name: str, The name of the module instance.\n",
    "            backend: Literal[\"torch\", \"numpy\"], The computational backend to use. Defaults to \"torch\".\n",
    "            dtype: type, The data type for the tensor values. Defaults to None (auto detection). \n",
    "                    For PyTorch, this corresponds to torch.dtype; for NumPy, it corresponds to np.dtype.\n",
    "            device: str | None, The target device (e.g., \"cpu\", \"cuda\") where the layer's parameters will be placed. \n",
    "                    If None, uses the default device. Defaults to None (auto detection).\n",
    "            autograd: bool, A flag indicating whether to use PyTorch's autograd for gradient computation. \n",
    "                    If True, manual gradient tracking is disabled. Defaults to False.\n",
    "\n",
    "        Attributes:\n",
    "            self.dtype: type, The data type of the tensor values.\n",
    "            self.device: str | None, The device where the parameters are placed.\n",
    "            self.autograd: bool, Whether PyTorch's autograd is enabled for this layer.\n",
    "            self._parameters.weight: nn_Parameter, The weight parameter matrix (shape: [in_features, out_features]).\n",
    "            self._parameters.bias: nn_Parameter | optional, The optional bias vector (shape: [out_features]) if has_bias is True.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__(module_name=module_name, backend=backend, dtype=dtype, device=device, autograd=autograd)\n",
    "\n",
    "    def forward(self, pred: Tensor, target: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Apply the Mean Squared Error (MSE) Loss function to evaluate the values predicted by the network.\n",
    "\n",
    "        This method computes the average squared difference between predicted values (`pred`) and actual values (`target`), \n",
    "        which is a common loss function for regression tasks. The output is a scalar value representing the loss, \n",
    "        averaged over all elements in the input tensors.\n",
    "\n",
    "        Args:\n",
    "            pred (Tensor): Predicted tensor containing model outputs.\n",
    "            target (Tensor): Target tensor containing ground truth values.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Scalar tensor representing the computed MSE loss.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If `pred` or `target` is not a valid MML.Tensor object, \n",
    "                        or if `pred` and `target` do not have the same shape.\n",
    "\n",
    "        Attributes:\n",
    "            self.numel (scalar): Total number of elements in the input tensors (product of tensor shapes).\n",
    "            self.mse (Tensor): Saved MSE Computed fpr backward computation.\n",
    "            self.pred (Tensor): Saved predicted tensor for backward computation.\n",
    "            self.target (Tensor): Saved target tensor for backward computation.\n",
    "            self.loss ([Tensor]): Save the computed loss for backpropagation uses.\n",
    "        \"\"\"\n",
    "\n",
    "        # Type check, pred and target must be an instance of Tensor\n",
    "        if isinstance(pred, Tensor) == False or isinstance(target, Tensor) == False:\n",
    "            raise ValueError(f\"In performing forward(), input `pred` or `target` must be in a MML `Tensor` format but you have {type(pred)} and {type(target)}\")\n",
    "\n",
    "        # Shape check, pred and target must have the same shape\n",
    "        if pred.shape != target.shape:\n",
    "            raise ValueError(f\"In performing forward(), input `pred` and `target` must have the same shape, but you have {pred.shape} and {target.shape}\")\n",
    "\n",
    "        # Compute the MSE loss\n",
    "        mse = ((pred - target) ** 2).mean()\n",
    "\n",
    "        # Save the pred, input, mse, and total number of elements for backward\n",
    "        self.__setattr__(\"numel\", np.array(pred.shape).prod())\n",
    "        self.__setattr__(\"mse\", mse)\n",
    "        self.__setattr__(\"pred\", pred)\n",
    "        self.__setattr__(\"target\", target)\n",
    "        self.__setattr__(\"loss\", [mse])\n",
    "\n",
    "        return mse\n",
    "\n",
    "    def backward(self, grad_output: Tensor | None = None) -> Tensor | None:\n",
    "        \"\"\"\n",
    "        Backward pass: compute gradients for weight, bias, and input.\n",
    "\n",
    "        This method performs the gradient computation for a module during backpropagation. \n",
    "        It calculates the gradients of the loss with respect to the weights, biases, and input tensor,\n",
    "        based on the provided `grad_output` (gradient from the next layer). The implementation\n",
    "        supports both PyTorch autograd and manual gradient calculation modes.\n",
    "\n",
    "        Args:\n",
    "            None: Since it is the first in calculating backward.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Gradient with respect to the input tensor, for recursive backward calculations in previous layers.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # If use autograd, pass; if manual mode, then calculate\n",
    "        if self.autograd == True:\n",
    "            self.loss[0].data.backward()\n",
    "            return None\n",
    "\n",
    "        # If grad_output is None (by default), assign it to 1\n",
    "        if grad_output is None:\n",
    "            grad_output = Tensor(1.0, backend=self.backend, dtype=self.dtype, device=self.device)\n",
    "        \n",
    "        # Else, it must be a scalar.\n",
    "        else:\n",
    "            if isinstance(grad_output, Tensor) == False:\n",
    "                raise ValueError(f\"In performing backward(), input `grad_output` must be in a MML `Tensor` format but you have {type(grad_output)}\")\n",
    "            if len(grad_output.shape) != 0:\n",
    "                raise ValueError(\"In performing backward(), input `grad_output` must be in a MML `Tensor` with a scalar stored in\")\n",
    "\n",
    "        # dL/dpred = 2*(pred - target) / N (N = number of elements)\n",
    "        grad_input = 2 * (self.pred - self.target) / self.numel\n",
    "        return grad_input * grad_output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"nn_Loss_MSE(Mean Square Error Loss).\"\n",
    "\n",
    "\n",
    "# Alias for nn_Loss_MSE\n",
    "MSE = nn_Loss_MSE\n",
    "\n",
    "\n",
    "# Implementation of Root Mean Square Error Loss\n",
    "class nn_Loss_RMSE(nn_Loss_BaseLoss):\n",
    "    \"\"\"\n",
    "    Root Mean Squared Error Loss.\n",
    "\n",
    "    The Root Mean Squared Error (RMSE) is the square root of the Mean Squared Error (MSE), \n",
    "    providing a measure of the magnitude of errors in the same units as the target variable. \n",
    "    It is widely used for evaluating regression models and is defined as:\n",
    "\n",
    "    Formula: L = \\sqrt{ \\frac{1}{n} \\sum_{i=1}^{n} (y_{true,i} - y_{pred,i})^2 }\n",
    "\n",
    "    Where:\n",
    "        - $ n $ is the number of samples\n",
    "        - $ y_{true,i} $ is the true value for sample i\n",
    "        - $ y_{pred,i} $ is the predicted value for sample i\n",
    "\n",
    "    RMSE addresses the interpretability limitation of MSE by scaling the error metric to the same units \n",
    "    as the target variable. It retains the sensitivity to outliers from squaring but offers a more intuitive \n",
    "    interpretation compared to MSE. Like MSE, it is differentiable and computationally efficient.\n",
    "\n",
    "    Formula: L = \\sqrt{ \\frac{1}{n} \\sum_{i=1}^{n} (y_{true,i} - y_{pred,i})^2 }\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    __attr__ = \"MML.nn_Loss_RMSE\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 *,\n",
    "                 module_name: str = \"nn_Loss_RMSE\",\n",
    "                 backend: Literal[\"torch\", \"numpy\"] = \"torch\",\n",
    "                 dtype: type | str = None,\n",
    "                 device: str | None = None,\n",
    "                 autograd: bool = False,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        A Root Mean Squared Error Loss Function.\n",
    "\n",
    "        Parameters:\n",
    "            module_name: str, The name of the module instance. Defaults to \"nn_Loss_RMSE\".\n",
    "            backend: Literal[\"torch\", \"numpy\"], The computational backend to use. Defaults to \"torch\".\n",
    "            dtype: type, The data type for the tensor values. Defaults to None (auto detection). \n",
    "                    For PyTorch, this corresponds to torch.dtype; for NumPy, it corresponds to np.dtype.\n",
    "            device: str | None, The target device (e.g., \"cpu\", \"cuda\") where the layer's parameters will be placed. \n",
    "                    If None, uses the default device. Defaults to None (auto detection).\n",
    "            autograd: bool, A flag indicating whether to use PyTorch's autograd for gradient computation. \n",
    "                    If True, manual gradient tracking is disabled. Defaults to False.\n",
    "\n",
    "        Attributes:\n",
    "            self.dtype: type, The data type of the tensor values.\n",
    "            self.device: str | None, The device where the parameters are placed.\n",
    "            self.autograd: bool, Whether PyTorch's autograd is enabled for this layer.\n",
    "            self._parameters.weight: nn_Parameter, The weight parameter matrix (shape: [in_features, out_features]).\n",
    "            self._parameters.bias: nn_Parameter | optional, The optional bias vector (shape: [out_features]) if has_bias is True.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__(module_name=module_name, backend=backend, dtype=dtype, device=device, autograd=autograd)\n",
    "\n",
    "    def forward(self, pred: Tensor, target: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Compute the Root Mean Squared Error (RMSE) between predictions and targets.\n",
    "\n",
    "        This method evaluates the square root of the average squared difference \n",
    "        between predicted values (`pred`) and actual values (`target`), providing \n",
    "        a loss measure in the same units as the original data.\n",
    "\n",
    "        Args:\n",
    "            pred (Tensor): Predicted tensor containing model outputs.\n",
    "            target (Tensor): Target tensor containing ground truth values.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Scalar tensor representing the computed MSE loss.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If `pred` or `target` is not a valid MML.Tensor object, \n",
    "                        or if `pred` and `target` do not have the same shape.\n",
    "\n",
    "        Attributes:\n",
    "            self.numel (scalar): Total number of elements in the input tensors (product of tensor shapes).\n",
    "            self.mse (Tensor): Saved MSE Computed fpr backward computation.\n",
    "            self.rmse (Tensor): Saved RMSE value for use in gradient calculation.\n",
    "            self.pred (Tensor): Saved predicted tensor for backward computation.\n",
    "            self.target (Tensor): Saved target tensor for backward computation.\n",
    "            self.loss (Tensor): Save the computed loss for backpropagation uses.\n",
    "        \"\"\"\n",
    "\n",
    "        # Type check, pred and target must be an instance of Tensor\n",
    "        if isinstance(pred, Tensor) == False or isinstance(target, Tensor) == False:\n",
    "            raise ValueError(f\"In performing forward(), input `pred` or `target` must be in a MML `Tensor` format but you have {type(pred)} and {type(target)}\")\n",
    "\n",
    "        # Shape check, pred and target must have the same shape\n",
    "        if pred.shape != target.shape:\n",
    "            raise ValueError(f\"In performing forward(), input `pred` and `target` must have the same shape, but you have {pred.shape} and {target.shape}\")\n",
    "\n",
    "        # Compute MSE and RMSE in Tensor\n",
    "        mse = ((pred - target) ** 2).mean()\n",
    "        rmse = mse ** 0.5\n",
    "\n",
    "        # Save the pred, input, mse, rmse, and total number of elements for backward\n",
    "        self.__setattr__(\"numel\", np.array(pred.shape).prod())\n",
    "        self.__setattr__(\"mse\", mse)\n",
    "        self.__setattr__(\"rmse\", rmse)\n",
    "        self.__setattr__(\"pred\", pred)\n",
    "        self.__setattr__(\"target\", target)\n",
    "        self.__setattr__(\"loss\", [rmse])\n",
    "\n",
    "        return rmse\n",
    "\n",
    "    def backward(self, grad_output: Tensor | None = None) -> Tensor | None:\n",
    "        \"\"\"\n",
    "        Backward pass: compute gradients for weight, bias, and input.\n",
    "\n",
    "        This method performs the gradient computation for a module during backpropagation. \n",
    "        It calculates the gradients of the loss with respect to the weights, biases, and input tensor,\n",
    "        based on the provided `grad_output` (gradient from the next layer). The implementation\n",
    "        supports both PyTorch autograd and manual gradient calculation modes.\n",
    "\n",
    "        Args:\n",
    "            None: Since it is the first in calculating backward.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Gradient with respect to the input tensor, for recursive backward calculations in previous layers.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # If use autograd, pass; if manual mode, then calculate\n",
    "        if self.autograd == True:\n",
    "            self.loss[0].data.backward()\n",
    "            return None\n",
    "        \n",
    "        # If grad_output is None (by default), assign it to 1\n",
    "        if grad_output is None:\n",
    "            grad_output = Tensor(1.0, backend=self.backend, dtype=self.dtype, device=self.device)\n",
    "        \n",
    "        # Else, it must be a scalar.\n",
    "        else:\n",
    "            if isinstance(grad_output, Tensor) == False:\n",
    "                raise ValueError(f\"In performing backward(), input `grad_output` must be in a MML `Tensor` format but you have {type(grad_output)}\")\n",
    "            if len(grad_output.shape) != 0:\n",
    "                raise ValueError(\"In performing backward(), input `grad_output` must be in a MML `Tensor` with a scalar stored in\")\n",
    "\n",
    "        # dL/dpred = (pred - target) / (N * RMSE)\n",
    "        grad_input = (self.pred - self.target) / (self.rmse * self.numel)\n",
    "        return grad_input * grad_output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"nn_Loss_RMSE(Root Mean Square Error Loss).\"\n",
    "\n",
    "\n",
    "# Alias for nn_Loss_RMSE\n",
    "RMSE = nn_Loss_RMSE\n",
    "\n",
    "\n",
    "# Implementation of Mean Absolute Error Loss\n",
    "class nn_Loss_MAE(nn_Loss_BaseLoss):\n",
    "    \"\"\"\n",
    "    Mean Absolute Error Loss.\n",
    "\n",
    "    The Mean Absolute Error (MAE) loss function quantifies the average absolute \n",
    "    difference between predicted values and true values. It is widely used in \n",
    "    regression tasks and is defined as: \n",
    "\n",
    "    Formula: L = \\frac{1}{n} \\sum_{i=1}^{n} |y_{true,i} - y_{pred,i}|\n",
    "\n",
    "    Where:\n",
    "        - $ n $ is the number of samples\n",
    "        - $ y_{true,i} $ is the true value for sample i\n",
    "        - $ y_{pred,i} $ is the predicted value for sample i\n",
    "\n",
    "    MAE is less sensitive to outliers compared to Mean Squared Error (MSE), as it \n",
    "    uses absolute differences rather than squared differences. However, it is not \n",
    "    differentiable at zero, which can affect gradient-based optimization methods. \n",
    "    It provides an interpretable measure of error in the same units as the target \n",
    "    variable.\n",
    "\n",
    "    Formula: L = \\frac{1}{n} \\sum_{i=1}^{n} |y_{true,i} - y_{pred,i}|\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    __attr__ = \"MML.nn_Loss_MAE\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 *,\n",
    "                 module_name: str = \"nn_Loss_MAE\",\n",
    "                 backend: Literal[\"torch\", \"numpy\"] = \"torch\",\n",
    "                 dtype: type | str = None,\n",
    "                 device: str | None = None,\n",
    "                 autograd: bool = False,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        A Mean Absolute Error Loss Function.\n",
    "\n",
    "        Parameters:\n",
    "            module_name: str, The name of the module instance. Defaults to \"nn_Loss_MAE\".\n",
    "            backend: Literal[\"torch\", \"numpy\"], The computational backend to use. Defaults to \"torch\".\n",
    "            dtype: type, The data type for the tensor values. Defaults to None (auto detection). \n",
    "                    For PyTorch, this corresponds to torch.dtype; for NumPy, it corresponds to np.dtype.\n",
    "            device: str | None, The target device (e.g., \"cpu\", \"cuda\") where the layer's parameters will be placed. \n",
    "                    If None, uses the default device. Defaults to None (auto detection).\n",
    "            autograd: bool, A flag indicating whether to use PyTorch's autograd for gradient computation. \n",
    "                    If True, manual gradient tracking is disabled. Defaults to False.\n",
    "\n",
    "        Attributes:\n",
    "            self.dtype: type, The data type of the tensor values.\n",
    "            self.device: str | None, The device where the parameters are placed.\n",
    "            self.autograd: bool, Whether PyTorch's autograd is enabled for this layer.\n",
    "            self._parameters.weight: nn_Parameter, The weight parameter matrix (shape: [in_features, out_features]).\n",
    "            self._parameters.bias: nn_Parameter | optional, The optional bias vector (shape: [out_features]) if has_bias is True.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__(module_name=module_name, backend=backend, dtype=dtype, device=device, autograd=autograd)\n",
    "\n",
    "    def forward(self, pred: Tensor, target: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Compute the Mean Absolute Error (MAE) between predictions and targets.\n",
    "\n",
    "        This method evaluates the average absolute difference \n",
    "        between predicted values (`pred`) and actual values (`target`), \n",
    "        offering a robust loss measure less sensitive to outliers.\n",
    "\n",
    "        Args:\n",
    "            pred (Tensor): Predicted tensor containing model outputs.\n",
    "            target (Tensor): Target tensor containing ground truth values.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Scalar tensor representing the computed MSE loss.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If `pred` or `target` is not a valid MML.Tensor object, \n",
    "                        or if `pred` and `target` do not have the same shape.\n",
    "\n",
    "        Attributes:\n",
    "            self.numel (scalar): Total number of elements in the input tensors (product of tensor shapes).\n",
    "            self.mae (Tensor): Saved Mean Absolute Error Tensor value for reference.\n",
    "            self.pred (Tensor): Saved predicted tensor for backward computation.\n",
    "            self.target (Tensor): Saved target tensor for backward computation.\n",
    "            self.loss (Tensor): Save the computed loss for backpropagation uses.\n",
    "        \"\"\"\n",
    "\n",
    "        # Type check, pred and target must be an instance of Tensor\n",
    "        if isinstance(pred, Tensor) == False or isinstance(target, Tensor) == False:\n",
    "            raise ValueError(f\"In performing forward(), input `pred` or `target` must be in a MML `Tensor` format but you have {type(pred)} and {type(target)}\")\n",
    "\n",
    "        # Shape check, pred and target must have the same shape\n",
    "        if pred.shape != target.shape:\n",
    "            raise ValueError(f\"In performing forward(), input `pred` and `target` must have the same shape, but you have {pred.shape} and {target.shape}\")\n",
    "\n",
    "        # Compute MAE in Tensor\n",
    "        abs_diff = (pred - target).abs()\n",
    "        mae = abs_diff.mean()\n",
    "\n",
    "        # Save the pred, input, mse, rmse, and total number of elements for backward\n",
    "        self.__setattr__(\"numel\", np.array(pred.shape).prod())\n",
    "        self.__setattr__(\"mae\", mae)\n",
    "        self.__setattr__(\"pred\", pred)\n",
    "        self.__setattr__(\"target\", target)\n",
    "        self.__setattr__(\"loss\", [mae])\n",
    "\n",
    "        return mae\n",
    "\n",
    "    def backward(self, grad_output: Tensor | None = None) -> Tensor | None:\n",
    "        \"\"\"\n",
    "        Backward pass: compute gradients for weight, bias, and input.\n",
    "\n",
    "        This method performs the gradient computation for a module during backpropagation. \n",
    "        It calculates the gradients of the loss with respect to the weights, biases, and input tensor,\n",
    "        based on the provided `grad_output` (gradient from the next layer). The implementation\n",
    "        supports both PyTorch autograd and manual gradient calculation modes.\n",
    "\n",
    "        Args:\n",
    "            None: Since it is the first in calculating backward.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Gradient with respect to the input tensor, for recursive backward calculations in previous layers.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # If use autograd, pass; if manual mode, then calculate\n",
    "        if self.autograd == True:\n",
    "            self.loss[0].data.backward()\n",
    "            return None\n",
    "\n",
    "        # If grad_output is None (by default), assign it to 1\n",
    "        if grad_output is None:\n",
    "            grad_output = Tensor(1.0, backend=self.backend, dtype=self.dtype, device=self.device)\n",
    "        \n",
    "        # Else, it must be a scalar.\n",
    "        else:\n",
    "            if isinstance(grad_output, Tensor) == False:\n",
    "                raise ValueError(f\"In performing backward(), input `grad_output` must be in a MML `Tensor` format but you have {type(grad_output)}\")\n",
    "            if len(grad_output.shape) != 0:\n",
    "                raise ValueError(\"In performing backward(), input `grad_output` must be in a MML `Tensor` with a scalar stored in\")\n",
    "\n",
    "        # dL/dpred = (pred - target).sign() / N\n",
    "        grad_input = (self.pred - self.target).sign() / self.numel\n",
    "        return grad_input * grad_output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"nn_Loss_MAE(Mean Absolute Error Loss).\"\n",
    "\n",
    "\n",
    "# Alias for nn_Loss_MAE\n",
    "MAE = nn_Loss_MAE\n",
    "\n",
    "\n",
    "# Implementation of Binary Cross Entropy Loss\n",
    "class nn_Loss_BinaryCrossEntropy(nn_Loss_BaseLoss):\n",
    "    \"\"\"\n",
    "    Binary Cross-Entropy for predictions in [0,1] and binary targets.\n",
    "\n",
    "    The Binary Cross-Entropy loss measures the difference between predicted \n",
    "    probabilities (in [0,1]) and true binary labels (0 or 1). It is widely used \n",
    "    in binary classification tasks and is defined as: \n",
    "\n",
    "    Formula: L = -\\frac{1}{n} \\sum_{i=1}^{n} [y_{true,i} \\log(p_i) + (1 - y_{true,i}) \\log(1 - p_i)]\n",
    "\n",
    "    Where:\n",
    "        - $ n $ is the number of samples\n",
    "        - $ y_{true,i} $ is the true binary label for sample i (0 or 1)\n",
    "        - $ p_i $ is the predicted probability for sample i (in [0,1])\n",
    "\n",
    "    This loss function penalizes incorrect predictions more heavily when the model \n",
    "    is confident but wrong. It is differentiable and suitable for optimization via \n",
    "    gradient-based methods. However, it requires care to avoid numerical instability \n",
    "    (e.g., adding a small epsilon to probabilities near 0 or 1).\n",
    "\n",
    "    Formula: L = -\\frac{1}{n} \\sum_{i=1}^{n} [y_{true,i} \\log(p_i) + (1 - y_{true,i}) \\log(1 - p_i)]\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    __attr__ = \"MML.nn_Loss_BinaryCrossEntropy\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 eps: float = 1e-5,\n",
    "                 *,\n",
    "                 module_name: str = \"nn_Loss_BinaryCrossEntropy\",\n",
    "                 backend: Literal[\"torch\", \"numpy\"] = \"torch\",\n",
    "                 dtype: type | str = None,\n",
    "                 device: str | None = None,\n",
    "                 autograd: bool = False,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        A Binary Cross Entropy Loss Function for binary classification.\n",
    "\n",
    "        Parameters:\n",
    "            eps: float, The epsilon amount applied to clip() to avoid log(0).\n",
    "            module_name: str, The name of the module instance. Defaults to \"nn_Loss_BinaryCrossEntropy\".\n",
    "            backend: Literal[\"torch\", \"numpy\"], The computational backend to use. Defaults to \"torch\".\n",
    "            dtype: type, The data type for the tensor values. Defaults to None (auto detection). \n",
    "                    For PyTorch, this corresponds to torch.dtype; for NumPy, it corresponds to np.dtype.\n",
    "            device: str | None, The target device (e.g., \"cpu\", \"cuda\") where the layer's parameters will be placed. \n",
    "                    If None, uses the default device. Defaults to None (auto detection).\n",
    "            autograd: bool, A flag indicating whether to use PyTorch's autograd for gradient computation. \n",
    "                    If True, manual gradient tracking is disabled. Defaults to False.\n",
    "\n",
    "        Attributes:\n",
    "            self.eps: float, The epsilon value applied.\n",
    "            self.dtype: type, The data type of the tensor values.\n",
    "            self.device: str | None, The device where the parameters are placed.\n",
    "            self.autograd: bool, Whether PyTorch's autograd is enabled for this layer.\n",
    "            self._parameters.weight: nn_Parameter, The weight parameter matrix (shape: [in_features, out_features]).\n",
    "            self._parameters.bias: nn_Parameter | optional, The optional bias vector (shape: [out_features]) if has_bias is True.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__(module_name=module_name, backend=backend, dtype=dtype, device=device, autograd=autograd)\n",
    "\n",
    "        # Record the eps value\n",
    "        self.__setattr__(\"eps\", eps)\n",
    "\n",
    "    def forward(self, pred: Tensor, target: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Compute the Binary Cross Entropy between predictions and targets.\n",
    "\n",
    "        Args:\n",
    "            pred (Tensor): Predicted tensor containing model outputs.\n",
    "            target (Tensor): Target tensor containing ground truth values.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Scalar tensor representing the computed MSE loss.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If `pred` or `target` is not a valid MML.Tensor object, \n",
    "                        or if `pred` and `target` do not have the same shape.\n",
    "\n",
    "        Attributes:\n",
    "            self.n_classes (scalar): The number of classes to be classified.\n",
    "            self.numel (scalar): Total number of elements in the input tensors (product of tensor shapes).\n",
    "            self.pred (Tensor): Saved predicted tensor for backward computation.\n",
    "            self.target (Tensor): Saved target tensor for backward computation.\n",
    "            self.loss (Tensor): Save the computed loss for backpropagation uses.\n",
    "        \"\"\"\n",
    "\n",
    "        # Type check, pred and target must be an instance of Tensor\n",
    "        if isinstance(pred, Tensor) == False or isinstance(target, Tensor) == False:\n",
    "            raise ValueError(f\"In performing forward(), input `pred` or `target` must be in a MML `Tensor` format but you have {type(pred)} and {type(target)}\")\n",
    "\n",
    "        # Shape check, pred and target must have the same shape\n",
    "        if pred.shape != target.shape:\n",
    "            raise ValueError(f\"In performing forward(), input `pred` and `target` must have the same shape, but you have {pred.shape} and {target.shape}\")\n",
    "\n",
    "        # n_classes check, must only have 1 dimension (DOES NOT SUPPORT ONE-HOT)\n",
    "        if len(pred.shape) == 2:\n",
    "            if pred.shape[1] != 1:\n",
    "                raise ValueError(f\"In performing forward(), input `pred` and `target` must be 1 dimensional or 2 dimension with the 2nd one be 1, but you have {pred.shape} and {target.shape}\")\n",
    "\n",
    "        # Compute clipped predictions to avoid log0\n",
    "        clipped_pred = pred.clip(self.eps, 1-self.eps)\n",
    "\n",
    "        # Compute binary cross-entropy loss\n",
    "        loss = - (target * clipped_pred.log() +\n",
    "                  (1 - target) * (1 - clipped_pred).log())\n",
    "        loss = loss.mean()\n",
    "\n",
    "        # Save the pred, input etc for backward\n",
    "        self.__setattr__(\"n_classes\", 1)\n",
    "        self.__setattr__(\"numel\", np.array(clipped_pred.shape).prod())\n",
    "        self.__setattr__(\"pred\", clipped_pred)\n",
    "        self.__setattr__(\"target\", target)\n",
    "        self.__setattr__(\"loss\", [loss])\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def backward(self, grad_output: Tensor | None = None) -> Tensor | None:\n",
    "        \"\"\"\n",
    "        Backward pass: compute gradients for weight, bias, and input.\n",
    "\n",
    "        This method performs the gradient computation for a module during backpropagation. \n",
    "        It calculates the gradients of the loss with respect to the weights, biases, and input tensor,\n",
    "        based on the provided `grad_output` (gradient from the next layer). The implementation\n",
    "        supports both PyTorch autograd and manual gradient calculation modes.\n",
    "\n",
    "        Args:\n",
    "            None: Since it is the first in calculating backward.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Gradient with respect to the input tensor, for recursive backward calculations in previous layers.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # If use autograd, pass; if manual mode, then calculate\n",
    "        if self.autograd == True:\n",
    "            self.loss[0].data.backward()\n",
    "            return None\n",
    "        \n",
    "        # If grad_output is None (by default), assign it to 1\n",
    "        if grad_output is None:\n",
    "            grad_output = Tensor(1.0, backend=self.backend, dtype=self.dtype, device=self.device)\n",
    "        \n",
    "        # Else, it must be a scalar.\n",
    "        else:\n",
    "            if isinstance(grad_output, Tensor) == False:\n",
    "                raise ValueError(f\"In performing backward(), input `grad_output` must be in a MML `Tensor` format but you have {type(grad_output)}\")\n",
    "            if len(grad_output.shape) != 0:\n",
    "                raise ValueError(\"In performing backward(), input `grad_output` must be in a MML `Tensor` with a scalar stored in\")\n",
    "\n",
    "        # dL/dpred = -(target/pred - (1-target)/(1-pred)) / N\n",
    "        grad_input = - (self.target / self.pred) + \\\n",
    "            ((1 - self.target) / (1 - self.pred))\n",
    "        grad_input = grad_input / self.numel\n",
    "        return grad_input * grad_output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"nn_Loss_BinaryCrossEntropy(Binary Cross Entropy Loss).\"\n",
    "\n",
    "\n",
    "# Alias for nn_Loss_BinaryCrossEntropy\n",
    "BinaryCrossEntropy = nn_Loss_BinaryCrossEntropy\n",
    "\n",
    "\n",
    "# Implementation of Multi Cross Entropy Loss\n",
    "class nn_Loss_MultiCrossEntropy(nn_Loss_BaseLoss):\n",
    "    \"\"\"\n",
    "    Multi Cross-Entropy for predictions in one_hot and probabilities targets.\n",
    "\n",
    "    The Multi-Class Cross-Entropy loss measures the difference between predicted \n",
    "    probability distributions (for multiple classes) and true one-hot encoded labels. \n",
    "    It is widely used in multi-class classification tasks and is defined as: \n",
    "\n",
    "    Formula: L = -\\frac{1}{n} \\sum_{i=1}^{n} \\sum_{c=1}^{C} y_{true,i,c} \\log(p_{i,c})\n",
    "\n",
    "    Where:\n",
    "        - $ n $ is the number of samples\n",
    "        - $ C $ is the number of classes\n",
    "        - $ y_{true,i,c} $ is the one-hot encoded true label for sample i (1 if class c is correct, 0 otherwise)\n",
    "        - $ p_{i,c} $ is the predicted probability for sample i belonging to class c\n",
    "\n",
    "    This loss function penalizes incorrect predictions by measuring the discrepancy between \n",
    "    the true distribution (one-hot) and the predicted distribution. It is differentiable and \n",
    "    suitable for optimization via gradient-based methods. However, numerical stability \n",
    "    must be ensured (e.g., adding a small epsilon to probabilities near 0 or 1) to avoid log(0).\n",
    "\n",
    "    Formula: L = -\\frac{1}{n} \\sum_{i=1}^{n} \\sum_{c=1}^{C} y_{true,i,c} \\log(p_{i,c})\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    __attr__ = \"MML.nn_Loss_MultiCrossEntropy\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 eps: float = 1e-5,\n",
    "                 raw_logits: bool = True,\n",
    "                 *,\n",
    "                 module_name: str = \"nn_Loss_MultiCrossEntropy\",\n",
    "                 backend: Literal[\"torch\", \"numpy\"] = \"torch\",\n",
    "                 dtype: type | str = None,\n",
    "                 device: str | None = None,\n",
    "                 autograd: bool = False,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        A Binary Cross Entropy Loss Function for multi classification.\n",
    "\n",
    "        Parameters:\n",
    "            eps: float, The epsilon amount applied to clip() to avoid log(0).\n",
    "            raw_logits: bool, If True, then not applied softmax, or applied softmax.\n",
    "            module_name: str, The name of the module instance. Defaults to \"nn_Loss_MultiCrossEntropy\".\n",
    "            backend: Literal[\"torch\", \"numpy\"], The computational backend to use. Defaults to \"torch\".\n",
    "            dtype: type, The data type for the tensor values. Defaults to None (auto detection). \n",
    "                    For PyTorch, this corresponds to torch.dtype; for NumPy, it corresponds to np.dtype.\n",
    "            device: str | None, The target device (e.g., \"cpu\", \"cuda\") where the layer's parameters will be placed. \n",
    "                    If None, uses the default device. Defaults to None (auto detection).\n",
    "            autograd: bool, A flag indicating whether to use PyTorch's autograd for gradient computation. \n",
    "                    If True, manual gradient tracking is disabled. Defaults to False.\n",
    "\n",
    "        Attributes:\n",
    "            self.eps: float, The epsilon value applied.\n",
    "            self.raw_logits: bool, Whether raw logits or not (not applied softmax or not).\n",
    "            self.dtype: type, The data type of the tensor values.\n",
    "            self.device: str | None, The device where the parameters are placed.\n",
    "            self.autograd: bool, Whether PyTorch's autograd is enabled for this layer.\n",
    "            self._parameters.weight: nn_Parameter, The weight parameter matrix (shape: [in_features, out_features]).\n",
    "            self._parameters.bias: nn_Parameter | optional, The optional bias vector (shape: [out_features]) if has_bias is True.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__(module_name=module_name, backend=backend, dtype=dtype, device=device, autograd=autograd)\n",
    "\n",
    "        # Record the eps value\n",
    "        self.__setattr__(\"eps\", eps)\n",
    "\n",
    "        # Record the status whether it is raw logits\n",
    "        self.__setattr__(\"raw_logits\", raw_logits)\n",
    "\n",
    "    def forward(self, pred: Tensor, target: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Compute the Multi-class Cross Entropy between predictions and targets.\n",
    "\n",
    "        Args:\n",
    "            pred (Tensor): Predicted tensor containing model outputs.\n",
    "            target (Tensor): Target tensor containing ground truth values.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Scalar tensor representing the computed MSE loss.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If `pred` or `target` is not a valid MML.Tensor object, \n",
    "                        or if `pred` and `target` do not have the same shape.\n",
    "\n",
    "        Attributes:\n",
    "            self.n_classes (scalar): The number of classes to be classified.\n",
    "            self.n_samples (scalar): Total number of samples in the data.\n",
    "            self.pred (Tensor): Saved predicted tensor for backward computation.\n",
    "            self.pred_logprobs (Tensor): Saved log probabilities tensor for backward computation.\n",
    "            self.target (Tensor): Saved target tensor for backward computation.\n",
    "            self.target_multiclass (Tensor): Save target but in multiclass form for backward computation.\n",
    "            self.loss (Tensor): Save the computed loss for backpropagation uses.\n",
    "        \"\"\"\n",
    "\n",
    "        # Type check, pred and target must be an instance of Tensor\n",
    "        if isinstance(pred, Tensor) == False or isinstance(target, Tensor) == False:\n",
    "            raise ValueError(f\"In performing forward(), input `pred` or `target` must be in a MML `Tensor` format but you have {type(pred)} and {type(target)}\")\n",
    "\n",
    "        # Shape check, pred and target must have the same shape\n",
    "        if pred.shape != target.shape:\n",
    "            raise ValueError(f\"In performing forward(), input `pred` and `target` must have the same shape, but you have {pred.shape} and {target.shape}\")\n",
    "\n",
    "        # n_classes check, must have 2 dim and greater than 1 2nd dim\n",
    "        if len(pred.shape) != 2:\n",
    "            raise ValueError(f\"In performing forward(), input `pred` and `target` must be 2 dimensional, but you have {pred.shape} and {target.shape}\")\n",
    "        if pred.shape[1] <= 1:\n",
    "            raise ValueError(f\"In performing forward(), input `pred` and `target` must have greater than 1 outputs in Multi Cross Entropy, but you have {pred.shape} and {target.shape}\")\n",
    "\n",
    "        # Input shape: (N, C) where C = number of classes\n",
    "        # Target shape: (N, C) with one-hot encoded\n",
    "        if self.raw_logits == True:\n",
    "            # Compute log-softmax for numerical stability (log probabilities)\n",
    "            pred_log_probs = pred.softmax(axis=1).clip(self.eps).log()\n",
    "        else:\n",
    "            # Input is probabilities; take log\n",
    "            pred_log_probs = pred.clip(self.eps).log()\n",
    "\n",
    "        # Turn the true one_hot result into a multi-class result (N, 1)\n",
    "        target_multiclass = self._to_labels(target)\n",
    "\n",
    "        # Gather the log probs along axis 1 by true indices\n",
    "        gathered_log_probs = pred_log_probs.gather_along(\n",
    "            pred_log_probs, axis=1, index=target_multiclass)\n",
    "\n",
    "        # Compute negative log-likelihood loss for each sample and take mean\n",
    "        losses = -gathered_log_probs\n",
    "        loss = losses.mean()\n",
    "\n",
    "        # Save the pred, input etc for backward\n",
    "        self.__setattr__(\"n_classes\", pred.shape[1])\n",
    "        self.__setattr__(\"n_samples\", pred.shape[0])\n",
    "        self.__setattr__(\"pred\", pred)\n",
    "        self.__setattr__(\"pred_logprobs\", pred_log_probs)\n",
    "        self.__setattr__(\"target\", target)\n",
    "        self.__setattr__(\"target_multiclass\", target_multiclass)\n",
    "        self.__setattr__(\"loss\", [loss])\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def backward(self, grad_output: Tensor | None = None) -> Tensor | None:\n",
    "        \"\"\"\n",
    "        Backward pass: compute gradients for weight, bias, and input.\n",
    "\n",
    "        This method performs the gradient computation for a module during backpropagation. \n",
    "        It calculates the gradients of the loss with respect to the weights, biases, and input tensor,\n",
    "        based on the provided `grad_output` (gradient from the next layer). The implementation\n",
    "        supports both PyTorch autograd and manual gradient calculation modes.\n",
    "\n",
    "        Args:\n",
    "            None: Since it is the first in calculating backward.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Gradient with respect to the input tensor, for recursive backward calculations in previous layers.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # If use autograd, pass; if manual mode, then calculate\n",
    "        if self.autograd == True:\n",
    "            self.loss[0].data.backward()\n",
    "            return None\n",
    "        \n",
    "        # If grad_output is None (by default), assign it to 1\n",
    "        if grad_output is None:\n",
    "            grad_output = Tensor(1.0, backend=self.backend, dtype=self.dtype, device=self.device)\n",
    "        \n",
    "        # Else, it must be a scalar.\n",
    "        else:\n",
    "            if isinstance(grad_output, Tensor) == False:\n",
    "                raise ValueError(f\"In performing backward(), input `grad_output` must be in a MML `Tensor` format but you have {type(grad_output)}\")\n",
    "            if len(grad_output.shape) != 0:\n",
    "                raise ValueError(\"In performing backward(), input `grad_output` must be in a MML `Tensor` with a scalar stored in\")\n",
    "\n",
    "        # Initialize grad_input with same shape as predictions\n",
    "        grad_input = self.pred_logprobs.to_zeros()\n",
    "\n",
    "        # Calculate the backward gradients\n",
    "        if self.raw_logits == True:\n",
    "            # grad = (softmax_prob - one_hot(target)) / N\n",
    "            grad_input = self.pred_logprobs.exp() - self.target\n",
    "            grad_input /= self.n_samples\n",
    "        else:\n",
    "            # For probabilities: grad = -1/p_target for target class, 0 for others, divided by N\n",
    "            grad_input = -self.target / (self.pred_logprobs.exp() + self.eps)\n",
    "            grad_input /= self.n_samples\n",
    "\n",
    "        return grad_input * grad_output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"nn_Loss_MultiCrossEntropy(Multi Cross Entropy Loss, with n_classes = {self.n_classes}).\"\n",
    "\n",
    "\n",
    "# Alias for nn_Loss_BinaryCrossEntropy\n",
    "MultiCrossEntropy = nn_Loss_MultiCrossEntropy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These Optimizers are self implemented and open-sourced\n",
    "# Available at https://github.com/dof-studio/MML/\n",
    "# By Nathmath Huang (bh2821)\n",
    "# License: Apache License Version 2.0\n",
    "\n",
    "# Base Class for All nn Optimizers\n",
    "class nn_Optm_BaseOptimizer(nn_Base):\n",
    "    \"\"\"\n",
    "    Base optimizer class.\n",
    "    \n",
    "    Any inherited optimizer takes all of the parameters as a reference and\n",
    "    use a method connected with gradients to update trainable parameters.\n",
    "    A typical __init__ at least requires a list/dict of parameters or a nn_Module.\n",
    "    \"\"\"\n",
    "    \n",
    "    __attr__ = \"MML.nn_Optm_BaseOptimizer\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 params: List[nn_Parameter] | Dict[Any, nn_Parameter] | nn_Module, \n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize an optimizer. Call this and pass in parameters as\n",
    "        a list/dict of nn_Parameter or a nn_Module which contains all parameters and submodules.\n",
    "\n",
    "        Parameters:\n",
    "            params: List[nn_Parameter] | Dict[Any, nn_Parameter] | nn_Module, if directly gives a list of parameters,\n",
    "                    which we believe generates by .parameters(), we record them as a list;\n",
    "                    If directly gives an nn_Module, we accepts and record all of the parameters as a list;\n",
    "\n",
    "        Attributes:\n",
    "            self.params: a List of nn_Parameters which may have gradients and needs to be updated.\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # Record the unfiltered list of parameters\n",
    "        if isinstance(params, list):\n",
    "            self.params = params\n",
    "        elif isinstance(params, dict):\n",
    "            self.params = params.values()\n",
    "        elif isinstance(params, nn_Module):\n",
    "            self.params = params.parameters()\n",
    "        else:\n",
    "            raise ValueError(f\"`params` for an optimizer can either be a list/dict of nn_Parameter or a nn_module, but you have {type(params)}\")\n",
    "            \n",
    "        # Filter all parameters having gradients\n",
    "        self.params = [p for p in self.params if p.requires_grad == True]\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"\n",
    "        Update all trainable parameters in one step (override in subclasses).\n",
    "        \n",
    "        Returns:\n",
    "            self\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"step() is not implemented in the base optimizer class\")\n",
    "\n",
    "    def zero_grad(self):\n",
    "        \"\"\"\n",
    "        Reset gradients of all parameters to zero.\n",
    "        \n",
    "        Returns:\n",
    "            self\n",
    "        \"\"\"\n",
    "        for p in self.params:\n",
    "            p.zero_grad()\n",
    "        return self\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"nn_Optm_BaseOptimizer(with {len(self.params)} trainable parameters).\"\n",
    "\n",
    "\n",
    "# Implementation of Stochastic Gradient Descent Optimzier\n",
    "class nn_Optm_SGD(nn_Optm_BaseOptimizer):\n",
    "    \"\"\"\n",
    "    Stochastic Gradient Descent optimizer with optional momentum.\n",
    "    \n",
    "    This optimizer uses stochastic gradient descent (SGD) to update model parameters \n",
    "    by minimizing a loss function. It supports optional momentum for faster convergence \n",
    "    and better stability in optimization.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    __attr__ = \"MML.nn_Optm_SGD\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 params: List[nn_Parameter] | Dict[Any, nn_Parameter] | nn_Module, \n",
    "                 *,\n",
    "                 lr: float = 0.01, \n",
    "                 momentum: float = 0.9,\n",
    "                 **kwargs):\n",
    "        \n",
    "        \"\"\"\n",
    "        Initialize a SGD optimizer with momentum. Call this and pass in parameters as\n",
    "        a list/dict of nn_Parameter or a nn_Module which contains all parameters and submodules.\n",
    "\n",
    "        Parameters:\n",
    "            params: List[nn_Parameter] | Dict[Any, nn_Parameter] | nn_Module, if directly gives a list of parameters,\n",
    "                    which we believe generates by .parameters(), we record them as a list;\n",
    "                    If directly gives an nn_Module, we accepts and record all of the parameters as a list;\n",
    "            lr: float, learning rate, the step size functioned on the gradients to update parameters;\n",
    "            momentum: float, momentum factor for acceleration in convergence.\n",
    "\n",
    "        Attributes:\n",
    "            self.params: a List of nn_Parameters which may have gradients and needs to be updated.\n",
    "            self.n: int, the total number of steps performed.\n",
    "            self.lr: float, the learning rate as a plain float.\n",
    "            self.momentum: float, the momentum rate as a plain float.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Call the base initialization to initialize all trainable parameters\n",
    "        super().__init__(params, **kwargs)\n",
    "        \n",
    "        # Record the SGD parameters\n",
    "        self.n = 0\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        \n",
    "        # Initialize velocity buffers if momentum is used\n",
    "        if momentum != 0:\n",
    "            self.velocities = [p.data.to_zeros() for p in self.params]\n",
    "        else:\n",
    "            self.velocities = None\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"\n",
    "        Update all trainable parameters in one step by SGD and momentum method.\n",
    "        \n",
    "        Returns:\n",
    "            self\n",
    "        \"\"\"\n",
    "        # For compatibility, n adds at the begining\n",
    "        self.n += 1\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Iterate over trainable parameters\n",
    "            for i, p in enumerate(self.params):\n",
    "    \n",
    "                # If uses momentum method\n",
    "                if self.momentum != 0:\n",
    "                    # Momentum update: v = momentum * v - lr * grad and update parameter: w = w + v\n",
    "                    if p.autograd == False:\n",
    "                        self.velocities[i] = self.velocities[i] * self.momentum - self.lr * p.grad\n",
    "                        p.data += self.velocities[i]\n",
    "                    else:\n",
    "                        self.velocities[i].data = self.velocities[i].data * self.momentum - self.lr * p.data.data.grad\n",
    "                        p.data.data.data += self.velocities[i].data\n",
    "                        \n",
    "                # Use plain vanilla SGD\n",
    "                else:\n",
    "                    # Vanilla SGD: w = w - lr * grad\n",
    "                    if p.autograd == False:\n",
    "                        p.data -= self.lr * p.grad\n",
    "                    else:\n",
    "                        p.data.data.data -= self.lr * p.data.data.grad\n",
    "                        \n",
    "        return self\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"nn_Optm_SGD(with {len(self.params)} trainable parameters).\"\n",
    "\n",
    "\n",
    "# Alias for nn_Optm_SGD\n",
    "SGD = nn_Optm_SGD\n",
    "\n",
    "\n",
    "# Implementation of Adaptive Momentum Optimzier\n",
    "class nn_Optm_Adam(nn_Optm_BaseOptimizer):\n",
    "    \"\"\"\n",
    "    Adaptive Momentum Estimate (Adam) Optimizer.\n",
    "\n",
    "    Adam is an optimization algorithm that combines the advantages of both RMSProp and SGD. \n",
    "    It maintains two moving averages: one for the gradient (momentum) and another for the square \n",
    "    of the gradient, which are updated over time. The learning rate is adaptively adjusted based \n",
    "    on these estimates to achieve faster convergence and better stability.\n",
    "    \"\"\"\n",
    "    \n",
    "    __attr__ = \"MML.nn_Optm_Adam\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 params: List[nn_Parameter] | Dict[Any, nn_Parameter] | nn_Module, \n",
    "                 *,\n",
    "                 lr: float = 0.001, \n",
    "                 beta1: float = 0.9, \n",
    "                 beta2: float = 0.999, \n",
    "                 eps: float = 1e-16,\n",
    "                 **kwargs):\n",
    "        \n",
    "        \"\"\"\n",
    "        Initialize an Adam optimizer with momentum. Call this and pass in parameters as\n",
    "        a list/dict of nn_Parameter or a nn_Module which contains all parameters and submodules.\n",
    "\n",
    "        Parameters:\n",
    "            params: List[nn_Parameter] | Dict[Any, nn_Parameter] | nn_Module, if directly gives a list of parameters,\n",
    "                    which we believe generates by .parameters(), we record them as a list;\n",
    "                    If directly gives an nn_Module, we accepts and record all of the parameters as a list;\n",
    "            lr: float, learning rate, the step size functioned on the gradients to update parameters;\n",
    "            beta1: float, momentum term, the decay rate for the first moment estimate.\n",
    "            beta2: float, adaptive scaling, it is the decay rate for the second moment estimate, 0.999 for 1000 steps averaging.\n",
    "\n",
    "        Attributes:\n",
    "            self.params: a List of nn_Parameters which may have gradients and needs to be updated.\n",
    "            self.n: int, the total number of steps performed.\n",
    "            self.lr: float, the learning rate as a plain float.\n",
    "            self.beta1: float, the momentum factor stored in plain float.\n",
    "            self.beta2: float, the adaptive scaling term stored in plain float.\n",
    "            self.eps: float, epsilon to avoid dividing by 0.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Call the base initialization to initialize all trainable parameters\n",
    "        super().__init__(params, **kwargs)\n",
    "        \n",
    "        # Record the Adam parameters\n",
    "        self.n = 0\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.eps = eps\n",
    "        \n",
    "        # Initialize first and second moment estimates for each param\n",
    "        self.m = [p.data.to_zeros() for p in self.params]  # first moment\n",
    "        self.v = [p.data.to_zeros() for p in self.params]  # second moment\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"\n",
    "        Update all trainable parameters in one step by Adaptive Momentum Estimate (Adam) method.\n",
    "        \n",
    "        Returns:\n",
    "            self\n",
    "        \"\"\"\n",
    "        # For compatibility, n adds at the begining\n",
    "        self.n += 1\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Iterate over trainable parameters\n",
    "            for i, p in enumerate(self.params):\n",
    "    \n",
    "                if p.autograd == False:\n",
    "                    # Update biased first moment: m = beta1*m + (1-beta1)*grad\n",
    "                    self.m[i] = self.m[i] * self.beta1 + (1.0 - self.beta1) * p.grad\n",
    "                    # Update biased second moment: v = beta2*v + (1-beta2)*grad^2\n",
    "                    self.v[i] = self.v[i] * self.beta2 + (1.0 - self.beta2) * (p.grad ** 2)            \n",
    "    \n",
    "                    # Compute bias-corrected moments\n",
    "                    m_hat = self.m[i] / (1.0 - (self.beta1 ** self.n))\n",
    "                    v_hat = self.v[i] / (1.0 - (self.beta2 ** self.n))\n",
    "                    \n",
    "                    # Update parameter: w = w - lr * m_hat / (sqrt(v_hat) + eps)\n",
    "                    p.data -= self.lr * m_hat / (v_hat ** 0.5 + self.eps)\n",
    "                \n",
    "                else:\n",
    "                    # Update biased first moment: m = beta1*m + (1-beta1)*grad\n",
    "                    self.m[i].data = self.m[i].data * self.beta1 + (1.0 - self.beta1) * p.data.data.grad\n",
    "                    # Update biased second moment: v = beta2*v + (1-beta2)*grad^2\n",
    "                    self.v[i].data = self.v[i].data * self.beta2 + (1.0 - self.beta2) * (p.data.data.grad ** 2)            \n",
    "    \n",
    "                    # Compute bias-corrected moments\n",
    "                    m_hat = self.m[i] / (1.0 - (self.beta1 ** self.n))\n",
    "                    v_hat = self.v[i] / (1.0 - (self.beta2 ** self.n))\n",
    "                    \n",
    "                    # Update parameter: w = w - lr * m_hat / (sqrt(v_hat) + eps)\n",
    "                    p.data.data.data -= self.lr * m_hat.data / (v_hat.data ** 0.5 + self.eps)\n",
    "                        \n",
    "        return self\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"nn_Optm_Adam(with {len(self.params)} trainable parameters, beta1 = {self.beta1}, beta2 = {self.beta2}).\"\n",
    "\n",
    "\n",
    "# Alias for nn_Optm_Adam\n",
    "Adam = nn_Optm_Adam\n",
    "\n",
    "\n",
    "# Implementation of Adaptive Momentum Optimzier with Weight Decay\n",
    "class nn_Optm_AdamW(nn_Optm_BaseOptimizer):\n",
    "    \"\"\"\n",
    "    Adaptive Momentum Estimate (AdamW) Optimizer with Weight Decay.\n",
    "\n",
    "    AdamW combines the Adam optimizer with a weight decay regularization term to prevent \n",
    "    overfitting and improve convergence. It applies weight decay to all parameters after \n",
    "    the gradient is computed, ensuring that large weights are penalized in the loss function. \n",
    "    The optimizer uses the same momentum terms as Adam but applies the weight decay during \n",
    "    the update step.\n",
    "\n",
    "    Formula:\n",
    "        For each parameter `p`:\n",
    "        1. Compute gradient `g` and update momentum terms `v`.\n",
    "        2. Apply weight decay: `p.data -= lr * (g + beta_2 * v)`.\n",
    "        3. Update parameters using Adam's updates: `p.data -= lr * (g + beta_1 * v)`.\n",
    "    \"\"\"    \n",
    "    \n",
    "    __attr__ = \"MML.nn_Optm_AdamW\"\n",
    "    \n",
    "    def __init__(self,                 \n",
    "                 params: List[nn_Parameter] | Dict[Any, nn_Parameter] | nn_Module, \n",
    "                 *, \n",
    "                 lr: float = 0.001, \n",
    "                 beta1: float = 0.9, \n",
    "                 beta2: float = 0.999, \n",
    "                 eps: float = 1e-16,\n",
    "                 weight_decay: float = 0.01,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize an AdamW optimizer with weight decay. Call this and pass in parameters as\n",
    "        a list/dict of nn_Parameter or a nn_Module which contains all parameters and submodules.\n",
    "\n",
    "        Parameters:\n",
    "            params: List[nn_Parameter] | Dict[Any, nn_Parameter] | nn_Module, if directly gives a list of parameters,\n",
    "                    which we believe generates by .parameters(), we record them as a list;\n",
    "                    If directly gives an nn_Module, we accepts and record all of the parameters as a list;\n",
    "            lr: float, learning rate, the step size functioned on the gradients to update parameters;\n",
    "            beta1: float, momentum term, the decay rate for the first moment estimate.\n",
    "            beta2: float, adaptive scaling, it is the decay rate for the second moment estimate, 0.999 for 1000 steps averaging;\n",
    "            weight_decay: float, L2 regularization coefficient (decoupled).\n",
    "\n",
    "        Attributes:\n",
    "            self.params: a List of nn_Parameters which may have gradients and needs to be updated.\n",
    "            self.n: int, the total number of steps performed.\n",
    "            self.lr: float, the learning rate as a plain float.\n",
    "            self.beta1: float, the momentum factor stored in plain float.\n",
    "            self.beta2: float, the adaptive scaling term stored in plain float.\n",
    "            self.eps: float, epsilon to avoid dividing by 0.\n",
    "            self.weight_decay: float, weight decay parameter.\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        # Call the base initialization to initialize all trainable parameters\n",
    "        super().__init__(params, **kwargs)\n",
    "        \n",
    "        # Record the AdamW parameters\n",
    "        self.n = 0\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.eps = eps\n",
    "        self.weight_decay = weight_decay\n",
    "        \n",
    "        # Initialize first and second moment estimates for each param\n",
    "        self.m = [p.data.to_zeros() for p in self.params]  # first moment\n",
    "        self.v = [p.data.to_zeros() for p in self.params]  # second moment\n",
    "        \n",
    "    def step(self):\n",
    "        \"\"\"\n",
    "        Update all trainable parameters in one step by Adaptive Momentum Estimate with Weight Decay (AdamW) method.\n",
    "        \n",
    "        Returns:\n",
    "            self\n",
    "        \"\"\"\n",
    "        # For compatibility, n adds at the begining\n",
    "        self.n += 1\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Iterate over trainable parameters\n",
    "            for i, p in enumerate(self.params):\n",
    "    \n",
    "                if p.autograd == False:\n",
    "                    # Update biased first moment: m = beta1*m + (1-beta1)*grad\n",
    "                    self.m[i] = self.m[i] * self.beta1 + (1.0 - self.beta1) * p.grad\n",
    "                    # Update biased second moment: v = beta2*v + (1-beta2)*grad^2\n",
    "                    self.v[i] = self.v[i] * self.beta2 + (1.0 - self.beta2) * (p.grad ** 2)            \n",
    "    \n",
    "                    # Compute bias-corrected moments\n",
    "                    m_hat = self.m[i] / (1.0 - (self.beta1 ** self.n))\n",
    "                    v_hat = self.v[i] / (1.0 - (self.beta2 ** self.n))\n",
    "                    \n",
    "                    # Apply decoupled weight decay\n",
    "                    p.data *= (1.0 - self.lr * self.weight_decay)\n",
    "                    \n",
    "                    # Update parameter: w = w - lr * m_hat / (sqrt(v_hat) + eps)\n",
    "                    p.data -= self.lr * m_hat / (v_hat ** 0.5 + self.eps)\n",
    "                \n",
    "                else:\n",
    "                    # Update biased first moment: m = beta1*m + (1-beta1)*grad\n",
    "                    self.m[i].data = self.m[i].data * self.beta1 + (1.0 - self.beta1) * p.data.data.grad\n",
    "                    # Update biased second moment: v = beta2*v + (1-beta2)*grad^2\n",
    "                    self.v[i].data = self.v[i].data * self.beta2 + (1.0 - self.beta2) * (p.data.data.grad ** 2)            \n",
    "    \n",
    "                    # Compute bias-corrected moments\n",
    "                    m_hat = self.m[i] / (1.0 - (self.beta1 ** self.n))\n",
    "                    v_hat = self.v[i] / (1.0 - (self.beta2 ** self.n))\n",
    "                    \n",
    "                    # Apply decoupled weight decay\n",
    "                    p.data.data.data *= (1.0 - self.lr * self.weight_decay)\n",
    "                    \n",
    "                    # Update parameter: w = w - lr * m_hat / (sqrt(v_hat) + eps)\n",
    "                    p.data.data.data -= self.lr * m_hat.data / (v_hat.data ** 0.5 + self.eps)\n",
    "                        \n",
    "        return self\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"nn_Optm_AdamW(with {len(self.params)} trainable parameters, beta1 = {self.beta1}, beta2 = {self.beta2}, weight_decay = {self.weight_decay}).\"\n",
    "\n",
    "\n",
    "# Alias for nn_Optm_AdamW\n",
    "AdamW = nn_Optm_AdamW\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`1. Implementation of LSTM (self-implemented)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This LSTM and Stacked LSTM layers are self implemented and open-sourced\n",
    "# Available at https://github.com/dof-studio/MML/\n",
    "# By Nathmath Huang (bh2821)\n",
    "# License: Apache License Version 2.0\n",
    "\n",
    "# Implementation of a Vanilla Long Short Term Memory using TBPTT\n",
    "class nn_Layer_LSTMCell(nn_Module):\n",
    "    \"\"\"\n",
    "    LSTM Cell Implementation\n",
    "    \n",
    "    This class implements a Long Short-Term Memory (LSTM) cell, designed to process\n",
    "    sequential data by maintaining and updating both hidden and cell states at each time step. \n",
    "    It contains multiple weight matrices (input, forget, cell, output gates) and corresponding \n",
    "    bias vectors stored in nn_Parameter containers, enabling the computation of forward \n",
    "    passes through sequences while supporting gradient-based learning via backward passes. \n",
    "    The LSTM architecture introduces gate mechanisms to control information flow, allowing\n",
    "    for better capture of long-term dependencies compared to standard RNNs.\n",
    "        \n",
    "    Note, TBPTT should be implemented externally with segmentation and carrying (manually reset_hidden).\n",
    "    \"\"\"\n",
    "\n",
    "    __attr__ = \"MML.nn_Layer_LSTMCell\"   \n",
    "\n",
    "    def __init__(self, \n",
    "                 in_features: int = 1, \n",
    "                 hid_features: int = 1, \n",
    "                 has_bias: str = True,\n",
    "                 init_scale: float = 0.1,\n",
    "                 tbptt_steps: int | None = None,\n",
    "                 return_sequences: bool = False, \n",
    "                 return_state: bool = False,\n",
    "                 accumulate_hidden: bool = False,\n",
    "                 gradient_clipping: float = 5.0,\n",
    "                 *,\n",
    "                 module_name: str = \"nn_Layer_LSTMCell\", \n",
    "                 backend: Literal[\"torch\", \"numpy\"] = \"torch\",\n",
    "                 dtype: type | str = None,\n",
    "                 device: str | None = None,\n",
    "                 autograd: bool = False,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        An LSTM (Long Short Term Machine) cell processes sequences by maintaining\n",
    "            a hidden state that is updated at each time step.\n",
    "            \n",
    "        Structure: \n",
    "            - **Input gate**: Controls how much new information is added to the cell state.  \n",
    "              $$ i_t = \\sigma(W_i x_t + U_i h_{t-1} + b_i) $$  \n",
    "            - **Forget gate**: Determines what information to discard from the cell state.  \n",
    "              $$ f_t = \\sigma(W_f x_t + U_f h_{t-1} + b_f) $$  \n",
    "            - **Cell gate (candidate)**: Computes a candidate value for the cell state.  \n",
    "              $$ \\tilde{c}_t = \\tanh(W_c x_t + U_c h_{t-1} + b_c) $$  \n",
    "            - **Output gate**: Regulates what part of the cell state is output as the hidden state.  \n",
    "              $$ o_t = \\sigma(W_o x_t + U_o h_{t-1} + b_o) $$  \n",
    "        \n",
    "        Parameters:\n",
    "            in_features: int, The number of input features for this layer. Defaults to 1.\n",
    "            hid_features: int, The number of hidden features for this layer. Defaults to 1.\n",
    "            has_bias: str, A flag indicating whether to include a bias term. Valid values are \"True\" or \"False\".\n",
    "                    If set to \"True\", the layer includes an additive bias term (b). Defaults to \"True\".\n",
    "            tbptt_steps: int | None, the number of steps performing Truncated BPTT (TBPTT), \n",
    "                    if None, then use true BPTT. Defaults: None.\n",
    "                    Note, TBPTT is not fully implemented. Please use BPTT instead.\n",
    "            return_sequences: bool, should the layers return only the final hidden state, or the full sequence \n",
    "                    of hidden states as well. Defaults to False.\n",
    "            return_state: bool, should the layers return output and final hidden state. Defaults to False.\n",
    "            accumulate_hidden: bool, should the network accumulate hidden state (when processing segments) or clear it before next forward.\n",
    "            gradient_clipping: float, The factor of performing gradient clipping to avoid gradient expolosion. Defaults to 5.0.\n",
    "            module_name: str, The name of the module instance. Defaults to \"nn_Layer_LSTMCell\".\n",
    "            backend: Literal[\"torch\", \"numpy\"], The computational backend to use. Defaults to \"torch\".\n",
    "            dtype: type, The data type for the tensor values. Defaults to None (auto detection). \n",
    "                    For PyTorch, this corresponds to torch.dtype; for NumPy, it corresponds to np.dtype.\n",
    "            device: str | None, The target device (e.g., \"cpu\", \"cuda\") where the layer's parameters will be placed. \n",
    "                    If None, uses the default device. Defaults to None (auto detection).\n",
    "            autograd: bool, A flag indicating whether to use PyTorch's autograd for gradient computation. \n",
    "                    If True, manual gradient tracking is disabled. Defaults to False.\n",
    "\n",
    "        Attributes:\n",
    "            self.in_features: int, The number of input features for this layer.\n",
    "            self.hid_features: int, The number of output features for this layer.\n",
    "            self.has_bias: str, A flag indicating whether a bias term is included (\"True\" or \"False\").\n",
    "            self.init_scale: float, A floatting number indicating the maximum value of initial random weights.\n",
    "            self.tbptt_steps: int | None, the number of TBPTT size, or left None.\n",
    "            self.return_sequences: bool, indicator of whether returns full sequence of hidden or not.\n",
    "            self.return_state: bool, indicator of whether returns final state or not.\n",
    "            self.accumulate_hidden: bool, indicator of whether accumulate states or reinitialize when next forward.\n",
    "            self.gradient_clipping: float, The threshold for gradient clipping.\n",
    "            self.backend: Literal[\"torch\", \"numpy\"], The computational backend used by the layer.\n",
    "            self.dtype: type, The data type of the tensor values.\n",
    "            self.device: str | None, The device where the parameters are placed.\n",
    "            self.autograd: bool, Whether PyTorch's autograd is enabled for this layer.\n",
    "            self._parameters.weight_ih: nn_Parameter, input weights\n",
    "            self._parameters.bias_ih: nn_Parameter | None, input bias\n",
    "            self._parameters.weight_hh: nn_Parameter, hidden weights\n",
    "            self._parameters.bias_hh: nn_Parameter | None, hidden bias\n",
    "            self._cache: dict of list of tensors, for backward propagation\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__(module_name = module_name, backend = backend, dtype = dtype, device = device, autograd = autograd)\n",
    "        \n",
    "        # Record shapes etc\n",
    "        self.__setattr__(\"in_features\", in_features)\n",
    "        self.__setattr__(\"hid_features\", hid_features)\n",
    "        self.__setattr__(\"has_bias\", has_bias)\n",
    "        self.__setattr__(\"init_scale\", init_scale)\n",
    "        self.__setattr__(\"tbptt_steps\", tbptt_steps)\n",
    "        self.__setattr__(\"return_sequences\", return_sequences)\n",
    "        self.__setattr__(\"return_state\", return_state)\n",
    "        self.__setattr__(\"accumulate_hidden\", accumulate_hidden)\n",
    "        self.__setattr__(\"gradient_clipping\", gradient_clipping)\n",
    "        \n",
    "        # Cache: to store values needed for backward\n",
    "        self.__setattr__(\"_cache\", {})\n",
    "        \n",
    "        # Initialize weight and bias parameters\n",
    "        \n",
    "        # LSTM weight parameters (following PyTorch's gating order: i, f, g, o)\n",
    "        # weight_ih: combines weights for input->[i, f, g, o] (shape: 4H x input_size)\n",
    "        # weight_hh: combines weights for hidden->[i, f, g, o] (shape: 4H x hidden_size)\n",
    "        # bias_ih, bias_hh: biases for the four gates (shape: 4H each)\n",
    "        self.__setattr__(\"weight_ih\", nn_Parameter(\n",
    "            Tensor.rand([4 * hid_features, in_features], backend=backend, dtype=dtype, device=device) * init_scale,\n",
    "            requires_grad = True,\n",
    "            dtype = None,\n",
    "            device = None,\n",
    "            autograd = autograd)\n",
    "            )\n",
    "        self.__setattr__(\"weight_hh\", nn_Parameter(\n",
    "            Tensor.rand([4 * hid_features, hid_features], backend=backend, dtype=dtype, device=device) * init_scale,\n",
    "            requires_grad = True,\n",
    "            dtype = None,\n",
    "            device = None,\n",
    "            autograd = autograd)\n",
    "            )\n",
    "        \n",
    "        if has_bias == True:\n",
    "            # If uses bias, then set the bias\n",
    "            self.__setattr__(\"bias_ih\", nn_Parameter(\n",
    "                Tensor.zeros([4 * hid_features], backend=backend, dtype=dtype, device=device),\n",
    "                requires_grad = True,\n",
    "                dtype = None,\n",
    "                device = None,\n",
    "                autograd = autograd)\n",
    "                )\n",
    "            self.__setattr__(\"bias_hh\", nn_Parameter(\n",
    "                Tensor.zeros([4 * hid_features], backend=backend, dtype=dtype, device=device),\n",
    "                requires_grad = True,\n",
    "                dtype = None,\n",
    "                device = None,\n",
    "                autograd = autograd)\n",
    "                )\n",
    "\n",
    "    def reset_hidden(self) -> None:\n",
    "        \"\"\"\n",
    "        Reset the stored hidden state if done TBPTT.\n",
    "        If invoked, when calling forward() next time, hiddden state will be starting from 0s.\n",
    "        \"\"\"\n",
    "        if self._cache.get('h0_final', None) is not None:\n",
    "            self._cache.pop('h0_final')\n",
    "        if self._cache.get('c0_final', None) is not None:\n",
    "            self._cache.pop('c0_final')\n",
    "\n",
    "    def clear_cache(self) -> None:\n",
    "        \"\"\"\n",
    "        Clear the cache dict to release Tensors and avoid issues.\n",
    "        If invoked, cache will be an empty dict. Do not invoke before applying backward().\n",
    "        \"\"\"\n",
    "        self._cache = {}\n",
    "\n",
    "    def forward(self, x: Tensor, h0: Tensor | None = None, c0: Tensor | None = None) -> Tensor| Tuple[Tensor, Tensor, Tensor]:\n",
    "        \"\"\"\n",
    "        Compute the layer output:\n",
    "            - **Input gate**: Controls how much new information is added to the cell state.  \n",
    "              $$ i_t = \\sigma(W_i x_t + U_i h_{t-1} + b_i) $$  \n",
    "            - **Forget gate**: Determines what information to discard from the cell state.  \n",
    "              $$ f_t = \\sigma(W_f x_t + U_f h_{t-1} + b_f) $$  \n",
    "            - **Cell gate (candidate)**: Computes a candidate value for the cell state.  \n",
    "              $$ \\tilde{c}_t = \\tanh(W_c x_t + U_c h_{t-1} + b_c) $$  \n",
    "            - **Output gate**: Regulates what part of the cell state is output as the hidden state.  \n",
    "              $$ o_t = \\sigma(W_o x_t + U_o h_{t-1} + b_o) $$  \n",
    "        You can safely choose NOT to return hidden states since we have memorized them if you do not pass.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape (batch_size, seq_len, input_size)) to be transformed by the layer.\n",
    "            h0 (Tensor | None): initial hidden state, Tensor of shape (batch, hidden_size) or left None.\n",
    "                If left None, the net will automatically use the previous final hidden state stored in the cache if selected.\n",
    "            c0 (Tensor | None): initial cell state, Tensor of shape (batch, hidden_size) or left None.\n",
    "                If left None, the net will automatically use the previous final hidden state stored in the cache if selected.\n",
    "            \n",
    "        Returns:\n",
    "            Tensor: Output tensor of shape (batch_size, hid_features) after applying the RNN transformation.\n",
    "            or \n",
    "            A tuple of Tensors of 3 having outputs, the hidden state, and the cell state.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the input `x` is not a valid MML.Tensor object.\n",
    "            ValueError: If the input `h0` is given but not equal to the (batch_size, hid_features).\n",
    "\n",
    "        Attributes:\n",
    "            self.input (Tensor): The input tensor `x` saved for use in backward propagation.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Type check, x must be an instance of Tensor\n",
    "        if isinstance(x, Tensor) == False:\n",
    "            raise ValueError(f\"In performing forward(), input `x` must be in a MML `Tensor` format but you have {type(x)}\")\n",
    "        \n",
    "        # If h0 is given, it must have (batch_size, hid_features) size\n",
    "        if h0 is not None:\n",
    "            if isinstance(h0, Tensor) == False:\n",
    "                raise ValueError(f\"In performing forward(), input `h0` must be in a MML `Tensor` format but you have {type(h0)}\")\n",
    "            if h0.shape != (x.shape[0], self.hid_features):\n",
    "                raise ValueError(f\"In performing forward(), if given an input `h0`, it must equal to (batch_size, hidden_size) but you have {h0.shape}\")\n",
    "        # Else if it has been stored previously, reuse it\n",
    "        else:\n",
    "            if self._cache.get('h0_final', None) is not None and self.accumulate_hidden == True:\n",
    "                h0 = self._cache['h0_final']\n",
    "                \n",
    "        # If c0 is given, it must have (batch_size, hid_features) size\n",
    "        if c0 is not None:\n",
    "            if isinstance(c0, Tensor) == False:\n",
    "                raise ValueError(f\"In performing forward(), input `c0` must be in a MML `Tensor` format but you have {type(c0)}\")\n",
    "            if c0.shape != (x.shape[0], self.hid_features):\n",
    "                raise ValueError(f\"In performing forward(), if given an input `c0`, it must equal to (batch_size, hidden_size) but you have {c0.shape}\")\n",
    "        # Else if it has been stored previously, reuse it\n",
    "        else:\n",
    "            if self._cache.get('c0_final', None) is not None and self.accumulate_hidden == True:\n",
    "                h0 = self._cache['c0_final']\n",
    "        \n",
    "        # Save input for backward\n",
    "        self.__setattr__(\"input\", x)\n",
    "        \n",
    "        # Rrcord the critical shapes\n",
    "        batch_size, seq_len, *_ = x.shape\n",
    "        \n",
    "        # Initialize hidden state\n",
    "        if h0 is None:\n",
    "            h = Tensor.zeros((batch_size, self.hid_features), backend=self.backend, dtype=self.dtype, device=self.device)\n",
    "        else:\n",
    "            h = h0\n",
    "            \n",
    "        # Initialize cell state\n",
    "        if c0 is None:\n",
    "            c = Tensor.zeros((batch_size, self.hid_features), backend=self.backend, dtype=self.dtype, device=self.device)\n",
    "        else:\n",
    "            c = c0\n",
    "            \n",
    "        # Cache lists for backward (to compute gradients)\n",
    "        window = self.tbptt_steps if (self.tbptt_steps is not None and self.tbptt_steps > 0) else seq_len\n",
    "        x_cache = deque(maxlen=window)\n",
    "        i_cache = deque(maxlen=window)\n",
    "        f_cache = deque(maxlen=window)\n",
    "        g_cache = deque(maxlen=window)\n",
    "        o_cache = deque(maxlen=window)\n",
    "        i_lin_cache = deque(maxlen=window)\n",
    "        f_lin_cache = deque(maxlen=window)\n",
    "        g_lin_cache = deque(maxlen=window)\n",
    "        o_lin_cache = deque(maxlen=window)\n",
    "        h_cache = deque(maxlen=window)\n",
    "        c_cache = deque(maxlen=window)\n",
    "        \n",
    "        # Cache the initial state\n",
    "        if self.training == True:\n",
    "            self._cache['h0'] = deepcopy(h)\n",
    "            self._cache['c0'] = deepcopy(c)\n",
    "            \n",
    "        # Prepare output list if return_sequences\n",
    "        outputs = [] if self.return_sequences == True else None\n",
    "        \n",
    "        # Forward propagate through time\n",
    "        for t in range(seq_len):\n",
    "            \n",
    "            # Crcord input at time t, shape (batch, input_size)\n",
    "            xt = x[:, t, ...]  \n",
    "            x_cache.append(xt)\n",
    "\n",
    "            # Linear combinations for all gates: (batch, 4*H)\n",
    "            # z_t = [i_lin, f_lin, g_lin, o_lin] = X_t * W_ih^T + h_prev * W_hh^T + (bias_ih + bias_hh)\n",
    "            z_t = (xt @ self._parameters[\"weight_ih\"].data.transpose()) + (h @ self._parameters[\"weight_hh\"].data.transpose()) + (self._parameters[\"bias_ih\"].data + self._parameters[\"bias_hh\"].data)\n",
    "            \n",
    "            # Split combined linear outputs into each gate pre-activation (each of shape (batch, H))\n",
    "            i_lin, f_lin, g_lin, o_lin = torch.split(z_t, self.hid_features, dim=1)\n",
    "            \n",
    "            # Apply activations to get gate values\n",
    "            i_t = i_lin.sigmoid()           # forget gate\n",
    "            f_t = f_lin.sigmoid()           # forget gate\n",
    "            g_t = g_lin.tanh()              # candidate gate (cell input)\n",
    "            o_t = o_lin.sigmoid()           # output gate\n",
    "                        \n",
    "            # Compute new cell state and hidden state\n",
    "            c_t = f_t * c + i_t * g_t  # (batch, hidden), formula: c_t = f_t ⊙ c_{t-1} + i_t ⊙ g_t\n",
    "            h_t = o_t *c_t.tanh()      # (batch, hideen), formula: h_t = o_t ⊙ tanh(c_t)\n",
    "\n",
    "            # Save gate activations and states in cache for backprop\n",
    "            i_cache.append(i_t)\n",
    "            f_cache.append(f_t)\n",
    "            g_cache.append(g_t)\n",
    "            o_cache.append(o_t)\n",
    "            i_lin_cache.append(i_lin)\n",
    "            f_lin_cache.append(f_lin)\n",
    "            g_lin_cache.append(g_lin)\n",
    "            o_lin_cache.append(o_lin)\n",
    "            c_cache.append(c_t)\n",
    "            h_cache.append(h_t)\n",
    "\n",
    "            # Append to outputs if needed\n",
    "            if self.return_sequences == True:\n",
    "                outputs.append(deepcopy(h_t))\n",
    "\n",
    "            # Prepare next step\n",
    "            h, c = h_t, c_t\n",
    "\n",
    "            # TBPTT: Truncated Back Propagation Through Time\n",
    "            # if we've reached tbptt_steps, detach hidden state\n",
    "            if self.tbptt_steps is not None and self.tbptt_steps > 0:\n",
    "                \n",
    "                # Detach at the boundary of each segment\n",
    "                if (t + 1) % self.tbptt_steps == 0:\n",
    "                    # Detach the hidden state to prevent backprop beyond this point\n",
    "                    # It is only used for autograd purposes\n",
    "                    h = h.detach()  \n",
    "                    c = c.detach()\n",
    "                    \n",
    "        # Save cache for backward\n",
    "        if self.training == True:\n",
    "            self._cache['x'] = x_cache\n",
    "            self._cache['i'] = i_cache\n",
    "            self._cache['f'] = f_cache\n",
    "            self._cache['g'] = g_cache\n",
    "            self._cache['o'] = o_cache\n",
    "            self._cache['i_lin'] = i_lin_cache\n",
    "            self._cache['f_lin'] = f_lin_cache\n",
    "            self._cache['g_lin'] = g_lin_cache\n",
    "            self._cache['o_lin'] = o_lin_cache\n",
    "            self._cache['c'] = c_cache\n",
    "            self._cache['h'] = h_cache\n",
    "            self._cache['seq_len'] = seq_len\n",
    "            self._cache['batch_size'] = batch_size\n",
    "            self._cache['h0_final'] = h  \n",
    "            self._cache['c0_final'] = c \n",
    "        # h: final hidden state after last step (stored for continuation reuse)\n",
    "        # c: final cell state after last step  (stored for continuation reuse)\n",
    "\n",
    "        # Prepare output data\n",
    "        if self.return_sequences == True:\n",
    "            # Stack outputs list to tensor: (batch, seq_len, hidden)\n",
    "            outputs_tensor = outputs[0].stack(*outputs[1:], axis=1)\n",
    "        else:\n",
    "            # Last output is the last hidden state\n",
    "            outputs_tensor = h\n",
    "\n",
    "        if self.return_state == True:\n",
    "            # Return output and final hidden state\n",
    "            return outputs_tensor, h, c\n",
    "        else:\n",
    "            # Return just the output tensor\n",
    "            return outputs_tensor\n",
    "    \n",
    "    def backward(self, grad_output: Tensor, grad_h: Tensor | None = None, grad_c: Tensor | None = None) -> Tensor | Tuple[Tensor, Tensor, Tensor]:\n",
    "        \"\"\"\n",
    "        Backward pass: compute gradients for weight, bias, and input.\n",
    "\n",
    "        This method performs the gradient computation for a dense layer during backpropagation. \n",
    "        It calculates the gradients of the loss with respect to the weights, biases, and input tensor,\n",
    "        based on the provided `grad_output` (gradient from the next layer). The implementation\n",
    "        supports both PyTorch autograd and manual gradient calculation modes.\n",
    "\n",
    "        Args:\n",
    "            grad_output (Tensor): Gradient tensor resulting from the output of the layer, used as input for backpropagation.\n",
    "                     - If return_sequences=True, shape is (batch, seq_len, hidden_size)\n",
    "                     - If return_sequences=False, shape is (batch, hidden_size) for last output.\n",
    "            grad_h (Tensor | None): Gradient of loss w.r.t. the final hidden state.\n",
    "                     - This is usually None unless the LSTM's final state is fed into another layer.\n",
    "            grad_c (Tensor | None): Gradient of loss w.r.t. the final cell state.\n",
    "                     - This is usually None unless the LSTM's final state is fed into another layer.\n",
    "\n",
    "        Returns:\n",
    "            Tuple of \n",
    "                (Tensor: Gradient with respect to the input tensor, for recursive backward calculations in previous layers, shape (batch, seq_len, input_size) tensor.\n",
    "                 Tensor: Gradient with respect to the final hidden state, ∂L/∂h0.\n",
    "                 Tensor: Gradient with respect to the final cell state, ∂L/∂c0.\n",
    "                 )\n",
    "        Raises:\n",
    "            ValueError: If `grad_output` is not a valid MML.Tensor object.\n",
    "            ValueError: If `grad_h` is given but not a valid MML.Tensor object.\n",
    "            ValueError: If `grad_c` is given but not a valid MML.Tensor object.\n",
    "        \"\"\"\n",
    "        \n",
    "        # If use autograd, pass; if manual mode, then calculate\n",
    "        if self.autograd == True:\n",
    "            return None\n",
    "        \n",
    "        # Type check, grad_output must be an instance of Tensor\n",
    "        if isinstance(grad_output, Tensor) == False:\n",
    "            raise ValueError(f\"In performing backward(), `grad_output` must be in a MML `Tensor` format but you have {type(grad_output)}\")\n",
    "        \n",
    "        # Type check, if given, grad_h must be an instance of Tensor\n",
    "        if grad_h is not None:\n",
    "            if isinstance(grad_h, Tensor) == False:\n",
    "                raise ValueError(f\"In performing backward(), if given `grad_h`, it must be in a MML `Tensor` format but you have {type(grad_h)}\")\n",
    "        \n",
    "        # Type check, if given, grad_c must be an instance of Tensor\n",
    "        if grad_c is not None:\n",
    "            if isinstance(grad_c, Tensor) == False:\n",
    "                raise ValueError(f\"In performing backward(), if given `grad_c`, it must be in a MML `Tensor` format but you have {type(grad_c)}\")\n",
    "        \n",
    "        # Retrieve some dimension values\n",
    "        x_cache = self._cache.get('x')\n",
    "        seq_len = self._cache.get('seq_len')\n",
    "        batch_size = self._cache.get('batch_size') \n",
    "        \n",
    "        # Initialize gradients for inputs and initial states\n",
    "        grad_x_all = Tensor.zeros((batch_size, seq_len, self.in_features), backend=self.backend, dtype=self.dtype, device=self.device)\n",
    "        grad_h_prev = Tensor.zeros((batch_size, self.hid_features), backend=self.backend, dtype=self.dtype, device=self.device)  # dL/dh_{t-1}\n",
    "        grad_c_prev = Tensor.zeros((batch_size, self.hid_features), backend=self.backend, dtype=self.dtype, device=self.device)  # dL/dc_{t-1}\n",
    "  \n",
    "        # If return_sequences=False, expand grad_output to sequence length (\n",
    "        # only last timestep has non-zero grad)\n",
    "        if self.return_sequences == False:\n",
    "            # grad_output is (batch, hidden_size) for last output\n",
    "            # We create an array of shape (batch, seq_len, hidden_size) with zeros except last timestep\n",
    "            go = Tensor.zeros((batch_size, seq_len, self.hid_features), backend=self.backend, dtype=self.dtype, device=self.device)\n",
    "            go[:, -1, ...] = grad_output  # only last time step has incoming grad\n",
    "            grad_output = go\n",
    "        \n",
    "        # If an external grad_h is provided, add it\n",
    "        if grad_h is not None:\n",
    "            grad_output[:, -1, ...] += grad_h\n",
    "        \n",
    "        # If an external grad_c is provided, add it\n",
    "        if grad_h is not None:\n",
    "            grad_c_prev += grad_h\n",
    "        \n",
    "        # maximum number of steps to walk back\n",
    "        max_steps = self.tbptt_steps if (self.tbptt_steps and self.tbptt_steps > 0) else seq_len\n",
    "        \n",
    "        # Truncated Backpropagate Through Time (TBPTT)\n",
    "        # We will iterate backwards through each time step.\n",
    "        # If return_sequences, include grad from that step's output as well.\n",
    "        steps_done = 0\n",
    "        for t in range(seq_len - 1, -1, -1):\n",
    "            \n",
    "            if steps_done >= max_steps:\n",
    "                break\n",
    "            steps_done += 1\n",
    "            \n",
    "            # Current step's hidden and cell from cache\n",
    "            h_t = self._cache['h'][t]    # shape: (batch, hidden)\n",
    "            c_t = self._cache['c'][t]    # shape: (batch, hidden)\n",
    "            \n",
    "            # Previous cell state and hidden state\n",
    "            if t == 0:\n",
    "                c_prev_t = self._cache['c0']\n",
    "                h_prev_t = self._cache['h0']\n",
    "            else:\n",
    "                c_prev_t = self._cache['c'][t-1]\n",
    "                h_prev_t = self._cache['h'][t-1]\n",
    "                \n",
    "            # Gradients from output of this timestep (dL/dh_t)\n",
    "            d_h = grad_output[:, t, ...] + grad_h_prev\n",
    "            \n",
    "            # Gradient from current cell output portion (dL/dc_t from h_t path)\n",
    "            # h_t = o_t * tanh(c_t) -> dL/dc_t (partial) = dL/dh_t * o_t * (1 - tanh(c_t)^2)\n",
    "            o_t = self._cache['o'][t]\n",
    "            tanh_c_t = c_t.tanh()\n",
    "            d_c = grad_c_prev + (d_h * o_t * (1 - tanh_c_t * tanh_c_t))\n",
    "            \n",
    "            # dL/do_t = dL/dh_t * tanh(c_t)\n",
    "            d_o = d_h * tanh_c_t\n",
    "            \n",
    "            # Now split d_c into contributions\n",
    "            i_t = self._cache['i'][t]\n",
    "            f_t = self._cache['f'][t]\n",
    "            g_t = self._cache['g'][t]\n",
    "            # dL/di_t = d_c * g_t\n",
    "            d_i = d_c * g_t\n",
    "            # dL/df_t = d_c * c_{t-1}\n",
    "            d_f = d_c * c_prev_t\n",
    "            # dL/dg_t = d_c * i_t\n",
    "            d_g = d_c * i_t\n",
    "            # dL/dc_{t-1} for next iteration (grad_c_prev for next step)\n",
    "            # c_t = f_t * c_{t-1} + ... -> partial derivative w.rt c_{t-1} is f_t\n",
    "            grad_c_prev = d_c * f_t  # (this becomes d_c for previous time step)\n",
    "            \n",
    "            # Backprop through gate activations\n",
    "            # i_t = sigmoid(i_lin), etc. Compute gradients w.rt pre-activation: d*_lin\n",
    "            # sigmoid' = s*(1-s); tanh' = 1 - g^2\n",
    "            i_lin = self._cache['i_lin'][t]\n",
    "            f_lin = self._cache['f_lin'][t]\n",
    "            o_lin = self._cache['o_lin'][t]\n",
    "            g_lin = self._cache['g_lin'][t]\n",
    "            d_i_lin = d_i * i_t * (1 - i_t)\n",
    "            d_f_lin = d_f * f_t * (1 - f_t)\n",
    "            d_o_lin = d_o * o_t * (1 - o_t)\n",
    "            d_g_lin = d_g * (1 - g_t * g_t)\n",
    "\n",
    "            # Concatenate gate gradients for linear part: (batch, 4H)\n",
    "            d_z = d_i_lin.hstack(d_f_lin, d_g_lin, d_o_lin)\n",
    "            \n",
    "            # Gradients w.rt weight matrices and biases\n",
    "            # weight_ih and weight_hh were used as: i_lin = X_t * W_ih^T + ..., so:\n",
    "            # grad_weight_ih += d_z^T * X_t\n",
    "            # grad_weight_hh += d_z^T * h_prev_t\n",
    "            self._parameters[\"weight_ih\"].grad += d_z.transpose() @ self._cache['x'][t]\n",
    "            self._parameters[\"weight_hh\"].grad += d_z.transpose() @ h_prev_t\n",
    "            \n",
    "            # Accumulate bias gradients (each just sum of d_z, since bias is added to each output)\n",
    "            if self.has_bias == True:\n",
    "                self._parameters[\"bias_ih\"].grad += d_z.sum(axis=0)\n",
    "                self._parameters[\"bias_hh\"].grad += d_z.sum(axis=0)\n",
    "\n",
    "            # Gradients w.rt input x_t and hidden state h_{t-1}\n",
    "            # dX_t = d_z * W_ih (because z = X * W_ih^T -> dX = dZ * W_ih)\n",
    "            grad_x_all[:, t, ...] = d_z @ self._parameters[\"weight_ih\"].data\n",
    "            # dH_prev = d_z * W_hh\n",
    "            grad_h_prev = d_z @ self._parameters[\"weight_hh\"].data\n",
    "            \n",
    "        # After accumulating all gradients\n",
    "        def _clip_gradients_inplace(params: nn_Parameter, max_norm: float) -> None:\n",
    "            \n",
    "            # Gather all gradient Tensors from nn_Parameter\n",
    "            grads = [p.grad for p in params if p is not None and p.grad is not None]\n",
    "            # List\n",
    "            \n",
    "            # Compute squared norms of each gradient and sum them\n",
    "            total_norm_sq = sum(((g ** 2).sum()) for g in grads)\n",
    "            # Tensor\n",
    "            \n",
    "            # Take the square root for the global norm\n",
    "            total_norm = total_norm_sq ** 0.5\n",
    "            \n",
    "            # If outside threshold, scale each gradient in place\n",
    "            if total_norm.to_list() > max_norm:\n",
    "                scale = max_norm / (total_norm + 1e-12)\n",
    "                for g in grads:\n",
    "                    g *= scale\n",
    "                    \n",
    "            return\n",
    "                    \n",
    "        # If self.gradient_clipping is not None, then perform gradient clipping\n",
    "        if self.gradient_clipping is not None and self.gradient_clipping > 0:\n",
    "            parameters = [self._parameters[\"weight_ih\"], \n",
    "                          self._parameters[\"weight_hh\"], \n",
    "                          self._parameters[\"bias_ih\"], \n",
    "                          self._parameters[\"bias_hh\"]]\n",
    "            _clip_gradients_inplace(parameters, self.gradient_clipping)\n",
    "        \n",
    "        return grad_x_all, grad_h_prev, grad_c_prev\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"nn_Layer_LSTMCell(shape: ({self.in_features}, {self.hid_features}) with{'out' if self.has_bias == False else ''} bias).\"\n",
    "      \n",
    "        \n",
    "# Implementation of a Stacked LSTM Layer using TBPTT  \n",
    "class nn_Layer_StackedLSTM(nn_Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    Stacked LSTM Implementation\n",
    "    \n",
    "    This class implements a Long Short-Term Memory (LSTM) cell, designed to process\n",
    "    sequential data by maintaining and updating both hidden and cell states at each time step. \n",
    "    It contains multiple weight matrices (input, forget, cell, output gates) and corresponding \n",
    "    bias vectors stored in nn_Parameter containers, enabling the computation of forward \n",
    "    passes through sequences while supporting gradient-based learning via backward passes. \n",
    "    The LSTM architecture introduces gate mechanisms to control information flow, allowing\n",
    "    for better capture of long-term dependencies compared to standard RNNs.\n",
    "        \n",
    "    Note, TBPTT should be implemented externally with segmentation and carrying (manually reset_hidden).\n",
    "    \"\"\"\n",
    "    \n",
    "    __attr__ = \"MML.nn_Layer_StackedLSTM\"   \n",
    "\n",
    "    def __init__(self, \n",
    "                 in_features: int = 1, \n",
    "                 hid_features: int = 1, \n",
    "                 num_layers: int = 1,\n",
    "                 has_bias: str = True,\n",
    "                 init_scale: float = 0.1,\n",
    "                 tbptt_steps: int | None = None,\n",
    "                 return_sequences: bool = False, \n",
    "                 return_state: bool = False,\n",
    "                 accumulate_hidden: bool = False,\n",
    "                 gradient_clipping: float = 5.0,\n",
    "                 *,\n",
    "                 module_name: str = \"nn_Layer_LSTMCell\", \n",
    "                 backend: Literal[\"torch\", \"numpy\"] = \"torch\",\n",
    "                 dtype: type | str = None,\n",
    "                 device: str | None = None,\n",
    "                 autograd: bool = False,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        An LSTM (Long Short Term Machine) cell processes sequences by maintaining\n",
    "            a hidden state that is updated at each time step.\n",
    "            \n",
    "        Structure: \n",
    "            - **Input gate**: Controls how much new information is added to the cell state.  \n",
    "              $$ i_t = \\sigma(W_i x_t + U_i h_{t-1} + b_i) $$  \n",
    "            - **Forget gate**: Determines what information to discard from the cell state.  \n",
    "              $$ f_t = \\sigma(W_f x_t + U_f h_{t-1} + b_f) $$  \n",
    "            - **Cell gate (candidate)**: Computes a candidate value for the cell state.  \n",
    "              $$ \\tilde{c}_t = \\tanh(W_c x_t + U_c h_{t-1} + b_c) $$  \n",
    "            - **Output gate**: Regulates what part of the cell state is output as the hidden state.  \n",
    "              $$ o_t = \\sigma(W_o x_t + U_o h_{t-1} + b_o) $$  \n",
    "        \n",
    "        Parameters:\n",
    "            in_features: int, The number of input features for this layer. Defaults to 1.\n",
    "            hid_features: int, The number of hidden features for this layer. Defaults to 1.\n",
    "            num_layers: int, The number of LSTM cells stacked together (h(n-1) becomes x in stack n).\n",
    "            has_bias: str, A flag indicating whether to include a bias term. Valid values are \"True\" or \"False\".\n",
    "                    If set to \"True\", the layer includes an additive bias term (b). Defaults to \"True\".\n",
    "            tbptt_steps: int | None, the number of steps performing Truncated BPTT (TBPTT), \n",
    "                    if None, then use true BPTT. Defaults: None.\n",
    "                    Note, TBPTT is not fully implemented. Please use BPTT instead.\n",
    "            return_sequences: bool, should the layers return only the final hidden state, or the full sequence \n",
    "                    of hidden states as well. Defaults to False.\n",
    "            return_state: bool, should the layers return output and final hidden state. Defaults to False.\n",
    "            accumulate_hidden: bool, should the network accumulate hidden state (when processing segments) or clear it before next forward.\n",
    "            gradient_clipping: float, The factor of performing gradient clipping to avoid gradient expolosion. Defaults to 5.0.\n",
    "            module_name: str, The name of the module instance. Defaults to \"nn_Layer_StackedLSTM\".\n",
    "            backend: Literal[\"torch\", \"numpy\"], The computational backend to use. Defaults to \"torch\".\n",
    "            dtype: type, The data type for the tensor values. Defaults to None (auto detection). \n",
    "                    For PyTorch, this corresponds to torch.dtype; for NumPy, it corresponds to np.dtype.\n",
    "            device: str | None, The target device (e.g., \"cpu\", \"cuda\") where the layer's parameters will be placed. \n",
    "                    If None, uses the default device. Defaults to None (auto detection).\n",
    "            autograd: bool, A flag indicating whether to use PyTorch's autograd for gradient computation. \n",
    "                    If True, manual gradient tracking is disabled. Defaults to False.\n",
    "\n",
    "        Attributes:\n",
    "            self.in_features: int, The number of input features for this layer.\n",
    "            self.hid_features: int, The number of output features for this layer.\n",
    "            self.num_layers: int, The number of stacked RNN cells for this layer.\n",
    "            self.has_bias: str, A flag indicating whether a bias term is included (\"True\" or \"False\").\n",
    "            self.init_scale: float, A floatting number indicating the maximum value of initial random weights.\n",
    "            self.tbptt_steps: int | None, the number of TBPTT size, or left None.\n",
    "            self.return_sequences: bool, indicator of whether returns full sequence of hidden or not.\n",
    "            self.return_state: bool, indicator of whether returns final state or not.\n",
    "            self.accumulate_hidden: bool, indicator of whether accumulate states or reinitialize when next forward.\n",
    "            self.gradient_clipping: float, The threshold for gradient clipping.\n",
    "            self.backend: Literal[\"torch\", \"numpy\"], The computational backend used by the layer.\n",
    "            self.dtype: type, The data type of the tensor values.\n",
    "            self.device: str | None, The device where the parameters are placed.\n",
    "            self.autograd: bool, Whether PyTorch's autograd is enabled for this layer.\n",
    "            self._parameters.weight_ih: nn_Parameter, input weights\n",
    "            self._parameters.bias_ih: nn_Parameter | None, input bias\n",
    "            self._parameters.weight_hh: nn_Parameter, hidden weights\n",
    "            self._parameters.bias_hh: nn_Parameter | None, hidden bias\n",
    "            self._cache: dict of list of tensors, for backward propagation\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__(module_name = module_name, backend = backend, dtype = dtype, device = device, autograd = autograd)\n",
    "        \n",
    "        # Record shapes etc\n",
    "        self.__setattr__(\"in_features\", in_features)\n",
    "        self.__setattr__(\"hid_features\", hid_features)\n",
    "        self.__setattr__(\"num_layers\", num_layers)\n",
    "        self.__setattr__(\"has_bias\", has_bias)\n",
    "        self.__setattr__(\"init_scale\", init_scale)\n",
    "        self.__setattr__(\"tbptt_steps\", tbptt_steps)\n",
    "        self.__setattr__(\"return_sequences\", return_sequences)\n",
    "        self.__setattr__(\"return_state\", return_state)\n",
    "        self.__setattr__(\"accumulate_hidden\", accumulate_hidden)\n",
    "        self.__setattr__(\"gradient_clipping\", gradient_clipping)\n",
    "        \n",
    "        # Initialize stacked RNN components, each returning full sequence + its final state\n",
    "        for i in range(num_layers):\n",
    "            self.__setattr__(\"LSTM_Layer_\"+str(i), \n",
    "                             nn_Layer_LSTMCell(\n",
    "                                 in_features=in_features if i == 0 else hid_features,\n",
    "                                 hid_features=hid_features,\n",
    "                                 has_bias=has_bias,\n",
    "                                 init_scale=init_scale,\n",
    "                                 tbptt_steps=tbptt_steps,\n",
    "                                 return_sequences=True,\n",
    "                                 return_state=True,\n",
    "                                 accumulate_hidden=accumulate_hidden,\n",
    "                                 gradient_clipping=gradient_clipping,\n",
    "                                 module_name=module_name+\"_\"+\"LSTM_Layer_\"+str(i),\n",
    "                                 backend=backend,\n",
    "                                 dtype=dtype,\n",
    "                                 device=device,\n",
    "                                 autograd=autograd,\n",
    "                                 **kwargs\n",
    "                                 )\n",
    "                             )\n",
    "\n",
    "    def reset_hidden(self) -> None:\n",
    "        \"\"\"\n",
    "        Reset the stored hidden state if done TBPTT.\n",
    "        If invoked, when calling forward() next time, hiddden state will be starting from 0s.\n",
    "        \"\"\"\n",
    "        for k in self._modules.keys():\n",
    "            self._modules[k].reset_hidden()\n",
    "\n",
    "    def clear_cache(self) -> None:\n",
    "        \"\"\"\n",
    "        Clear the cache dict to release Tensors and avoid issues.\n",
    "        If invoked, cache will be an empty dict. Do not invoke before applying backward().\n",
    "        \"\"\"\n",
    "        for k in self._modules.keys():\n",
    "            self._modules[k].clear_cache()\n",
    "\n",
    "    def forward(self, x: Tensor, h0: List[Tensor] | None = None, c0: List[Tensor] | None = None) -> Tensor| Tuple[Tensor, List[Tensor], List[Tensor]]:\n",
    "        \"\"\"\n",
    "        Compute the layer output: h_t = \\tanh(W_{ih} x_t + b_{ih} + W_{hh} h_{t-1} + b_{hh}) \\tag{1} and return it.\n",
    "        You can safely choose NOT to return hidden states since we have memorized them if you do not pass.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape (batch_size, seq_len, input_size)) to be transformed by the layer.\n",
    "            h0 (List[Tensor] | None): initial hidden state, List of Tensors of shape (batch, hidden_size) or left None.\n",
    "            c0 (List[Tensor] | None): initial cell state, List of Tensors of shape (batch, hidden_size) or left None.\n",
    "            \n",
    "        Returns:\n",
    "            Tensor: Output tensor of shape (batch_size, hid_features) after applying the RNN transformation.\n",
    "            or \n",
    "            A tuple of 3 having outputs and the List of Tensors containing hidden state and cell state.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the input `x` is not a valid MML.Tensor object.\n",
    "            ValueError: If the input `h0` is given but not a list of Tensors.\n",
    "            ValueError: If the input `c0` is given but not a list of Tensors.\n",
    "        \"\"\"\n",
    "        # Type check, x must be an instance of Tensor\n",
    "        if isinstance(x, Tensor) == False:\n",
    "            raise ValueError(f\"In performing forward(), input `x` must be in a MML `Tensor` format but you have {type(x)}\")\n",
    "        \n",
    "        # If h0 is given, it must have (batch_size, hid_features) size\n",
    "        if h0 is not None:\n",
    "            if isinstance(h0, list) == False:\n",
    "                raise ValueError(f\"In performing forward(), input `h0` must be in a List of MML `Tensor` format but you have {type(h0)}\")\n",
    "            if isinstance(h0[0], Tensor) == False:\n",
    "                raise ValueError(f\"In performing forward(), if given an input `h0`, it must be a list of MML `Tensor`s but you have {type(h0[0])}\")\n",
    "            \n",
    "        # If c0 is given, it must have (batch_size, hid_features) size\n",
    "        if c0 is not None:\n",
    "            if isinstance(c0, list) == False:\n",
    "                raise ValueError(f\"In performing forward(), input `c0` must be in a List of MML `Tensor` format but you have {type(c0)}\")\n",
    "            if isinstance(c0[0], Tensor) == False:\n",
    "                raise ValueError(f\"In performing forward(), if given an input `c0`, it must be a list of MML `Tensor`s but you have {type(c0[0])}\")\n",
    "             \n",
    "        # Clear cache to release spaces\n",
    "        self.clear_cache()\n",
    "             \n",
    "        # Prepare output and final states/cells\n",
    "        output = x\n",
    "        final_states = []\n",
    "        final_cells = []\n",
    "\n",
    "        # Perform RNN Layer by layer\n",
    "        for idx, key in enumerate(self._modules.keys()):\n",
    "            h = h0[idx] if (h0 is not None) else None\n",
    "            c = c0[idx] if (c0 is not None) else None\n",
    "            # Perform a forward pass for each layer sequentially\n",
    "            output, h_n, c_n = self._modules[key].forward(output, h, c)\n",
    "            final_states.append(h_n)\n",
    "            final_cells.append(c_n)\n",
    "            \n",
    "        # if only last step is wanted, slice it off the sequence\n",
    "        if self.return_sequences == False:\n",
    "            # (batch, hidden_size)\n",
    "            output = output[:, -1, ...] \n",
    "            \n",
    "        # If return state, then return a list of states\n",
    "        if self.return_state == True:\n",
    "            return output, final_states, final_cells\n",
    "        # Otherwise, only the output\n",
    "        else:\n",
    "            return output\n",
    "        \n",
    "    def backward(self, grad_output: Tensor, grad_h: List[Tensor] | None = None, grad_c: List[Tensor] | None = None) -> Tensor | Tuple[Tensor, List[Tensor], List[Tensor]]:\n",
    "        \"\"\"\n",
    "        Backward pass: compute gradients for weight, bias, and input.\n",
    "\n",
    "        This method performs the gradient computation for a dense layer during backpropagation. \n",
    "        It calculates the gradients of the loss with respect to the weights, biases, and input tensor,\n",
    "        based on the provided `grad_output` (gradient from the next layer). The implementation\n",
    "        supports both PyTorch autograd and manual gradient calculation modes.\n",
    "\n",
    "        Args:\n",
    "            grad_output (Tensor): Gradient tensor resulting from the output of the layer, used as input for backpropagation.\n",
    "                     - If return_sequences=True, shape is (batch, seq_len, hidden_size)\n",
    "                     - If return_sequences=False, shape is (batch, hidden_size) for last output.\n",
    "            grad_h (List[Tensor] | None): Gradient of loss w.r.t. the final hidden state.\n",
    "                     - This is usually None unless the LSTM's final state is fed into another layer.\n",
    "                     - If used, it is List of Tensors with shape (batch, hidden_size)\n",
    "            grad_c (List[Tensor] | None): Gradient of loss w.r.t. the final cell state.\n",
    "                     - This is usually None unless the LSTM's final state is fed into another layer.\n",
    "                     - If used, it is List of Tensors with shape (batch, hidden_size)\n",
    "                     \n",
    "        Returns:\n",
    "            Tuple of \n",
    "                (Tensor: Gradient with respect to the input tensor, for recursive backward calculations in previous layers, shape (batch, seq_len, input_size) tensor.\n",
    "                 List[Tensor]: Gradients with respect to each final hidden state, List[∂L/∂h0] (batch, hidden).\n",
    "                 List[Tensor]: Gradients with respect to each final cell state, List[∂L/∂c0] (batch, hidden).\n",
    "                 )\n",
    "        Raises:\n",
    "            ValueError: If `grad_output` is not a valid MML.Tensor object.\n",
    "            ValueError: If `grad_h` is given but not a valid list of MML.Tensor object.\n",
    "            ValueError: If `grad_c` is given but not a valid list of MML.Tensor object.\n",
    "        \"\"\"\n",
    "        \n",
    "        # If use autograd, pass; if manual mode, then calculate\n",
    "        if self.autograd == True:\n",
    "            return None\n",
    "        \n",
    "        # Type check, grad_output must be an instance of Tensor\n",
    "        if isinstance(grad_output, Tensor) == False:\n",
    "            raise ValueError(f\"In performing backward(), `grad_output` must be in a MML `Tensor` format but you have {type(grad_output)}\")\n",
    "        \n",
    "        # Type check, if given, grad_h must be an instance of Tensor\n",
    "        if grad_h is not None:\n",
    "            if isinstance(grad_h, list) == False:\n",
    "                raise ValueError(f\"In performing backward(), if given `grad_h`, it must be in a List of MML `Tensor`s format but you have {type(grad_h)} with each {type(grad_h[0])}\")\n",
    "            if isinstance(grad_h[0], Tensor) == False:\n",
    "                raise ValueError(f\"In performing backward(), if given `grad_h`, it must be in a List of MML `Tensor`s format but you have {type(grad_h)} with each {type(grad_h[0])}\")\n",
    "        \n",
    "        # Type check, if given, grad_c must be an instance of Tensor\n",
    "        if grad_c is not None:\n",
    "            if isinstance(grad_c, list) == False:\n",
    "                raise ValueError(f\"In performing backward(), if given `grad_c`, it must be in a List of MML `Tensor`s format but you have {type(grad_c)} with each {type(grad_c[0])}\")\n",
    "            if isinstance(grad_c[0], Tensor) == False:\n",
    "                raise ValueError(f\"In performing backward(), if given `grad_c`, it must be in a List of MML `Tensor`s format but you have {type(grad_c)} with each {type(grad_c[0])}\")\n",
    "        \n",
    "        # prepare per-layer state gradients\n",
    "        if grad_h is None:\n",
    "            grad_h = [None] * self.num_layers\n",
    "\n",
    "        # prepare per-layer cell gradients\n",
    "        if grad_c is None:\n",
    "            grad_c = [None] * self.num_layers\n",
    "                 \n",
    "        # grad flowing into top layer's output\n",
    "        next_grad_out = grad_output\n",
    "        grad_h0_list = [None] * self.num_layers\n",
    "        grad_c0_list = [None] * self.num_layers\n",
    "\n",
    "        # walk backward through layers\n",
    "        keys = list(self._modules.keys())\n",
    "        for i in reversed(range(self.num_layers)):\n",
    "            layer    = self._modules[keys[i]]\n",
    "            grad_h_n = grad_h[i]\n",
    "            grad_c_n = grad_c[i]\n",
    "            grad_out  = next_grad_out\n",
    "\n",
    "            # if only last-step output was used externally, expand it\n",
    "            if self.return_sequences == False and i == self.num_layers - 1:\n",
    "                seq_len = layer._cache[\"seq_len\"]\n",
    "                batch_size = layer._cache[\"batch_size\"]\n",
    "                hidden = layer.hid_features\n",
    "                full = Tensor.zeros((batch_size, seq_len, hidden), backend=self.backend, dtype=self.dtype, device=self.device)\n",
    "                full[:, -1, ...] = grad_out\n",
    "                grad_out = full\n",
    "\n",
    "            # call sub-layer backward (must return three)\n",
    "            grad_x, grad_h0, grad_c0 = layer.backward(grad_out, grad_h_n, grad_c_n)\n",
    "\n",
    "            grad_h0_list[i] = grad_h0\n",
    "            grad_c0_list[i] = grad_c0\n",
    "            next_grad_out = grad_x\n",
    "\n",
    "        # next_grad_out is gradient w.rt the very input x\n",
    "        # grad_h0_list is gradient w.rt the hidden state of each layer\n",
    "        # grad_c0_list is gradient w.rt the cell state of each layer\n",
    "        return next_grad_out, grad_h0_list, grad_c0_list\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"nn_Layer_StackedLSTM(shape: ({self.in_features}, {self.hid_features}), num_layers: {self.num_layers} with{'out' if self.has_bias == False else ''} bias).\"\n",
    "    \n",
    "\n",
    "# Alias for nn_Layer_StackedLSTM\n",
    "StackedLSTM = nn_Layer_StackedLSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Evauator is self implemented and open-sourced\n",
    "# Available at https://github.com/dof-studio/MML/\n",
    "# By Nathmath Huang (bh2821)\n",
    "# License: Apache License Version 2.0\n",
    "\n",
    "# Neural Network Fast Evaluation Pipeline\n",
    "class nn_SInterf_Evaluator(nn_Base, Regression, Classification):\n",
    "    \"\"\"\n",
    "    Neural Network Simple Interface - Evaluation pipeline.\n",
    "    \n",
    "    This evaluator accepts a neural network module, a ctiterion module (loss function),\n",
    "    an optimizer instance and conduct controlled automatic training and evaluation job.\n",
    "    You may use fit(), predict(), or other APIs to experience an easy-to-use and optimized\n",
    "    neural network training process with full automation loaded.\n",
    "    \"\"\"\n",
    "    \n",
    "    __attr__ = \"MML.nn_SInterf_Evaluator\"\n",
    "    \n",
    "    def __init__(self, name: str = \"Evaluator\",\n",
    "                       task: str = \"classification\",\n",
    "                       module: nn_Module | None = None,\n",
    "                       criterion: nn_Loss_BaseLoss | None = None,\n",
    "                       optimizer: nn_Optm_BaseOptimizer | None = None,\n",
    "                       **kwargs) -> None:\n",
    "        \"\"\"\n",
    "        Initialize an easy-interface evaluator pipeline object by passing in modules.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "            task: str, one of {\"classification\", \"regression\"}, showing the learning task.\n",
    "            module: nn_Module | None, you must pass an nn_Module which is the root node of your neural network structure.\n",
    "            criterion: nn_Loss_BaseLoss, you must pass an instance of a loss function that is the child of the base class.\n",
    "            optimizer: nn_Optm_BaseOptimizer, you must pass an instance of an optimizer that is the child of the base class.\n",
    "            Optional:\n",
    "                **kwargs: other key word arguments, reserved for compatibility use.\n",
    "            \n",
    "        Raise\n",
    "        ----------\n",
    "            ValueError, if task it not valid.\n",
    "            TypeError, if any of the parameter is None or does not have the correct type.\n",
    "        \"\"\"\n",
    "        # Task check\n",
    "        if task not in {\"classification\", \"regression\"}:\n",
    "            raise ValueError(f\"In initializing an evaluator, `task` must be either 'classification' or 'regression', but got {task}\")\n",
    "\n",
    "        # Type check (must be the type specified but not None or others)\n",
    "        if module is not None and not isinstance(module, nn_Module):\n",
    "            raise TypeError(\"In initializing an evaluator, `module` must be an instance of nn_Module\")\n",
    "        elif module is None:\n",
    "            raise TypeError(\"In initializing an evaluator, `module` must be initialized and cannot be None\")\n",
    "    \n",
    "        if criterion is not None and not isinstance(criterion, nn_Loss_BaseLoss):\n",
    "            raise TypeError(\"In initializing an evaluator, `criterion` must be an instance of nn_Loss_BaseLoss\")\n",
    "        elif criterion is None:\n",
    "            raise TypeError(\"In initializing an evaluator, `criterion` must be initialized and cannot be None\")\n",
    "        \n",
    "        if optimizer is not None and not isinstance(optimizer, nn_Optm_BaseOptimizer):\n",
    "            raise TypeError(\"In initializing an evaluator, `optimizer` must be an instance of nn_Optm_BaseOptimizer\")\n",
    "        elif optimizer is None:\n",
    "            raise TypeError(\"In initializing an evaluator, `optimizer` must be initialized and cannot be None\")\n",
    "            \n",
    "        # Call the nn_Base to keep the format consistent\n",
    "        super().__init__()\n",
    "        \n",
    "        # Record name, task, module, criterion, optimizer\n",
    "        self.name = name\n",
    "        self.task = task\n",
    "        self.module = module\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        # Create a reference of reference_X and reference_y WITHOUT COPY\n",
    "        self.reference_X = None   # NO COPY\n",
    "        self.reference_y = None   # ON COPY\n",
    "        \n",
    "        # Create a recoder of batch_size\n",
    "        self.batch_size = None\n",
    "        \n",
    "        # Create a counter of how may epoches and steps trainned\n",
    "        self.n_epoch = 0\n",
    "        self.n_step = 0\n",
    "        \n",
    "        # Create a dictionary to collect training loss and evaluation information (if any)\n",
    "        self.losses_ = {}    # Stepwise, index: step number\n",
    "        self.evalhist_ = {}  # Some_epoch-wise, index: epoch number\n",
    "        \n",
    "        # Create a record of random state\n",
    "        self.random_state = None\n",
    "        \n",
    "        # Create a record of to_device which means we need to redevice the data before training/testing\n",
    "        self.to_device = None\n",
    "        \n",
    "    def _fit_epoch_prep(self, X: Tensor, y: Tensor,\n",
    "                        batch_size: int | None = None,\n",
    "                        shuffle: bool = True, \n",
    "                        random_state: int | None = None, \n",
    "                        **kwargs) -> Tuple[Tensor, Tensor]:\n",
    "        \"\"\"\n",
    "        Prepare datasets and calculate initial values (for regression tasks and classification tasks).\n",
    "\n",
    "        Parameters:\n",
    "            ----------\n",
    "            X: Tensor, the feature tensor (the 1st dimension is sample).\n",
    "            y: Tensor, the target values (for regression, numerical; for classification, one-hot or multi-label).\n",
    "            batch_size: int, the number of samples trained each time. Must be greater than 1. If None, then use all.\n",
    "            shuffle: bool, whether data will be shuffled for each round (same device same type). By default, True (if batch_size is None then omitted).\n",
    "            random_seed: int | None, the random seed set to perform shuffle, can be None which means to randomly choose one.\n",
    "\n",
    "        Returns:\n",
    "            ----------\n",
    "            Tuple(X, y) shuffled copy or original reference\n",
    "        \"\"\"\n",
    "        \n",
    "        # We don't conduct type checks but checks if X or y are None\n",
    "        if X is None or y is None:\n",
    "            raise ValueError(\"In _fit_epoch_prep(), input `X` or `y` is/are None-type.\")\n",
    "        \n",
    "        # If no need to shuffle, then JUST RETURN without shuffling and copying\n",
    "        if shuffle == False or batch_size is None:\n",
    "            return X,y\n",
    "        elif batch_size >= X.shape[0]:\n",
    "            return X,y\n",
    "        \n",
    "        # Else, we shuffle based on the seed\n",
    "        if random_state is not None:\n",
    "            np.random.seed(random_state)\n",
    "        idx = list(range(X.shape[0]))\n",
    "        np.random.shuffle(idx)\n",
    "        \n",
    "        return X[idx], y[idx]\n",
    "     \n",
    "    def _fit_slice_batch(self, X: Tensor, y: Tensor,\n",
    "                         start: int | None = None,\n",
    "                         batch_size: int | None = None,\n",
    "                         to_device: str | None = None,\n",
    "                         **kwargs) -> Tuple[Tensor, Tensor]:\n",
    "        \"\"\"\n",
    "        Slice the input to create one mini-batch for training/testing.\n",
    "        \n",
    "        Parameters:\n",
    "            ----------\n",
    "            X: Tensor, the feature tensor (the 1st dimension is sample).\n",
    "            y: Tensor, the target values (for regression, numerical; for classification, one-hot or multi-label).\n",
    "            start: int | None, the starting index (begining) to be sliced for this round.\n",
    "            batch_size: int | None, the number of samples trained each time. Must be greater than 1. If None, then use all.\n",
    "            to_device: str | None, if non-None, we will perform device transformation after slicing them.\n",
    "            \n",
    "        Returns:\n",
    "            ----------\n",
    "            Tuple(X, y) sliced copy or original reference.\n",
    "        \"\"\"\n",
    "        \n",
    "        # We don't conduct type checks but checks if X or y are None\n",
    "        if X is None or y is None:\n",
    "            raise ValueError(\"In _fit_slice_batch(), input `X` or `y` is/are None-type.\")\n",
    "            \n",
    "        # If no need to slice, then JUST RETURN without shuffling and copying\n",
    "        if batch_size is None and start is None:\n",
    "            if to_device is None:\n",
    "                return X,y\n",
    "            else:\n",
    "                return X.to(backend=X._backend, dtype=X.dtype, device=to_device), y.to(backend=y._backend, dtype=y.dtype, device=to_device)\n",
    "        elif batch_size >= X.shape[0] and start is None:\n",
    "            if to_device is None:\n",
    "                return X,y\n",
    "            else:\n",
    "                return X.to(backend=X._backend, dtype=X.dtype, device=to_device), y.to(backend=y._backend, dtype=y.dtype, device=to_device)\n",
    "        \n",
    "        # Then, we need to slice.\n",
    "        if start is None:\n",
    "            raise ValueError(\"In _fit_slice_batch(), input `start` is None while a small batch_size is specified\")\n",
    "        if batch_size is None:\n",
    "            raise ValueError(\"In _fit_slice_batch(), input `batch_size` is None while a start is specified\")\n",
    "        if start >= X.shape[0]:\n",
    "            raise ValueError(f\"In _fit_slice_batch(), input `start` {start} is greater than the number of samples in `X`\")\n",
    "        \n",
    "        # Slice and return a copy.\n",
    "        if start + batch_size <= X.shape[0]:\n",
    "            idx = list(range(start, start + batch_size))\n",
    "        else:\n",
    "            idx = list(range(start, X.shape[0]))\n",
    "        if to_device is None:\n",
    "            return X[idx],y[idx]\n",
    "        else:\n",
    "            return X[idx].to(backend=X._backend, dtype=X.dtype, device=to_device), y[idx].to(backend=y._backend, dtype=y.dtype, device=to_device)\n",
    "    \n",
    "    def _fit_slice_batch_X(self, X: Tensor,\n",
    "                         start: int | None = None,\n",
    "                         batch_size: int | None = None,\n",
    "                         to_device: str | None = None,\n",
    "                         **kwargs) -> Tensor:\n",
    "        \"\"\"\n",
    "        Slice the input to create one mini-batch for training/testing.\n",
    "        \n",
    "        Parameters:\n",
    "            ----------\n",
    "            X: Tensor, the feature tensor (the 1st dimension is sample).\n",
    "            y: Tensor, the target values (for regression, numerical; for classification, one-hot or multi-label).\n",
    "            start: int | None, the starting index (begining) to be sliced for this round.\n",
    "            batch_size: int | None, the number of samples trained each time. Must be greater than 1. If None, then use all.\n",
    "            to_device: str | None, if non-None, we will perform device transformation after slicing them.\n",
    "            \n",
    "        Returns:\n",
    "            ----------\n",
    "            X sliced copy or original reference.\n",
    "        \"\"\"\n",
    "        \n",
    "        # We don't conduct type checks but checks if X or y are None\n",
    "        if X is None:\n",
    "            raise ValueError(\"In _fit_slice_batch_X(), input `X` is/are None-type.\")\n",
    "            \n",
    "        # If no need to slice, then JUST RETURN without shuffling and copying\n",
    "        if batch_size is None and start is None:\n",
    "            if to_device is None:\n",
    "                return X\n",
    "            else:\n",
    "                return X.to(backend=X._backend, dtype=X.dtype, device=to_device)\n",
    "        elif batch_size >= X.shape[0] and start is None:\n",
    "            if to_device is None:\n",
    "                return X\n",
    "            else:\n",
    "                return X.to(backend=X._backend, dtype=X.dtype, device=to_device)\n",
    "        \n",
    "        # Then, we need to slice.\n",
    "        if start is None:\n",
    "            raise ValueError(\"In _fit_slice_batch(), input `start` is None while a small batch_size is specified\")\n",
    "        if batch_size is None:\n",
    "            raise ValueError(\"In _fit_slice_batch(), input `batch_size` is None while a start is specified\")\n",
    "        if start >= X.shape[0]:\n",
    "            raise ValueError(f\"In _fit_slice_batch(), input `start` {start} is greater than the number of samples in `X`\")\n",
    "        \n",
    "        # Slice and return a copy.\n",
    "        if start + batch_size <= X.shape[0]:\n",
    "            idx = list(range(start, start + batch_size))\n",
    "        else:\n",
    "            idx = list(range(start, X.shape[0]))\n",
    "        if to_device is None:\n",
    "            return X[idx]\n",
    "        else:\n",
    "            return X[idx]    \n",
    "    \n",
    "    def _fit_train_one_step(self, X: Tensor, y: Tensor, **kwargs) -> Tuple[int, int, float]:\n",
    "        \"\"\"\n",
    "        Train the model for 1 complete step without switching to evaluation mode.\n",
    "        \n",
    "        Parameters:\n",
    "            ----------\n",
    "            X: Tensor, the mini-batch feature tensor (the 1st dimension is sample).\n",
    "            y: Tensor, the mini-batch target values (for regression, numerical; for classification, one-hot or multi-label).\n",
    "            \n",
    "        Returns:\n",
    "            ----------\n",
    "            Tuple[int, int, float]: (epoch, step, train_loss)\n",
    "\n",
    "        \"\"\"\n",
    "        # We don't conduct type checks but checks if X or y are None\n",
    "        if X is None or y is None:\n",
    "            raise ValueError(\"In _fit_slice_batch(), input `X` or `y` is/are None-type.\")\n",
    "        \n",
    "        # Module must be in training mode\n",
    "        if self.module.training == False:\n",
    "            raise RuntimeError(\"Called _fit_train_one_step() to perform one step training but the module is in non-training mode.\")\n",
    "        \n",
    "        # Module must have the same dtype, device with X\n",
    "        if self.module.dtype != X.dtype or self.module.device != X.device:\n",
    "            raise RuntimeError(\"Called _fit_train_one_step() to perform one step training but the module and your data have different dtype/device.\")\n",
    "        \n",
    "        # Perform a forward pass on the inputs\n",
    "        out = self.module.forward(X)\n",
    "        \n",
    "        # Calculate loss of this step\n",
    "        loss = self.criterion(out, y)\n",
    "        \n",
    "        # Perform the backward propagation of the loss function\n",
    "        lossgrad = self.criterion.backward()\n",
    "    \n",
    "        # Perform the backward propagation of the neural network module\n",
    "        self.module.backward(lossgrad)\n",
    "        \n",
    "        # Apply one step on optimizer to update the parameters\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Apply zero grad to clear the gradients\n",
    "        self.module.zero_grad()\n",
    "        \n",
    "        # Increment the step += 1\n",
    "        self.n_step += 1\n",
    "        return self.n_epoch, self.n_step, loss.to_list()\n",
    "        \n",
    "    def _fit_switch_to_mode(self, mode: Literal[\"train\", \"eval\"] = \"train\", **kwargs) -> None:\n",
    "        \"\"\"\n",
    "        Switch the module to train mode or evaluation mode.\n",
    "        \"\"\"\n",
    "        if mode not in {\"train\", \"eval\"}:\n",
    "            raise ValueError(f\"In _fit_switch_to_mode(), you gave a mode {mode} which is neither `train` nor `eval`.\")\n",
    "    \n",
    "        if mode == \"train\":\n",
    "            self.module.train()\n",
    "        elif mode == \"eval\":\n",
    "            self.module.eval()\n",
    "        return\n",
    "    \n",
    "    def _eval_one_batch(self, evalset: Dict[str, Tuple[Tensor, Tensor]] | None = None, evalmetrics: List[str] | str | None = None, one_hot: bool = True, **kwargs):\n",
    "        \"\"\"\n",
    "        Evaluate the `evalset` after training for one batch.    \n",
    "\n",
    "        Returns\n",
    "            -------\n",
    "            result_dict : dict  # Key: evalset name\n",
    "                                # Value dict {metric_name: metric_value}\n",
    "            or \n",
    "            {} if failed or did not evaluate\n",
    "        \"\"\"\n",
    "        # First switch to evaluation mode\n",
    "        self._fit_switch_to_mode(mode = \"eval\")\n",
    "        \n",
    "        # If:\n",
    "        # 1. sequential_batch non-None\n",
    "        # 2. evalset is at least len = 1\n",
    "        # 3. evalmetrics is non-None and at least len = 1\n",
    "        # Do evaluation\n",
    "        result_dict = {}\n",
    "        if evalmetrics is not None and evalset is not None:\n",
    "            if len(evalset) > 0 and len(evalmetrics) > 0:\n",
    "                # Record the result for each eval group\n",
    "                result_dict = {}    # Key: evalset name\n",
    "                                    # Value dict {metric_name: metric_value}\n",
    "                for eval_name in evalset.keys():\n",
    "                    X_sub, y_sub = evalset[eval_name]\n",
    "                    y_pred = self.predict(X_sub)\n",
    "                    \n",
    "                    # Inner metric dict, for values of result dict\n",
    "                    metrics = {}\n",
    "                    for metric_name in evalmetrics:\n",
    "                        \n",
    "                        # Evaluation: regression\n",
    "                        if self.task == \"regression\":\n",
    "                            eval_metric = RegressionMetrics(y_pred, y_sub, metric_type = metric_name).compute()\n",
    "                            # Matrix | Tensor\n",
    "                        \n",
    "                        # Evaluation: this is classification\n",
    "                        else:\n",
    "                            if (y_pred.shape[1] == 2 and one_hot == False) or y_pred.shape[1] == 1:\n",
    "                                # Binary and non-one hot\n",
    "                                eval_metric = BinaryClassificationMetrics(self._to_binary_prob(y_pred), y_sub, metric_type = metric_name).compute()\n",
    "                            else:\n",
    "                                # Since the aggregation output is alway one-hot, use Multiple then\n",
    "                                eval_metric = MultiClassificationMetrics(y_pred, y_sub, metric_type = metric_name).compute()\n",
    "                            # Matrix | Tensor\n",
    "                        metrics[metric_name] = eval_metric\n",
    "                    # For all metrics, put them into result_dict\n",
    "                    result_dict[eval_name] = metrics\n",
    "        \n",
    "        # If it is empty, then exit since nothing valid\n",
    "        if len(result_dict) == 0:\n",
    "            return {}\n",
    "        \n",
    "        # Else, return the dict\n",
    "        else:\n",
    "            return result_dict\n",
    "\n",
    "    def fit(self, \n",
    "            X: Tensor, \n",
    "            y: Tensor,\n",
    "            epoches: int = 100,\n",
    "            batch_size: int | None = None,\n",
    "            shuffle: bool = True,\n",
    "            random_state: int | None = None,\n",
    "            to_device: str | None = None,\n",
    "            *,\n",
    "            one_hot: bool = True,\n",
    "            verbosity: int | None = None,\n",
    "            evalper: int = 1,\n",
    "            evalset: Dict[str, Tuple[Tensor, Tensor]] | None = None,\n",
    "            evalmetrics: List[str] | str | None = None,\n",
    "            early_stop: int | None = None,\n",
    "            early_stop_logic: str = \"some\",\n",
    "            continue_to_train: bool | None = True,\n",
    "            **kwargs):\n",
    "        \"\"\"\n",
    "        Train n_estimators gradient boosting trees sequentially.\n",
    "        \n",
    "        Evaluation Remark:\n",
    "            ----------\n",
    "            You may want to evaluate datasets while training. If so, please do the following things:\n",
    "                1. set `verbosity` = 1 to print the evaluation\n",
    "                2. set the `evalset` to a dict of tuples of your dataset that is going to be evaluated\n",
    "                3. set the `evalmetrics` either to a string of metrics or a list of strings\n",
    "            You may want the algorithm to decide to stop training automatically. If so, please do things above, plus:\n",
    "                1. set `early_stop` to a number of batches, like 1 or 2, which acts like: \n",
    "                    if the metrics for all/some/any/most of the evaluation sets do not decrease anymore, \n",
    "                    the training process will be terminated and return\n",
    "                2. set `early_stop_logic` to determine the way of processing non-decreasing datasets/metrics\n",
    "                3. If you hope to continue to train again, call this `fit` again with `continue_to_train` set to True\n",
    "\n",
    "        Parameters:\n",
    "            ----------\n",
    "            X: Tensor, the feature tensor (the 1st dimension is sample).\n",
    "            y: Tensor, the target values (for regression, numerical; for classification, one-hot or multi-label).\n",
    "            epoches: int, the number of rounds (maximum rounds) to be trainned. Default is 100.\n",
    "            batch_size: int, the number of samples trained each time. Must be greater than 1. If None, then use all.\n",
    "            shuffle: bool, whether data will be shuffled for each round (same device same type). By default, True (if batch_size is None then omitted).\n",
    "            random_seed: int | None, the random seed set to perform shuffle, can be None which means to randomly choose one.\n",
    "            Optional:\n",
    "                one_hot : bool, if y is one-hot encoded for classification tasks.\n",
    "                verbosity: int | None, if >= 1 and having `evalset`, then will report metrics each batch.\n",
    "                evalper: int, the number of rounds to perform before evaluation conducted again.\n",
    "                evalset: Dict[name : Tuple[X, y],\n",
    "                              ...], | None, if provided, it may be used as evaluation set. XGBoost style.\n",
    "                evalmetrics: list of str | str | None, metrics used to do the evaluation. Will be printed.\n",
    "                early_stop: int | None, if non-None, then if metrics NOT gained for `early_stop` times, the forest will stop training.\n",
    "                early_stop_logic: str, the logic when deciding on multiple metrics, can be {\"any\", \"some\", \"most\", \"all\"}.\n",
    "                continue_to_train: bool | None, if non-None and True, the machine will try to restore the place it was and continue\n",
    "                                   to train new estimators until a new stopping criterion meets or until reaches the max number of allowed estimators.\n",
    "                \n",
    "        Returns:\n",
    "            ----------\n",
    "            self\n",
    "        \"\"\"\n",
    "\n",
    "        # Type Check (must be an Tensor type).\n",
    "        if isinstance(X, Tensor) == False or isinstance(y, Tensor) == False:\n",
    "            raise ValueError(\"Input dataset must be Tensor for neural networks. Use Tensor(data) or Tensor(data) to convert.\")\n",
    "        if type(X) != type(y):\n",
    "            raise ValueError(\"Input feature `X` and target `y` must have the same type, use Tensor instead.\")\n",
    "        \n",
    "        # Dimension Check.\n",
    "        if len(X.shape) < 2:\n",
    "            raise ValueError(\"Input feature `X` must be at least 2 dimensional (the first is for samples).\")\n",
    "        if len(y.shape) == 1:\n",
    "            raise ValueError(\"Input target `y` must also be a 2d data. If only one label or value, use data.reshape([-1, 1])\")\n",
    "                    \n",
    "        # Batch size Check.\n",
    "        if batch_size is not None:\n",
    "            if int(batch_size) < 1:\n",
    "                raise ValueError(\"Input `batch_size` must be an interger which is greater or equal to 1.\")\n",
    "                        \n",
    "        # Stopping Logic Check.\n",
    "        if early_stop_logic not in (\"any\", \"some\", \"most\", \"all\"):\n",
    "            raise ValueError(\"Stopping logic `early_stop_logic` must be one of ('any', 'some', 'most', 'all')\")\n",
    "    \n",
    "        # Record the original data, random seeds, and to_device\n",
    "        self.reference_X = X\n",
    "        self.reference_y = y\n",
    "        self.random_state = random_state\n",
    "        self.to_device = to_device\n",
    "        \n",
    "        # Record batch size\n",
    "        self.batch_size = int(batch_size) if batch_size is not None else None\n",
    "        \n",
    "        # Special evalmetrics type conversion\n",
    "        if isinstance(evalmetrics, str) == True:\n",
    "            evalmetrics = [evalmetrics]\n",
    "            \n",
    "        # Verbosity Conversion\n",
    "        verbosity = verbosity if verbosity is not None else 0\n",
    "        \n",
    "        # Create Evaluation Related Objects\n",
    "        undecreased_no = 0\n",
    "        last_eval_dict = {} # Please use deepcopy() here to avoid being errorly referred\n",
    "        \n",
    "        # Helper: Print and decide the evaluated results\n",
    "        def _decide_stop_with_print(batch: int, undecreased_no: int, eval_dict: dict, last_eval_dict: dict, **kwargs):\n",
    "            \"\"\"\n",
    "            Compare the metics and decide if stop or not.\n",
    "\n",
    "            Parameters\n",
    "                ----------\n",
    "                batch: int, batch no, for printing uses.\n",
    "                undecreased_no : int, cumulative number that loss did NOT decrease before evaluation.\n",
    "                eval_dict : dict, the passed evaluation dict.\n",
    "\n",
    "            Returns\n",
    "                -------\n",
    "                Tuple of (int, bool):\n",
    "                    int, updated undecreased_no\n",
    "                    bool, whether to stop (True) training or continue (False)\n",
    "\n",
    "            \"\"\"\n",
    "            # Dict is empty, abort\n",
    "            if len(eval_dict) == 0:\n",
    "                return undecreased_no, False\n",
    "            if len(last_eval_dict) == 0:\n",
    "                return undecreased_no, False\n",
    "            \n",
    "            # Difference dict copy\n",
    "            diff_dict = deepcopy(eval_dict)\n",
    "            \n",
    "            # Calculate the difference (this - last)\n",
    "            # and\n",
    "            # If verbosity, print the new evaluation dict\n",
    "            undes_count = 0\n",
    "            allmetric_count = 0\n",
    "            for evalset_name in eval_dict.keys():\n",
    "                eval_result = eval_dict[evalset_name]\n",
    "                if verbosity >= 1:\n",
    "                    print(\"Evalset: [\", evalset_name, \" : Metrics {\", end = \" \", sep = \"\")\n",
    "                for metric_name in eval_result.keys():\n",
    "                    metric_value = eval_result[metric_name]\n",
    "                    diff_dict[evalset_name][metric_name] = metric_value - last_eval_dict[evalset_name][metric_name]\n",
    "                    if diff_dict[evalset_name][metric_name].to_list() > 0:\n",
    "                        undes_count += 1\n",
    "                    allmetric_count += 1\n",
    "                    if verbosity >= 1:\n",
    "                        print(metric_name, \":\", round(metric_value.to_list(), 4), \", \", end = \" \", sep = \"\")\n",
    "                if verbosity >= 1:\n",
    "                    print(\"}]\", end = \"\\n\")\n",
    "                    \n",
    "            # If no early stop, directly return 0, False\n",
    "            if early_stop is None:\n",
    "                return 0, False\n",
    "                    \n",
    "            # If meets the requirement, stop training\n",
    "            if early_stop_logic == \"any\":\n",
    "                if undes_count > 0:\n",
    "                    undecreased_no += 1\n",
    "                    if undecreased_no >= early_stop:\n",
    "                        return undecreased_no, True\n",
    "                    else:\n",
    "                        return undecreased_no, False\n",
    "            elif early_stop_logic == \"some\":\n",
    "                if undes_count * 3 >= allmetric_count:\n",
    "                    undecreased_no += 1\n",
    "                    if undecreased_no >= early_stop:\n",
    "                        return undecreased_no, True\n",
    "                    else:\n",
    "                        return undecreased_no, False\n",
    "            elif early_stop_logic == \"most\":\n",
    "                if undes_count * 2 >= allmetric_count:\n",
    "                    undecreased_no += 1\n",
    "                    if undecreased_no >= early_stop:\n",
    "                        return undecreased_no, True\n",
    "                    else:\n",
    "                        return undecreased_no, False\n",
    "            elif early_stop_logic == \"all\":\n",
    "                if undes_count * 1 >= allmetric_count:\n",
    "                    undecreased_no += 1\n",
    "                    if undecreased_no >= early_stop:\n",
    "                        return undecreased_no, True\n",
    "                    else:\n",
    "                        return undecreased_no, False\n",
    "                    \n",
    "            # If survives here, return 0, False to refresh the undecreased_no\n",
    "            return 0, False\n",
    "                \n",
    "        #######################################################################\n",
    "        #        \n",
    "        # Iteratively train the neural network\n",
    "        rounds = 0\n",
    "        while rounds < epoches:\n",
    "            \n",
    "            # Verbosity\n",
    "            if verbosity >= 1:\n",
    "                print(f\"Training on Total Epoch: {self.n_epoch}, Round: {rounds}\")\n",
    "\n",
    "            ###################################################################\n",
    "            #\n",
    "            # If needs to shuffle the data, then shuffle it\n",
    "            epo_X, epo_y = self._fit_epoch_prep(X, y, \n",
    "                    batch_size=batch_size, shuffle=shuffle, random_state=self._random_state_next(), **kwargs)\n",
    "            \n",
    "            # Calculate number of steps in this round\n",
    "            if batch_size is None:\n",
    "                this_steps = 1\n",
    "            elif batch_size >= epo_X.shape[0]:\n",
    "                this_steps = 1\n",
    "            else:\n",
    "                this_steps = int(np.ceil(epo_X.shape[0] / batch_size))\n",
    "            \n",
    "            ###################################################################\n",
    "            #\n",
    "            # Formally strat to train this epoch, we first transfer to train mode\n",
    "            self._fit_switch_to_mode(mode = \"train\")\n",
    "            \n",
    "            # For steps in one epoch, train iteratively\n",
    "            if this_steps == 1:\n",
    "                # Just train and get the results\n",
    "                stp_X, stp_y = self._fit_slice_batch(epo_X, epo_y, start = None, batch_size = None, to_device = to_device, **kwargs)\n",
    "                _epoch, _step, _loss = self._fit_train_one_step(stp_X, stp_y, **kwargs)\n",
    "                # Record the step loss\n",
    "                self.losses_[_step] = _loss\n",
    "                \n",
    "            else:\n",
    "                for step in range(this_steps):\n",
    "                    # Prepare step-sliced data\n",
    "                    stp_X, stp_y = self._fit_slice_batch(epo_X, epo_y, start = step * batch_size, \n",
    "                         batch_size = batch_size, to_device = to_device, **kwargs)\n",
    "                    # Train one step\n",
    "                    _epoch, _step, _loss = self._fit_train_one_step(stp_X, stp_y, **kwargs)\n",
    "                    # Record the step loss\n",
    "                    self.losses_[_step] = _loss\n",
    "                                        \n",
    "            # Self-increment epoch\n",
    "            self.n_epoch += 1\n",
    "            \n",
    "            ###################################################################\n",
    "            #\n",
    "            # Evaluation if needed\n",
    "            if rounds % evalper != 0 or rounds == 0:\n",
    "                # Count self increasing\n",
    "                rounds += 1\n",
    "                continue\n",
    "            \n",
    "            # Evaluate and decide if stop training from now\n",
    "            eval_dict = self._eval_one_batch(evalset = evalset, evalmetrics = evalmetrics, one_hot = one_hot, **kwargs)\n",
    "            \n",
    "            # Try stop maker and receive the advice\n",
    "            undecreased_no, decision = _decide_stop_with_print(rounds, undecreased_no = undecreased_no, eval_dict = eval_dict, last_eval_dict = last_eval_dict)\n",
    "            \n",
    "            # Record the evaluation result\n",
    "            self.evalhist_[self.n_epoch] = deepcopy(eval_dict)\n",
    "            \n",
    "            # Copy last evaluated dict\n",
    "            last_eval_dict = deepcopy(eval_dict)\n",
    "            \n",
    "            # Count self increasing\n",
    "            rounds += 1\n",
    "            \n",
    "            # Make decision to terminate or not\n",
    "            if decision == True:\n",
    "                break\n",
    "            \n",
    "        return self\n",
    "            \n",
    "    def eval(self) -> None:\n",
    "        \"\"\"\n",
    "        Switch the module to evaluation mode.\n",
    "        \"\"\"\n",
    "        self.module.eval()\n",
    "        return\n",
    "    \n",
    "    def predict(self, X: Tensor, **kwargs) -> Tensor:\n",
    "        \"\"\"\n",
    "        Predict target values for samples in X in batches.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor, output of predictions.\n",
    "            \n",
    "        Raises:\n",
    "            RuntimeError: if you did NOT switched to evalation mode.\n",
    "        \"\"\"\n",
    "        # Check the module training status\n",
    "        if self.module.training == True:\n",
    "            raise RuntimeError(\"In predict(), you called this in training mode. Please call .eval() to make the model safe in evaluation mode.\")\n",
    "        \n",
    "        # Type Check (must be an Tensor type).\n",
    "        if isinstance(X, Tensor) == False:\n",
    "            raise ValueError(\"Input dataset must be a Tensor. Use Tensor(data) to convert.\")\n",
    "        \n",
    "        # If does not need mini-batch, directly go with X to deviced\n",
    "        if self.batch_size is None:\n",
    "            epo_X = X if self.to_device is None else X.to(X._backend, dtype=X.dtype, device=self.to_device)\n",
    "            return self.module.forward(epo_X)\n",
    "        elif self.batch_size >= X.shape[0]:\n",
    "            epo_X = X if self.to_device is None else X.to(X._backend, dtype=X.dtype, device=self.to_device)\n",
    "            return self.module.forward(epo_X)\n",
    "        \n",
    "        # We have to batchly predict to avoid exceeding the limit of memory\n",
    "        else:\n",
    "            pred = None\n",
    "            start = 0\n",
    "            while start < X.shape[0]:\n",
    "                stp_X = self._fit_slice_batch_X(X, start = start, batch_size = self.batch_size, to_device = self.to_device, **kwargs)\n",
    "                stp_pred = self.module.forward(stp_X)\n",
    "                if pred is None:\n",
    "                    pred = stp_pred\n",
    "                else:\n",
    "                    pred = pred.vstack(stp_pred)\n",
    "                start += self.batch_size\n",
    "            return pred\n",
    "\n",
    "    def predict_loss(self, X: Tensor, y: Tensor, **kwargs) -> Tuple[Tensor, Tensor]:\n",
    "        \"\"\"\n",
    "        Predict target values for samples in X and calculate the loss by a given target y.\n",
    "        \n",
    "        Returns:\n",
    "            Tuple[Tensor, Tensor]: output of predictions, loss.\n",
    "            \n",
    "        Raises:\n",
    "            RuntimeError: if you did NOT switched to evalation mode.\n",
    "        \"\"\"\n",
    "        # Check the module training status\n",
    "        if self.module.training == True:\n",
    "            raise RuntimeError(\"In predict(), you called this in training mode. Please call .eval() to make the model safe in evaluation mode.\")\n",
    "        \n",
    "        # Type Check (must be an Tensor type).\n",
    "        if isinstance(X, Tensor) == False:\n",
    "            raise ValueError(\"Input dataset must be either Matrix and Tensor. Use Matrix(data) or Tensor(data) to convert.\")\n",
    "        \n",
    "        # Conduct a forward pass and return result\n",
    "        pred = self.predict(X, kwargs)    \n",
    "        \n",
    "        # Compute loss function\n",
    "        loss = self.criterion.forward(pred, y if self.to_device is None else y.to(backend=y._backend, dtype=y.dtype, device=self.to_device))\n",
    "        return pred, loss\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"NN Simple Interf Evaluator(name = {self.name}, task = {self.task}, has trained n_epoch = {self.n_epoch}, n_step = {self.n_step}).\"\n",
    "    \n",
    "    \n",
    "# Alias for nn_SInterf_Evaluator\n",
    "Evaluator = nn_SInterf_Evaluator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2.1 Download USD-EUR exchange rate daily data and load into the Notebook Environment do Pre-processing`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Price</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Price_Rate</th>\n",
       "      <th>Open_Rate</th>\n",
       "      <th>High_Rate</th>\n",
       "      <th>Low_Rate</th>\n",
       "      <th>Price_MA</th>\n",
       "      <th>Open_MA</th>\n",
       "      <th>High_MA</th>\n",
       "      <th>Low_MA</th>\n",
       "      <th>Price_Rate_MA</th>\n",
       "      <th>Open_Rate_MA</th>\n",
       "      <th>High_Rate_MA</th>\n",
       "      <th>Low_Rate_MA</th>\n",
       "      <th>Next Day Average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5/26/2006</th>\n",
       "      <td>1.2732</td>\n",
       "      <td>1.2807</td>\n",
       "      <td>1.2837</td>\n",
       "      <td>1.2700</td>\n",
       "      <td>-0.005623</td>\n",
       "      <td>0.003998</td>\n",
       "      <td>0.001326</td>\n",
       "      <td>-0.003687</td>\n",
       "      <td>1.276360</td>\n",
       "      <td>1.275835</td>\n",
       "      <td>1.282910</td>\n",
       "      <td>1.269505</td>\n",
       "      <td>0.000629</td>\n",
       "      <td>0.000758</td>\n",
       "      <td>0.000604</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>1.274300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5/29/2006</th>\n",
       "      <td>1.2751</td>\n",
       "      <td>1.2733</td>\n",
       "      <td>1.2770</td>\n",
       "      <td>1.2718</td>\n",
       "      <td>0.001492</td>\n",
       "      <td>-0.005778</td>\n",
       "      <td>-0.005219</td>\n",
       "      <td>0.001417</td>\n",
       "      <td>1.277190</td>\n",
       "      <td>1.276360</td>\n",
       "      <td>1.283300</td>\n",
       "      <td>1.270320</td>\n",
       "      <td>0.000672</td>\n",
       "      <td>0.000432</td>\n",
       "      <td>0.000313</td>\n",
       "      <td>0.000657</td>\n",
       "      <td>1.281850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5/30/2006</th>\n",
       "      <td>1.2873</td>\n",
       "      <td>1.2749</td>\n",
       "      <td>1.2912</td>\n",
       "      <td>1.2740</td>\n",
       "      <td>0.009568</td>\n",
       "      <td>0.001257</td>\n",
       "      <td>0.011120</td>\n",
       "      <td>0.001730</td>\n",
       "      <td>1.278480</td>\n",
       "      <td>1.277210</td>\n",
       "      <td>1.284515</td>\n",
       "      <td>1.271230</td>\n",
       "      <td>0.001031</td>\n",
       "      <td>0.000688</td>\n",
       "      <td>0.000960</td>\n",
       "      <td>0.000731</td>\n",
       "      <td>1.284750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5/31/2006</th>\n",
       "      <td>1.2810</td>\n",
       "      <td>1.2873</td>\n",
       "      <td>1.2904</td>\n",
       "      <td>1.2803</td>\n",
       "      <td>-0.004894</td>\n",
       "      <td>0.009726</td>\n",
       "      <td>-0.000620</td>\n",
       "      <td>0.004945</td>\n",
       "      <td>1.279330</td>\n",
       "      <td>1.278505</td>\n",
       "      <td>1.285680</td>\n",
       "      <td>1.272295</td>\n",
       "      <td>0.000687</td>\n",
       "      <td>0.001036</td>\n",
       "      <td>0.000921</td>\n",
       "      <td>0.000851</td>\n",
       "      <td>1.279175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6/1/2006</th>\n",
       "      <td>1.2809</td>\n",
       "      <td>1.2811</td>\n",
       "      <td>1.2826</td>\n",
       "      <td>1.2721</td>\n",
       "      <td>-0.000078</td>\n",
       "      <td>-0.004816</td>\n",
       "      <td>-0.006045</td>\n",
       "      <td>-0.006405</td>\n",
       "      <td>1.279905</td>\n",
       "      <td>1.279400</td>\n",
       "      <td>1.286185</td>\n",
       "      <td>1.273045</td>\n",
       "      <td>0.000470</td>\n",
       "      <td>0.000723</td>\n",
       "      <td>0.000405</td>\n",
       "      <td>0.000606</td>\n",
       "      <td>1.286500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5/5/2025</th>\n",
       "      <td>1.1315</td>\n",
       "      <td>1.1319</td>\n",
       "      <td>1.1365</td>\n",
       "      <td>1.1292</td>\n",
       "      <td>0.001682</td>\n",
       "      <td>0.002746</td>\n",
       "      <td>-0.001406</td>\n",
       "      <td>0.001685</td>\n",
       "      <td>1.131440</td>\n",
       "      <td>1.129345</td>\n",
       "      <td>1.137625</td>\n",
       "      <td>1.125210</td>\n",
       "      <td>0.001882</td>\n",
       "      <td>0.001706</td>\n",
       "      <td>0.001429</td>\n",
       "      <td>0.001864</td>\n",
       "      <td>1.133675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5/6/2025</th>\n",
       "      <td>1.1370</td>\n",
       "      <td>1.1315</td>\n",
       "      <td>1.1382</td>\n",
       "      <td>1.1280</td>\n",
       "      <td>0.004861</td>\n",
       "      <td>-0.000353</td>\n",
       "      <td>0.001496</td>\n",
       "      <td>-0.001063</td>\n",
       "      <td>1.133510</td>\n",
       "      <td>1.131395</td>\n",
       "      <td>1.139570</td>\n",
       "      <td>1.127170</td>\n",
       "      <td>0.001886</td>\n",
       "      <td>0.001875</td>\n",
       "      <td>0.001767</td>\n",
       "      <td>0.001788</td>\n",
       "      <td>1.133525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5/7/2025</th>\n",
       "      <td>1.1300</td>\n",
       "      <td>1.1370</td>\n",
       "      <td>1.1379</td>\n",
       "      <td>1.1292</td>\n",
       "      <td>-0.006157</td>\n",
       "      <td>0.004861</td>\n",
       "      <td>-0.000264</td>\n",
       "      <td>0.001064</td>\n",
       "      <td>1.135255</td>\n",
       "      <td>1.133450</td>\n",
       "      <td>1.140985</td>\n",
       "      <td>1.129065</td>\n",
       "      <td>0.001601</td>\n",
       "      <td>0.001871</td>\n",
       "      <td>0.001285</td>\n",
       "      <td>0.001726</td>\n",
       "      <td>1.126950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5/8/2025</th>\n",
       "      <td>1.1228</td>\n",
       "      <td>1.1301</td>\n",
       "      <td>1.1337</td>\n",
       "      <td>1.1212</td>\n",
       "      <td>-0.006372</td>\n",
       "      <td>-0.006069</td>\n",
       "      <td>-0.003691</td>\n",
       "      <td>-0.007085</td>\n",
       "      <td>1.135410</td>\n",
       "      <td>1.135200</td>\n",
       "      <td>1.141460</td>\n",
       "      <td>1.130410</td>\n",
       "      <td>0.000160</td>\n",
       "      <td>0.001604</td>\n",
       "      <td>0.000443</td>\n",
       "      <td>0.001235</td>\n",
       "      <td>1.124125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5/9/2025</th>\n",
       "      <td>1.1248</td>\n",
       "      <td>1.1228</td>\n",
       "      <td>1.1293</td>\n",
       "      <td>1.1196</td>\n",
       "      <td>0.001781</td>\n",
       "      <td>-0.006460</td>\n",
       "      <td>-0.003881</td>\n",
       "      <td>-0.001427</td>\n",
       "      <td>1.134850</td>\n",
       "      <td>1.135330</td>\n",
       "      <td>1.140550</td>\n",
       "      <td>1.130445</td>\n",
       "      <td>-0.000479</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>-0.000788</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>1.115600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4946 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Price    Open    High     Low  Price_Rate  Open_Rate  High_Rate  \\\n",
       "5/26/2006  1.2732  1.2807  1.2837  1.2700   -0.005623   0.003998   0.001326   \n",
       "5/29/2006  1.2751  1.2733  1.2770  1.2718    0.001492  -0.005778  -0.005219   \n",
       "5/30/2006  1.2873  1.2749  1.2912  1.2740    0.009568   0.001257   0.011120   \n",
       "5/31/2006  1.2810  1.2873  1.2904  1.2803   -0.004894   0.009726  -0.000620   \n",
       "6/1/2006   1.2809  1.2811  1.2826  1.2721   -0.000078  -0.004816  -0.006045   \n",
       "...           ...     ...     ...     ...         ...        ...        ...   \n",
       "5/5/2025   1.1315  1.1319  1.1365  1.1292    0.001682   0.002746  -0.001406   \n",
       "5/6/2025   1.1370  1.1315  1.1382  1.1280    0.004861  -0.000353   0.001496   \n",
       "5/7/2025   1.1300  1.1370  1.1379  1.1292   -0.006157   0.004861  -0.000264   \n",
       "5/8/2025   1.1228  1.1301  1.1337  1.1212   -0.006372  -0.006069  -0.003691   \n",
       "5/9/2025   1.1248  1.1228  1.1293  1.1196    0.001781  -0.006460  -0.003881   \n",
       "\n",
       "           Low_Rate  Price_MA   Open_MA   High_MA    Low_MA  Price_Rate_MA  \\\n",
       "5/26/2006 -0.003687  1.276360  1.275835  1.282910  1.269505       0.000629   \n",
       "5/29/2006  0.001417  1.277190  1.276360  1.283300  1.270320       0.000672   \n",
       "5/30/2006  0.001730  1.278480  1.277210  1.284515  1.271230       0.001031   \n",
       "5/31/2006  0.004945  1.279330  1.278505  1.285680  1.272295       0.000687   \n",
       "6/1/2006  -0.006405  1.279905  1.279400  1.286185  1.273045       0.000470   \n",
       "...             ...       ...       ...       ...       ...            ...   \n",
       "5/5/2025   0.001685  1.131440  1.129345  1.137625  1.125210       0.001882   \n",
       "5/6/2025  -0.001063  1.133510  1.131395  1.139570  1.127170       0.001886   \n",
       "5/7/2025   0.001064  1.135255  1.133450  1.140985  1.129065       0.001601   \n",
       "5/8/2025  -0.007085  1.135410  1.135200  1.141460  1.130410       0.000160   \n",
       "5/9/2025  -0.001427  1.134850  1.135330  1.140550  1.130445      -0.000479   \n",
       "\n",
       "           Open_Rate_MA  High_Rate_MA  Low_Rate_MA  Next Day Average  \n",
       "5/26/2006      0.000758      0.000604     0.000617          1.274300  \n",
       "5/29/2006      0.000432      0.000313     0.000657          1.281850  \n",
       "5/30/2006      0.000688      0.000960     0.000731          1.284750  \n",
       "5/31/2006      0.001036      0.000921     0.000851          1.279175  \n",
       "6/1/2006       0.000723      0.000405     0.000606          1.286500  \n",
       "...                 ...           ...          ...               ...  \n",
       "5/5/2025       0.001706      0.001429     0.001864          1.133675  \n",
       "5/6/2025       0.001875      0.001767     0.001788          1.133525  \n",
       "5/7/2025       0.001871      0.001285     0.001726          1.126950  \n",
       "5/8/2025       0.001604      0.000443     0.001235          1.124125  \n",
       "5/9/2025       0.000135     -0.000788     0.000039          1.115600  \n",
       "\n",
       "[4946 rows x 17 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and preprocess the Excahnge rate Dataset\n",
    "#\n",
    "\n",
    "# Load the stock index dataset into the environment.\n",
    "url = \"https://raw.githubusercontent.com/dof-studio/dtafina/refs/heads/main/MachineLearning/eur_usd_processesd.csv\"\n",
    "raw_index = pd.read_csv(url)\n",
    "raw_index.index = raw_index[\"Date\"].to_list()\n",
    "\n",
    "# Astype into float32.\n",
    "raw_index = raw_index.drop([\"Date\"], axis=1).astype(\"float32\")\n",
    "raw_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(backend=torch, shape=torch.Size([4946, 16]), data=\n",
      "tensor([[ 1.2732e+00,  1.2807e+00,  1.2837e+00,  ...,  7.5842e-04,\n",
      "          6.0422e-04,  6.1658e-04],\n",
      "        [ 1.2751e+00,  1.2733e+00,  1.2770e+00,  ...,  4.3159e-04,\n",
      "          3.1305e-04,  6.5661e-04],\n",
      "        [ 1.2873e+00,  1.2749e+00,  1.2912e+00,  ...,  6.8844e-04,\n",
      "          9.5965e-04,  7.3116e-04],\n",
      "        ...,\n",
      "        [ 1.1300e+00,  1.1370e+00,  1.1379e+00,  ...,  1.8706e-03,\n",
      "          1.2850e-03,  1.7263e-03],\n",
      "        [ 1.1228e+00,  1.1301e+00,  1.1337e+00,  ...,  1.6037e-03,\n",
      "          4.4260e-04,  1.2347e-03],\n",
      "        [ 1.1248e+00,  1.1228e+00,  1.1293e+00,  ...,  1.3469e-04,\n",
      "         -7.8775e-04,  3.9298e-05]], device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "# Let's split the data into features and target \n",
    "\n",
    "# Create the Features Tensor\n",
    "features_index_org = Tensor(raw_index.drop([\"Next Day Average\"], axis = 1).to_numpy(), backend=backend, device=device, dtype=torch.float32)\n",
    "\n",
    "# Create the Targets Tensor\n",
    "targets_index = Tensor(raw_index[[\"Next Day Average\"]].to_numpy(), backend=backend, device=device, dtype=torch.float32)\n",
    "\n",
    "# Show how is the feature Tensor like\n",
    "print(features_index_org)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2.2 Create sequences of 30 past days to predict the next day's rate`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build input sequences of 20 days of past prices (sample) to predict the next day's movement\n",
    "\n",
    "features_index_w, targets_index_w = MLBase.make_rolling_window(features_index_org, targets_index, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2.4 Split the dataset: 80% training / 20% testing`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(backend=torch, shape=torch.Size([3934, 30, 16]), data=\n",
      "tensor([[[ 1.2732e+00,  1.2807e+00,  1.2837e+00,  ...,  7.5842e-04,\n",
      "           6.0422e-04,  6.1658e-04],\n",
      "         [ 1.2751e+00,  1.2733e+00,  1.2770e+00,  ...,  4.3159e-04,\n",
      "           3.1305e-04,  6.5661e-04],\n",
      "         [ 1.2873e+00,  1.2749e+00,  1.2912e+00,  ...,  6.8844e-04,\n",
      "           9.5965e-04,  7.3116e-04],\n",
      "         ...,\n",
      "         [ 1.2793e+00,  1.2797e+00,  1.2824e+00,  ..., -4.0954e-04,\n",
      "          -4.3201e-04, -7.7940e-05],\n",
      "         [ 1.2719e+00,  1.2795e+00,  1.2841e+00,  ..., -1.0742e-04,\n",
      "           1.6866e-05, -2.0734e-04],\n",
      "         [ 1.2777e+00,  1.2724e+00,  1.2784e+00,  ..., -2.4453e-04,\n",
      "          -9.2142e-05,  3.6891e-04]],\n",
      "\n",
      "        [[ 1.2751e+00,  1.2733e+00,  1.2770e+00,  ...,  4.3159e-04,\n",
      "           3.1305e-04,  6.5661e-04],\n",
      "         [ 1.2873e+00,  1.2749e+00,  1.2912e+00,  ...,  6.8844e-04,\n",
      "           9.5965e-04,  7.3116e-04],\n",
      "         [ 1.2810e+00,  1.2873e+00,  1.2904e+00,  ...,  1.0356e-03,\n",
      "           9.2077e-04,  8.5100e-04],\n",
      "         ...,\n",
      "         [ 1.2719e+00,  1.2795e+00,  1.2841e+00,  ..., -1.0742e-04,\n",
      "           1.6866e-05, -2.0734e-04],\n",
      "         [ 1.2777e+00,  1.2724e+00,  1.2784e+00,  ..., -2.4453e-04,\n",
      "          -9.2142e-05,  3.6891e-04],\n",
      "         [ 1.2807e+00,  1.2779e+00,  1.2863e+00,  ...,  5.1108e-04,\n",
      "           7.0474e-04,  6.7253e-04]],\n",
      "\n",
      "        [[ 1.2873e+00,  1.2749e+00,  1.2912e+00,  ...,  6.8844e-04,\n",
      "           9.5965e-04,  7.3116e-04],\n",
      "         [ 1.2810e+00,  1.2873e+00,  1.2904e+00,  ...,  1.0356e-03,\n",
      "           9.2077e-04,  8.5100e-04],\n",
      "         [ 1.2809e+00,  1.2811e+00,  1.2826e+00,  ...,  7.2347e-04,\n",
      "           4.0546e-04,  6.0622e-04],\n",
      "         ...,\n",
      "         [ 1.2777e+00,  1.2724e+00,  1.2784e+00,  ..., -2.4453e-04,\n",
      "          -9.2142e-05,  3.6891e-04],\n",
      "         [ 1.2807e+00,  1.2779e+00,  1.2863e+00,  ...,  5.1108e-04,\n",
      "           7.0474e-04,  6.7253e-04],\n",
      "         [ 1.2735e+00,  1.2814e+00,  1.2818e+00,  ...,  7.7844e-04,\n",
      "           6.5595e-04,  6.1528e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.1915e+00,  1.1868e+00,  1.1922e+00,  ..., -1.2812e-03,\n",
      "          -1.2733e-03, -1.3461e-03],\n",
      "         [ 1.1938e+00,  1.1918e+00,  1.1954e+00,  ..., -1.2307e-03,\n",
      "          -1.2863e-03, -1.3628e-03],\n",
      "         [ 1.1925e+00,  1.1939e+00,  1.1971e+00,  ..., -1.2859e-03,\n",
      "          -1.2029e-03, -1.1178e-03],\n",
      "         ...,\n",
      "         [ 1.1842e+00,  1.1817e+00,  1.1851e+00,  ..., -3.3350e-04,\n",
      "          -2.4615e-04, -3.0254e-04],\n",
      "         [ 1.1886e+00,  1.1845e+00,  1.1894e+00,  ..., -5.5327e-05,\n",
      "           4.0224e-05,  2.0025e-05],\n",
      "         [ 1.1870e+00,  1.1888e+00,  1.1910e+00,  ...,  1.6413e-04,\n",
      "           1.4955e-04,  1.8897e-04]],\n",
      "\n",
      "        [[ 1.1938e+00,  1.1918e+00,  1.1954e+00,  ..., -1.2307e-03,\n",
      "          -1.2863e-03, -1.3628e-03],\n",
      "         [ 1.1925e+00,  1.1939e+00,  1.1971e+00,  ..., -1.2859e-03,\n",
      "          -1.2029e-03, -1.1178e-03],\n",
      "         [ 1.1930e+00,  1.1927e+00,  1.1957e+00,  ..., -1.0913e-03,\n",
      "          -1.0657e-03, -1.0639e-03],\n",
      "         ...,\n",
      "         [ 1.1886e+00,  1.1845e+00,  1.1894e+00,  ..., -5.5327e-05,\n",
      "           4.0224e-05,  2.0025e-05],\n",
      "         [ 1.1870e+00,  1.1888e+00,  1.1910e+00,  ...,  1.6413e-04,\n",
      "           1.4955e-04,  1.8897e-04],\n",
      "         [ 1.1867e+00,  1.1864e+00,  1.1898e+00,  ...,  1.2555e-05,\n",
      "           7.3914e-05,  3.2176e-05]],\n",
      "\n",
      "        [[ 1.1925e+00,  1.1939e+00,  1.1971e+00,  ..., -1.2859e-03,\n",
      "          -1.2029e-03, -1.1178e-03],\n",
      "         [ 1.1930e+00,  1.1927e+00,  1.1957e+00,  ..., -1.0913e-03,\n",
      "          -1.0657e-03, -1.0639e-03],\n",
      "         [ 1.1933e+00,  1.1932e+00,  1.1976e+00,  ..., -1.0826e-03,\n",
      "          -9.4123e-04, -8.4955e-04],\n",
      "         ...,\n",
      "         [ 1.1870e+00,  1.1888e+00,  1.1910e+00,  ...,  1.6413e-04,\n",
      "           1.4955e-04,  1.8897e-04],\n",
      "         [ 1.1867e+00,  1.1864e+00,  1.1898e+00,  ...,  1.2555e-05,\n",
      "           7.3914e-05,  3.2176e-05],\n",
      "         [ 1.1860e+00,  1.1868e+00,  1.1894e+00,  ...,  2.5198e-05,\n",
      "          -6.0218e-06,  1.9673e-04]]], device='cuda:0'))\n",
      "Tensor(backend=torch, shape=torch.Size([3934, 1]), data=\n",
      "tensor([[1.2803],\n",
      "        [1.2772],\n",
      "        [1.2747],\n",
      "        ...,\n",
      "        [1.1872],\n",
      "        [1.1869],\n",
      "        [1.1858]], device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset: 80% training / 20% testing\n",
    "\n",
    "# We must use time-series to avoid data leakage\n",
    "feature_train, feature_test, target_train, target_test = MLBase.train_test_split_for_timeseries(\n",
    "    features_index_w, targets_index_w, 0.2)\n",
    "\n",
    "print(feature_train)\n",
    "print(target_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2.3 Train your LSTM`\n",
    "\n",
    "and\n",
    "\n",
    "`2.3.1 Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE)`\n",
    "\n",
    "and \n",
    "\n",
    "`2.3.2 Comment on the performance of your model and compare it to PyTorch`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an Evaluation Pipeline\n",
    "def eval_pipeline(evaluator: nn_SInterf_Evaluator | Any, X: Tensor, y_true: Tensor, use = \"my\", loader = None) -> tuple:\n",
    "\n",
    "    if use == \"my\":\n",
    "        evaluator.eval()\n",
    "        pred = evaluator.predict(X)\n",
    "        mse = RegressionMetrics(pred, y_true, \"mse\").compute().to_list()\n",
    "        rmse = RegressionMetrics(pred, y_true, \"rmse\").compute().to_list()\n",
    "        mae = RegressionMetrics(pred, y_true, \"mae\").compute().to_list()\n",
    "        r2 = RegressionMetrics(pred, y_true, \"r2\").compute().to_list()\n",
    "        return pred, mse, rmse, mae, mape, r2\n",
    "    else:\n",
    "\n",
    "        evaluator.eval()\n",
    "        preds = []\n",
    "        trues = []\n",
    "        with torch.no_grad():\n",
    "            for x, y in loader:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                y_hat = evaluator(x)\n",
    "                preds.append(y_hat.detach().cpu())\n",
    "                trues.append(y.cpu())\n",
    "       \n",
    "        # concatenate all batches\n",
    "        y_pred = torch.cat(preds, dim=0).view(-1)\n",
    "        y_true = torch.cat(trues, dim=0).view(-1)\n",
    "\n",
    "        # compute basic errors\n",
    "        diff    = y_pred - y_true\n",
    "        mse_val = torch.mean(diff ** 2).item()\n",
    "        rmse_val = mse_val ** 0.5\n",
    "        mae_val = torch.mean(diff.abs()).item()\n",
    "\n",
    "        # R2\n",
    "        ss_res = torch.sum(diff ** 2)\n",
    "        ss_tot = torch.sum((y_true - y_true.mean()) ** 2)\n",
    "        r2_val = (1 - ss_res / ss_tot).item() if ss_tot != 0 else float('nan')\n",
    "        return y_pred, mse_val, rmse_val, mae_val, r2_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct A Simple but Well Crafted Stacked LTSM Model\n",
    "\n",
    "class Model_1(nn_Module):\n",
    "\n",
    "    def __init__(self, input_num, **kwargs):\n",
    "        # Nathmath Huang (This is a template definition)\n",
    "        super().__init__(module_name=\"Model_1\", **kwargs)\n",
    "        # Stacked LSTM\n",
    "        self.rnn_1 = StackedLSTM(input_num, hid_features=128, num_layers=1, has_bias=True,\n",
    "                                init_scale=0.1, module_name=\"LSTM_1\", **kwargs)\n",
    "\n",
    "        # Output Layers\n",
    "        self.dense_1 = Dense(128, 64, init_scale=0.2, **kwargs)\n",
    "        self.actv_1 = Tanh(**kwargs)\n",
    "        self.dense_2 = Dense(64, 1, init_scale=0.2, **kwargs)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        out = self.rnn_1(inputs)\n",
    "        out = self.dense_1(out)\n",
    "        out = self.actv_1(out)\n",
    "        out = self.dense_2(out)\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a comparable PyTorch Model\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "class PyTorch_1(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        # Single‐layer LSTM producing hidden states of size 128\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=128,\n",
    "                            num_layers=1, bias=True, batch_first=True, device=device)\n",
    "        # Two fully connected layers with Tanh activation in between\n",
    "        self.fc1  = nn.Linear(128, 64, device=device)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.fc2  = nn.Linear(64, 1, device=device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)   \n",
    "        last = out[:, -1, :]     \n",
    "        hidden = self.tanh(self.fc1(last))\n",
    "        out = self.fc2(hidden)\n",
    "        return out\n",
    "\n",
    "def pytorch_train(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for inp, tgt in loader:\n",
    "        inp, tgt = inp.to(device), tgt.to(device)\n",
    "        pred = model(inp)\n",
    "        loss = criterion(pred, tgt)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def pytorch_evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inp, tgt in loader:\n",
    "            inp, tgt = inp.to(device), tgt.to(device)\n",
    "            pred = model(inp)\n",
    "            total_loss += criterion(pred, tgt).item()\n",
    "            print(\"Target mean±std:\", tgt.mean().item(), tgt.std().item())\n",
    "            print(\"Init pred mean±std:\", pred.mean().item(), pred.std().item())\n",
    "    return total_loss / len(loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histogram of Errors\n",
    "\n",
    "def plot_error_histograms(data1, data2, bins1=100, bins2=100, cmap_name='plasma', which='Target', names=[\"Data1\", \"Data2\"]):\n",
    "\n",
    "    # Flatten and clean both datasets\n",
    "    data1 = data1.flatten()\n",
    "    data2 = data2.flatten()\n",
    "    data1 = data1[~np.isnan(data1)]\n",
    "    data2 = data2[~np.isnan(data2)]\n",
    "    data1 = data1[data1 > 0]\n",
    "    data2 = data2[data2 > 0]\n",
    "\n",
    "    # Define binning shared across both datasets\n",
    "    combined_data = np.concatenate((data1, data2))\n",
    "    bins = np.logspace(np.log10(combined_data.min()), np.log10(combined_data.max()), num=50)\n",
    "\n",
    "    # Plot first histogram\n",
    "    plt.hist(data1, bins=bins1, color='skyblue', edgecolor='black', alpha=0.6, label=names[0])\n",
    "\n",
    "    # Plot second histogram\n",
    "    plt.hist(data2, bins=bins2, color='salmon', edgecolor='darkred', alpha=0.6, label=names[1])\n",
    "\n",
    "    # Set log scale\n",
    "    plt.xscale('log')\n",
    "    plt.yscale('log')\n",
    "\n",
    "    # Labeling and aesthetics\n",
    "    plt.xlabel(which)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Comparison of Two Distributions of ' + which)\n",
    "    plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3.1.1 Results from My LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = Model_1(input_num=16, backend=backend, device=device, autograd=False)\n",
    "crit1 = RMSE(backend=backend, device=device, autograd=False)\n",
    "optm1 = AdamW(model1.parameters(), lr = 4e-4, eps = 1e-10, backend=backend, device=device, autograd=False)\n",
    "eval1 = Evaluator(\"Eval_1\", task = \"regression\", module=model1, criterion=crit1, optimizer=optm1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on Total Epoch: 2796, Round: 0\n",
      "Training on Total Epoch: 2797, Round: 1\n",
      "Training on Total Epoch: 2798, Round: 2\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2799, Round: 3\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2800, Round: 4\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2801, Round: 5\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2802, Round: 6\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2803, Round: 7\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2804, Round: 8\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2805, Round: 9\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2806, Round: 10\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2807, Round: 11\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2808, Round: 12\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2809, Round: 13\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2810, Round: 14\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2811, Round: 15\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2812, Round: 16\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2813, Round: 17\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2814, Round: 18\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2815, Round: 19\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2816, Round: 20\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2817, Round: 21\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2818, Round: 22\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2819, Round: 23\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2820, Round: 24\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2821, Round: 25\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2822, Round: 26\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2823, Round: 27\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2824, Round: 28\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2825, Round: 29\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2826, Round: 30\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2827, Round: 31\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2828, Round: 32\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2829, Round: 33\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2830, Round: 34\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2831, Round: 35\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2832, Round: 36\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2833, Round: 37\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2834, Round: 38\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2835, Round: 39\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2836, Round: 40\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2837, Round: 41\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2838, Round: 42\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2839, Round: 43\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2840, Round: 44\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2841, Round: 45\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2842, Round: 46\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2843, Round: 47\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2844, Round: 48\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2845, Round: 49\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2846, Round: 50\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2847, Round: 51\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2848, Round: 52\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2849, Round: 53\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2850, Round: 54\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2851, Round: 55\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2852, Round: 56\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2853, Round: 57\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2854, Round: 58\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2855, Round: 59\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2856, Round: 60\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2857, Round: 61\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2858, Round: 62\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2859, Round: 63\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2860, Round: 64\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2861, Round: 65\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2862, Round: 66\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2863, Round: 67\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2864, Round: 68\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2865, Round: 69\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2866, Round: 70\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2867, Round: 71\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2868, Round: 72\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2869, Round: 73\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2870, Round: 74\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2871, Round: 75\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2872, Round: 76\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2873, Round: 77\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2874, Round: 78\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2875, Round: 79\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2876, Round: 80\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2877, Round: 81\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2878, Round: 82\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2879, Round: 83\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2880, Round: 84\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2881, Round: 85\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2882, Round: 86\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2883, Round: 87\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2884, Round: 88\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2885, Round: 89\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2886, Round: 90\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2887, Round: 91\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2888, Round: 92\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2889, Round: 93\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2890, Round: 94\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2891, Round: 95\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2892, Round: 96\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2893, Round: 97\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2894, Round: 98\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2895, Round: 99\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2896, Round: 100\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2897, Round: 101\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2898, Round: 102\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2899, Round: 103\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2900, Round: 104\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2901, Round: 105\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2902, Round: 106\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2903, Round: 107\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2904, Round: 108\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2905, Round: 109\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2906, Round: 110\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2907, Round: 111\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2908, Round: 112\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2909, Round: 113\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2910, Round: 114\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2911, Round: 115\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2912, Round: 116\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2913, Round: 117\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2914, Round: 118\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2915, Round: 119\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2916, Round: 120\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2917, Round: 121\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2918, Round: 122\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2919, Round: 123\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2920, Round: 124\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2921, Round: 125\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2922, Round: 126\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2923, Round: 127\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2924, Round: 128\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2925, Round: 129\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2926, Round: 130\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2927, Round: 131\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2928, Round: 132\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2929, Round: 133\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0052,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2930, Round: 134\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2931, Round: 135\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2932, Round: 136\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2933, Round: 137\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2934, Round: 138\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2935, Round: 139\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2936, Round: 140\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2937, Round: 141\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2938, Round: 142\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2939, Round: 143\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2940, Round: 144\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2941, Round: 145\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2942, Round: 146\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2943, Round: 147\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2944, Round: 148\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2945, Round: 149\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2946, Round: 150\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2947, Round: 151\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2948, Round: 152\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2949, Round: 153\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2950, Round: 154\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2951, Round: 155\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2952, Round: 156\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2953, Round: 157\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2954, Round: 158\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2955, Round: 159\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2956, Round: 160\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2957, Round: 161\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2958, Round: 162\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2959, Round: 163\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2960, Round: 164\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2961, Round: 165\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2962, Round: 166\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2963, Round: 167\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2964, Round: 168\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2965, Round: 169\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2966, Round: 170\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2967, Round: 171\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2968, Round: 172\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2969, Round: 173\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2970, Round: 174\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2971, Round: 175\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2972, Round: 176\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2973, Round: 177\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2974, Round: 178\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2975, Round: 179\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2976, Round: 180\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2977, Round: 181\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2978, Round: 182\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2979, Round: 183\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2980, Round: 184\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2981, Round: 185\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2982, Round: 186\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2983, Round: 187\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2984, Round: 188\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2985, Round: 189\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2986, Round: 190\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2987, Round: 191\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2988, Round: 192\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2989, Round: 193\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2990, Round: 194\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2991, Round: 195\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2992, Round: 196\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2993, Round: 197\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2994, Round: 198\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2995, Round: 199\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2996, Round: 200\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2997, Round: 201\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2998, Round: 202\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 2999, Round: 203\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3000, Round: 204\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3001, Round: 205\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3002, Round: 206\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3003, Round: 207\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3004, Round: 208\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3005, Round: 209\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3006, Round: 210\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3007, Round: 211\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3008, Round: 212\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3009, Round: 213\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3010, Round: 214\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3011, Round: 215\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3012, Round: 216\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3013, Round: 217\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3014, Round: 218\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3015, Round: 219\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3016, Round: 220\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3017, Round: 221\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3018, Round: 222\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3019, Round: 223\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3020, Round: 224\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3021, Round: 225\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3022, Round: 226\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3023, Round: 227\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3024, Round: 228\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3025, Round: 229\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3026, Round: 230\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3027, Round: 231\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3028, Round: 232\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3029, Round: 233\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3030, Round: 234\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3031, Round: 235\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3032, Round: 236\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3033, Round: 237\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3034, Round: 238\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3035, Round: 239\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3036, Round: 240\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3037, Round: 241\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3038, Round: 242\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3039, Round: 243\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3040, Round: 244\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3041, Round: 245\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3042, Round: 246\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3043, Round: 247\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3044, Round: 248\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3045, Round: 249\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3046, Round: 250\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3047, Round: 251\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3048, Round: 252\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3049, Round: 253\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3050, Round: 254\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3051, Round: 255\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3052, Round: 256\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3053, Round: 257\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3054, Round: 258\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3055, Round: 259\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3056, Round: 260\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3057, Round: 261\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3058, Round: 262\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3059, Round: 263\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3060, Round: 264\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3061, Round: 265\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3062, Round: 266\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3063, Round: 267\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3064, Round: 268\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3065, Round: 269\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3066, Round: 270\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3067, Round: 271\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3068, Round: 272\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3069, Round: 273\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3070, Round: 274\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3071, Round: 275\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3072, Round: 276\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3073, Round: 277\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3074, Round: 278\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3075, Round: 279\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3076, Round: 280\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3077, Round: 281\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3078, Round: 282\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3079, Round: 283\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3080, Round: 284\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3081, Round: 285\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3082, Round: 286\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3083, Round: 287\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3084, Round: 288\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3085, Round: 289\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3086, Round: 290\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3087, Round: 291\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3088, Round: 292\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3089, Round: 293\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3090, Round: 294\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3091, Round: 295\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3092, Round: 296\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3093, Round: 297\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3094, Round: 298\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3095, Round: 299\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3096, Round: 300\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3097, Round: 301\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3098, Round: 302\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3099, Round: 303\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3100, Round: 304\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3101, Round: 305\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3102, Round: 306\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3103, Round: 307\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3104, Round: 308\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3105, Round: 309\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3106, Round: 310\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3107, Round: 311\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3108, Round: 312\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3109, Round: 313\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3110, Round: 314\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3111, Round: 315\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3112, Round: 316\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3113, Round: 317\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3114, Round: 318\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3115, Round: 319\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3116, Round: 320\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3117, Round: 321\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3118, Round: 322\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3119, Round: 323\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3120, Round: 324\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3121, Round: 325\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3122, Round: 326\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3123, Round: 327\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3124, Round: 328\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3125, Round: 329\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3126, Round: 330\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3127, Round: 331\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3128, Round: 332\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3129, Round: 333\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3130, Round: 334\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3131, Round: 335\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3132, Round: 336\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3133, Round: 337\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3134, Round: 338\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3135, Round: 339\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3136, Round: 340\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3137, Round: 341\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3138, Round: 342\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3139, Round: 343\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3140, Round: 344\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3141, Round: 345\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3142, Round: 346\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3143, Round: 347\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3144, Round: 348\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3145, Round: 349\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3146, Round: 350\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3147, Round: 351\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3148, Round: 352\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3149, Round: 353\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3150, Round: 354\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3151, Round: 355\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3152, Round: 356\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3153, Round: 357\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3154, Round: 358\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3155, Round: 359\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3156, Round: 360\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3157, Round: 361\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.004,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3158, Round: 362\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3159, Round: 363\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3160, Round: 364\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3161, Round: 365\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3162, Round: 366\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3163, Round: 367\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3164, Round: 368\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3165, Round: 369\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3166, Round: 370\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3167, Round: 371\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3168, Round: 372\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3169, Round: 373\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3170, Round: 374\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3171, Round: 375\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3172, Round: 376\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3173, Round: 377\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3174, Round: 378\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3175, Round: 379\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3176, Round: 380\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3177, Round: 381\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3178, Round: 382\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3179, Round: 383\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3180, Round: 384\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3181, Round: 385\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3182, Round: 386\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3183, Round: 387\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3184, Round: 388\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3185, Round: 389\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3186, Round: 390\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3187, Round: 391\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3188, Round: 392\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3189, Round: 393\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3190, Round: 394\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3191, Round: 395\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3192, Round: 396\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3193, Round: 397\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3194, Round: 398\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.003,  }]\n",
      "Training on Total Epoch: 3195, Round: 399\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.0029,  }]\n",
      "Training on Total Epoch: 3196, Round: 400\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.0029,  }]\n",
      "Training on Total Epoch: 3197, Round: 401\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.0029,  }]\n",
      "Training on Total Epoch: 3198, Round: 402\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.0029,  }]\n",
      "Training on Total Epoch: 3199, Round: 403\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.0029,  }]\n",
      "Training on Total Epoch: 3200, Round: 404\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.0029,  }]\n",
      "Training on Total Epoch: 3201, Round: 405\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.0029,  }]\n",
      "Training on Total Epoch: 3202, Round: 406\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.0029,  }]\n",
      "Training on Total Epoch: 3203, Round: 407\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.0029,  }]\n",
      "Training on Total Epoch: 3204, Round: 408\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.0029,  }]\n",
      "Training on Total Epoch: 3205, Round: 409\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.0029,  }]\n",
      "Training on Total Epoch: 3206, Round: 410\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.0029,  }]\n",
      "Training on Total Epoch: 3207, Round: 411\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.0029,  }]\n",
      "Training on Total Epoch: 3208, Round: 412\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.0029,  }]\n",
      "Training on Total Epoch: 3209, Round: 413\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.0029,  }]\n",
      "Training on Total Epoch: 3210, Round: 414\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.0029,  }]\n",
      "Training on Total Epoch: 3211, Round: 415\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.0029,  }]\n",
      "Training on Total Epoch: 3212, Round: 416\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.0029,  }]\n",
      "Training on Total Epoch: 3213, Round: 417\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.0029,  }]\n",
      "Training on Total Epoch: 3214, Round: 418\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.0029,  }]\n",
      "Training on Total Epoch: 3215, Round: 419\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.0029,  }]\n",
      "Training on Total Epoch: 3216, Round: 420\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.0029,  }]\n",
      "Training on Total Epoch: 3217, Round: 421\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.0029,  }]\n",
      "Training on Total Epoch: 3218, Round: 422\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.0029,  }]\n",
      "Training on Total Epoch: 3219, Round: 423\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.0029,  }]\n",
      "Training on Total Epoch: 3220, Round: 424\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.0029,  }]\n",
      "Training on Total Epoch: 3221, Round: 425\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.0029,  }]\n",
      "Training on Total Epoch: 3222, Round: 426\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.0029,  }]\n",
      "Training on Total Epoch: 3223, Round: 427\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.0029,  }]\n",
      "Training on Total Epoch: 3224, Round: 428\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.0029,  }]\n",
      "Training on Total Epoch: 3225, Round: 429\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.0029,  }]\n",
      "Training on Total Epoch: 3226, Round: 430\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.0029,  }]\n",
      "Training on Total Epoch: 3227, Round: 431\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.0029,  }]\n",
      "Training on Total Epoch: 3228, Round: 432\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.0029,  }]\n",
      "Training on Total Epoch: 3229, Round: 433\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.0029,  }]\n",
      "Training on Total Epoch: 3230, Round: 434\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.0029,  }]\n",
      "Training on Total Epoch: 3231, Round: 435\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.0029,  }]\n",
      "Training on Total Epoch: 3232, Round: 436\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.0029,  }]\n",
      "Training on Total Epoch: 3233, Round: 437\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.0029,  }]\n",
      "Training on Total Epoch: 3234, Round: 438\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.0029,  }]\n",
      "Training on Total Epoch: 3235, Round: 439\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.0029,  }]\n",
      "Training on Total Epoch: 3236, Round: 440\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.0029,  }]\n",
      "Training on Total Epoch: 3237, Round: 441\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.0029,  }]\n",
      "Training on Total Epoch: 3238, Round: 442\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.0029,  }]\n",
      "Training on Total Epoch: 3239, Round: 443\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.0029,  }]\n",
      "Training on Total Epoch: 3240, Round: 444\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.0029,  }]\n",
      "Training on Total Epoch: 3241, Round: 445\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.0029,  }]\n",
      "Training on Total Epoch: 3242, Round: 446\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.0029,  }]\n",
      "Training on Total Epoch: 3243, Round: 447\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.0029,  }]\n",
      "Training on Total Epoch: 3244, Round: 448\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.0029,  }]\n",
      "Training on Total Epoch: 3245, Round: 449\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.0029,  }]\n",
      "Training on Total Epoch: 3246, Round: 450\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.0029,  }]\n",
      "Training on Total Epoch: 3247, Round: 451\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.0029,  }]\n",
      "Training on Total Epoch: 3248, Round: 452\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.0029,  }]\n",
      "Training on Total Epoch: 3249, Round: 453\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.0029,  }]\n",
      "Training on Total Epoch: 3250, Round: 454\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.0029,  }]\n",
      "Training on Total Epoch: 3251, Round: 455\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.0029,  }]\n",
      "Training on Total Epoch: 3252, Round: 456\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.0029,  }]\n",
      "Training on Total Epoch: 3253, Round: 457\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.0029,  }]\n",
      "Training on Total Epoch: 3254, Round: 458\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.0029,  }]\n",
      "Training on Total Epoch: 3255, Round: 459\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.0029,  }]\n",
      "Training on Total Epoch: 3256, Round: 460\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.0029,  }]\n",
      "Training on Total Epoch: 3257, Round: 461\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.0029,  }]\n",
      "Training on Total Epoch: 3258, Round: 462\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.0029,  }]\n",
      "Training on Total Epoch: 3259, Round: 463\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.0029,  }]\n",
      "Training on Total Epoch: 3260, Round: 464\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.0029,  }]\n",
      "Training on Total Epoch: 3261, Round: 465\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.0029,  }]\n",
      "Training on Total Epoch: 3262, Round: 466\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.0029,  }]\n",
      "Training on Total Epoch: 3263, Round: 467\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.0029,  }]\n",
      "Training on Total Epoch: 3264, Round: 468\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.0029,  }]\n",
      "Training on Total Epoch: 3265, Round: 469\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.0029,  }]\n",
      "Training on Total Epoch: 3266, Round: 470\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.0029,  }]\n",
      "Training on Total Epoch: 3267, Round: 471\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.0029,  }]\n",
      "Training on Total Epoch: 3268, Round: 472\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.0029,  }]\n",
      "Training on Total Epoch: 3269, Round: 473\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.0029,  }]\n",
      "Training on Total Epoch: 3270, Round: 474\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.0029,  }]\n",
      "Training on Total Epoch: 3271, Round: 475\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.0029,  }]\n",
      "Training on Total Epoch: 3272, Round: 476\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.0029,  }]\n",
      "Training on Total Epoch: 3273, Round: 477\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.0029,  }]\n",
      "Training on Total Epoch: 3274, Round: 478\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.0029,  }]\n",
      "Training on Total Epoch: 3275, Round: 479\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.0029,  }]\n",
      "Training on Total Epoch: 3276, Round: 480\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.0029,  }]\n",
      "Training on Total Epoch: 3277, Round: 481\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.0029,  }]\n",
      "Training on Total Epoch: 3278, Round: 482\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.0029,  }]\n",
      "Training on Total Epoch: 3279, Round: 483\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.0029,  }]\n",
      "Training on Total Epoch: 3280, Round: 484\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.0029,  }]\n",
      "Training on Total Epoch: 3281, Round: 485\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.0029,  }]\n",
      "Training on Total Epoch: 3282, Round: 486\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.0029,  }]\n",
      "Training on Total Epoch: 3283, Round: 487\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.0029,  }]\n",
      "Training on Total Epoch: 3284, Round: 488\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.0029,  }]\n",
      "Training on Total Epoch: 3285, Round: 489\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.0029,  }]\n",
      "Training on Total Epoch: 3286, Round: 490\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.0029,  }]\n",
      "Training on Total Epoch: 3287, Round: 491\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.0029,  }]\n",
      "Training on Total Epoch: 3288, Round: 492\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.0029,  }]\n",
      "Training on Total Epoch: 3289, Round: 493\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.0029,  }]\n",
      "Training on Total Epoch: 3290, Round: 494\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.0029,  }]\n",
      "Training on Total Epoch: 3291, Round: 495\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.0029,  }]\n",
      "Training on Total Epoch: 3292, Round: 496\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.0029,  }]\n",
      "Training on Total Epoch: 3293, Round: 497\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.0029,  }]\n",
      "Training on Total Epoch: 3294, Round: 498\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.0029,  }]\n",
      "Training on Total Epoch: 3295, Round: 499\n",
      "Evalset: [Train : Metrics { mse:0.0,  rmse:0.0051,  mae:0.0037,  }]\n",
      "Evalset: [Test : Metrics { mse:0.0,  rmse:0.0039,  mae:0.0029,  }]\n"
     ]
    }
   ],
   "source": [
    "# Train and Evaluate the model - LSTM Model with 1 Stacked Layers\n",
    "# \n",
    "\n",
    "# Train the Hybrid LSTM Model\n",
    "\n",
    "# Start to train the LSTM Model\n",
    "eval1.fit(\n",
    "    X = feature_train - 1, y = target_train - 1,\n",
    "    epoches = 500,\n",
    "    batch_size = 4096,\n",
    "    shuffle = False,\n",
    "    one_hot = False,\n",
    "    random_state = None,\n",
    "    verbosity = 1,\n",
    "    evalper = 1,\n",
    "    evalset = {\n",
    "        \"Train\": (feature_train - 1, target_train - 1),\n",
    "        \"Test\" : (feature_test - 1, target_test - 1)\n",
    "    },\n",
    "    evalmetrics = [\"mse\", \"rmse\", \"mae\"],\n",
    "    early_stop = 5,\n",
    "    early_stop_logic = \"most\"\n",
    ")\n",
    "\n",
    "eval1.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and Evaluate the model - LSTM Model with 1 Stacked Layers\n",
    "# \n",
    "\n",
    "# Save the model and evaluator\n",
    "model1.save(model1, \"./models/LSTM_Model_ID01_ckpt6_m1.bin\")\n",
    "eval1.save(eval1, \"./models/LSTM_Eval_ID01_ckpt6_m1.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Config: {Benchmark Evaluation: My LSTM with 1 Stacked Layers }\n",
      "MSE    : 1.5e-05\n",
      "RMSE   : 0.003924\n",
      "MAE    : 0.002935\n",
      "R2     : 0.991512\n"
     ]
    }
   ],
   "source": [
    "# Train and Evaluate the model - LSTM Model with 1 Stacked Layers\n",
    "# \n",
    "\n",
    "# Print the results for the test-set\n",
    "eval1.eval()\n",
    "pred, mse, rmse, mae, mape, r2 = eval_pipeline(eval1, feature_test - 1, target_test - 1)\n",
    "print(\"Model Config: {Benchmark Evaluation: My LSTM with 1 Stacked Layers }\")\n",
    "print(\"MSE    :\", round(mse, 6))\n",
    "print(\"RMSE   :\", round(rmse, 6))\n",
    "print(\"MAE    :\", round(mae, 6))\n",
    "print(\"R2     :\", round(r2, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3.1.3 Results from the Pytorch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and Evaluate the model - PyTorch LSTM Model\n",
    "# \n",
    "\n",
    "# Create a model\n",
    "model2 = PyTorch_1(16)\n",
    "crit2 = nn.MSELoss()\n",
    "optm2 = torch.optim.Adam(model2.parameters(), lr=1e-3)\n",
    "\n",
    "# Load My Tensors into Torch DataLoader\n",
    "train_loader = DataLoader(TensorDataset(feature_train.data, target_train.data - 1), batch_size=4096)\n",
    "test_loader  = DataLoader(TensorDataset(feature_test.data, target_test.data - 1), batch_size=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target mean±std: 0.08173996210098267 0.042615462094545364\n",
      "Init pred mean±std: 0.08291716128587723 0.041078630834817886\n",
      "Epoch 01: Train Loss = 0.0000, Val Loss = 0.0000\n",
      "Target mean±std: 0.08173996210098267 0.042615462094545364\n",
      "Init pred mean±std: 0.08291665464639664 0.041079070419073105\n",
      "Epoch 02: Train Loss = 0.0000, Val Loss = 0.0000\n",
      "Target mean±std: 0.08173996210098267 0.042615462094545364\n",
      "Init pred mean±std: 0.08291618525981903 0.04107951745390892\n",
      "Epoch 03: Train Loss = 0.0000, Val Loss = 0.0000\n",
      "Target mean±std: 0.08173996210098267 0.042615462094545364\n",
      "Init pred mean±std: 0.08291587233543396 0.041079968214035034\n",
      "Epoch 04: Train Loss = 0.0000, Val Loss = 0.0000\n",
      "Target mean±std: 0.08173996210098267 0.042615462094545364\n",
      "Init pred mean±std: 0.08291522413492203 0.04108038544654846\n",
      "Epoch 05: Train Loss = 0.0000, Val Loss = 0.0000\n",
      "Target mean±std: 0.08173996210098267 0.042615462094545364\n",
      "Init pred mean±std: 0.0829150378704071 0.041080836206674576\n",
      "Epoch 06: Train Loss = 0.0000, Val Loss = 0.0000\n",
      "Target mean±std: 0.08173996210098267 0.042615462094545364\n",
      "Init pred mean±std: 0.08291445672512054 0.0410812571644783\n",
      "Epoch 07: Train Loss = 0.0000, Val Loss = 0.0000\n",
      "Target mean±std: 0.08173996210098267 0.042615462094545364\n",
      "Init pred mean±std: 0.0829140841960907 0.04108169302344322\n",
      "Epoch 08: Train Loss = 0.0000, Val Loss = 0.0000\n",
      "Target mean±std: 0.08173996210098267 0.042615462094545364\n",
      "Init pred mean±std: 0.08291368186473846 0.041082125157117844\n",
      "Epoch 09: Train Loss = 0.0000, Val Loss = 0.0000\n",
      "Target mean±std: 0.08173996210098267 0.042615462094545364\n",
      "Init pred mean±std: 0.08291320502758026 0.04108254611492157\n",
      "Epoch 10: Train Loss = 0.0000, Val Loss = 0.0000\n",
      "Target mean±std: 0.08173996210098267 0.042615462094545364\n",
      "Init pred mean±std: 0.08291276544332504 0.04108297452330589\n",
      "Epoch 11: Train Loss = 0.0000, Val Loss = 0.0000\n",
      "Target mean±std: 0.08173996210098267 0.042615462094545364\n",
      "Init pred mean±std: 0.08291240781545639 0.041083406656980515\n",
      "Epoch 12: Train Loss = 0.0000, Val Loss = 0.0000\n",
      "Target mean±std: 0.08173996210098267 0.042615462094545364\n",
      "Init pred mean±std: 0.08291187137365341 0.04108381271362305\n",
      "Epoch 13: Train Loss = 0.0000, Val Loss = 0.0000\n",
      "Target mean±std: 0.08173996210098267 0.042615462094545364\n",
      "Init pred mean±std: 0.08291158825159073 0.04108424112200737\n",
      "Epoch 14: Train Loss = 0.0000, Val Loss = 0.0000\n",
      "Target mean±std: 0.08173996210098267 0.042615462094545364\n",
      "Init pred mean±std: 0.08291107416152954 0.041084643453359604\n",
      "Epoch 15: Train Loss = 0.0000, Val Loss = 0.0000\n",
      "Target mean±std: 0.08173996210098267 0.042615462094545364\n",
      "Init pred mean±std: 0.0829106867313385 0.04108505696058273\n",
      "Epoch 16: Train Loss = 0.0000, Val Loss = 0.0000\n",
      "Target mean±std: 0.08173996210098267 0.042615462094545364\n",
      "Init pred mean±std: 0.08291039615869522 0.04108547419309616\n",
      "Epoch 17: Train Loss = 0.0000, Val Loss = 0.0000\n",
      "Target mean±std: 0.08173996210098267 0.042615462094545364\n",
      "Init pred mean±std: 0.08290984481573105 0.041085872799158096\n",
      "Epoch 18: Train Loss = 0.0000, Val Loss = 0.0000\n",
      "Target mean±std: 0.08173996210098267 0.042615462094545364\n",
      "Init pred mean±std: 0.08290956169366837 0.041086286306381226\n",
      "Epoch 19: Train Loss = 0.0000, Val Loss = 0.0000\n",
      "Target mean±std: 0.08173996210098267 0.042615462094545364\n",
      "Init pred mean±std: 0.08290916681289673 0.04108669236302376\n",
      "Epoch 20: Train Loss = 0.0000, Val Loss = 0.0000\n",
      "Target mean±std: 0.08173996210098267 0.042615462094545364\n",
      "Init pred mean±std: 0.08290862292051315 0.041087083518505096\n",
      "Epoch 21: Train Loss = 0.0000, Val Loss = 0.0000\n",
      "Target mean±std: 0.08173996210098267 0.042615462094545364\n",
      "Init pred mean±std: 0.08290845155715942 0.041087500751018524\n",
      "Epoch 22: Train Loss = 0.0000, Val Loss = 0.0000\n",
      "Target mean±std: 0.08173996210098267 0.042615462094545364\n",
      "Init pred mean±std: 0.08290783315896988 0.041087884455919266\n",
      "Epoch 23: Train Loss = 0.0000, Val Loss = 0.0000\n",
      "Target mean±std: 0.08173996210098267 0.042615462094545364\n",
      "Init pred mean±std: 0.08290752023458481 0.0410882905125618\n",
      "Epoch 24: Train Loss = 0.0000, Val Loss = 0.0000\n",
      "Target mean±std: 0.08173996210098267 0.042615462094545364\n",
      "Init pred mean±std: 0.08290724456310272 0.041088689118623734\n",
      "Epoch 25: Train Loss = 0.0000, Val Loss = 0.0000\n",
      "Target mean±std: 0.08173996210098267 0.042615462094545364\n",
      "Init pred mean±std: 0.08290661871433258 0.04108906909823418\n",
      "Epoch 26: Train Loss = 0.0000, Val Loss = 0.0000\n",
      "Target mean±std: 0.08173996210098267 0.042615462094545364\n",
      "Init pred mean±std: 0.08290651440620422 0.041089482605457306\n",
      "Epoch 27: Train Loss = 0.0000, Val Loss = 0.0000\n",
      "Target mean±std: 0.08173996210098267 0.042615462094545364\n",
      "Init pred mean±std: 0.08290594071149826 0.04108985513448715\n",
      "Epoch 28: Train Loss = 0.0000, Val Loss = 0.0000\n",
      "Target mean±std: 0.08173996210098267 0.042615462094545364\n",
      "Init pred mean±std: 0.08290565758943558 0.041090257465839386\n",
      "Epoch 29: Train Loss = 0.0000, Val Loss = 0.0000\n",
      "Target mean±std: 0.08173996210098267 0.042615462094545364\n",
      "Init pred mean±std: 0.08290529251098633 0.04109064117074013\n",
      "Epoch 30: Train Loss = 0.0000, Val Loss = 0.0000\n",
      "Target mean±std: 0.08173996210098267 0.042615462094545364\n",
      "Init pred mean±std: 0.08290480822324753 0.04109102487564087\n",
      "Epoch 31: Train Loss = 0.0000, Val Loss = 0.0000\n",
      "Target mean±std: 0.08173996210098267 0.042615462094545364\n",
      "Init pred mean±std: 0.08290454000234604 0.04109141603112221\n",
      "Epoch 32: Train Loss = 0.0000, Val Loss = 0.0000\n",
      "Target mean±std: 0.08173996210098267 0.042615462094545364\n",
      "Init pred mean±std: 0.08290409296751022 0.04109179228544235\n",
      "Epoch 33: Train Loss = 0.0000, Val Loss = 0.0000\n",
      "Target mean±std: 0.08173996210098267 0.042615462094545364\n",
      "Init pred mean±std: 0.08290377259254456 0.04109217971563339\n",
      "Epoch 34: Train Loss = 0.0000, Val Loss = 0.0000\n",
      "Target mean±std: 0.08173996210098267 0.042615462094545364\n",
      "Init pred mean±std: 0.08290337026119232 0.04109255596995354\n",
      "Epoch 35: Train Loss = 0.0000, Val Loss = 0.0000\n",
      "Target mean±std: 0.08173996210098267 0.042615462094545364\n",
      "Init pred mean±std: 0.08290306478738785 0.04109293967485428\n",
      "Epoch 36: Train Loss = 0.0000, Val Loss = 0.0000\n",
      "Target mean±std: 0.08173996210098267 0.042615462094545364\n",
      "Init pred mean±std: 0.08290261775255203 0.041093308478593826\n",
      "Epoch 37: Train Loss = 0.0000, Val Loss = 0.0000\n",
      "Target mean±std: 0.08173996210098267 0.042615462094545364\n",
      "Init pred mean±std: 0.08290233463048935 0.04109368845820427\n",
      "Epoch 38: Train Loss = 0.0000, Val Loss = 0.0000\n",
      "Target mean±std: 0.08173996210098267 0.042615462094545364\n",
      "Init pred mean±std: 0.0829019546508789 0.041094060987234116\n",
      "Epoch 39: Train Loss = 0.0000, Val Loss = 0.0000\n",
      "Target mean±std: 0.08173996210098267 0.042615462094545364\n",
      "Init pred mean±std: 0.08290157467126846 0.04109443724155426\n",
      "Epoch 40: Train Loss = 0.0000, Val Loss = 0.0000\n",
      "Target mean±std: 0.08173996210098267 0.042615462094545364\n",
      "Init pred mean±std: 0.08290129154920578 0.041094820946455\n",
      "Epoch 41: Train Loss = 0.0000, Val Loss = 0.0000\n",
      "Target mean±std: 0.08173996210098267 0.042615462094545364\n",
      "Init pred mean±std: 0.08290085941553116 0.04109518975019455\n",
      "Epoch 42: Train Loss = 0.0000, Val Loss = 0.0000\n",
      "Target mean±std: 0.08173996210098267 0.042615462094545364\n",
      "Init pred mean±std: 0.0829005241394043 0.0410955585539341\n",
      "Epoch 43: Train Loss = 0.0000, Val Loss = 0.0000\n",
      "Target mean±std: 0.08173996210098267 0.042615462094545364\n",
      "Init pred mean±std: 0.08290017396211624 0.04109593480825424\n",
      "Epoch 44: Train Loss = 0.0000, Val Loss = 0.0000\n",
      "Target mean±std: 0.08173996210098267 0.042615462094545364\n",
      "Init pred mean±std: 0.08289975672960281 0.04109630361199379\n",
      "Epoch 45: Train Loss = 0.0000, Val Loss = 0.0000\n",
      "Target mean±std: 0.08173996210098267 0.042615462094545364\n",
      "Init pred mean±std: 0.08289948105812073 0.041096676141023636\n",
      "Epoch 46: Train Loss = 0.0000, Val Loss = 0.0000\n",
      "Target mean±std: 0.08173996210098267 0.042615462094545364\n",
      "Init pred mean±std: 0.0828990712761879 0.041097041219472885\n",
      "Epoch 47: Train Loss = 0.0000, Val Loss = 0.0000\n",
      "Target mean±std: 0.08173996210098267 0.042615462094545364\n",
      "Init pred mean±std: 0.08289875090122223 0.041097406297922134\n",
      "Epoch 48: Train Loss = 0.0000, Val Loss = 0.0000\n",
      "Target mean±std: 0.08173996210098267 0.042615462094545364\n",
      "Init pred mean±std: 0.08289845287799835 0.04109778627753258\n",
      "Epoch 49: Train Loss = 0.0000, Val Loss = 0.0000\n",
      "Target mean±std: 0.08173996210098267 0.042615462094545364\n",
      "Init pred mean±std: 0.08289797604084015 0.04109814018011093\n",
      "Epoch 50: Train Loss = 0.0000, Val Loss = 0.0000\n",
      "Target mean±std: 0.08173996210098267 0.042615462094545364\n",
      "Init pred mean±std: 0.08289779722690582 0.041098520159721375\n",
      "Epoch 51: Train Loss = 0.0000, Val Loss = 0.0000\n",
      "Target mean±std: 0.08173996210098267 0.042615462094545364\n",
      "Init pred mean±std: 0.08289727568626404 0.04109887778759003\n",
      "Epoch 52: Train Loss = 0.0000, Val Loss = 0.0000\n",
      "Target mean±std: 0.08173996210098267 0.042615462094545364\n",
      "Init pred mean±std: 0.08289702981710434 0.041099246591329575\n",
      "Epoch 53: Train Loss = 0.0000, Val Loss = 0.0000\n",
      "Target mean±std: 0.08173996210098267 0.042615462094545364\n",
      "Init pred mean±std: 0.0828966572880745 0.041099611669778824\n",
      "Epoch 54: Train Loss = 0.0000, Val Loss = 0.0000\n",
      "Target mean±std: 0.08173996210098267 0.042615462094545364\n",
      "Init pred mean±std: 0.08289627730846405 0.04109996557235718\n",
      "Epoch 55: Train Loss = 0.0000, Val Loss = 0.0000\n",
      "Target mean±std: 0.08173996210098267 0.042615462094545364\n",
      "Init pred mean±std: 0.08289603888988495 0.041100338101387024\n",
      "Epoch 56: Train Loss = 0.0000, Val Loss = 0.0000\n",
      "Target mean±std: 0.08173996210098267 0.042615462094545364\n",
      "Init pred mean±std: 0.08289558440446854 0.04110069200396538\n",
      "Epoch 57: Train Loss = 0.0000, Val Loss = 0.0000\n",
      "Target mean±std: 0.08173996210098267 0.042615462094545364\n",
      "Init pred mean±std: 0.08289535343647003 0.04110105708241463\n",
      "Epoch 58: Train Loss = 0.0000, Val Loss = 0.0000\n",
      "Target mean±std: 0.08173996210098267 0.042615462094545364\n",
      "Init pred mean±std: 0.0828949511051178 0.04110141098499298\n",
      "Epoch 59: Train Loss = 0.0000, Val Loss = 0.0000\n",
      "Target mean±std: 0.08173996210098267 0.042615462094545364\n",
      "Init pred mean±std: 0.08289463818073273 0.04110176861286163\n",
      "Epoch 60: Train Loss = 0.0000, Val Loss = 0.0000\n",
      "Target mean±std: 0.08173996210098267 0.042615462094545364\n",
      "Init pred mean±std: 0.08289434015750885 0.04110213741660118\n",
      "Epoch 61: Train Loss = 0.0000, Val Loss = 0.0000\n",
      "Target mean±std: 0.08173996210098267 0.042615462094545364\n",
      "Init pred mean±std: 0.08289390802383423 0.041102487593889236\n",
      "Epoch 62: Train Loss = 0.0000, Val Loss = 0.0000\n",
      "Target mean±std: 0.08173996210098267 0.042615462094545364\n",
      "Init pred mean±std: 0.08289370685815811 0.041102856397628784\n",
      "Epoch 63: Train Loss = 0.0000, Val Loss = 0.0000\n",
      "Target mean±std: 0.08173996210098267 0.042615462094545364\n",
      "Init pred mean±std: 0.08289319276809692 0.04110319912433624\n",
      "Epoch 64: Train Loss = 0.0000, Val Loss = 0.0000\n",
      "Target mean±std: 0.08173996210098267 0.042615462094545364\n",
      "Init pred mean±std: 0.08289308845996857 0.04110356792807579\n",
      "Epoch 65: Train Loss = 0.0000, Val Loss = 0.0000\n",
      "Target mean±std: 0.08173996210098267 0.042615462094545364\n",
      "Init pred mean±std: 0.08289246261119843 0.04110391065478325\n",
      "Epoch 66: Train Loss = 0.0000, Val Loss = 0.0000\n",
      "Target mean±std: 0.08173996210098267 0.042615462094545364\n",
      "Init pred mean±std: 0.08289246261119843 0.04110429063439369\n",
      "Epoch 67: Train Loss = 0.0000, Val Loss = 0.0000\n",
      "Target mean±std: 0.08173996210098267 0.042615462094545364\n",
      "Init pred mean±std: 0.0828917846083641 0.041104625910520554\n",
      "Epoch 68: Train Loss = 0.0000, Val Loss = 0.0000\n",
      "Target mean±std: 0.08173996210098267 0.042615462094545364\n",
      "Init pred mean±std: 0.08289173245429993 0.041105005890131\n",
      "Epoch 69: Train Loss = 0.0000, Val Loss = 0.0000\n",
      "Target mean±std: 0.08173996210098267 0.042615462094545364\n",
      "Init pred mean±std: 0.08289124816656113 0.041105348616838455\n",
      "Epoch 70: Train Loss = 0.0000, Val Loss = 0.0000\n",
      "Target mean±std: 0.08173996210098267 0.042615462094545364\n",
      "Init pred mean±std: 0.08289093524217606 0.041105709969997406\n",
      "Epoch 71: Train Loss = 0.0000, Val Loss = 0.0000\n",
      "Target mean±std: 0.08173996210098267 0.042615462094545364\n",
      "Init pred mean±std: 0.08289071917533875 0.04110607132315636\n",
      "Epoch 72: Train Loss = 0.0000, Val Loss = 0.0000\n",
      "Target mean±std: 0.08173996210098267 0.042615462094545364\n",
      "Init pred mean±std: 0.08289015293121338 0.04110641032457352\n",
      "Epoch 73: Train Loss = 0.0000, Val Loss = 0.0000\n",
      "Target mean±std: 0.08173996210098267 0.042615462094545364\n",
      "Init pred mean±std: 0.08289017528295517 0.04110679030418396\n",
      "Epoch 74: Train Loss = 0.0000, Val Loss = 0.0000\n",
      "Target mean±std: 0.08173996210098267 0.042615462094545364\n",
      "Init pred mean±std: 0.08288943022489548 0.041107114404439926\n",
      "Epoch 75: Train Loss = 0.0000, Val Loss = 0.0000\n",
      "Target mean±std: 0.08173996210098267 0.042615462094545364\n",
      "Init pred mean±std: 0.08288948237895966 0.04110749810934067\n",
      "Epoch 76: Train Loss = 0.0000, Val Loss = 0.0000\n",
      "Target mean±std: 0.08173996210098267 0.042615462094545364\n",
      "Init pred mean±std: 0.08288886398077011 0.04110782966017723\n",
      "Epoch 77: Train Loss = 0.0000, Val Loss = 0.0000\n",
      "Target mean±std: 0.08173996210098267 0.042615462094545364\n",
      "Init pred mean±std: 0.082888662815094 0.04110819473862648\n",
      "Epoch 78: Train Loss = 0.0000, Val Loss = 0.0000\n",
      "Target mean±std: 0.08173996210098267 0.042615462094545364\n",
      "Init pred mean±std: 0.0828883945941925 0.041108548641204834\n",
      "Epoch 79: Train Loss = 0.0000, Val Loss = 0.0000\n",
      "Target mean±std: 0.08173996210098267 0.042615462094545364\n",
      "Init pred mean±std: 0.08288787305355072 0.041108883917331696\n",
      "Epoch 80: Train Loss = 0.0000, Val Loss = 0.0000\n",
      "Target mean±std: 0.08173996210098267 0.042615462094545364\n",
      "Init pred mean±std: 0.08288785070180893 0.04110926017165184\n",
      "Epoch 81: Train Loss = 0.0000, Val Loss = 0.0000\n",
      "Target mean±std: 0.08173996210098267 0.042615462094545364\n",
      "Init pred mean±std: 0.0828871801495552 0.041109584271907806\n",
      "Epoch 82: Train Loss = 0.0000, Val Loss = 0.0000\n",
      "Target mean±std: 0.08173996210098267 0.042615462094545364\n",
      "Init pred mean±std: 0.08288715779781342 0.041109953075647354\n",
      "Epoch 83: Train Loss = 0.0000, Val Loss = 0.0000\n",
      "Target mean±std: 0.08173996210098267 0.042615462094545364\n",
      "Init pred mean±std: 0.0828867256641388 0.04111029952764511\n",
      "Epoch 84: Train Loss = 0.0000, Val Loss = 0.0000\n",
      "Target mean±std: 0.08173996210098267 0.042615462094545364\n",
      "Init pred mean±std: 0.08288634568452835 0.04111064597964287\n",
      "Epoch 85: Train Loss = 0.0000, Val Loss = 0.0000\n",
      "Target mean±std: 0.08173996210098267 0.042615462094545364\n",
      "Init pred mean±std: 0.08288615196943283 0.04111101105809212\n",
      "Epoch 86: Train Loss = 0.0000, Val Loss = 0.0000\n",
      "Target mean±std: 0.08173996210098267 0.042615462094545364\n",
      "Init pred mean±std: 0.08288571238517761 0.04111135005950928\n",
      "Epoch 87: Train Loss = 0.0000, Val Loss = 0.0000\n",
      "Target mean±std: 0.08173996210098267 0.042615462094545364\n",
      "Init pred mean±std: 0.08288545906543732 0.04111171141266823\n",
      "Epoch 88: Train Loss = 0.0000, Val Loss = 0.0000\n",
      "Target mean±std: 0.08173996210098267 0.042615462094545364\n",
      "Init pred mean±std: 0.08288510888814926 0.04111206531524658\n",
      "Epoch 89: Train Loss = 0.0000, Val Loss = 0.0000\n",
      "Target mean±std: 0.08173996210098267 0.042615462094545364\n",
      "Init pred mean±std: 0.0828847736120224 0.041112419217824936\n",
      "Epoch 90: Train Loss = 0.0000, Val Loss = 0.0000\n",
      "Target mean±std: 0.08173996210098267 0.042615462094545364\n",
      "Init pred mean±std: 0.08288446813821793 0.04111277312040329\n",
      "Epoch 91: Train Loss = 0.0000, Val Loss = 0.0000\n",
      "Target mean±std: 0.08173996210098267 0.042615462094545364\n",
      "Init pred mean±std: 0.08288414776325226 0.041113127022981644\n",
      "Epoch 92: Train Loss = 0.0000, Val Loss = 0.0000\n",
      "Target mean±std: 0.08173996210098267 0.042615462094545364\n",
      "Init pred mean±std: 0.08288374543190002 0.0411134772002697\n",
      "Epoch 93: Train Loss = 0.0000, Val Loss = 0.0000\n",
      "Target mean±std: 0.08173996210098267 0.042615462094545364\n",
      "Init pred mean±std: 0.0828835740685463 0.04111383482813835\n",
      "Epoch 94: Train Loss = 0.0000, Val Loss = 0.0000\n",
      "Target mean±std: 0.08173996210098267 0.042615462094545364\n",
      "Init pred mean±std: 0.0828830897808075 0.04111417755484581\n",
      "Epoch 95: Train Loss = 0.0000, Val Loss = 0.0000\n",
      "Target mean±std: 0.08173996210098267 0.042615462094545364\n",
      "Init pred mean±std: 0.08288289606571198 0.04111453890800476\n",
      "Epoch 96: Train Loss = 0.0000, Val Loss = 0.0000\n",
      "Target mean±std: 0.08173996210098267 0.042615462094545364\n",
      "Init pred mean±std: 0.08288253098726273 0.041114892810583115\n",
      "Epoch 97: Train Loss = 0.0000, Val Loss = 0.0000\n",
      "Target mean±std: 0.08173996210098267 0.042615462094545364\n",
      "Init pred mean±std: 0.08288215100765228 0.04111524298787117\n",
      "Epoch 98: Train Loss = 0.0000, Val Loss = 0.0000\n",
      "Target mean±std: 0.08173996210098267 0.042615462094545364\n",
      "Init pred mean±std: 0.08288200199604034 0.04111560806632042\n",
      "Epoch 99: Train Loss = 0.0000, Val Loss = 0.0000\n",
      "Target mean±std: 0.08173996210098267 0.042615462094545364\n",
      "Init pred mean±std: 0.08288145065307617 0.04111594334244728\n",
      "Epoch 100: Train Loss = 0.0000, Val Loss = 0.0000\n",
      "Target mean±std: 0.08173996210098267 0.042615462094545364\n",
      "Init pred mean±std: 0.08288145065307617 0.04111594334244728\n",
      "Final Test Loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Train and Evaluate the model - PyTorch LSTM Model\n",
    "# \n",
    "\n",
    "# Train and Evaluate The model\n",
    "for epoch in range(1, 100 + 1):\n",
    "    train_loss = pytorch_train(model2, train_loader, optm2, crit2, device)\n",
    "    val_loss   = pytorch_evaluate(model2, test_loader, crit2, device)\n",
    "    print(f\"Epoch {epoch:02d}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}\")\n",
    "\n",
    "# Final evaluation on test set\n",
    "final_loss = pytorch_evaluate(model2, test_loader, crit2, device)\n",
    "print(f\"Final Test Loss: {final_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Config: {Benchmark Evaluation: PyTorch LSTM }\n",
      "MSE    : 1.6e-05\n",
      "RMSE   : 0.004053\n",
      "MAE    : 0.002928\n",
      "R2     : 0.990946\n"
     ]
    }
   ],
   "source": [
    "# Train and Evaluate the model - PyTorch LSTM Model\n",
    "# \n",
    "\n",
    "# Print the results for the test-set\n",
    "pred, mse, rmse, mae, r2 = eval_pipeline(model2, None, None, use = \"Torch\", loader=test_loader)\n",
    "print(\"Model Config: {Benchmark Evaluation: PyTorch LSTM }\")\n",
    "print(\"MSE    :\", round(mse, 6))\n",
    "print(\"RMSE   :\", round(rmse, 6))\n",
    "print(\"MAE    :\", round(mae, 6))\n",
    "print(\"R2     :\", round(r2, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A short Comparision between LSTM and PyTorch Model\n",
    "\n",
    "# One sentence: The achieved a comparable level.\n",
    "\n",
    "# Focus on figures:\n",
    "# My custom LSTM with one stacked layer outperforms the PyTorch LSTM across most evaluation metrics.\n",
    "#   It records a lower MSE of 1.5e-05 compared to 1.6e-05, and a slightly improved \n",
    "#   RMSE of 0.003924 versus 0.004053. Additionally, my model achieves a higher R-squared value of 0.991512, \n",
    "#   indicating a stronger fit to the data than the PyTorch LSTM, which scores 0.990946. \n",
    "#   The only exception is the MAE, where the PyTorch LSTM shows a minor advantage at 0.002928 against my model’s 0.002935."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2.3.4 Plot the Training loss curve`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training/validation loss over epochs\n",
    "\n",
    "# Collect train-valid losses\n",
    "def collect_losses(evaluator: nn_SInterf_Evaluator, \n",
    "                   metric = \"logloss\",\n",
    "                   collect = [\"Train\", \"Valid\"]) -> Tuple[dict, pd.DataFrame]:\n",
    "\n",
    "    evalhist = deepcopy(evaluator.evalhist_)\n",
    "    evaldict = {}\n",
    "    evaldict[\"No\"] = []\n",
    "    for c in collect:\n",
    "        evaldict[c] = []\n",
    "\n",
    "    # For each, extract data as float types\n",
    "    for k in evalhist.keys():\n",
    "        obj = evalhist[k]\n",
    "        # No self-increment\n",
    "        evaldict[\"No\"].append(k)\n",
    "        for c in collect:\n",
    "            data = obj[c][metric].to_list()\n",
    "            evaldict[c].append(data)\n",
    "\n",
    "    return evaldict, pd.DataFrame(evaldict)\n",
    "\n",
    "# Plot train/train-valid losses\n",
    "def plot_loss(df, xlabel_: str | None = None, what: str = \"Loss\", log_y: bool = True):\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    from matplotlib.collections import LineCollection\n",
    "    from matplotlib.colors import Normalize\n",
    "\n",
    "    if 'No' in df.columns:\n",
    "        x = df['No'].values\n",
    "        xlabel = 'Epoch'\n",
    "    else:\n",
    "        x = np.arange(len(df))\n",
    "        xlabel = 'Index' if xlabel_ is None else xlabel_\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "    # Plot training loss with gradient color\n",
    "    y_train = df['Train'].values\n",
    "    pts_train = np.array([x, y_train]).T.reshape(-1, 1, 2)\n",
    "    segs_train = np.concatenate([pts_train[:-1], pts_train[1:]], axis=1)\n",
    "    norm_train = Normalize(vmin=y_train.min(), vmax=y_train.max())\n",
    "    lc_train = LineCollection(segs_train, cmap='viridis', norm=norm_train)\n",
    "    lc_train.set_array(y_train)\n",
    "    lc_train.set_linewidth(2)\n",
    "    ax.add_collection(lc_train)\n",
    "    cbar_train = fig.colorbar(lc_train, ax=ax, pad=0.02)\n",
    "    cbar_train.set_label(what)\n",
    "\n",
    "    # Plot validation loss if available\n",
    "    if 'Valid' in df.columns:\n",
    "        y_valid = df['Valid'].values\n",
    "        pts_valid = np.array([x, y_valid]).T.reshape(-1, 1, 2)\n",
    "        segs_valid = np.concatenate([pts_valid[:-1], pts_valid[1:]], axis=1)\n",
    "        norm_valid = Normalize(vmin=y_valid.min(), vmax=y_valid.max())\n",
    "        lc_valid = LineCollection(segs_valid, cmap='plasma', norm=norm_valid)\n",
    "        lc_valid.set_array(y_valid)\n",
    "        lc_valid.set_linewidth(2)\n",
    "        ax.add_collection(lc_valid)\n",
    "\n",
    "    ax.set_xlim(x.min(), x.max())\n",
    "    y_min = y_train.min()\n",
    "    y_max = y_train.max()\n",
    "    if 'Valid' in df.columns:\n",
    "        y_min = min(y_min, y_valid.min())\n",
    "        y_max = max(y_max, y_valid.max())\n",
    "    ax.set_ylim(y_min, y_max)\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(what)\n",
    "    if log_y == True:\n",
    "        ax.set_yscale('log')\n",
    "    ax.set_title(what + ' Over {}s'.format(xlabel))\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABDoAAAHqCAYAAADlKy4xAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAoQJJREFUeJzs3XecXPV97//3OVN3Z2dmey/qZdUAIQGmmI6JKy5x4tyY4N91mnDsS8ovvknsNF8cO86PJN4EXxObOIkTbN/rEleKAdEREgIkgXrZ3qfs7O60c35/zO5IioRQmd2zM3o98zgPds6c+c7nzAo58+b7/XwN27ZtAQAAAAAAlADT6QIAAAAAAAAKhaADAAAAAACUDIIOAAAAAABQMgg6AAAAAABAySDoAAAAAAAAJYOgAwAAAAAAlAyCDgAAAAAAUDIIOgAAAAAAQMkg6AAAAAAAACWDoAMAAOAMnnjiCRmGoe985ztOlwIAAM4CQQcAAGfhwQcflGEYeumll5wu5aw888wzuuOOO9TQ0CCfz6dFixbpN37jN3Ts2DGnSzvFbJDwZsd//Md/OF0iAAAoIm6nCwAAAIX193//9/rkJz+pJUuW6BOf+ISampr0+uuv64EHHtBDDz2kH//4x3rb297mdJmn+J3f+R1t2rTplPNXXXWVA9UAAIBiRdABAEAJeeaZZ/SpT31K11xzjX7605+qvLw8/9xv/dZv6eqrr9YHP/hB7d69W1VVVfNWVyKRUCAQOOM11157rT74wQ/OU0UAAKBUsXQFAIACevnll3X77bcrFAqpoqJCN910k55//vmTrkmn0/qzP/szLV++XH6/XzU1Nbrmmmv0yCOP5K8ZGBjQXXfdpdbWVvl8PjU1Nem9732vjhw5csb3/4u/+AsZhqF//ud/PinkkKSlS5fqC1/4gvr7+/WVr3xFkvTXf/3XMgxDR48ePWWsT3/60/J6vRofH8+fe+GFF/SOd7xD4XBY5eXlevvb365nnnnmpNf96Z/+qQzD0J49e/SRj3xEVVVVuuaaa87q83srhmHo7rvv1r/9279p5cqV8vv92rhxo7Zu3XrKtWfzu5CkSCSi//E//ocWLVokn8+n1tZWffSjH9XIyMhJ11mWpc997nNqbW2V3+/XTTfdpAMHDpx0zf79+/WBD3xAjY2N8vv9am1t1S/90i8pGo0W5P4BAMBbY0YHAAAFsnv3bl177bUKhUL6gz/4A3k8Hn3lK1/R9ddfryeffFJXXHGFpFwQcO+99+q///f/rs2bNysWi+mll17Sjh07dMstt0iSPvCBD2j37t36xCc+oUWLFmloaEiPPPKIjh07pkWLFp32/ScnJ/XYY4/p2muv1eLFi097zYc//GH9+q//un74wx/qD//wD/WLv/iL+oM/+AN961vf0u///u+fdO23vvUt3XrrrfmZHz//+c91++23a+PGjfrsZz8r0zT19a9/XTfeeKOeeuopbd68+aTXf+hDH9Ly5cv1v/7X/5Jt22/5+cXj8VPCBUmqqamRYRj5x08++aQeeugh/c7v/I58Pp/+4R/+Qe94xzv04osvau3atef0u5iYmNC1116r119/XR/72Md02WWXaWRkRD/4wQ/U09Oj2tra/Pt+/vOfl2ma+r3f+z1Fo1F94Qtf0K/8yq/ohRdekCSlUinddtttSiaT+sQnPqHGxkb19vbqhz/8oSKRiMLh8Ft+BgAAoABsAADwlr7+9a/bkuxt27a96TXve9/7bK/Xax88eDB/rq+vzw4Gg/Z1112XP7dhwwb7ne9855uOMz4+bkuyv/jFL55TjTt37rQl2Z/85CfPeN369evt6urq/OOrrrrK3rhx40nXvPjii7Yk+xvf+IZt27ZtWZa9fPly+7bbbrMty8pfNzk5aS9evNi+5ZZb8uc++9nP2pLsX/7lXz6ruh9//HFb0pse/f39+Wtnz7300kv5c0ePHrX9fr99xx135M+d7e/iM5/5jC3J/r//9/+eUtfsfc7Wt3r1ajuZTOaf/9u//Vtbkv3aa6/Ztm3bL7/8si3J/va3v31W9w0AAOYGS1cAACiAbDarhx9+WO973/u0ZMmS/PmmpiZ95CMf0dNPP61YLCZJqqys1O7du7V///7TjlVWViav16snnnjipGUjbyUej0uSgsHgGa8LBoP5WqTcLI/t27fr4MGD+XMPPfSQfD6f3vve90qSdu7cqf379+sjH/mIRkdHNTIyopGRESUSCd10003aunWrLMs66X1+8zd/86xrl6TPfOYzeuSRR045qqurT7ruqquu0saNG/OP29vb9d73vlc/+9nPlM1mz+l38X/+z//Rhg0bdMcdd5xSz4mzSCTprrvuktfrzT++9tprJUmHDh2SpPyMjZ/97GeanJw8p3sHAACFQ9ABAEABDA8Pa3JyUitXrjzludWrV8uyLHV3d0uS/vzP/1yRSEQrVqzQunXr9Pu///t69dVX89f7fD791V/9lX7yk5+ooaFB1113nb7whS9oYGDgjDXMBhyzgcebicfjJ4UhH/rQh2Saph566CFJkm3b+va3v53vbyEpH8rceeedqqurO+l44IEHlEwmT+lD8WbLZ97MunXrdPPNN59ynBguSNLy5ctPee2KFSs0OTmp4eHhc/pdHDx4ML/c5a20t7ef9Hh2Sc9sGLV48WLdc889euCBB1RbW6vbbrtNXV1d9OcAAGCeEXQAADDPrrvuOh08eFBf+9rXtHbtWj3wwAO67LLL9MADD+Sv+dSnPqV9+/bp3nvvld/v15/8yZ9o9erVevnll9903GXLlsntdp8UmvxXyWRSe/fuVWdnZ/5cc3Ozrr32Wn3rW9+SJD3//PM6duyYPvzhD+evmZ2t8cUvfvG0sy4eeeQRVVRUnPReZWVl5/bBLHAul+u05+0T+o986Utf0quvvqr/+T//p6ampvQ7v/M7WrNmjXp6euarTAAALnoEHQAAFEBdXZ3Ky8u1d+/eU5574403ZJqm2tra8ueqq6t111136d///d/V3d2t9evX60//9E9Pet3SpUv1u7/7u3r44Ye1a9cupVIpfelLX3rTGgKBgG644QZt3br1tLuoSLkGo8lkUu9617tOOv/hD39Yr7zyivbu3auHHnpI5eXleve7331SLZIUCoVOO+vi5ptvlsfjecvPqRBOt+Rn3759Ki8vz88yOdvfxdKlS7Vr166C1rdu3Tr98R//sbZu3aqnnnpKvb29uv/++wv6HgAA4M0RdAAAUAAul0u33nqrvv/975+0Bezg4KC++c1v6pprrskvAxkdHT3ptRUVFVq2bJmSyaSk3O4p09PTJ12zdOlSBYPB/DVv5o//+I9l27Z+7dd+TVNTUyc9d/jwYf3BH/yBmpqa9Bu/8RsnPfeBD3xALpdL//7v/65vf/vbete73qVAIJB/fuPGjVq6dKn++q//WhMTE6e87/Dw8BnrKqTnnntOO3bsyD/u7u7W97//fd16661yuVzn9Lv4wAc+oFdeeUXf/e53T3kf+yx2ijlRLBZTJpM56dy6detkmuZb/t4AAEDhsL0sAADn4Gtf+5p++tOfnnL+k5/8pP7yL/9SjzzyiK655hr99m//ttxut77yla8omUzqC1/4Qv7azs5OXX/99dq4caOqq6v10ksv6Tvf+Y7uvvtuSbnZCTfddJN+8Rd/UZ2dnXK73frud7+rwcFB/dIv/dIZ67vuuuv013/917rnnnu0fv16/dqv/Zqampr0xhtv6Ktf/aosy9KPf/zjfH+JWfX19brhhhv0N3/zN4rH4yctW5Ek0zT1wAMP6Pbbb9eaNWt01113qaWlRb29vXr88ccVCoX0n//5n+f7sUqSnnrqqVMCHklav3691q9fn3+8du1a3XbbbSdtLytJf/Znf5a/5mx/F7//+7+v73znO/rQhz6kj33sY9q4caPGxsb0gx/8QPfff782bNhw1vX//Oc/1913360PfehDWrFihTKZjP7lX/5FLpdLH/jAB87nIwEAAOfD2U1fAAAoDrPby77Z0d3dbdu2be/YscO+7bbb7IqKCru8vNy+4YYb7Gefffaksf7yL//S3rx5s11ZWWmXlZXZq1atsj/3uc/ZqVTKtm3bHhkZsbds2WKvWrXKDgQCdjgctq+44gr7W9/61lnXu3XrVvu9732vXVtba3s8Hru9vd3++Mc/bh85cuRNX/PVr37VlmQHg0F7amrqtNe8/PLL9vvf/367pqbG9vl8dkdHh/2Lv/iL9mOPPZa/ZnZ72eHh4bOq9a22l/3sZz+bv1aSvWXLFvtf//Vf7eXLl9s+n8++9NJL7ccff/yUcc/md2Hbtj06OmrffffddktLi+31eu3W1lb7zjvvtEdGRk6q779uG3v48GFbkv31r3/dtm3bPnTokP2xj33MXrp0qe33++3q6mr7hhtusB999NGz+hwAAEBhGLZ9jvMyAQAAHGIYhrZs2aIvf/nLTpcCAAAWKHp0AAAAAACAkkHQAQAAAAAASgZBBwAAAAAAKBnsugIAAIoGrcUAAMBbYUYHAAAAAAAoGQQdAAAAAACgZLB05Qwsy1JfX5+CwaAMw3C6HAAAAABAkbJtW/F4XM3NzTLNws45mJ6eViqVKth4Xq9Xfr+/YOPNN4KOM+jr61NbW5vTZQAAAAAASkR3d7daW1sLNt709LQWd1RoYChbsDEbGxt1+PDhog07CDrOIBgMSpL27rxe1Y13y1t2i8MVAQAAAACKUSwWU1tbW/57ZqGkUikNDGV1dPsihYIXPlMkFrfUsfGIUqkUQUcpml2u4g365fHuUCj0AYcrAgAAAAAUs7lqi1ARNFQRvPCxLRV/2waCjrPh71YsbShoJWSaAaerAQAAAADgJFnbUrYAu7BnbevCB3EYu66cBcPKKKO4YlNPOF0KAAAAAAA4A4KOs2CkTbntpEannnK6FAAAAAAATmHJLthR7Ag6zoKRMuRLpzQ4+YIsK+N0OQAAAAAA4E0QdJwFI+OXkZaUHlc09brT5QAAAAAAcBKrgP9X7GhGehZc2XIZqYSqrKz6Jl9SlX+d0yUBAAAAAJCXtW1l7QtfdlKIMZzGjI6z4MmGpYyp+nRWR2IvOV0OAAAAAAB4E8zoOAtu71pZmTGVpSY1mditZCYhn5ttZgEAAAAAC0OhGonSjPRiUbZerkyZlHSpJeVWz9RupysCAAAAACDPkq1sAQ6CjhLV1dWlzs5Obdq0SZJkBDfJYwWkjKnmqbSOJPY6XCEAAAAAADgdgo7T2LJli/bs2aNt27ZJkgxfgzyeJcpm3KpLJBSZOOpwhQAAAAAAHDe7dKUQR7Ej6DhLruB6JTNeuactlUe6lbKSTpcEAAAAAAD+C4KOsxW+RLJDUtKlqki/jiWY1QEAAAAAWBhmt5ctxFHsCDrOklG9Tqa7RlPyqD06qn3xfU6XBAAAAACAJMkq4FHsCDrOkmF65Aos06QqVDNpaXBwl9MlAQAAAACA/4Kg4xyUV63XlOXThF2hurF+p8sBAAAAAECSCrK17OxR7NxOF1BMzNp1srIeDdkZVY0NaSwZVbUv7HRZAAAAAICLXNbOHYUYp9gxo+McGDXLVG75NWm55YsndCDe7XRJAAAAAADgBAQd58AwXTL8darMeOW1y9Tf/7rTJQEAAAAAQDPSExB0nCNPwzrVxifkT2ZkDrLFLAAAAADAeZYMZQtwWDKcvpULRtBxjkLtm6S0S42xKaV79ztdDgAAAAAAOAFBxznyNK3SQKBO07ZXnaNRTWVSTpcEAAAAALjIWXbhjmJH0HGOTI9P42VV2ucukzlt68gIy1cAAAAAAFgoCDrOg6eiQe6sS92mV4OHdjldDgAAAADgIleI/hyzR7Ej6DgPnoYlWh6d1qJpW9ZAj9PlAAAAAAAucgQdxxF0nIfqxZdoXD6FEhm5jxxyuhwAAAAAADDD7XQBC1FXV5e6urqUzWZP+3xT6zI9E6rR+HRKwfHJea4OAAAAAICTWbYhy77w2RiFGMNpzOg4jS1btmjPnj3atm3baZ93uVyasg1NpzKKGh6NDfXPc4UAAAAAABzH0pXjCDrOk1HToqWxlOojCe3bs9PpcgAAAAAAgAg6zlto6RqNucoVnDY0feCg0+UAAAAAAC5iWZkFO4pd8d+BQzpWrFGf3689FRUa72XpCgAAAAAACwHNSM9TY1uHBuPTmnJ7ZE9bsm1bhlH8a5kAAAAAAMXHLlAzUptmpBcvwzA0EKxSfTSt5b3jGu5nVgcAAAAAwBk0Iz2OoOMCJBcv1tFgSOMuv7p373W6HAAAAAAALnosXbkAnpoaGYndOuZ1q6Knz+lyAAAAAAAXqaxtKmtf+FyGrF2AYhxG0HEB6pctU2DySTVOptW995jT5QAAAAAALlKWDFkFWLRhqfiTDoKOC9C+YokeDZapKmWrfHTK6XIAAAAAALjoEXRcgLpAhSrklpFNazRrKTuVlKvM53RZAAAAAICLTKEaidKM9CJnGIbGKirUGLe1ZDipyOFep0sCAAAAAFyEZnt0FOIodsV/Bw6zO5r1WoVf+8r8OrT7kNPlAAAAAABwUSPouEBNi1rUOZ5R80RWx96gISkAAAAAYP7lmpEW5ih2BB0XaPn6lYr7/fKmDbkOjci2i79DLQAAAAAAxYqg4wItWdqqSb9H3TUBxTK2njnErA4AAAAAwPyyZCpbgKMQW9Q6jV1XLpBhGAqGwqrcPyRJ2r73iK5Z2uFwVQAAAACAi0mhGolmS2CVQvFHNQtA06aV6qsq16jHo33b9ymVzTpdEgAAAAAAFyWCjgII1FWqJppRyutWR8ajV7r7nS4JAAAAAHARsWaWnRTiKHbFfwcLQNmSBhlTUs1YVql9w9pxrNfpkgAAAAAAF5GsbRTsKHYEHQUQWFwrX2erDlf65J6wtf1In9MlAQAAAABwUaIZaQF4gmXKDk2qadSSp9KtH/UOybZtGUbxJ2EAAAAAgIVvdteUCx+HZqQlqaurS52dndq0adNZvyZ8aYfsjKHa4azWuYLqHo3OYYUAAAAAABxn2WbBjmJX/HcwB7Zs2aI9e/Zo27ZtZ/2a4KoGTS6uVG+5qehr/To4NDqHFQIAAAAAgNMh6CiQ4NI6laUNZX0etUXd2tc/4nRJAAAAAICLxOzSlUIcxY4eHQUSXtckI2loKpVVfdZUJD7pdEkAAAAAAFx0ij+qWSDcZR5VrqiXabvkmZJ6X2bnFQAAAADA/LBUmC1mLadvpAAIOgqo8erFsmxTRtZU4PW40+UAAAAAAC4SlsyCHcWu+O9gAWm8YZGC5WUql1vNw4ZS6YzTJQEAAAAAcFEh6CigsvoKJcMe2Ulb3qSh15456HRJAAAAAICLQNY2C3YUu+K/gwUmuL5e2bSh+HRaBx8+7HQ5AAAAAICLgCWjYEexI+gosEU3L5Hpdsnv82tyV8TpcgAAAAAAuKiwvWyBLV/dpMd9WXkmszIGDcW7Ywq2hZwuCwAAAABQwgq17ISlKzhFfWWFAutrVGZ4lLal3mfYZhYAAAAAgPlC0FFghmHIsyooc1LKZqV9z3Q7XRIAAAAAoMRlZRbsKHbFfwcL0Nobl8ozbchM2BraPy7btp0uCQAAAABQwizbKNhR7Ag65sDla9o1VmsoEDGU7JlW/NiE0yUBAAAAAHBRIOiYA8Fyv+xVAWVlyD1o6cBWlq8AAAAAAOaOVaBlK1YJxATFfwcLVNPGekUrTU26pTceO+Z0OQAAAACAEmbZZsGOYlf8d7BArby6Tabb1GSloZFDcafLAQAAAADgokDQMUfWr2uVkZT8SZeSsbTSUxmnSwIAAAAAlKisjIIdxY6gY44EA35N1EjeMcl7xFLfy8NOlwQAAAAAKFEsXTmu+O9gAQusD2kskFHWkHb9nD4dAAAAAADMNYKOObR0U5MyLmk4lNaxXSNOlwMAAAAAKFFZFWr5yvn7/Oc/L8Mw9KlPfapAd3V+CDrm0Ibr21U57lX1uFfj+yacLgcAAAAAgDmxbds2feUrX9H69eudLoWgYy4tWdqgiRbJzhpy788qk7qQbAwAAAAAgNNzskfHxMSEfuVXfkVf/epXVVVVNQd3d24IOuaQaRrK1JuK1GY0WSu98Wyv0yUBAAAAAEpQ1jYLdpyrLVu26J3vfKduvvnmObizc+d2uoBS17i0SrEXxpTyW9r9VK/WXt/udEkAAAAAAJxRLBY76bHP55PP5zvluv/4j//Qjh07tG3btvkq7S0xo2OOXf2e5Up7JfeUS3se7XG6HAAAAABACbJlyCrAYcuQJLW1tSkcDuePe++995T37O7u1ic/+Un927/9m/x+/3zf8ptiRsccW3Z1kxKVWcmWXCOSZdkyTcPpsgAAAAAAJeR8l52cbhwpF2KEQqH8+dPN5ti+fbuGhoZ02WWXHX99NqutW7fqy1/+spLJpFwu1wXXdK4IOuZYRX2ZyrMexaeTSqaTOnBgSCtWNDhdFgAAAAAAbyoUCp0UdJzOTTfdpNdee+2kc3fddZdWrVql//f//X8dCTkkgo55Ub0hrOyzEU1409rx08MEHQAAAACAgrJsQ5Z94asHzmWMYDCotWvXnnQuEAiopqbmlPPziR4d82DZjY2yJyR3ytTrj/c5XQ4AAAAAoMRkZRbsKHbM6JgHmz+wVE9/Zq88UZeiuxNKJjPy+fjoAQAAAACl5YknnnC6hBKIaopAzZKgfEu9slyS74hbr73W7XRJAAAAAIASMrt0pRBHsSv5oOOHP/yhVq5cqeXLl+uBBx5wrI6G9ZXKVNqybWnbT444VgcAAAAAAKWspNdPZDIZ3XPPPXr88ccVDoe1ceNG3XHHHaqpqZn3WlZd1aT9Tw0oGcro4DND8/7+AAAAAIDSZcmUVYC5DIUYw2nFfwdn8OKLL2rNmjVqaWlRRUWFbr/9dj388MOO1LL06nq5E6bkMjR+KKGhkbgjdQAAAAAASk/WNgp2FLsFHXRs3bpV7373u9Xc3CzDMPS9733vlGu6urq0aNEi+f1+XXHFFXrxxRfzz/X19amlpSX/uKWlRb29vfNR+ima1lfKnjTkGXXLPW7qVfp0AAAAAABQcAs66EgkEtqwYYO6urpO+/xDDz2ke+65R5/97Ge1Y8cObdiwQbfddpuGhhbe0hCX21Tr9dWyJVWMlum5nx9wuiQAAAAAQImgGelxCzrouP322/WXf/mXuuOOO077/N/8zd/o4x//uO666y51dnbq/vvvV3l5ub72ta9Jkpqbm0+awdHb26vm5uY3fb9kMqlYLHbSUUgr3tYoo9lQuiKrgZ1RZbNWQccHAAAAAFycbNuUVYDDthd0THBWivYOUqmUtm/frptvvjl/zjRN3XzzzXruueckSZs3b9auXbvU29uriYkJ/eQnP9Ftt932pmPee++9CofD+aOtra2gNdetCMqVNpX2ZxU9OqW9e/sLOj4AAAAAABe7og06RkZGlM1m1dDQcNL5hoYGDQwMSJLcbre+9KUv6YYbbtAll1yi3/3d3z3jjiuf/vSnFY1G80d3d2H7aLRdXq3koCX3sEfuIZd27jxa0PEBAAAAABenrIyCHcWupLeXlaT3vOc9es973nNW1/p8Pvl8vjmrpXZphTqurdHh50ZUFinTof2Dc/ZeAAAAAABcjIp2Rkdtba1cLpcGB08OCwYHB9XY2OhQVWdmGIZa1ldJVYaUNvXKEz2KRBJOlwUAAAAAKHKWXaiGpE7fyYUr2qDD6/Vq48aNeuyxx/LnLMvSY489pquuusrBys6ssTOkQI1PVigre8DQLraZBQAAAABcoEI0Ip09it2CXroyMTGhAweOb8N6+PBh7dy5U9XV1Wpvb9c999yjO++8U5dffrk2b96s++67T4lEQnfddZeDVZ9Z2yWVMtOmDL/knfTq1Z3duubaVU6XBQAAAABASVjQQcdLL72kG264If/4nnvukSTdeeedevDBB/XhD39Yw8PD+sxnPqOBgQFdcskl+ulPf3pKg9Jz1dXVpa6uLmWz2Qsa53TaNlYr2j0teU2VBf16aduhgr8HAAAAAODiYsmQVYBGooUYw2mGbdslsAJnbsRiMYXDYUWjUYVCoYKN++V3b9WrP+6TWWUpeW1c9339VxWuLC/Y+AAAAACAhWWuvl/OjvuRn39E3grvBY+Xmkjpmzd+s+B1zqfiX3xThJZeXSu5pEzUUChZoQP7BpwuCQAAAACAkkDQ4YDFV9aootErOyP175jQ0SMjTpcEAAAAAChiNCM9bkH36ChViy6vVrDer7GxSZUlyjU8GHO6JAAAAABAEbOU2x62EOMUu+KPaoqQv8KjYKNfKVdKWcvSaH/C6ZIAAAAAACgJBB0OqVsckLKGMu6sYoeSTpcDAAAAAChi9syuKxd62MzoKE1dXV3q7OzUpk2b5uw9Oi6vkWvSJyNjaqK78NvYAgAAAABwMSLoOI0tW7Zoz5492rZt25y9x5qbG2WZlmzb0PQAQQcAAAAA4PxZtlGwo9gRdDikpimgZHhSlplVJiF17444XRIAAAAAoEix68pxxX8HRco0DWWrU7JNKZO1teM/e5wuCQAAAACAokfQ4aCOzRXy+z1yu0ztenTQ6XIAAAAAAEWKpSvHuZ0u4GI2ODqiVCoor8urke6EpuJplQU9TpcFAAAAACgys7umFGKcYseMDgd5PC5l/GlZlq1QY5kObht1uiQAAAAAAIoaQcdpzMf2spLk8biVLU/KjnuUTKTVvSc6p+8HAAAAAChNLF05jqDjNOZje1lJcntcypRPSbapQy+Pa/fjA3P6fgAAAACA0kTQcRxBh4PcHpdsX0bpYEJ1SwI6+PKY0yUBAAAAAFDUaEbqII/XJRmSZVgKVHvlD7gVHZlWuNbvdGkAAAAAgCJSqNkYpTCjg6DDQR63S5JkeTM68FJuNkf3rojC1zc6WRYAAAAAAEWLpSsOcntyQUe2fFpLNlapfklA3a/TkBQAAAAAcG7o0XEcMzoc5PHmPn7bl9GB7WOqrPNrtHfS4aoAAAAAAMXGlmTpwkMK+8JLcRwzOhzknl264kvKH3RrfHhabzw/7HBVAAAAAAAUL2Z0OGj5igZNRicVi0wqnfCqYWmFvOX8SgAAAAAA54ZmpMfxrfo0urq61NXVpWw2O6fvk05mtPO5A5ItLS5v1KGd4zq0c1yT8bTKg545fW8AAAAAQOkg6DiOpSunsWXLFu3Zs0fbtm2b0/dZu7FDl125TDV1QfmqLK3YXKu1b2/QwMH4nL4vAAAAAAClihkdDlqysknpZFoet0vjI1GNvjYlSeo7ENOSS6odrg4AAAAAUCyY0XEcQYeDKqsrVFMfUnQsoZ6xYS1e36lA2KfxoWmnSwMAAAAAoCgRdDisfVm9hvvGVdti6PCLEUlS68qQs0UBAAAAAIoKMzqOo0eHwzqW1mtyIqna1nIFql1aemm1LLsUdi4GAAAAAMwX2zYKdhQ7ZnQ4bMXaVh3ZOyBJck2s0oGXRzURTTlcFQAAAAAAxYkZHQ6rbQyr87IOhSrLJXdaHp8pl9tQJjO3W9sCAAAAAEqHJaNgR7Ej6HCYYRjyeF0KVQdUXm0ombTUvS+msX4akgIAAAAAzs5sj45CHMWOoGMBaGqtVs/BIUVjsfy5oaMTDlYEAAAAAEBxokfHaXR1damrq0vZ7PwsH+lY0aQV61oV6/HJipXJNNwa7k3My3sDAAAAAIpfoRqJ0oy0RG3ZskVbtmxRLBZTOBye8/draKnUvle7ZSWCsoY8kqRBZnQAAAAAAM4S28seR9CxADS2VWtpZ7OyUz4N7XapdUm1UtMZp8sCAAAAAKDoEHQsAE1tNTq4u1d2xq3M4ApFB4cVqvU7XRYAAAAAoEiwdOU4go4FwF/uVfOiWhkydKTXlmxDI72TTpcFAAAAAEDRIehYIIJBv/a+0i3b71NzW7MmJlNOlwQAAAAAKBJ2gXp0lMKMDraXXSDqmqskSZZlqnt/TN17o0ol52fXFwAAAABAcbMl2XYBDqdvpAAIOhaIRSsa1NhaKbc3o6oGn5ZtqNboAMtXAAAAAAA4FyxdWSDKg34NdI/Jkldjg0mNDSY12juppo6g06UBAAAAABY4S4YMFWB72QKM4TRmdCwQja3VWnlJ+0nBxnAfMzoAAAAAAG9tdteVQhzFjhkdC0RVXUh7Xz6q9GRINa2L1bKkThORpNNlAQAAAABQVAg6FojaprAkyXBlNdyT0XBPvzo31zlcFQAAAACgGFi2IaMAszEKsXOL01i6skBU1ga1cn27mtqPL10Z7WfpCgAAAAAA54IZHafR1dWlrq4uZbPzt72r2+3ScN+4Rvrj8gQCWrqmVVm7FDb2AQAAAADMtdntYQsxTrFjRsdpbNmyRXv27NG2bdvm9X1rGsLyB9xKTXq158Vh7X9ldF7fHwAAAABQnGhGehxBxwJS11yp1FRKMtOSpBF2XQEAAAAA4JywdGUBCddUSJJMT0L1HdVKJS2lkll5fS6HKwMAAAAALGSFmo1RCjM6CDoWkPqWKrUtq1fPvgr1HpqQJI0NTqmxvcLhygAAAAAACxm7rhzH0pUFJFQVUPf+QSXisfw5dl4BAAAAAODsMaNjAamuD0mSDFdSwWqXLMvUyEDC4aoAAAAAAAsdu64cR9CxgFQ3hLR8Q7vGBnw68GpGUm7pCgAAAAAAZ5ILOgrRo6MAxTiMoGMBqaoLaf/Oo0ong5IWSZJGBwg6AAAAAAA4WwQdC0hVXUiGYcgw0vL4szJcXg2zdAUAAAAA8BbYdeU4mpEuIG6PS5ddt1Kdl7coOW1qOpHRKEtXAAAAAAA4awQdC8zoQER7X94vyZYtWxPRpNMlAQAAAAAWOLuAR7Ej6FhgahrCMgzJcE/L5TXVezT21i8CAAAAAFzUZpeuFOI4F//4j/+o9evXKxQKKRQK6aqrrtJPfvKTObrLs0PQscAsXdemzs1L5fN7lElbGumflF0KbW8BAAAAACWntbVVn//857V9+3a99NJLuvHGG/Xe975Xu3fvdqwmgo4FxjAM7XnxoNKpSbm9hqobyhUdm3a6LAAAAADAQubQ2pV3v/vd+oVf+AUtX75cK1as0Oc+9zlVVFTo+eefL8RdnRd2XVlgqhtCkiTDTCuZtNXfPaGxwSlV1pQ5XBkAAAAAYMEq0K4ruoAxstmsvv3tbyuRSOiqq6668FrOE0HHaXR1damrq0vZbHbe37u+tVotS+qU3G9oeqYP6XD/pJZ0Vs97LQAAAACAi1MsdnK/SJ/PJ5/Pd9prX3vtNV111VWanp5WRUWFvvvd76qzs3M+yjwtlq6cxpYtW7Rnzx5t27Zt3t+7qjak3oNDyiQnFQwbWrGhRhGWrgAAAAAAzsC2C3dIUltbm8LhcP6499573/S9V65cqZ07d+qFF17Qb/3Wb+nOO+/Unj175unOT8WMjgWmpimspkW1mpwO6Y09lqKvjKjvSNzpsgAAAAAAC9j57JjyZuNIUnd3t0KhUP78m83mkCSv16tly5ZJkjZu3Kht27bpb//2b/WVr3zlgus5HwQdC0xVfViDx0Y1nUxIyv1BGR5IOFsUAAAAAOCiMrtd7PmwLEvJZLLAFZ09go4FxuUyVdtcqb4jcfn9U1qzaYnYXRYAAAAAcEa2cUGNRE8a5xx8+tOf1u2336729nbF43F985vf1BNPPKGf/exnF17LeSLoWIDWbl4mO3tAw7vLtP2pfqXTltMlAQAAAABwiqGhIX30ox9Vf3+/wuGw1q9fr5/97Ge65ZZbHKuJoGMBMk1Dwz2jMo1GWbZbQ70TTpcEAAAAAFjATmwkeqHjnIt/+qd/uvA3LTCCjgVo2fp29R4a0tRLaZm+csWjKWWzllwuNskBAAAAAJyGPXMUYpwixzfnBShUU6HXtx3S5ERKo4NJTcRSGhmYdLosAAAAAAAWPGZ0LEDtyxu14tIO7X7Vku0x1La8QYM9E2poqXC6NAAAAADAAlTo7WWLGTM6FqCWpQ3at/OIXG5bQ4OWtj/dr76jcafLAgAAAAAsZHYBjhJA0LEA+QM+daxs1lQ8KslWY1tAo0NTTpcFAAAAAMCCx9KVBeqSt69S2u7Wiy9KA90JHXpjzOmSAAAAAAALFEtXjmNGxwJVVRdWKhGVZMgwbI0xowMAAAAAgLdE0LFALV/fpqEjA3KZU7Ikvf7qsNMlAQAAAAAWqkL05yiRPh0EHQvUiksXa9HqZhlG7k+a6TKVTGacLgsAAAAAsCAZBTyKG0HHAlVRWa7mxfVqaHTLtqWj+yPqPhhzuiwAAAAAABY0go4FzON3KzUZU6A8q1WXVOnogYjTJQEAAAAAFiKWruQRdCxgyzd0KD0VU3zSpT07x/T6Tvp0AAAAAABOg6Ajj6BjAVu2vl1rL6tXZSit2jq3Duxhi1kAAAAAAM6EoGMBa1/ZrEM792sibsm2sjQjBQAAAACcnm0U7ihyBB0LWHVjWB2rmuQ20xoZtfTGK8Ma7J1wuiwAAAAAwAJj24U7ih1BxwJmGIZqmqrU1CS5Pbb6eib08vP9TpcFAAAAAMCCRdBxGl1dXers7NSmTZucLkW1zZWaGB6UmZ1UVY1P257udbokAAAAAMBCQzPSPIKO09iyZYv27Nmjbdu2OV2Klqxt0/rL61UR9itc6dZrLw05XRIAAAAAAAsWQccCV9dSrQM7DmgqGpOVzSg6Nq3RoUmnywIAAAAALCQ0I80j6FjgWpc1aM1Vy9W6KKBs2lKo0qcXnuxxuiwAAAAAwAJi2IU7ih1BxwLXuKhOu5/br2j3UcWj0/KWufXqS4NOlwUAAAAAwILkdroAnJm/3KfNt65XLDqtx18wlLUsHTkYdbosAAAAAMBCUqhGoszowHyYmkzqjRf2KTs9qeH+hGLj0xrojTtdFgAAAABgoaBHRx4zOorA6k1LJduWsc+lA4diymZs7doxpMaWoNOlAQAAAACwoDCjowgEQmV67em9mujvViDgUu+xOA1JAQAAAADH2QU8itx5zejo7u6WYRhqbW2VJL344ov65je/qc7OTv36r/96QQuE1LaiSeuuWampSUt7B7xq8pfpjV2jTpcFAAAAAFgoirhHR6EzhvOa0fGRj3xEjz/+uCRpYGBAt9xyi1588UX90R/9kf78z//8fIbEGdS31ei1p/fqwI79SsSnFQh65HIZsqwSiNoAAAAAABe1QmcM5xV07Nq1S5s3b5Ykfetb39LatWv17LPP6t/+7d/04IMPns+QOIPmJfVad/UKrblyufx+Wy+/OKCnHj2mowcjTpcGAAAAAFgIinjpSqEzhvMKOtLptHw+nyTp0Ucf1Xve8x5J0qpVq9Tf338+Q+IMyoNl6tk/oN3P75cVH9Gy1dUqC7i1d9eI06UBAAAAAHBBCp0xnFfQsWbNGt1///166qmn9Mgjj+gd73iHJKmvr081NTXnMyTewqZb16vziqUKh90aH53S0hVVGuidcLosAAAAAMBCUMTbyxY6YzivoOOv/uqv9JWvfEXXX3+9fvmXf1kbNmyQJP3gBz/ITzdBYdm2rd3P7Ve0t1ejQ1Pa9fKwdjw/4HRZAAAAAIAFwLALd8y3QmcM57XryvXXX6+RkRHFYjFVVVXlz//6r/+6ysvLz2dIvIUVly7S0T29Gh+Oy6OAapsrNf85GwAAAAAAhVXojOG8go6pqSnZtp0v4OjRo/rud7+r1atX67bbbjufIfEWqhsrtXf7IUlSNLRYhw72qqzcLdu2ZRhEHgAAAABwUSvi7WULnTGc19KV9773vfrGN74hSYpEIrriiiv0pS99Se973/v0j//4j+czJN5C24omta1okq/Mq9qqrC6/plnL19RosI8+HQAAAACA4lXojOG8go4dO3bo2muvlSR95zvfUUNDg44ePapvfOMb+ru/+7vzGRJvoWlJg/oODaqiKiDDtvTi073auW1ARw9FnS4NAAAAAIDzVuiM4byWrkxOTioYDEqSHn74Yb3//e+XaZq68sordfTo0fMZEm/B43WrbUWTjuzp1YTLo03XbJKVtTXQw4wOAAAAALjYGSpMI1EnGiMUOmM4rxkdy5Yt0/e+9z11d3frZz/7mW699VZJ0tDQkEKh0PkMibOw4rLFWrVpqRYtq9G2p/u0/bl+Hdw37nRZAAAAAACct0JnDOcVdHzmM5/R7/3e72nRokXavHmzrrrqKkm55OXSSy89nyFxFqrqw3rjxQPq3XNIS5YFtenqFiWnM06XBQAAAABwmm0U7phnhc4Yzmvpygc/+EFdc8016u/vz+9vK0k33XST7rjjjvMZEmdh6SUdWnv1CsXGp/Xoq3EdOhB3Zl4RAAAAAGBhKeJdVwqdMZxX0CFJjY2NamxsVE9PjySptbVVmzdvPt/hcBaaF9dr1zP75Pa4VFZWq4pwuVxukg4AAAAAQHErZMZwXktXLMvSn//5nyscDqujo0MdHR2qrKzUX/zFX8iyrPMqBG+tdUWTGhfVycpacptpDQwk9NzWHmUyfOYAAAAAcFGzC3jMs0JnDOc1o+OP/uiP9E//9E/6/Oc/r6uvvlqS9PTTT+tP//RPNT09rc997nPnMyzeQlnAr3BdUMGqgKJHTUUSkmXZGupPqLkt6HR5AAAAAACHGHaBdl1xIOgodMZwXkHHP//zP+uBBx7Qe97znvy59evXq6WlRb/9279N0DGHKmtDeuEnOzWdbdC6S9YrlbbV2x0j6AAAAAAAFKVCZwznFXSMjY1p1apVp5xftWqVxsbGzmdInKXOK5YpEZvSroPSaztHJEn9vRMOVwUAAAAAcFQRNyMtdMZwXj06NmzYoC9/+cunnP/yl7+s9evXn8+QOEs1TVXa9cxeRXsGFAy6tGJ1taLj006XBQAAAABwUhH36Ch0xnBeMzq+8IUv6J3vfKceffTR/P62zz33nLq7u/XjH//4fIbEWepY3aLWpQ1KHZ1WXzyr+OtjOrw/4nRZAAAAAACcl0JnDOc1o+Ptb3+79u3bpzvuuEORSESRSETvf//7tXv3bv3Lv/zL+QyJs9S6oklDPaNqbArkz/X3xh2sCAAAAADgtNlmpIU45luhMwbDtu2C3cYrr7yiyy67TNlstlBDOioWiykcDisajSoUCjldTt5/W/5JDRwbV597pTqvXK6WtqD+9sHbnS4LAAAAAPAm5ur75ey4i//8czL9/gsez5qe1uHP/NGC+B58vhnDeS1dgbPWXbNK9ceGdeT5Mr34dK/aFi2cEAYAAAAA4ADbyB2FGKfIndfSlWJzxx13qKqqSh/84AedLqUgglUB7X52v4xMrgnpUH9CBZyYAwAAAAAoNkXcjLTQLoqg45Of/KS+8Y1vOF1GwSzZ0KG1V69UXY2h+gavykMejY1MOV0WAAAAAACOO6elK+9///vP+HwkErmQWubM9ddfryeeeMLpMgqmob1Wu57Zq5jVroh8kqShwYRq6sodrgwAAAAA4IRCNRKdz2akc5UxnNOMjnA4fMajo6NDH/3oR8+pgK1bt+rd7363mpubZRiGvve9751yTVdXlxYtWiS/368rrrhCL7744jm9R6lpWdaolZcvUWWlV5IUrvJpeDDhcFUAAAAAAMcU4dKVucgYpHOc0fH1r3/9nN/grSQSCW3YsEEf+9jHTpvmPPTQQ7rnnnt0//3364orrtB9992n2267TXv37lV9fb0k6ZJLLlEmkznltQ8//LCam5sLXrPTaluqdGRPj6YngjJUoeh4UoN9BB0AAAAAgOIxFxmDtAB2Xbn99tt1++1vvjXq3/zN3+jjH/+47rrrLknS/fffrx/96Ef62te+pj/8wz+UJO3cubMgtSSTSSWTyfzjWCxWkHELzTRNbbiuU9EnemVP5jriDg4QdAAAAADARatAS1doRjrHUqmUtm/frptvvjl/zjRN3XzzzXruuecK/n733nvvSdNk2traCv4eheIP+KT0tDxmVu2LgkpOnTqjBQAAAABwkSjCpStzZUEHHSMjI8pms2poaDjpfENDgwYGBs56nJtvvlkf+tCH9OMf/1itra1vGpJ8+tOfVjQazR/d3d0XVP9c6ljdLDM9pbTl0rEjce1/Y8zpkgAAAAAAcJzjS1fmw6OPPnpW1/l8Pvl8vjmupjAaOmpVW1em/cO5x0MsXQEAAACAi1ehZmMwo2Nu1dbWyuVyaXBw8KTzg4ODamxsdKiqhaF1eZNiI1H5lFB9janKGr/TJQEAAAAA4LgFHXR4vV5t3LhRjz32WP6cZVl67LHHdNVVVzlYmfNaljepaXGd3H6fBkaz2vrYUadLAgAAAAA4xLALdxQ7x5euTExM6MCBA/nHhw8f1s6dO1VdXa329nbdc889uvPOO3X55Zdr8+bNuu+++5RIJPK7sFysQtUVSsSmZE1PSgpqMpHWRDyliqDX6dIAAAAAAHCM40HHSy+9pBtuuCH/+J577pEk3XnnnXrwwQf14Q9/WMPDw/rMZz6jgYEBXXLJJfrpT396SoPSQurq6lJXV5ey2eycvUchrL+uU8PPTmpsMqvFa9o0NDChimC102UBAAAAAOAYx4OO66+/XrZ95rkxd999t+6+++55qkjasmWLtmzZolgspnA4PG/ve6785T6NDfRqRA0aea5PQwOTWrKcoAMAAAAALjo0I81b0D06cGZLL+lQ2+Kq/OPBgQkHqwEAAAAAOIUeHccRdBSx2uZqjRzuVrniWtLhVzyadLokAAAAAAAcRdBRxFqWN2r5uhZNKqhDR6d15FDU6ZIAAAAAAE6xC3CUAIKOItaytEHdrx3KPx4aSDhYDQAAAAAAziPoKGL+gF9X3rxaNYG0Kv2pt2zqCgAAAAAoUYWYzVEiszoc33VlISqW7WUlKZua1tiET7Zh6tXtA06XAwAAAABwQKEaidKMtERt2bJFe/bs0bZt25wu5S2tuWqFyryWJGmgj11XAAAAAAAXN4KOIhesCsiTisprT8vKWpqaSjtdEgAAAABgvrF0JY+go8i1r26RP+BXyvArMZnVUD8NSQEAAADgYjO7dKUQx7m49957tWnTJgWDQdXX1+t973uf9u7dOzc3eZYIOopcY0edMokJGcoqVGFocJDlKwAAAACA+fHkk09qy5Ytev755/XII48onU7r1ltvVSLh3H+EpxlpkWtcUq9wlVtDUUPjiaz6ewk6AAAAAOCiU6hlJ+c4xk9/+tOTHj/44IOqr6/X9u3bdd111xWgoHNH0FHkvD6P/C4r/7i/N+5gNQAAAAAARzgUdPxX0WhUklRdXV2AYs4PQUcJWLq6TgeficuwLUVG6dEBAAAAALgwsVjspMc+n08+n++Mr7EsS5/61Kd09dVXa+3atXNZ3hnRo+M0urq61NnZqU2bNjldyllpbA5qUgEljKBe3d7vdDkAAAAAgHlW6GakbW1tCofD+ePee+99yxq2bNmiXbt26T/+4z/m+G7PjBkdp7FlyxZt2bJFsVhM4XDY6XLeUudlrdK3c9ODeo5FHa4GAAAAAFDsuru7FQqF8o/fajbH3XffrR/+8IfaunWrWltb57q8MyLoKAEti2oVsONKy6toxOt0OQAAAACA+VbgHh2hUOikoONNL7dtfeITn9B3v/tdPfHEE1q8eHEBirgwBB0loGlpgwyvT6m0V0OjKVmWLdM0nC4LAAAAADBfHGpGumXLFn3zm9/U97//fQWDQQ0MDEiSwuGwysrKClDQuaNHRwloXFQndyquMntSNYGshoZoSAoAAAAAmHv/+I//qGg0quuvv15NTU3546GHHnKsJmZ0lIBAqFx1TWHtH3BrKi7198TV2FjhdFkAAAAAgHlyYiPRCx3nXNh2IaaRFBYzOkqEz8jkfz56eNzBSgAAAAAA884u4FHkCDpKxMp19Qr6s/K6LO3fPeh0OQAAAAAAOIKg4zS6urrU2dmpTZs2OV3KWasMeRSfdimVNXVo74jT5QAAAAAA5tHs0pVCHMWOoOM0tmzZoj179mjbtm1Ol3LWVm1oUk0go/pQVpPxaafLAQAAAADMJ5au5NGMtEQsXVWv0UTu17l795jD1QAAAAAA4AyCjhLRsrReXiOjpO1SKm05XQ4AAAAAYD4VajZGCczoYOlKiWjoqJPbSsiybfWNpJROZ50uCQAAAACAeUfQUSLKAn75vKZkGJIM9fdOOF0SAAAAAGCeGAU8ih1BRwlZsqhCle4pBZRQb3fU6XIAAAAAAPOFZqR5BB0lxOMxFMmUKaGA3nit3+lyAAAAAACYdwQdJWTpynpJks9l6dihEYerAQAAAADMF8Mu3FHs2HWlhDQ0lEm2rWTW1IE32GIWAAAAAC4a7LqSx4yOErJsdcNMM1JpaDDhcDUAAAAAAMw/go7T6OrqUmdnpzZt2uR0Kedk5bpmldsTClhxpaemnS4HAAAAADCfaEQqiaDjtLZs2aI9e/Zo27ZtTpdyTlqW1iklnxJmUId6kk6XAwAAAADAvKNHRwkJ14VUUW4qMiVNJiXbtmUYpbALMgAAAADgTArVSLQUmpEyo6OEmKapkGtKAXtC5ca0RkcmnS4JAAAAADAfCrFspUSWrxB0lJiK6pASRoUmrDIdOzzudDkAAAAAAMwrgo4SEwwc/5Ue3DPoYCUAAAAAgPkyu3SlEEexI+goMa2tQQXsuMrtCR07NOx0OQAAAACA+cDSlTyakZaYhrYqJYzckpVjh1i6AgAAAAC4uBB0lJim1nD+58G+uIOVAAAAAADmC7uuHEfQUWI6ltWq0p+Wx23IsiynywEAAAAAzIdCLTsh6MBCs3R1gyLTHknSwQNRh6sBAAAAAGB+EXSUmI7ldfmfJxIZBysBAAAAAMwbZnTksevKaXR1damzs1ObNm1yupRz5vd75J2Jr1IWv14AAAAAwMWFb8KnsWXLFu3Zs0fbtm1zupTzUlWelsee0vR0UsmplNPlAAAAAADm2Gwz0kIcxY6gowRlTY+mDb+mDb+Ge8acLgcAAAAAMNfsAh5FjqCjBIVD3pmfDHUfHHa0FgAAAAAA5hNBRwkKV/ryPx/dP+RgJQAAAACA+WDYdsGOYkfQUYJa20Iqc2flddkaGphwuhwAAAAAwFxj6Uoe28uWoPIKn6YyLklSzyGWrgAAAAAALh4EHSWooTmc/3moP+5gJQAAAACA+VCoHVPYdQULUkPr8aAjY7scrAQAAAAAgPnFjI4S1NZepZASsixLPYeiTpcDAAAAAJhrheqvUQIzOgg6SlBZuUcxBSRTGo+MOl0OAAAAAGCOsXTlOJaulKBQ+Pj2sv5whaYnkw5WAwAAAADA/CHoKEEnBh0jQxOKDLF8BQAAAABKGtvL5hF0lKBQ6HjQkZVLYwMR54oBAAAAAMy52aUrhTiKHUFHCQqGvGpp9CvosxQIlykyHHO6JAAAAAAA5gXNSEuQ3+/WwHBS2ayp7HRWoz1jTpcEAAAAAJhL7LqSR9BRggzDUHmZW/GJtLIyNTYYcbokAAAAAMAcK4VlJ4XA0pXT6OrqUmdnpzZt2uR0KectGPTmfnB7NZWYdrYYAAAAAADmCUHHaWzZskV79uzRtm3bnC7lvM3uvJLKSP0HBh2uBgAAAAAwp2y7cEeRI+goUZdsbFSFNaFye1LD/RGnywEAAAAAYF7Qo6NEDQ9PacKsyD1wTTpbDAAAAABgThVqa9hS6PNB0FGi8j06JO17+Zhs25ZhGA5WBAAAAACYM+y6ksfSlRIVCBwPOqaTliZjUw5WAwAAAADA/GBGR4mqOGFGR8OyZo0ORBQIlztYEQAAAABgrhhW7ijEOMWOGR0lqqLieNDRfXBYkcGIc8UAAAAAAOaWXcCjyBF0lKjKKr/8PlOybVkyNT4QdbokAAAAAADmHEtXSpTX59J00pIMQw1Lm5WIsfMKAAAAAJQqdl05jqCjRAXKPfmfew8Na/DosIPVAAAAAADmlG3njkKMU+RYulKiAif06MjK1PggS1cAAAAAAKWPGR0lqjxwfEZHbXu9TJfhYDUAAAAAgLnE0pXjCDpKVCBwfEZH/7ExHdiRcLAaAAAAAADmB0tXSlSgwqPFSyrlM9KyZbB0BQAAAABKGdvL5jGjo0SVl3t0+FBEkkfBhmo1L83Ktm0ZBktYAAAAAKDUsHTlOIKOElV+wq4rY0Nx7ew7pMnYpALhgINVAQAAAAAwt1i6UqICJzQjzcolSSxfAQAAAIBSNbu9bCGOIseMjhJ14q4r/mBAa9esUnQkrtYVx69JpbI6dHBcmYylpuagamrKHKgUAAAAAHChWLpyHEFHifJ6XXK7TWUylibiSe16+g2N9o3ln7dtW//w9y/q6NGovF6XTNPQH33mOlVUeM8wKgAAAAAACxtLV0qUYRi6/IomLVsckEdJSdLYQCT//LPPdOvRRw/rsUcO6RsPvqodOwb0zX/d5VC1AAAAAIALwq4reQQdJezgoYj2HpnQtC+k1VcuV3o6lX/uW/+xR9mMLa/XVFWVT9NTGT3y8EFls5aDFQMAAAAAcGEIOk6jq6tLnZ2d2rRpk9OlXJDZPh2ptK3Xn9+v7r39kqShoYS2PnlU2aylhsag3nZNu/x+t/p643r2mW4nSwYAAAAAnIfZHh2FOIodQcdpbNmyRXv27NG2bducLuWClJfnWrBkZ37N44MRSdKTjx/R6OikJuIp3XjTYr3r3StkmoYSEyk9/NNDTpULAAAAADhfll24o8gRdJSw8vJcY1HbMLXyiuUqC+Z2Vfn5o0cUj6WUydi67vpFuuW2JfJ4TKXTll59ZVCpVNbJsgEAAAAAOG8EHSVsdkaHJO154aBe27pHkvTU1qNqbQupurZMl17WqEDAqzXr6uXxupRM5np1AAAAAACKCM1I8wg6SlhZuSf/syVT44NRDQ9NaNHiSjU1Vaimpkxud+6PwM23LFFl2KdkMquf/oSgAwAAAACKiaEC9ehw+kYKgKCjhC1dWqWrr23Toga3lm1eodVXLteOF45p65PH9PxzvWporMhf+/brO1TfENBkIqWD+8c00B93sHIAAAAAAM6P+60vQbGKRpN65qncLirB7kMqt6e16B2DWr+hXmVlHq1bV5+/1u029bar23T0aFS2LX3vu3v1m799uVOlAwAAAADOhW3njkKMU+SY0VHCysqO51jWzK/69T1DevWVIb3wfK+amitOuv6d716u8oBHU1NpPfoIu68AAAAAQLFwanvZrVu36t3vfream5tlGIa+973vzcn9nQuCjhIWCHjzP9ctalC4LqR4JKFrrmvX5iua1d4eOun65StqtHx5tWLRpAYHEtqze3i+SwYAAAAAFJFEIqENGzaoq6vL6VLyWLpSwspO2HWl/+ioKq2YxvfHtO/YkCSpuSV0ymuuubZdTz/Vrdo6t3764wPqXFM3b/UCAAAAAM5ToXZMOccxbr/9dt1+++0FeOPCYUZHCSsvO77rStPyFq27drUi8Yyk3LKWcNh3ymvecfsy+XwuuVymHvnZIVlW8a/PAgAAAACcm1gsdtKRTCadLumsEXSUsBO3lz22f1CvPfW6sslpXXJZo66/YZEM49SNg+obArr6mjZls5Yi0Wn9/FF6dQAAAADAQmfYdsEOSWpra1M4HM4f9957r8N3ePZYulLCqqr8uvqaNsmQ0ntj6ghX6IX9bo3uGJD3ipY3fd173rdSn//c0wqFvPrnr7+iG29eItM8NRQZG0voyOGobNtWMORTe3tYfr/nNCMCAAAAAOaUNXMUYhxJ3d3dCoWOtzvw+U5dEbBQEXSUMJfL1DNP57aX3dzq0/5dhyVfrudGbV3Zm77uppsX6/98+3X19cbVPzChz/zPn+tP//IGud2mjh6N6PHHDunll4cUi07LMAxlMpayWVter6m6+nKtWVOvG25cpPaOyvm4TQAAAABAgYVCoZOCjmJC0FHCTmxGWtlUI1/ZYvUPZNSwqEHLlle/6etcLlO//huX6b7/73l5EoZ27hzUu27/purqymXbUjZjybJsWbatdNqSy2XKZRpKpzOaPJzWkUNR/eg/96upuULr1tXrbde0adXqWpkmK6UAAAAAYC6cuOzkQscpdgQdJaz8hB4dLr9P+3f3a8C7UgOvjerGW5ef8bUbNzXrlz+yVt/8t13yuLOamEgpFkvJ73MpY9nyeUyFg37V1ZcrGPRpYiKp7mMxJRJpuUxTMqSB/rj6++L66U8PKljhUWt7pRYtrtSqldVavaZO1dXlc/0RAAAAAMDFwaFdVyYmJnTgwIH848OHD2vnzp2qrq5We3t7AQo6dwQdJazshF1XAtUhNa/u0BsHc4+rqvzKZrOKDMfk8XkUqqo45fW/8K4Vamis0Pe/u1eTk2mlUhllLVurV9fqHbcv07LlNae8Zv++UT35+BG98sqghoYSMk1Dpi0lJjPa98ao9r0xqod/ekCmYSgY8qmlJahFiyu1enWN1m1oUFmZd84+DwAAAABAYb300ku64YYb8o/vueceSdKdd96pBx980JGaCDpKWPkJS1c8gTJNJY93pon2j+ifPv1NZdIZJSeTqm2t0XUfvEodq1tPGmPj5c26bGOTBgcnlM3Yam4Jnna3llnLV9Ro+YpcAHLo0Jief6ZHu/cMq+dYTMlUVrIl286FhLFoUtHotF5/fUQ/+dF+GaappqYKLV5SqTWddbr08kZVVTHrAwAAAADekj3zZasQ45yD66+/XvYCW+5C0FHCTtxeNp2R/KGAGoMZTcantOPHL2jd8oCmYlPy+t0aH4xq30sH1XnVCv3Cx29WuOZ40xnDMNTYGDzn91+ypFpLluR6gWQyWe3bO6rXXx/RoQNj6u2d0PBQQulM7l9GwzSUzVrq7Y2ptzemp7cek+kyVFsX0OLFlVrdWauNlzepvv7UmScAAAAAcLEz7NxRiHGKHUFHCSsv8+htV7fKMAxVV/tldbRoYE+PpApVVlmaTiSVnErJ7XMrncrINAy98sQevfrEHjUtbVBDR50qG8KqqAqosi6kmuZq1bXWyOVynXMtbrdLnWvq1bmmPn8unc5qz55hvfH6iA7uG9ORYxFFxpMyTUO2bFmWrcGBCQ0NJPTi8736+tdeUU21X4sWV2rNmjpdclmTWluLswswAAAAAGBuEHSUsPKAR88+0yNJ8npdamuvlpR7vGRVk25571q97T2blJxO6Uf/+xHteup1WbatbDqroWMjGuoekWmaMpQLHiTJMA0FQuUKVlcoVFOhcG1Q4bqwqpurVNtSpdrmsw9CPB6XNmxo1IYNjflz/X0x7dgxoL1vjOjggXGNj00pa0m2bBmSxsamNTY2oJe3D+hf//lVVVb5tWhxlVZ11ujyTS0EHwAAAAAuTg4tXVmICDpK2Im7rkxOphWPpfKP/9sfvltLl9dKknx+r97/O+/U1e/dpKf/74s69NoxTU1MKZPOSrYty7JkGJIlybSk+PiEJsYT6j3YL9Mwcw03DMm2bZkuU4FguQKV5QrWVKiyLqyqhrCqGytV11ar2pbqM24z29Qc0jubQ3rnu1ZIkkZHE9qxfUB7dg/r0KGIhgYTsrL2TPBiKBJJ6pWX+/Xyjn499G+7Fa7ya8nSSnWurtPmq1pY6gIAAAAAFxmCjhLmdpvyel1KpbKanEyrrNytFStrNJlIqabu1ACgoaNeH/gf71I2m9XA0WENHxtWfGxS0dGYYqMxTYwmFBufUCI6qWw6K8M28k1nDDvXy8O2bMWjCcWjCQ0cGZIMI58IGjJkuA0FgmUKVgcVqg4q3BBSZX1Y9W01qmuvVWVN6KRmpzU1Ad1y61LdcutSSVI8ntTL2/u1e9ewDhwYU39fXJmsZCiXt4yPTWn72LR2vDSgf/3X11RbW66ly6u1YX29Lt/crFDIP+efOwAAAADMN8PKHYUYp9gRdJS48oAnH3S88fqo9u0dlWFIoZDvTV/jcrnUsqRRLUsaT/u8bdsaHRjXaM+oRvrGFBmOKzYSU2x0QhORhCYjk0qnM5KZCz4MYyYQMWxZGWkiMqmJ8Un1HxzILUkxTNm2LUOGPD63KioDCtUEFaoLqrK+UrXN1aprq1F9R42CQZ+uu36Rrrt+kSQpkUhpx/Z+vfbakA7sG1NfX1y2bcmWITsrDQ8lNDw8qeef7db//srLammt0PLlNbr0sgZdtrFZHs+59xsBAAAAgAWHpSt5BB0lLlDuUWR8WpOJdH6mRCjkk2m++Raxb8UwDNU2Vau2qVorT/O8bduKjMY1fHRY44NRjQ1EFBmKKDYaV3w8oURkUpl0JjeWzJlZIbk+IOlUWuNDEY0NRk6Y2THzL5phKhAuU7CyQqHaoKrqw6ppqVZTW4023rlG5RXliseT2vFSv3buHNTBA2MaHk4om5/xYamnO6ae7pie+PlhebwudSyqVGdnnTZf2aSlS2vOuHUuAAAAAGDhI+gocbNbzE5NpZXJ5uYghcNvPpujEAzDUFVtSFW1p28MalmWoiNRDR4d0UjvmMYHxhUZjis+GldsfEJT8WnZln28Aaoky5ZM28rNBolMauDwUO75/NIYQ/6AT6GagEI1IS1qCGnTrTVyhTrUO5zRwUNxHTgwrsj4lGwjN2oqmdX+vaPav3dU3//eGwqF/Vq+rErrNzToqqvbFA6zzAUAAABAkbCV/2/EFzxOkSPoKHFr19aprMytTMbSoYPjkqTQHAcdb8U0TVXVV6mqvkradOrz6XRGg0eGNNQ9qrH+MY0NRBQbjis2llsak0lmZBuSYRuy7NzSGNm2phPTmk5Ma6h7TLKt3G4xMzlIWblPa2oqlG33azDh0mjM1viEJLdbLpcp2Ybi0aS2b+/X9pf69c9ff1XNLRVatbpGV17ZrjXras/YRBUAAAAAnGTYtowCLDspxBhOI+gocUNDCb326pAk6YorW+R2m1q0pNLZot6Cx+NW6/JmtS5vPuU527Y1NhTR0JEhDfeMaaw/oshwVLGRuOJjE0pOJmdmehxfEmPI1tRkUtNTSRm25DUNNVq2GgxpKCKNJt2aSLqUyJhyedzylfnkC/h0LJNWT3dMjz5yROXlbi1fUa0NGxr0tmvaVFVVPu+fCwAAAADgrRF0lLhAhTf/8wvP90pyfkbHhTAMQzUNVappqNLqK059Ph5NaPDwkIaODWukd0zRoZgiwzHFxuNKTiRlG0Yu6ZQhwyU1hGw1GBnZSiudMdQ3Jg1F4xrsk5IpyeU15PG45fa5deSNAW199IAe+IpPS5fXqnNtva64skWrVzPbAwAAAIDDaEaaR9BR4gIzPTpOdKYdV4pdMBxQ8JLFWnbJ4lOem4gl1H9wUCPdoxruHdP44Liiw3HFxhJKTiblddtaXC8tapBkGxqP2xqI2RqJpxWJpDVhT8lWri3I4de69fgPTH3V51Ew7NXy5dW66up23fKeTjU0Vc37fQMAAAAAcgg6StyJMzpmlXLQcSYVoYCWX7pEyy9dcspzsfEJ9R0c0Ej3sEZ6xjU2FFH1UEz14wmlk2mls9JARBqISuMJaSotWSlLmVRSiXhSgz1xPfnYYX3pfz2lmpCpxa3lunRDrRavrFdje50altSpob1OLhfb2QIAAACYA7Ykq0DjFDmCjhIXCJw6o2Oud10pRqGqCoUuXyZdvuyk85ZlaXxwXL0HBjTcPaqRvjFFBmM6eGBM+7unNRKT4tOSbCmVtuVyScPjloYjE3ph14QC3iOqD0st1VJ9yFSoNqSq+rCqmipV11qjpsX1alzaoPJAmTM3DgAAAKAk0Iz0OIKOEtfaFtIVV7ZoaHBCIyNTqqouU31DwOmyioZpmqppqlFNU80pz6VSaQ0cGtSBXX16euth7XxlVAPDU5qcTMvK5rbFTaSkI8O5w+exVBOIqLk6oobQUbldM9vmmqYCIb/CtSFVN1WprqVaDYvr1bK8SeGa02/RCwAAAAA4PYKOEpfN2vkmpJIUj6fk9/NrLwSv16P2Va1qX9WqGz+4WZKUyWS1Y3uftv78kF5+sVf9/TGlptJKp9LKpDLqjVjqjUgeU6oOSI2VUmu1pUR0ShPRSfUeGJRh2JJtyJYtf8A3E4BUqqa5Wg0ddWpZ3qDa5trctroAAAAAIOWWnBSkGemFD+G0kv/G293drV/91V/V0NCQ3G63/uRP/kQf+tCHnC5r3gSDp/boqKz0O1DJxcHtdmnzFW3afEWbJOnggTE9+2y3dr48oL6euDKZjKYnk0om0pqeSupgJKV9I1mF3BlVV0gdtba8bskwJcnQdCKl5OSIho4OS4Yhe2bHGLffpcqasMINYdW31KiuvVatKxpV317HDjAAAADAxYhdV/JKPuhwu9267777dMkll2hgYEAbN27UL/zCLygQuDiWbwRP03g0TNAxb5Yuq9bSZdX61Y9u0MhIQk89eUwvb+/XocMRpVIZGTIk5WZvTE8mtXs6rVCZrfqA1BxIK52YVHIqJSm3Vs6euTo9ldFw75hGesd0cMehfAji9rgVqg2qqj6kmuYqNS5qVOPSOjUvaaQRKgAAAICLQskHHU1NTWpqapIkNTY2qra2VmNjYxdN0BE6YUZHe0dYHR1hVVfT+NIJtbUB3fGB1brjA6s1NZXWU1uP6qUX+rR336impjLyB3wyAn5Ztq3+lDSUkdraFmvlykotaXbJjk9quGdU44MRjQ9FNRmblm1oZmpZLnXNpDMa7R/XWH9EB185OvPOhlxuU6HqoCobQqprqVF9e62alzWqcUmDPJ6S/2sAAAAAKH2Wco0CCzFOkXP8G87WrVv1xS9+Udu3b1d/f7+++93v6n3ve99J13R1demLX/yiBgYGtGHDBv393/+9Nm/efM7vtX37dmWzWbW1tRWo+oWvuqZc117Xrt27h3XsaFTHjkZVWcWuK04rK/Po1tuW6dbblimTyer5Z3v0wou92vPasCYmUjJkKGvZOnospiNHozJNQ83NFbps42K9+yOL1dIaUiI+qd79feo7MKiRnrHcjjAjMU1FJ2VZubkfMnLT17KZrCJDEY0PRXT4tWMyZmeAuE0FqoKqaQirqrlKTYvq1LikQc3LGuXxnLpjDwAAAICFiV1XjnM86EgkEtqwYYM+9rGP6f3vf/8pzz/00EO65557dP/99+uKK67Qfffdp9tuu0179+5VfX29JOmSSy5RJpM55bUPP/ywmpubJUljY2P66Ec/qq9+9atze0MLTDDo1VNbj510rqam3KFqcDput0vXXNeha67rkGVZ2rG9X88+061drw0pEknKMCTbttXbG1dvT1w//ME+NbWEdNmljbrhxsW6/rKTt8SdnpxWz74+9R8a1lD3sMb6IhofjmpibEKSfdKSu2zWVnQ4quhQVNp1LBcAG4YM01CwqkJVjWHVtlSrrq1WzUsb1bKsUR4vAQgAAACAhcuw7YUT1xiGccqMjiuuuEKbNm3Sl7/8ZUmSZVlqa2vTJz7xCf3hH/7hWY2bTCZ1yy236OMf/7h+9Vd/9YzXJZPJ/ONYLKa2tjZFo1GFQsW5zWdfb1yrlnXlH3u9Lg1Hfo8dO4qAZVnavWtIW588pl2vDWlsbEqyjdx0tNl+HYbU3BzUpZc16sabFqu1Lfym4yWnk+o9MKi+A/0aPjai0b5xjQ9FFR+fyO1zq5kFMIaRe2zMnjEl25ZhSIZpKlgdUFVDpWqaq1TfUafGxfVqWdYkn//UxrcAAAAAcmKxmMLhcMG/X86Oe9Oa35fbdeGz9zPZpB7b/cWi/h7s+IyOM0mlUtq+fbs+/elP58+Zpqmbb75Zzz333FmNYdu2fu3Xfk033njjGUMOSbr33nv1Z3/2ZxdU80JTXXO8H4fX69LV17YRchQJ0zS1bn2j1q1vlCTtem1QzzzdrVdfGdTI8NTMNrRSb29cfb1x/eg/96ulNaTLNuZCj+aWk/9S8vl9WrK2XUvWtp90PplMq+9Av/oPDWro6PDxAGRsQlY2m7vIMGZ2q7IVGYkrNhLXkd3duWaqhiHDsFVRVaHK+pBqmmtU11qjpiV1al7erPIAzW8BAAAAzJ8FHXSMjIwom82qoaHhpPMNDQ164403zmqMZ555Rg899JDWr1+v733ve5Kkf/mXf9G6detOufbTn/607rnnnvzj2Rkdxczvd+ua69r09NZupVJZTcRTTpeE87R2XYPWrsv9u/D668N66smjeuXlQQ2PTOZmYchWT09MvT0x/ecP9qu1LajLN7XohhsXqbGx4k3H9fk8WrymXYvXnByApFNp9R0cUN+Bfg0eG9Vo35jGByKKjydkWbkORfbMXt22bSs2Gld0NK5jr/fO7A6Tq6miMqDKupCqGitV316rhkV1al3ZomD44mgIDAAAAMwLtpfNW9BBRyFcc801+S9lb8Xn88nnK71GnQP98fzPGy5tOMOVKBarV9dp9eo6SdLre4b05JNHtfPlQY2PTuWv6TkWU8+xmL7/f99QR0dYl29u0fU3tquu7s1DjxN5vB51rG5Tx+qTw750OqP+gwPqPzSogSNDGuuPaGwgovhoXFbWnmmAKmkm7piIJJSIJNS9rz/Xb0S5ZWplAZ8q68KqbMj1AWlYVKempfWqa65l1hEAAABwrgg68hZ00FFbWyuXy6XBwcGTzg8ODqqxsdGhqorPBz60Rg/87x2KjE+ztWwJWt1Zr9Wduca8e/YM68knjmjnywOKjE9Ldm65yeEjER05Mq7/8509WrQ4rM1XtOrGmxarsvLcl5V4PG61r2pV+6rWk85nMhkNHhlWz/5cD5CR3lGND+aWwKTTGZmajT4k2bamJqY1NTGt/iODx5+wJbfXo1BtUFV1IVU35WaBNC6pV/OSRnl99AEBAAAAcGYLOujwer3auHGjHnvssXyDUsuy9Nhjj+nuu+92trgi8u73rNAvfWSt2ttDmp4+dXcalI7Ozjp1dtbNNDId1tYnjuqVVwcVjUzLtg0ZtnToQESHD0b07Yd2a+myal15ZYtuuGmxAoELCxHcbrdaljWpZVnTSecty9Lg0WH1Hx7S4JEhjfaNaWwwquhwTOnptOz8QhdbtgxlUmmN949prG9Mh17VCemIoUBlucK1oVwz1JYqNbTVqnFpg2qbqpkFAgAAgIubpZn/v7kA4xQ5x4OOiYkJHThwIP/48OHD2rlzp6qrq9Xe3q577rlHd955py6//HJt3rxZ9913nxKJhO666y4Hqy4u6zccX67i8bgcrATzJdfItEHr1jfItm3t3Nmvpx4/qldfHVYsnpQMQ9msrX17x7R/75j+/Zu7tHxFja64skXX39ChsrLCzZwwTVNNixvUtPjUZVPjQ+PqPTCgwaMjGunJhRvR0bgSscl8j4/ZVTCGbE2MJzQxnlDv/n7N7LsrGbllNsGZZqhV9WHVtFWraVG9Gpc2qiLIdsoAAAAofYZtyyjAspNCjOE0x7eXfeKJJ3TDDTeccv7OO+/Ugw8+KEn68pe/rC9+8YsaGBjQJZdcor/7u7/TFVdcMWc1dXV1qaurS9lsVvv27SvqbXWAE2WzlrZv69NTW49p964hTUykNTsRYnbihNfr0sqVNXrbNe265rp2eb3zH44lJqY0cHBQ/YeHNNo7qtH+iCJDEcXHEsqks7kIxLBl2LndYCRbxyd0GLJtW4aRmyNSFihTqLpC4fqQqhsrVddeq4b2OjUurpPPX3o9eQAAALAwzfX2sjevuKdg28s+uu9vivp7sONBx0I2V38QgYUgk7H0wnPdeurpbr2xe0STUylJpmajAxmSz+fW6s5ave3qVr3t6nbHZwRZlqXhnlENHBrU4NFhjQ5EFR0cV2QkrsnYVK5v0kwAIkMz0/fsfANU27ZnW4HIMA2VB8sVqqlQZX1YNU2VqmmuVuPiBjUsqpPH4/iENwAAAJSQOQ86lv+PwgUd+/+/ov4eTNBxBgQduFgkk2k9+0yPnnumR6+/PqxUKivbsmWYUm6GhFRe7lFnZ63ednWbrnxbq9zuhbUManpyWr0HBjTcParhnhGN9UcVHYkqNjKh9HRKln1Cx1PNrnwxZtt/zCyDycU8pmmoIlyuYE1Q1fVhVTVXqbY1txymri3XJBkAAAA4F3MedCz9VOGCjoP3FfX3YIKOMyDowMVoejqjrU8c0XPP9erAvlGlUrPdiOx8TlBe7lHn2pnQ46o2uVymkyW/pdj4hPoODmike1gjPeMaG4goOhJXfHxCmVRaso3jMz9md8eduVdDhuwTTrrdpgJVFaqsC6qyLqyalirVtdeqaUmDqurCMs2F/VkAAADAGQQd84eg4wwIOnCxSySSevLxo3rhhV7t3zemTMae2R3l+FKQ8nKP1qyp01XXtOnKK1sW3EyPM7FtW2OD4+o/NKThYyMaHRjPbYk7E4Jk09nZhTy5mR4z9zybguRCkOPNrT0+tyqqg6qqDamqsVJ1bTWq76hRy7ImlQXY2hkAAOBiNudBx5JPFi7oOPS3Rf09mKDjDAg6gONisWk9+cRRvfB8jw4fiiiTtjTbwjQXfEhlZW6tXlWrzVe16Opr2uX1Fm+fC8uyNNI3roHDgxo+NqrR/jGND0YVG4krEUnIsnP7d+X/Ap3p/2HZkmnM7Bdj54IQU4bKQn6Fa0O5fiDNlapvr1Pz0kbVtdYwCwQAAOAiQNAxfwg6ToNdV4Azi8WSevLnR/TCi706fHBcmawt27ZkGEZ++xafz63ly6t1+eZmXXtduyoqSmeHk0wmo4EjQxo8Mqzh7hGN9Uc0PhzTxOiEJmKT+X4fOqkvyPH+ILak6YkplVWUy+11KVRdoaqGsKoaq1TfXqPGJQ1qXtLArjAAAAAlZO6Djt+R2yxA0GEl9eihvyvq78EEHWfAjA7grcXj03ri50f00rYBHTwwpnTGkuyZGQ3Kfd93u0x1LA7rso3Nuu7t7aqvr3C46rmTa4rar8GjIxrtHZsJQSK5pqip9Ez4YSo1nZK3zKvcEpiZ03m2DNNUsKpC4bqQapqrVNdao6YljWpd0cgyGAAAgCI050HH4k8ULug4/PdF/T2YoOMMCDqAc5NIpLT1ySPa9mKf9u8dUzJlyZBkmJKVtWW6crMcmluCWrO2QVde1aLONXW5mSAlzrIsjQ9F1XdwQMPHRmaaoo4rOhLXRDSh2dkeso/P/JBO2Bb3hM+oPFyeWwLTWKm6jlq1LG1Sy4omlQf883tTAAAAOGsEHfOHoOMMCDqA8zc9ndbzz/Xohef69MbeESUmUid8adfMrA8pHPZp+fJqXbqxUVe9ra2klricrempafUeGNTAoUGN9I5ppHdMkaGIYqMJ2VZW9mzH05nPbvZvbSPfHFWSYShYVa5wbVi1LVVqXFSv5uWNal7WJJ/P69i9AQAAIGfOg46OuwsXdBz9clF/DyboOAOCDqAwLMvW9u29euHZHu3eParRsalcG8/ZSQwz17ldplpbg1rVWavLN7Vo7bq6i7pRZzabVd/hXAAycHhYoz2jigzHFRuJKZPOSIZyIcjM1r+GfeKEkNwPpmkqWF2h6sZK1bXXqqGjTu2rmlXffnF/tgAAAPNtzoOO9t8uXNBx7B+K+nswQccZEHQAc+PA/lE9/2yPXts1pO5jMWUz1szsBM3s1ZoLQcrK3Fq0pEpr1tRp4+VNWryk6qJY5vJWLMvS4NFh9R0Y0MCRYY30jGi0f1zx0Qlls1Z+qYs98znOboabS0IkyZDH51FlfVC1LTWqb61Vy4pGta1qVkW4dPunAAAAOImgY/4QdJwBQQcw9+LxpJ59uluv7BzQ/v1jisdSuS/ohmTIkG3ZsmXINKRA0KPFi6u0fEWNLrm0QStW1BB8nCA3A2RIffv7NXRkSEM9oxofiCg+lpBlnfhX/fElMCcvhZEC4XJVN1Wprq1azUsa1bayRY2LG/icAQAALtCcBx1tv1W4oKP7H4v6ezBBxxkQdADzy7ZtHdg/qhdf6NPru0d09FhUyWQm19NjdknGzAwFS1Ig4FFrW1jLllRqxeoard/QoEDg4uvx8VZSyZR69/epZ2+/Bo4Oa6R3XJHBiKbi08q1/7CP7/piSzINybYkGTIMQ26vW1X1YdW2VKtxcYNaljeovbNN/jI+awAAgLNF0DF/CDpOo6urS11dXcpms9q3b19R/4KBYpZOZ/Xyy/16deeg9u8bU3d3TFbWyi9xMeyZ5Ria+X5uGqqtKVdbe0iLl1RpxcpqrVpdK5/P4+h9LFSR0ZiOvdGj/gODGjw2orG+cUWHo8pksvnGKYYkayZc0sykDsOQDNNUqLpCNc3Vqu+oU8uyBnWsaVNlbdihuwEAAFjY5jzoaPnNwgUdvfcX9fdggo4zYEYHsLBMT6f16s5BvfrqoA4djKinJ6ZkMpN70la+L4Ux843csm25Xabq6svV2FyhRe2VWrSkSitXVau6utzBO1m4stmseg70q29vv3oPD2qke1Tj/RFNxidPTjtOkjtnGLbKQ+WqbqxSfUetmpY2atHqFtW3XxxbCAMAAJzJnAcdzb9RuKCj7ytF/T3Y7XQBAHC2/H6PNl/Zqs1XtkqSsllLe/eOavdruRkfPT0xjY9OzfTbNGTIVjZraWAwocGBCb3y8mCuUaekQIVXDQ3lamgIqrk1qEUdlVq6vOqiD0BcLpc6VraqY2XrSefHhqPqeb1XPfv7NXhkWKP9Y4qPxmd6f8zMqrGlRDShydiUevb2ytarMiR5/B7VNFeqrqVWTcvq1b6yRS0rmuVyueb/BgEAAFDymNFxBszoAIpPNDqt3buGtHffmHqORNTXN6FIdHrmC7mR6/cxs/mIoZlGnKZmen/YKivzqLq2TPV1FapvKFdzU4XaFoXV0RGm/8d/MT01re43+tSzt1cDh4c11D2qyFBUmXRGhkzJsHK7vdi5WTa2YcuwDdlGbivhyoawaltr1LS4Xq0rmtXR2SoffT8AAECJmvMZHU2/IbfpveDxMlZKj/YX94wOgo4zIOgASkMsNq03Xh/WwYMR9XbH1Neb0MhIQql0Nr8V62zjD3smApntfZpv1GkYqgh4VFlVpuoav+rqArkgpCWk1tagGhsrZJqmo/e5EGSzWfUdHFD3673qPzyU6/3RP6bpyZRMI7eLjmGe8Onax5uBmIapUG2Fappr1NhRp+blDVq0tl2hqqCTtwQAAFAQcx50NP564YKOgf9d1N+DWboCoOSFQn5tvqJNm69oy5+zbVu9PTEdPDimY8di6uuNa3hoUmOjU5qcSuU6fdi5jh+zUz8SibQmEin19saOf0Gf2aLV5XIpHPYpVOlTdbVftTXlqmsIqKE+oObWCjU1heRylX4Q4nK51LaiRW0rWvLnLMvScO+oju7u0cChQQ0eG9ZoX0STscRMU9lczmTbtiJDcUWGYjq08/BsP1SVh8pU01yturYaNS6qV/uqVjUtqSdYAgAAwGkRdAC4KBmGoda2sFrbTt0lZHx8UocPR9VzLKr+vriGBhMaG59WZDypqal0bimGYci2cg04ZRvKZC2NjU5pbHxKRw5rZqZI7gu8IUOmy1Aw6FM47FNVlV/BsE/NTUHVNwbU1BRUa2tQPl9p/pVsmqYa2urU0FZ30vnYWEyHX+tW/4EBDRwZ0kjfuOJjE7KtmXk1M81PJ2PTmoz1qntv70w7EEMuj0tV9aF8ANKyrFFNyxpV01B1XjXatq2XH3tNl928/vgsHwAAgGJiWZKsAo1T3Fi6cgYsXQHwX0Uik+rujquvJ6b+wQmNDEzOhCBTisZTyqSzM5uTGPlGIHZ+f1Yp1yfE1sREWhUBrzQbhhhSIODNzQoJ+1RbXabaunLVNZSpsTGktvaQQiG/Y/c9X6ankjq6u1s9+/o1eGRIw71jigxFlU1nJduWbRpS1s7vsCPNbC2s3Da4ZQGfQnUhVdeHVdua2/q2cXGDGtprzzgD5MDLh3Xfb/1vDR8bUaAyoK/tuW9e7hcAAFw85nzpSt3/U7ilK8P/VNTfg0vzPx8CwByprCxXZWW51q1rOOU527Y1MBBXf29c/f0TGhqa1OjopMbHkopGpxWLJpVMZWTbhrweU/bMsg3DsGXbhhKJlBITKfX1xnMDGsrPLrBtW36/W+GwT5WVftXUlKmmplz1DQE1NVWorSOkysri3zHGX+bTysuXaeXly/LnLMtS38FBdb/Rrf5DQxruGdP4YESTsUnZtnE8UzJsTU8mlTw2osEjwzK2zfYCseVyu1VRWaFQbYVCNUFVN1SqprlS1c3Vqm+v0Qs/2q6ju7s1nUgqOZ3STx98XBXhcl1zxxVOfRQAAAA4T8zoOI2uri51dXUpm81q3759RZ1kAVhYxsYS6uueUN9ArifI6MiURkYnFYsmFYlOa3oqM7Pc5fhMD1t2fhlHroGnlftp5gt+7su+LZ/Pk18aU11Tprq6cjU0BdXSElR7e0jl5Ree8C8kE9GEjr3Ro/5Dgxo6OqLRvnFFR+JKJqZneqvMLjGa/SCPN0DNfX65/iuWbWvXU6/LNE3FxyYkSabLVOPSBi1Z06ZNt29QZDih6z54hUb6xrV4bZsqKgMyTfOsl7hMRCaUnE6rpvHcl9bYtq3eAwOqbamWv5xdaQAAKFZzPqOj9mOFm9Ex8rWi/h5M0HEGLF0BMN+mptLqPhbVwMCEhoYSGhqY6Q8yNqVoNKl4PKXZPXJtO79J7in/NGbWyswu7jAMQ+XlHlVW+VRdXab62oDqGgJqbgmqtS2kpqbS2TVmbDiq/gP9GjwyotG+UY0NRhUdjmkqNqlMxjr+Sc0EHbZty8pacrtcev35/YpHJiQZymaz8ng9al3ZqNjwhDa/8zLteW6vll2yWINHh9W0pEFTsUnVttZIMuT1e+Sv8Mnr98rj88hX5lU2a+m1rXtUWR9WIBxQajqlxWvbVd0U1nDPqAzDlNvjVsuyBi3fuPSU38HhXcf0g3/4mRo66pSIJHTjr1wrX7lXpstUWYVf4Zqz/9+mbDa3y5DbffrJnLZta7hnVP6AT6Hq89/pZqh7RJUNYcVHJ1TTdH49U85GNpPVRCShUE2QnioAgKJA0DF/CDrOgKADwEKTSmXU2xtTX8+EBgcnNDIyqZHhSY2PJzU+PqnERFq5liCzm+POsI/3tZgNQWTYsq3cl32vx1Qo7Fd1lU+19bndYhqbg2prC6pjUZXc7uIPQbLZrAaODWvk2KjG+sc1PhRVbDSuicikQlUVWrSuXZdcv0YHdh5W38FBvf78Prk8Lr3y811aeeVy9e7tV7g+lJsNYhozX64NGWZulk1uBx4j1wPMtOX2eOT1ezQ+FJXL49Jo75jCNRUaH4qrqiEsf8AvX5kn/4Xd5TJVUV0hr88jl8etTCqtXU/vVSKWUDKRUsfaNg0eHtLV79usiuoKDXePyDANNS9pUCI+rUwyLdM05XK7VBbyq7qxSr5yj6IjExo6OizDNOTxuGV6XGpe0qAVGxervLJChiH1HhjQCz/cLtM0VdNcpUQ0oXf8PzepaVGDxocjev4/t2sqNq3ycJnKAn697Y5N8ng8p3zGL//8Nb3w45flL/Mom7H0zt+8RY0d9ZqMT+nVp/YoWBlQOpXRhrevOSWcmIgktPelA8qmLVmWpc23X/qm4dsrT+7Wzsd3q6ohJH+5X7feeX3+ue43ejWdSCpcH1J9W+0pr52IJhQfjStUG1IgdPJyL8uyZBgGwQkAYE7MedBRfVfhgo6xrxf192CCjjMg6ABQbNLpjHq64+rpiWqgf1JDwxMaHZ7U+Pi0IpFpTU1mNLsn7klb5Jqz8xyU73shO7dsxuUyFQp6VVVTptraMjU1BtXUUqG2trA6FlXK43E5dbtzKp1Ky+P1qGd/n4aOjWjo2Ij6Dg5IpqFEZFLx0bhM01B0JC6P3yPTMJTNWrnPzHTJ4/coXBvS1MS0JFsHXzmqJevblZpOK5vJyspYGu4eVVVjWF6/V7ZlybJmV9nYkp1bVuP3e/XGSwe0aE2bxvrHVd1Ypb5DA2pe0qjpqaS8fq/cbpcM05Bl2zN1ZPOBViaTlWlKpulSJp2Ry+1Sejott9ctl9ulbNZSIj6ZXy41MT6hcE1Q8fGEGjrqNBGdlNfnlq/cJ8MwNBmfkpW11Ly0Qb6AXy6XIcM0FR+La/ez+xSqCerIrmNadslixcfiWntdp3re6FVlfVhur0vJqbRSkymte3unKuvDMkxTkYFxvfCjHaqoCiiVTMtX5pNhGrrtzusVrK6Qx+eRy23I5XZpz3P79PA3ntRo37gSkYRaVzTppl+9Tjf88jX60f2P6MjubpUFfLItKVQX1H/7zAfk9/s1NZXUd/76B0on08qmsrJsWzf9t2u1bMNiTU8l9cRDz2i0b0wVlRXyl/t040eukcfr0VD3iPZvP6TkZFLl4XJ1vm2lQlUVsm1bh3d1ayo2KU+5V+0rm+UvzzUMTiVTGh+IyON3q6q+iuAEACBp7oOOm6ruLFjQ8dj4Pxf192CCjjMg6ABQauLxpI4djaqvN67+gbiGBxIaHZvW+NiUYrGkstlc807bmPmafHxKiOzcTrozX58NGbJlmIbCYZ9qawKqrS1TQ1OF2tpC6lgcVktLqGSWw5yJZVmKj08oNhZXMpHS9OS0UlNpyZACoYDKAl61LG9WNpPV7mf3qmd/v7LprKobw/L4vZqMTWng8KBG+sY1EUkom8ooa1lyuVyqba7W8ssWa+WmZXr+RzuUTWcUj07q8CuH1bCoQamplFLTud1+0sm0sumsvOVeudwu2ZaV+4WZksfjVjqV1lD3mJqX1CubsWTNhDKzMxjKAn6NDkU0HZtS4+IGTSem881wXR6Xevf3STLUsrxJmWRaMgwZMmTJkilTlmXldsBxGeo7MKDmpY1KRBIzAYnk8ri159m96nzbKqWmU3J7XHK5TNmS0qmMMsmMXF6XXn9un9Zds0oTsSkFgmVyu12Sy8i1WLGlyGhU6cmUUtNpJSdTqmmuUmQ4pvr2WsVHJxSuDer15/dr2cYlmhiPK1gTUrAyoLGhqDLJtMpDZep+o1ftq1oVHYmpaWmDIkNRebwemR5TpuFSOplSJp1RuCakeCShqvqQ/AGfJmPTio7GFK4JKTWdki/gU6gyoMlEUhORCZUHy2WahqysJa8/t6tSciqlskCZDJdyn7ltyGVKMg35/F65PKbs7MyvyshtRe3xeWS6XJrNR2Z/B74yr8K1IUWGozIMM3et1yOXy5ThygWWliV5vR6ZbjMfgJkuUy53rqeM6XHJ43GrsqFSi9e0EcIAwDwi6Jg/BB1nQNAB4GKSyWTV2x3XsZ6o+nvjGujPLY0ZHZ1SNDqtTCr3RdY2DJn5zV2PdweZ/Xm2S6rba6qq0q+6unLV1wfU3Fyh9o5KLVlWdVFslVto2UxW05PT8pf75XK7NDkxpWOv90i2oXBDSI3tdTIMQ5GRqPoODCg6MqF0MiN/uVd1bTVqW9ks02Vq/47DOrzrqMb6I5pOTMswDAVC5erobFHn21bJlvTCf76kY6/3KBaZVCBYppYVTVp3zWqlkkntfGy3uvf1KTYSVyaT2/bXNF1yeQzVNFVr7bWr1b66RS/+eIeGjo5oIjopj9ettlXNKg+Wa/Bobuec1FRKVjY3hcXtMVURDqimpVotyxr1xrYDio/GNZVIyrak/J8wIzerI1hZrurm3EyJ0b4xxUcnlMla8njdCoTKlJxKyVfm1eTEtNJT6ZmeLJLH71FqOq3xwYialzRoejI506Q2F8T07uuXJLWubFF6Opmb7eQ2ZWdtHXr1qJZdukhWdqY/jpGb+RQZiipYXSG3xz2zhCmXBqaSabndbrnchrJZW+ZsnpAPFnJLx2ZWQMl0mZJly7IsWbZkmsZMUGjLsm0ZtmS6XQrXhjQ+GDk+jpWrx5h5g+lEUr4TGg/PBhmzuzflPk9Lvgq/jrx6TJfetG4mlCuXr9ynsgq/xoci8njcKguWye1xKWvZyqQy8vrc+S5ApkuSbcgwTBnm8fEN05Bp5oIpwzRzC+hm/mmYhmSYMg1bhmHmPnrTlmm4cs2DZ5aEzf4NYxiS6XJJtiXDNDW7VCw3vp0PUnPjSlIuKDJtyXCZMw2JXTINQ/ZMc+Lc0jMz9xpbMt25z9gwTBnG7MdqyuUxZ5r55P58595Hufc0jJkxjdxsqdnO0C6XJFtutznTTNrOhY62ZLhcM39mcrPkbNuS2+3JvbfLlf871O3O/b5crtx7mqaZ/xlA8ZvzoKPyo3IbBQg67JQei3yjqL8HE3ScAUEHAORYlqX+/gl1H4mqpy+u/r64hkcmNTazLCaTtSVZMmTKMGxZliFj9nuCjv9TM/8sD3hVXe1XQ0NFvhfIosVhtbVVyjT5L8zFIJPJKDmdViaVkpWVXO5cWDH75dO2bQ11j2gyPqXalmoFKysk5Xql9B8a1MDhIU1PJpXNWioL+NTQXqv21bkZBtHRmPbvOKS+AwOankoqk7RkZ7OSIfkDfi1Z36FVVyyXYRja89xe7d9xWPGxCdW112jl5Uu18vJlOvzaMe165g11v9Gj5GRK/gqfWle06NIb16q6sVLbHn5Fe1/MBSq+cp/aVrXo0pvWyjBNvfDD7eo7NKj0VFrhuqDaVrUoVFOhwaO55UuJyKQ8Po8q60O5JS2hMo0PRjUZn5YsW2VBv6ZnwpZMMq10MiPLsnPxoCXZRm5pkukyZGVtpZNppVNZGYYtrz83m0OylU1ZuaVHhiGX1yWP161gdVCjfaOysrkv7qYr90XdliRbmopPyVfhk2mYM/1jck/Y2eNhiGSoZ1+voqNxmS5TwcoKheuCCoTKZbpMpaZTcnlcMl0uuVyGLMuWlbVnwpfcEq3ZGT0nTPvKL3czZnY5yvUFUj4Y0vF/nDxjzDj+D3tm9thsaDp7ff7amfBB+Xefuf7EmSknlGXP9iwyTr5+9vF/vX72zSxJrpm+RqdcrxPHnnnRmcb/r6WfdP3xz+e0189+WsbsHeeeMWZSmdkwafazzuU0uc/fdLlm+jLlbtIwlP8zYJrm8R2ozNxYpmnKto7/OTFMyZz9izwfJs2cm90NzMjNGjM1G3jlXmgoFy7Nvpdk5AMnczZ8c+VCLWk25NJMHcYJ1+fuO3e/tsyZWWBm/j0smaZrJqQyTrofY2YsW7lAKj9DyjSP1z4TWuU/F5eZ35VrtibDMHP36MoFWC7TzP0+THNmfB3v3WSaMo2Zz9c05MqPbc/MwsrdrzkTbplGLsA0NBPc5QPO3N8PMkyZM2HY7DnTnKnR7daJwVpu1pYkl2vmnjTzvCdX4Ey95kwwaZqm3G43s7rm2ZwHHeFfLVzQEf2Xov4eTNBxBgQdAPDWLMtSb29c/397dx4dV3XnCfx736td+2Jr8SovyNhgg4E4CksAu700TbOYYWk3x6Q70ISlQ5+EQJgESNJnYKCHTDJjnKbTDUyHgY45QGiGpVlsE4gx2I3xirGN8KbVkqWSSlWleu/+5o+3VJUtGxMkG0vfzzmypXq37rv31VVJ76v77tvdeBD79vc6d4xp6UVHZwrxrjQ03L9yu78ODvQ7lbhnAEHT9G+NW1NTgNpxxZg4sRSTJ5cjHB74biFEXxWZTOawv8CLCPp6k1CGQjQW8U8qMv0ZxDt7oG1BQUkMscIoLMtC94EeJLoSEK0RCAURjASQSVtI9qaQ6c8gFA75M3G0rZFO9vtBilIK2rJhWTa0ZUNrG9p2Z4nYGtoG7IyFjGXh9yvew6W3zMcb//p7JLr7ECuOQWwbgWAAkYIwMukMxK0vEAw4oYmt3ZNyeNfa5J7tu2ftcE/as7O+crIJdznk7Bbna/f9wbsddM57heTeSUq0U4nKL69UznV13swSKLipkhv45N5m+ijl3ZNq29bOSalk2wMgu2/JL39Y/cjOMnEToPzEN3eT91yvn34CcmzlBwyORNzX50uUHyCYyg2yvP4de3mnb85r5l7WqHKCrYHK45D2GMgvr9wRlBNW6ZwAxz/JyWvU55TPC+Cc8qIAw3sZRfvhi/ejLZlII1oY9sdSfvnPbw+cHOfI5XMHxSHl/fEFHPICuMdTkL0c9bDy2ZlM2Sq84Ms75l445u5X5SzY7AY+ALIBlrdY9yHhk7h1KPd72J/B5QVofkCVH0Z5oZPhhauGs2NnhpVyXwsDMNxZW4aRnXGl4IRHMKAMcduh/LDKUIAynaDJ26cyTMBwvlNMMzuDy2urCph5bfRvNW8aTogkyg2pnDKmafrHwDAUJKhx5nmzGHQcB/ytkYiIvhTDMDBuXAnGjSs5bFs6beGzz7qxd3eXc9vc1gTa2/pw8GASfUkLSrLrgQgUMpaN1rYEWlsT2LK5Hd6JkGkAJaURVI6KoaqqEGPHFmHCxBJMnlLOy2DoK2OgO9EopVBQFDu8bCiIiuryvMcCgQAqqstQUV02ZG30LP7bP4Nt22j4s7NhZZxbD6cSacQ7egAA/akMtG0jk7acyzgA6Iw44YnA+d6Fc0IqEIh2gg3nciTnZE1s9zTMuRUUtAYg2UtxtIhz1irOyaNAQYm4s1/g383IK2M7FQCi/JNTrTXEW8RXO2ugQDkBh61z63dOEr02eF+LAGJrwDDcdW3cXborA2tb+8dMtPO5ePWJM/MDOntyqkW7lyS5/feOheTu172MSMTfl3Na5TwXEOcyJgDembfWcMIjry73c4gbKbmPe8c695IvsTVEea+V3wGIsxu3XreP7muZ7a8Bd+fua+yGDzknzn547UxWyOZeftCTU97PLyR77u41VQmUKGRDIvjt8oKynDP/bHmVLZfdpvxAJG+THqh89rHDyovy958NAgx/UzZjE7emgcqr3C7C62Ju/XC/n7xa8troBVH+F4eUl2zwoJS44YvySysDftgBeJfM5ZQHnHHnHQC38ZZfHn6Yd1jQlBNMecFPbpDl1e/84/6kz8mxnH27dfiB56Evgnccc773jhCs5R4vb0abV97IO6b5g8Abw07O6ox7f1y7O/C+d/wQB9n2eCGI11el3LpywkuvfHF1weGdG0xaO++BX5YMQh0nGIMOIiIaMuFwAPX1FaivrzhsW2trDz77rBu7P+tGc1MPWtsSONDWh554PzS0exLl/BXEtjUOHkyhszOFHds7nF+a3V/0iotDKCuPoqamEDU1hZhYV4qJdaWoqir8wlNye3v70dGRxIQJh4c2RMONaZqI5YQwJRVA1fhRJ7BFdLKwLAuAcymaF75oN/CBbbtlbHjBlK2dVMW5u5QXMgm07T5Ha4gW2F5gZNn+SaPtBVBukKW17YdXXogjWkNswNbOvlXO87QXmGnAtjWUAmzL+V9rDVsDBsQNxrTTF9ubPeH0zfbCKK39k1vvc+32C4AfXjnr7TihkXIDMe9uWuJexqZt8UMqwD0WToed8t4sLdsGcvft1i9uaOiFEV6A5QeKcENAALbtBgw6u08/uJJsXV5g59TnrBfkB185+8oNN/2JJqJzQj1A3BlVogVwZ4R5+Vvu87IxkEuLH0SIO4tDcmah5M4mAZxQEV4dSjnBjzcZC/DX4xEvbFHi1ujW707L8QMcw8hpjvKn5igIxFDORK6cFvuTvODFGk5dhvd1Tl+g5Av/XkJ/PAYdA1i2bBmWLVsG232jJiKiwVdVVYSqqiLMmTM27/Fksh+Nu7rQ+FkX9u+No7m5F+3tCRw8mHIXgoT3uzOUKMTjafTEM9jd2OVPj1UiiEQCqKiIYdToGKprC1FbW4SJE0tRN6kUwWD+j7+X/98O3H3nm+g6mMK3bzoDP7rvm1wrhIjoCAKBQN7/RMdKa+2fY9m2DctyQxjLCc20GxpZGYFo2wlqLO1uA8QN17wwyrK0O7tLIGJDtHLvcAVobbtBjYKVsZz92AItbuDmzpzSlu3OiHIed8Iht17bC7fc53ghmhs2iXZnltka8NYwEredcKZ22F64pzXCJSHgV0N4gP3L5wajnpMb1+g4Cq7RQUT01WHbGnt2d+PTXZ3YvSeOtuZetLQm0NmRRLrf+cuhN1nXu/Lfuf47O79YRCNgGCguiaCyMorRVQUoKAzik+2dWL5svb+vuX9Sh7lzJ+KW27/GwIOIiIgGxVAvRnpx7NpBW6Pjrb5nTurzYMawRER0UjBNA3WTylA3KX/9AhFBU1Mcn37qrAPS3JRAW2sCBw70IZHod6ecuguhKQO2Brq6UzjYlcSnjV1Ipy38fvUeRCImUinnr0xvvt6I99bsw789swU3/s1s7NpxEH9++SkIRwKYWFeKgoIQp58SERERfUUx6CAiopOaUgpjxpRgzJjD19WIx1PYubMTe3bH0dTUg9aWXnQcSKLrYBK27Vw7bFuCr329FoUFYbS3J9Dc1IO2tgSSyQw2bWzD3T94E7FYEGve24d4dxp1daXo6EjilPpyxOP9mH1WNToOOOt6xApCCIVMVFUVoLgkDAVg565O2JYg0duP+mmV+Po3xvohSTLZj3A44N9O8Eg2bWxFYVEYdXWlQ3AEiYiIaFjgpSs+Bh1ERDRsFRdHMHt2LWbPrs173Lklbhz79/Vi+/YDqKkuwhlnVsEwDGQyNqKxAF59eRfefWcPtm/vQDppId6dRnl5BB0dSRQVhbB3bw8CAQPrPmhGMmnhk086YZgKqaRzi9FAwKmrt6cftWOKYFsaL774CRK9/Rg3vgRFxSEc7EyhojKKWCyI6TNGY9y4IhQVhVFQGEIkYmDVyt34P49vREVFFN84bzzGjCnCtFMrYFuCadMrEQyaR+g5ERERjTjObbG+fD3DIOjgGh1HwTU6iIhGrkzGhm0L0mkLBztT2LGjA3v2dGPnjk4kevuRTFrQWtDSnEAkGkAsFkRvbz/CIRPBkIFMWiNja5SVRtDc1Itdn3bgzNk1SKUsBAImTFMh2WchGgsgnbIgAgSCBoIhE/1pG+mkhfYDCRxo70NvIoPx44uxv6kXF144AemkhVDERKbfxuQp5YhEAmhu7kWsIIiCWABV1UWoqS5ASVkUkYiBfXt7sGVzO0pLwxg3vhSXXjYVBQXhvP7+57omvL16N/otjW80jMW554/3Z55oLejvt2AYBoJB44iX7SQS/fj927tRWBhCX5+FufPqYJpHn63yx742qZSFYNCACBCNHn5bVyIioq+aIV+jI3z14K3Rkf7tSX0ezBkdREREAwgGTQSDQCQSQElJBBMHuGykv99CX18Gjbu6kExlsHd3NxJ9FgBBss9CW1sCDd8Yi0g0ANvWWLumCU1NPUinLShDIdGXQShoIhoJIJmyYCjnNoCBgEK0PIJwxMSMGaOxdWs7Tp0+CqNGFeCTjw8g3pPB1KllEAE2b2pDxtLo7UkjmbQQjQYQDJqIxYKwbY2urjTSaRs9PWloW1BWFsYj/2MNysuiqKophLY19uyOo7W1B+3tfTCUgfKKKOqnVaC+vhxdXWns2nUQez7rRjBoYMy4Ylx19XREoyaqqwpRWBSCgkI4EsCK327BY8v/E1oLDEPh0ccuweKrpmHTxjb89pktOHAgifETSvC3d3wNpaXRvGP58bZ2/K9fvI99e+OYOrUCP/1vFyEWGzjA+OX/XIv//sAfABGcd/44PL3iKoTDzq806z9oQjCk0NOTwbnnjR/wNTMMA4aBz71kiIiI6KQi7r2UB6WekxtndBwFZ3QQEdFgi8dTUEqhqMiZUdHdncL+vd14770m9Pdb6O5Ko68vg8rKKA50JPGX18/E5Cnl+PVj/4ni4hC2bGnHWWfVYOVbu9HVlXQuj4n3IxQOwAwqHDyQgjIVDMNZqFVBoTueRtA0EQwbOHCgD6NHFaC4OAQtzmU8fX0WWpt7ES0IIhIO4ODBJCorYyguDqO9LYGmph709Tl3tikoCGH06BiKisMoLgkjFDKglIEd2w+gqyuN3kQa4XAQqVQGBQUhFBQG0duTwcHOJGxbEA6bqKktwqTJpSgpjiBWGERbSy82b25Hc1MvACAQMHDa6aNx3nljUVIaQawgiGgsCAWgtTWBR//3OsS7084smIDCf733fJx9Tg1Wr9qDXz+2Hl0H+xGJmrjiymn4h5/Pc2eVKPz0/rfxmyc3Ip22MLW+Ao8/eRnqp1VCRPDY8vVYufIzGAZw4UV1uPFvZkMphb6+fuzc2QkFBcNUmDat0p+lIiJQSvn/ExERHc1Qz+i4KHAVAurLz3K0JIOV1rMn9Xkwg46jYNBBRERfVamUhab9cdi2oG5SGQIB5+TbtjU+2tCClpZeZPo10mkb5543DjW1RQCAdeuasPLNRrS29qKnpx/JhIXKUTHMmDEKf7JwMgoLg/iHh9Zgz+5upFI2lALq6kpwyaWnIJWy8K9PfoTODmdtEdHO/tJpG6ZpoDeRxtSp5bjwoon41yc3wjANZPotNO3vQU1tIcKRAPbt7YFSQOWoAoRCChCFnt40mpt6UDmqAAHTRGtLD8KRAKprihAOmzBNAwpAIKiwfVsHenrTiEWDKCgMo601AcMAKiqiaG9Poqc3DdPdb2FhGKWlzpon3d1ptLf3IZ2yYNmCUNBAaVkElRVR9PVl0Nrai2TS+StYNGqiurYIpSVhtLT0oq01AdsGgkGFsWOLMW5cMeI9aezZHUci0Y9w2MTYccWYMrUC2hbs2dON7u60X37WGdUIhQy0NPeis9MJok6ZWo7pp41CYUEQXQfT6DyYRCwWwIQJZagdW4BgwEQqbUMEKHQDo1gsiEDAWZfFm4wSCBh5M1Oee3Ybrrzq1GMaQwxoiIiOLwYdxw+DjqNg0EFERCORiKBpfw+6u1OYNLkMkUj2l6ZEoh+7dnZi40etAIB4vB89PWlU1xSivr4C02eMRlFRCJ980oE3XvsUe/fGcc7XanHxvDqUlkawbWs7XnjuY2zc2IZIJIBkXwZQCtNOrcRfXn86xowtxnPPbsNrr+7EgfY+lFdEYZoKlqXR29MP0zRQWhrB4qunIxIO4P8+tQm9Pc7lOb29GZw+czTOv2AC3l+7F+++sxfhSBBFRSF0daaQSGQwb8EkhEMGVq/ag+7ulN+GdNrGqdMr0N8v2Lq1HQETUIaBeHcS4UgQ4XAAvb1pWBlBKGQinbZgWdo/XkopBIMGLEvDsgTanTlsGIBpKmgtsO3sMVYK7qyb7If3uDIAhfxthuFsczY4/3ufBkwTUEA0EkAqlYFpOmuXGKaCkRNkmKZCIGAiFDJRO6YQnZ0pLF5cj/Mvmoh17zdh2rRKGCaw+9MunHl2LbZtbcPo0YWIFoTQ0tSDmppCJJIZQIBYQQCppI1oNACtBSLAqFExHDyYgmm4jXP3WVwSQaInA1EahmHAVO52pRAwnYICQcAwnb4rp2dmwFkPxplAo9xjo6CgYJoKUAqG4Rx7wzCglLM/AH4gZJrKPdYKShkwDHE/VzBNLuZLRMfXkAcd5pWDF3TYz53U58EMOo6CQQcREdGJkU5b2LmzA/GuflhaYFsWOjvSqJtUhqmnlKGw0Ln0p70tgVUrP0NrSy8uuHAiTp85Gko5wcja9/Zh3ftNaGtLoLgkjPkLJuOMM6uhlMLOnZ145aUdaGzsQklJGIv/y6mYekoFMhmNTRtb8ey/bUVvIoNvXjgBZ3+tFhMnlmDr1na88tJObN3SjtoxRVh0yRTMOqMKH2/rwFtvNmL9B02IxkJoaBiLGaeNwoEDCfzh3X3YvLkdojVqaoowfkIxUkkLO3d2Yv++XthaI1YQRGFBGP0ZC50HkuhLWhAtCLgLv2b6bWQsDcvSEC1QCtAigDhruog4QUMkYiLRZyEYNJHpt7IBipeIwM9I/KAlFApg0uQypNMWqqsLUVoawb59cZw6fRSa9vcgFg0iGgugozOJ8rIoEskMxBZEYwGkUhZCQWf9GS2CeFcaHZ1JTJ5SBjjNg2EodBzoQ1eXE5pBnLDC+/XTm1EitgDuxBQnOAIgTkihkZfvOM+D0ze3mLNVVHbjQOXh3IzAKwbxwiMFQ7mllMBQgIiC4TwIw1BOX5TTHudzp7ihnEDM64u33TTc+t0AxmunkwEpKNOA8g+SgqEEShlQzs7dfbttUOLWraCQ/dwwFJS3HxNQovw2KMP73Ll8zQnPFAy3YYZSgAEY7uvhhT8CgWEaMOCka16fvXqhANMwnH647YChYCovdDOcet02CICAF0K5fXHq9dplwDQFSgwo0zlQpmnCgHaOHQAjoGC4jTVNAwYUDBMQKJjuMTCUE34JFAIB5ezGdI+TqWAowylnOK+nF4wFAt6xNPxtpmm4j4kTzDEQo0HEoOP4YdBxFAw6iIiI6GTStD+OqupCaO0EI7at3cBDkMloJBIZJJM2Hn7wXdTWFuL995uRTmWgRRB0Z3uMG1eMffviKCkNI9OvkclolJZF3BkwQYjWsLUgFDRh29rJB5SB5qYexONpnFJfAcA5sRYIWlt60R3vx9SpZTCUgv+LZ84sluwvo+7Jv1JukuH876yFktNRlZveZCMNJwRyv1JOv519uoGKCOCGBV7QMVB523ZOlAHx17rJrd9w2+zXr5Ub1Ej2kiDJ9s1r7UDls/3MLw84gVZu+6HEbXB++Wxf3PLeMTxC+73+5h5KUQpKAxraD6CUG/74oZL30mgAhoLSAp13PJXztd9nJ2zxciin+XJI/V5oJQOWF+/4eE3O6bS4M31yB0dueQNOvyDw2+WPveyL4hU8crDmBmJKvFlXOaESgFTKRiwWhCjAVM6xMeAGeV4oBGe2FpTyyyjkzDiCUy+8MEwpKMMNpdx9GW4o5+0fSmDA8MMeZ7/iB2yGodz9uLPCDAXD/c6EaThhkIgb5jj7UoYBZQgCyoCYgAnlrPvkhkcCJ4xym+UEX6YXsCEbKin384AbEgIwAwYM04RS4oRUygmqTO857iw003SmtQVM5RwTOLO0DCXuvp0QShkKpgk3qMr93HA/VzAME6bpPO+rsAD1UAcdF6orBi3oWCXPn9TnwbzrChEREdEwUTvG+YXUNA0EB/hdt8LJIPDrx/8ctq1hGApbNrcjFFLYuuUAAkEn7DjQnkAgaCCdspBO2Sgtj+JAewKRaNC53XDauWwlmbKcv7AbwNr39qM3kcF554+D1uL8Rd/W2LihDd3xNL7eMBZKGbC1DWhAi/ZPLm3tntxqgSjnf8A5KYe7aK5AQbRzEm5r57laO6el4s5wETi3Q4Zbh7YBGNn6RIsfDHgzYWxvmxs6iADaP9vNXobk/W1Qay948UIk99ni7tutHyLOvQ/cx/MCHiP7tX/ertwTd8APKrIhULbeAcsr+Cf9/owVl5FTh8CZPZIbReS1SwkM5JYHlBskSW75nCTAL6/c8t6/4pb2MymVuzc3kHDCDGdv2foVnE75z/efrZwXxz0Ifn3ZJ2UzDBGIcsYM3BNt//h56Y53GLxrwZDzELJfiziBgPN6CqCzwYsA6Ovrh8pZANqZviP5KZQbBIn/teS024uFcoKaYynvH+NseW9c+INTKT90zCuPnCDLK49Dgi+RAYOpvGOfe/yPEhzl9jHvKbnJ4SFjF4c8lLubgYLEQ8vnPg8a/qVx3swpZTizgryOG4Z3zAHTDWGUG1p52+HNJFL5z/GCKn+2lRdwuSGQ6X4/l1cM8Qwh0Ricu64MQh0nGIOOASxbtgzLli2DZVkAnISMiIiIaDgaPyECAKiuGZPz6KgvXM/V1009wpbpX7xRI4RlWRARaO2ENc4MHOdrrbUf5Ni2cxpn2xa0ds9htRvmiPjrpGg3lLAsDYWcx7VAi4aIgvYvd3KeC61ga9sJkGx3fxrQtga8OrRTt/jBkzh15bTRC4IsW0MJYLmJkXPu6wRDEMCyxZ/Z4YQRgLbcYAICpRUsbTvPgUBsAMppBwSwRfygTLTyt4ko56TZDSSywZT2t3kzR2w3ONNa3DRNOcEbnHZq7TzsXUalbbhlvEAOgGhot7wyBOGglQ3RtBMEeHUAXv+850r2cVvcDMOLI7ywRCDKQG5U4K3X46/pA6+NXoiUk0AdMTjwgi6vXG56BsBwZ9L4wYTktE3lhArZsCgbVrn/2soPvLKPZ9vsGah8btlsmJETjxwyw2fg8GOA8ocEI97xzJZ3g6+8GVPHHjT5LXJnQOUfc+XMUNJAZZXp7vuQQoPEQib/gH+Zek5yvHTlKPbt24dx48ad6GYQERERERHRMLF3716MHTt20OpLpVKoq6tDS0vLoNVZXV2NxsZGRCKRQavzeGLQcRRaazQ1NaGoqMifskU0kHg8jnHjxmHv3r0n7XVsdPxwvNAXwfFCx4pjhb4Ijhc6Vhwrg0dE0NPTg9ra2kFfMySVSqG/v3/Q6guFQidtyAHw0pWjMgxjUJM2Gv6Ki4v5A4COGccLfREcL3SsOFboi+B4oWPFsTI4SkpKhqTeSCRyUgcTg+3ELz1LRERERERERDRIGHQQERERERER0bDBoINoEITDYdx3330Ih8Mnuil0EuB4oS+C44WOFccKfREcL3SsOFboZMTFSImIiIiIiIho2OCMDiIiIiIiIiIaNhh0EBEREREREdGwwaCDiIiIiIiIiIYNBh1ER3D//fdDKZX3MW3aNH97KpXCrbfeioqKChQWFmLx4sVobW3Nq2PPnj245JJLEIvFMHr0aNx5552wLOt4d4WGwNtvv41LL70UtbW1UErhhRdeyNsuIrj33ntRU1ODaDSKefPmYceOHXllOjs7sWTJEhQXF6O0tBR//dd/jd7e3rwyGzduxPnnn49IJIJx48bhoYceGuqu0RD4vPFyww03HPZ+s3DhwrwyHC8jwwMPPIBzzjkHRUVFGD16NC6//HJs3749r8xg/fxZtWoVZs+ejXA4jClTpuCJJ54Y6u7RIDqWsXLhhRce9t5y880355XhWBkZli9fjpkzZ6K4uBjFxcVoaGjAK6+84m/n+woNNww6iI5ixowZaG5u9j/eeecdf9vf/d3f4d///d+xYsUKrF69Gk1NTbjyyiv97bZt45JLLkF/fz/+8Ic/4Mknn8QTTzyBe++990R0hQZZIpHArFmzsGzZsgG3P/TQQ/jlL3+JX/3qV1i7di0KCgqwYMECpFIpv8ySJUuwZcsWvP7663jppZfw9ttv46abbvK3x+NxzJ8/HxMmTMD69evx8MMP4/7778djjz025P2jwfV54wUAFi5cmPd+8/TTT+dt53gZGVavXo1bb70V7733Hl5//XVkMhnMnz8fiUTCLzMYP38aGxtxySWX4KKLLsKGDRtwxx134Nvf/jZee+2149pf+uMdy1gBgBtvvDHvvSU3AOVYGTnGjh2LBx98EOvXr8e6detw8cUX47LLLsOWLVsA8H2FhiEhogHdd999MmvWrAG3dXV1STAYlBUrVviPbdu2TQDImjVrRETk5ZdfFsMwpKWlxS+zfPlyKS4ulnQ6PaRtp+MLgDz//PP+11prqa6ulocffth/rKurS8LhsDz99NMiIrJ161YBIB988IFf5pVXXhGllOzfv19ERB599FEpKyvLGy933XWX1NfXD3GPaCgdOl5ERJYuXSqXXXbZEZ/D8TJytbW1CQBZvXq1iAzez58f/OAHMmPGjLx9XXPNNbJgwYKh7hINkUPHiojIN7/5Tfnud797xOdwrIxsZWVl8utf/5rvKzQscUYH0VHs2LEDtbW1mDRpEpYsWYI9e/YAANavX49MJoN58+b5ZadNm4bx48djzZo1AIA1a9bg9NNPR1VVlV9mwYIFiMfjfnpOw1NjYyNaWlryxkdJSQnmzJmTNz5KS0tx9tln+2XmzZsHwzCwdu1av8wFF1yAUCjkl1mwYAG2b9+OgwcPHqfe0PGyatUqjB49GvX19fjOd76Djo4OfxvHy8jV3d0NACgvLwcweD9/1qxZk1eHV8arg04+h44Vz1NPPYXKykqcdtpp+OEPf4i+vj5/G8fKyGTbNp555hkkEgk0NDTwfYWGpcCJbgDRV9WcOXPwxBNPoL6+Hs3NzfjJT36C888/H5s3b0ZLSwtCoRBKS0vznlNVVYWWlhYAQEtLS94PA2+7t42GL+/1Hej1zx0fo0ePztseCARQXl6eV6auru6wOrxtZWVlQ9J+Ov4WLlyIK6+8EnV1ddi1axfuueceLFq0CGvWrIFpmhwvI5TWGnfccQfOPfdcnHbaaQAwaD9/jlQmHo8jmUwiGo0ORZdoiAw0VgDgL/7iLzBhwgTU1tZi48aNuOuuu7B9+3Y899xzADhWRppNmzahoaEBqVQKhYWFeP755zF9+nRs2LCB7ys07DDoIDqCRYsW+Z/PnDkTc+bMwYQJE/Db3/6Wb9RENKiuvfZa//PTTz8dM2fOxOTJk7Fq1SrMnTv3BLaMTqRbb70VmzdvzlsfimggRxoruev4nH766aipqcHcuXOxa9cuTJ48+Xg3k06w+vp6bNiwAd3d3Xj22WexdOlSrF69+kQ3i2hI8NIVomNUWlqKU045BTt37kR1dTX6+/vR1dWVV6a1tRXV1dUAgOrq6sNWq/a+9srQ8OS9vgO9/rnjo62tLW+7ZVno7OzkGCJMmjQJlZWV2LlzJwCOl5Hotttuw0svvYSVK1di7Nix/uOD9fPnSGWKi4sZ5p9kjjRWBjJnzhwAyHtv4VgZOUKhEKZMmYKzzjoLDzzwAGbNmoVf/OIXfF+hYYlBB9Ex6u3txa5du1BTU4OzzjoLwWAQb775pr99+/bt2LNnDxoaGgAADQ0N2LRpU97Jyeuvv47i4mJMnz79uLefjp+6ujpUV1fnjY94PI61a9fmjY+uri6sX7/eL/PWW29Ba+3/ItrQ0IC3334bmUzGL/P666+jvr6elyEMc/v27UNHRwdqamoAcLyMJCKC2267Dc8//zzeeuutwy5HGqyfPw0NDXl1eGW8Ouir7/PGykA2bNgAAHnvLRwrI5fWGul0mu8rNDyd6NVQib6qvve978mqVauksbFR3n33XZk3b55UVlZKW1ubiIjcfPPNMn78eHnrrbdk3bp10tDQIA0NDf7zLcuS0047TebPny8bNmyQV199VUaNGiU//OEPT1SXaBD19PTIhx9+KB9++KEAkEceeUQ+/PBD2b17t4iIPPjgg1JaWiq/+93vZOPGjXLZZZdJXV2dJJNJv46FCxfKmWeeKWvXrpV33nlHpk6dKtddd52/vaurS6qqquT666+XzZs3yzPPPCOxWEz+8R//8bj3l76co42Xnp4e+f73vy9r1qyRxsZGeeONN2T27NkydepUSaVSfh0cLyPDd77zHSkpKZFVq1ZJc3Oz/9HX1+eXGYyfP59++qnEYjG58847Zdu2bbJs2TIxTVNeffXV49pf+uN93ljZuXOn/PSnP5V169ZJY2Oj/O53v5NJkybJBRdc4NfBsTJy3H333bJ69WppbGyUjRs3yt133y1KKfmP//gPEeH7Cg0/DDqIjuCaa66RmpoaCYVCMmbMGLnmmmtk586d/vZkMim33HKLlJWVSSwWkyuuuEKam5vz6vjss89k0aJFEo1GpbKyUr73ve9JJpM53l2hIbBy5UoBcNjH0qVLRcS5xeyPf/xjqaqqknA4LHPnzpXt27fn1dHR0SHXXXedFBYWSnFxsXzrW9+Snp6evDIfffSRnHfeeRIOh2XMmDHy4IMPHq8u0iA62njp6+uT+fPny6hRoyQYDMqECRPkxhtvzLuFnwjHy0gx0DgBII8//rhfZrB+/qxcuVLOOOMMCYVCMmnSpLx90Fff542VPXv2yAUXXCDl5eUSDodlypQpcuedd0p3d3dePRwrI8Nf/dVfyYQJEyQUCsmoUaNk7ty5fsghwvcVGn6UiMjxmz9CRERERERERDR0uEYHEREREREREQ0bDDqIiIiIiIiIaNhg0EFEREREREREwwaDDiIiIiIiIiIaNhh0EBEREREREdGwwaCDiIiIiIiIiIYNBh1ERERERERENGww6CAiIiIiIiKiYYNBBxEREZ0QSim88MILJ7oZRERENMww6CAiIhqBbrjhBiilDvtYuHDhiW4aERER0ZcSONENICIiohNj4cKFePzxx/MeC4fDJ6g1RERERIODMzqIiIhGqHA4jOrq6ryPsrIyAM5lJcuXL8eiRYsQjUYxadIkPPvss3nP37RpEy6++GJEo1FUVFTgpptuQm9vb16Zf/mXf8GMGTMQDodRU1OD2267LW/7gQMHcMUVVyAWi2Hq1Kl48cUXh7bTRERENOwx6CAiIqIB/fjHP8bixYvx0UcfYcmSJbj22muxbds2AEAikcCCBQtQVlaGDz74ACtWrMAbb7yRF2QsX74ct956K2666SZs2rQJL774IqZMmZK3j5/85Ce4+uqrsXHjRvzpn/4plixZgs7OzuPaTyIiIhpelIjIiW4EERERHV833HADfvOb3yASieQ9fs899+Cee+6BUgo333wzli9f7m/7+te/jtmzZ+PRRx/FP/3TP+Guu+7C3r17UVBQAAB4+eWXcemll6KpqQlVVVUYM2YMvvWtb+Hv//7vB2yDUgo/+tGP8LOf/QyAE54UFhbilVde4VohRERE9EfjGh1EREQj1EUXXZQXZABAeXm5/3lDQ0PetoaGBmzYsAEAsG3bNsyaNcsPOQDg3HPPhdYa27dvh1IKTU1NmDt37lHbMHPmTP/zgoICFBcXo62t7Y/tEhERERGDDiIiopGqoKDgsEtJBks0Gj2mcsFgMO9rpRS01kPRJCIiIhohuEYHERERDei999477OtTTz0VAHDqqafio48+QiKR8Le/++67MAwD9fX1KCoqwsSJE/Hmm28e1zYTERERcUYHERHRCJVOp9HS0pL3WCAQQGVlJQBgxYoVOPvss3Heeefhqaeewvvvv49//ud/BgAsWbIE9913H5YuXYr7778f7e3tuP3223H99dejqqoKAHD//ffj5ptvxujRo7Fo0SL09PTg3Xffxe233358O0pEREQjCoMOIiKiEerVV19FTU1N3mP19fX4+OOPATh3RHnmmWdwyy23oKamBk8//TSmT58OAIjFYnjttdfw3e9+F+eccw5isRgWL16MRx55xK9r6dKlSKVS+PnPf47vf//7qKysxFVXXXX8OkhEREQjEu+6QkRERIdRSuH555/H5ZdffqKbQkRERPSFcI0OIiIiIiIiIho2GHQQERERERER0bDBNTqIiIjoMLyylYiIiE5WnNFBRERERERERMMGgw4iIiIiIiIiGjYYdBARERERERHRsMGgg4iIiIiIiIiGDQYdRERERERERDRsMOggIiIiIiIiomGDQQcRERERERERDRsMOoiIiIiIiIho2GDQQURERERERETDxv8H6Zyli7fmTEEAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the Train/Test RMSE Loss\n",
    "\n",
    "losses_, losses_df = collect_losses(eval1, collect=[\"Train\", \"Test\"], metric=\"rmse\")\n",
    "losses_df.columns = [\"No\", \"Train\", \"Valid\"]\n",
    "plot_loss(losses_df, xlabel_=\"Epoches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2.3.5 Predicted vs. True prices on the test set (scatter plot)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Prediction vs. True values (scatter plot) - using model0\n",
    "# \n",
    "\n",
    "# Prediction by model1\n",
    "pred_y = eval1.predict(feature_test - 1) + 1\n",
    "\n",
    "# Plot prediction versus true\n",
    "def plot_pred_vs_true(y_true: np.ndarray,\n",
    "                      y_pred: np.ndarray,\n",
    "                      *,\n",
    "                      cmap: str = 'plasma',\n",
    "                      base_marker_size: int = None,\n",
    "                      alpha: float = None,\n",
    "                      rasterized: bool = True):\n",
    "    \"\"\"\n",
    "    Scatter true vs. predicted values over sample index, with:\n",
    "      - hollow circles for true\n",
    "      - '×' for predicted\n",
    "      - color‑encoded by true value magnitude\n",
    "      - auto‑adjusted marker_size & alpha when n is large\n",
    "      - optional rasterization for very large n\n",
    "    \n",
    "    This version always passes the Axes to the colorbar to avoid\n",
    "    the “Unable to determine Axes” error.\n",
    "    \"\"\"\n",
    "    if y_true.shape != y_pred.shape:\n",
    "        raise ValueError(\"y_true and y_pred must have the same shape\")\n",
    "    \n",
    "    n = y_true.shape[0]\n",
    "    # auto‑tune marker size & alpha for big datasets\n",
    "    from matplotlib import colors\n",
    "    if base_marker_size is None:\n",
    "        base_marker_size = 80 if n < 5_000 else 20\n",
    "    if alpha is None:\n",
    "        alpha = 1.0 if n < 5_000 else 0.3\n",
    "\n",
    "    # Normalize true‑value magnitude for coloring\n",
    "    norm = colors.Normalize(vmin=y_true.min(), vmax=y_true.max())\n",
    "    cmap_obj = plt.colormaps[cmap]\n",
    "\n",
    "    # Create a Figure + Axes\n",
    "    fig, ax = plt.subplots(figsize=(15, 6))\n",
    "\n",
    "    idx = np.arange(n)\n",
    "\n",
    "    # True values: hollow circles\n",
    "    ax.scatter(idx, y_true,\n",
    "               marker='o',\n",
    "               facecolors='none',\n",
    "               edgecolors=cmap_obj(norm(y_true)),\n",
    "               s=base_marker_size,\n",
    "               linewidths=1.2,\n",
    "               alpha=alpha,\n",
    "               rasterized=rasterized,\n",
    "               label='True')\n",
    "\n",
    "    # Predicted values: × markers\n",
    "    ax.scatter(idx, y_pred,\n",
    "               marker='x',\n",
    "               c=cmap_obj(norm(y_true)),\n",
    "               s=base_marker_size,\n",
    "               linewidths=1.2,\n",
    "               alpha=alpha,\n",
    "               rasterized=rasterized,\n",
    "               label='Predicted')\n",
    "\n",
    "    # Create a ScalarMappable and attach it to this Axes\n",
    "    sm = plt.cm.ScalarMappable(norm=norm, cmap=cmap_obj)\n",
    "    sm.set_array(y_true)  # Associate the data for the colorbar\n",
    "\n",
    "    # Now tell the figure which Axes to steal space from\n",
    "    cbar = fig.colorbar(sm, ax=ax, pad=0.02)\n",
    "    cbar.set_label('True value magnitude', rotation=270, labelpad=15)\n",
    "\n",
    "    ax.set_title('Predicted vs True Values (Validation and Testset)', fontsize=16)\n",
    "    ax.set_xlabel('Sample index', fontsize=14)\n",
    "    ax.set_ylabel('Value', fontsize=14)\n",
    "    ax.legend(fontsize=12, loc='upper left')\n",
    "    ax.grid(alpha=0.2)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Since the number is large, we plot the last 100 entries.\n",
    "plot_pred_vs_true(target_test.flatten()[-100:].to_numpy_array(), pred_y.flatten()[-100:].to_numpy_array())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`End of Homework 4 Q2 by Nathmath Huang (bh2821)`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

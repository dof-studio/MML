{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning Homework IV\n",
    "\n",
    "### 1. Vanilla RNN (bh2821)\n",
    "\n",
    "* This document is open sourced under Apeche License Version 2.0\n",
    "* Author: Nathmath Huang (bh2821)\n",
    "* Date  : May 11, 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Prior to Anything`:\n",
    "\n",
    "....* Even though I used torch, I just used its capability of computing on CUDA but `NEVER used its Autograd functionality`.\n",
    "\n",
    "....* In my implementation, the base class `nn_Parameter` is completely compatible for `numpy` and `torch` backend,\n",
    "\n",
    "....* so you can see I have manually programmed the gradient system and even a mimic scalable Autograd framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`P. Libraries`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from typing import List, Dict, Tuple, Any, Optional, Union, Literal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "import lzma\n",
    "import random\n",
    "from collections import deque\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I have cuda\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whether using `numpy` as the backend or `torch`\n",
    "backend = \"torch\"; _backend = torch\n",
    "device = \"cuda\"    \n",
    "\n",
    "# Feel free to change those parameters since I have debugged and ensured all of them are useful.\n",
    "# Though, their result may NOT be completely the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`P. Matrix Wrapper Library (self-implemented)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This object class is self implemented and open-sourced\n",
    "# Available at https://github.com/dof-studio/MML/\n",
    "# By Nathmath Huang (bh2821)\n",
    "# License: Apache License Version 2.0\n",
    "\n",
    "class Object:\n",
    "    \"\"\"\n",
    "    Base Type for all advanced n-dimensional data types.\n",
    "    \"\"\"\n",
    "   \n",
    "    __attr__ = \"MML.Object\"  \n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "   \n",
    "    def __repr__(self):\n",
    "        return f\"Object (Abstract Data Type).\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This matrix class is self implemented and open-sourced\n",
    "# Available at https://github.com/dof-studio/MML/\n",
    "# By Nathmath Huang (bh2821)\n",
    "# License: Apache License Version 2.0\n",
    "\n",
    "class Matrix(Object):\n",
    "    \"\"\"\n",
    "    A production-level Matrix class that provides a unified interface for common matrix operations used in machine learning.\n",
    "    The underlying data is stored as either a numpy.ndarray or a torch.Tensor depending on the chosen backend.\n",
    "    This class supports element-wise arithmetic, matrix multiplication, transpose, determinant, inverse, trace, and SVD.\n",
    "    Internal optimizations avoid repeated string comparisons by setting boolean flags during initialization.\n",
    "    \"\"\"\n",
    "    __attr__ = \"MML.Matrix\"    \n",
    "    \n",
    "    def e(self):\n",
    "        \"\"\"\n",
    "        Returns natural exponent value as a single-value Matrix.\n",
    "        \n",
    "        Returns:\n",
    "            -------\n",
    "            Matrix with 0 shape. exp value.\n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            return Matrix(np.e, backend = self._backend, dtype = self.dtype, device = self.device)\n",
    "        else:\n",
    "            return Matrix(torch.e, backend = self._backend, dtype = self.dtype, device = self.device)\n",
    "    \n",
    "    def pi(self):\n",
    "        \"\"\"\n",
    "        Returns pi value as a single-value Matrix.\n",
    "        \n",
    "        Returns:\n",
    "            -------\n",
    "            Matrix with 0 shape. pi value.\n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            return Matrix(np.pi, backend = self._backend, dtype = self.dtype, device = self.device)\n",
    "        else:\n",
    "            return Matrix(torch.pi, backend = self._backend, dtype = self.dtype, device = self.device)\n",
    "    \n",
    "    def __init__(self, data, backend=\"numpy\", *, dtype=None, device=None):\n",
    "        \"\"\"\n",
    "        Initializes a Matrix instance with the specified backend.\n",
    "        \n",
    "        Args:\n",
    "            data (array-like): Input data to be converted into a matrix.\n",
    "            backend (str): The backend to use (\"numpy\" or \"torch\").\n",
    "            dtype(str): The type of data to be stored in (any type or None).\n",
    "            device (str): Device where the data is stored on (\"cpu\" or \"cuda\", or None).\n",
    "            \n",
    "        Raises:\n",
    "            ValueError: If an unsupported backend is provided.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self._backend = backend.lower()\n",
    "        if self._backend not in (\"numpy\", \"torch\"):\n",
    "            raise ValueError(\"Unsupported backend. Please choose 'numpy' or 'torch'.\")\n",
    "        self._is_numpy = (self._backend == \"numpy\")\n",
    "        self._is_torch = (self._backend == \"torch\")\n",
    "        \n",
    "        # Convert input data to the appropriate type.\n",
    "        # By Nathmath Huang\n",
    "        if self._is_numpy:\n",
    "            self.data = np.array(data, dtype=dtype)\n",
    "        else:\n",
    "            if torch is None:\n",
    "                raise ImportError(\"PyTorch is not installed but backend 'torch' was requested.\")\n",
    "            self.data = data.to(device, dtype=dtype) if isinstance(data, torch.Tensor) else torch.tensor(data, device=device, dtype=dtype)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        Returns a string representation of the Matrix showing the backend, shape, and data.\n",
    "        \"\"\"\n",
    "        return f\"Matrix(backend={self._backend}, shape={self.shape}, data=\\n{self.data})\"\n",
    "    \n",
    "    def __getitem__(self, key):\n",
    "       \"\"\"\n",
    "       Allows subscription using a[i, j]. \n",
    "       If the result is an array, it returns a new Matrix; otherwise, the scalar value.\n",
    "       \"\"\"\n",
    "       result = self.data[key]\n",
    "       if (self._is_numpy and isinstance(result, np.ndarray)) or (self._is_torch and torch is not None and isinstance(result, torch.Tensor)):\n",
    "           return Matrix(result, backend=self._backend)\n",
    "       return result\n",
    "   \n",
    "    def __setitem__(self, key, value):\n",
    "        \"\"\"\n",
    "        Allows assignment using a[i, j] = value.\n",
    "        If the value is a Matrix instance, its underlying data is used.\n",
    "        \"\"\"\n",
    "        if isinstance(value, Matrix):\n",
    "            value = value.data\n",
    "        self.data[key] = value\n",
    "   \n",
    "    def submatrix(self, key):\n",
    "      \"\"\"\n",
    "      Retrieves a sub-array of the matrix using the given key while ensuring the result remains two-dimensional.\n",
    "      For example, using a[:, 1] will return a Matrix of shape (m, 1) rather than (m,).\n",
    "      \n",
    "      Args:\n",
    "          key: Indexing key (can be an int, slice, or tuple of such) for sub-array extraction.\n",
    "      \n",
    "      Returns:\n",
    "          Matrix: A new Matrix instance representing the sub-array with two dimensions.\n",
    "      \"\"\"\n",
    "      result = self.data[key]\n",
    "      # For numpy backend: if result is 1D but the original matrix is 2D, adjust the shape.\n",
    "      if self._is_numpy and isinstance(result, np.ndarray):\n",
    "          if result.ndim == 1 and len(self.data.shape) == 2:\n",
    "              if isinstance(key, tuple):\n",
    "                  if len(key) == 2:\n",
    "                      if isinstance(key[0], slice) and isinstance(key[1], int):\n",
    "                          # Selecting a column -> reshape to (m, 1)\n",
    "                          result = result[:, np.newaxis]\n",
    "                      elif isinstance(key[0], int) and isinstance(key[1], slice):\n",
    "                          # Selecting a row -> reshape to (1, n)\n",
    "                          result = result[np.newaxis, :]\n",
    "                      else:\n",
    "                          result = np.atleast_2d(result)\n",
    "                  else:\n",
    "                      result = np.atleast_2d(result)\n",
    "              else:\n",
    "                  # key is a single index (e.g., a[1]) -> treat as row selection.\n",
    "                  result = result[np.newaxis, :]\n",
    "      # For torch backend: similar adjustments using unsqueeze.\n",
    "      elif self._is_torch and torch is not None and isinstance(result, torch.Tensor):\n",
    "          if result.dim() == 1 and len(self.data.shape) == 2:\n",
    "              if isinstance(key, tuple):\n",
    "                  if len(key) == 2:\n",
    "                      if isinstance(key[0], slice) and isinstance(key[1], int):\n",
    "                          result = result.unsqueeze(1)  # Make column vector.\n",
    "                      elif isinstance(key[0], int) and isinstance(key[1], slice):\n",
    "                          result = result.unsqueeze(0)  # Make row vector.\n",
    "                      else:\n",
    "                          result = result.unsqueeze(0)\n",
    "                  else:\n",
    "                      result = result.unsqueeze(0)\n",
    "              else:\n",
    "                  result = result.unsqueeze(0)\n",
    "      return Matrix(result, backend=self._backend) \n",
    "   \n",
    "    @property\n",
    "    def shape(self):\n",
    "        \"\"\"\n",
    "        Returns the shape of the matrix.\n",
    "        \"\"\"\n",
    "        return self.data.shape\n",
    "    \n",
    "    @property\n",
    "    def dtype(self):\n",
    "        \"\"\"\n",
    "        Returns the data type of the matrix elements.\n",
    "        \"\"\"\n",
    "        return self.data.dtype\n",
    "    \n",
    "    @property\n",
    "    def device(self):\n",
    "        \"\"\"\n",
    "        Returns the data device of the matrix elements.\n",
    "        \"\"\"\n",
    "        if self._backend == \"numpy\":\n",
    "            return \"cpu\"\n",
    "        else:\n",
    "            return self.data.device.type\n",
    "\n",
    "    def reshape(self, shape):\n",
    "        \"\"\"\n",
    "        Converts the matrix into a new shape.\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: A new Matrix object with the specified shape.\n",
    "        \"\"\"\n",
    "        return Matrix(self.data.reshape(shape), backend=self._backend)\n",
    "    \n",
    "    def astype(self, dtype):\n",
    "        \"\"\"\n",
    "        Converts the underlying data to the specified type.\n",
    "        \n",
    "        For the numpy backend, it uses np.ndarray.astype.\n",
    "        For the torch backend, it maps the input (which can be a torch.dtype, a string, or a numpy type)\n",
    "        to the corresponding torch dtype and uses tensor.to(dtype=...).\n",
    "        \n",
    "        Args:\n",
    "            dtype: The desired data type. For numpy, any valid numpy dtype is accepted.\n",
    "                   For torch, this can be a torch.dtype, a string (e.g., \"float32\", \"int64\"),\n",
    "                   or a numpy dtype.\n",
    "                   \n",
    "        Returns:\n",
    "            A new Matrix instance with the data converted to the specified type.\n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            new_data = self.data.astype(dtype)\n",
    "            return Matrix(new_data, backend=\"numpy\")\n",
    "        else:\n",
    "            # Map the input dtype to a torch dtype.\n",
    "            torch_dtype = None\n",
    "            if isinstance(dtype, torch.dtype):\n",
    "                torch_dtype = dtype\n",
    "            elif isinstance(dtype, str):\n",
    "                mapping = {\n",
    "                    \"float32\": torch.float32,\n",
    "                    \"float\": torch.float32,\n",
    "                    \"float64\": torch.float64,\n",
    "                    \"double\": torch.float64,\n",
    "                    \"int32\": torch.int32,\n",
    "                    \"int\": torch.int32,\n",
    "                    \"int64\": torch.int64,\n",
    "                    \"long\": torch.int64,\n",
    "                    \"bool\": torch.bool,\n",
    "                    \"complex64\": torch.complex64,\n",
    "                    \"complex128\": torch.complex128\n",
    "                }\n",
    "                if dtype in mapping:\n",
    "                    torch_dtype = mapping[dtype]\n",
    "                else:\n",
    "                    raise ValueError(f\"Unsupported dtype string: {dtype}\")\n",
    "            elif isinstance(dtype, (np.dtype, type)):\n",
    "                np_dtype = np.dtype(dtype)\n",
    "                mapping = {\n",
    "                    np.dtype(\"float32\"): torch.float32,\n",
    "                    np.dtype(\"float64\"): torch.float64,\n",
    "                    np.dtype(\"int32\"): torch.int32,\n",
    "                    np.dtype(\"int64\"): torch.int64,\n",
    "                    np.dtype(\"bool\"): torch.bool,\n",
    "                    np.dtype(\"complex64\"): torch.complex64,\n",
    "                    np.dtype(\"complex128\"): torch.complex128,\n",
    "                }\n",
    "                if np_dtype in mapping:\n",
    "                    torch_dtype = mapping[np_dtype]\n",
    "                else:\n",
    "                    raise ValueError(f\"Unsupported numpy dtype: {np_dtype}\")\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported dtype argument: {dtype}\")\n",
    "            new_data = self.data.to(dtype=torch_dtype)\n",
    "            return Matrix(new_data, backend=\"torch\")\n",
    "\n",
    "    def to(self, backend, *, dtype=None, device=None):\n",
    "        \"\"\"\n",
    "        Converts the matrix to the specified backend and optionally sets the device for torch tensors.\n",
    "        \n",
    "        Args:\n",
    "            backend (str): The target backend (\"numpy\" or \"torch\").\n",
    "            dtype (str, optional): The target type (any numpy or torch type nor None for auto inferenence).\n",
    "            device (str, optional): The target device for torch tensors (e.g., \"cpu\" or \"cuda\").\n",
    "                                    Ignored if converting to numpy.\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: A new Matrix object with data in the target backend and on the specified device.\n",
    "        \"\"\"\n",
    "        target = backend.lower()\n",
    "        if target == self._backend:\n",
    "            # Already in the target backend: for torch, adjust device if specified.\n",
    "            if self._is_torch:\n",
    "                new_data = self.data.to(device = device, dtype = dtype)\n",
    "                return Matrix(new_data, backend=\"torch\")\n",
    "            return Matrix(self.data, backend=self._backend, device=device, dtype=dtype)\n",
    "        if target == \"numpy\":\n",
    "            # Converting from torch to numpy: always bring to CPU.\n",
    "            if self._is_torch:\n",
    "                return Matrix(self.data.cpu().to(dtype = dtype).numpy(), backend=\"numpy\")\n",
    "        elif target == \"torch\":\n",
    "            if torch is None:\n",
    "                raise ImportError(\"PyTorch is not installed.\")\n",
    "            if self._is_numpy:\n",
    "                new_data = torch.tensor(self.data, device = device, dtype = dtype)\n",
    "                return Matrix(new_data, backend=\"torch\")\n",
    "        raise ValueError(\"Unsupported backend conversion.\")\n",
    "\n",
    "    def to_rational(self):\n",
    "        \"\"\"\n",
    "        Converts all values stored in the matrix to rational numbers (fractions.Fraction).\n",
    "        For the numpy backend, returns a new Matrix with a numpy array of Fraction objects (dtype=object).\n",
    "        For the torch backend, due to limitations of torch tensors with non-numeric types,\n",
    "        the conversion is performed via numpy and the underlying data becomes a numpy array of Fraction objects,\n",
    "        while the backend attribute is preserved as 'torch'.\n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            vec_func = np.vectorize(lambda x: float(x.real))\n",
    "            new_data = vec_func(self.data)\n",
    "            return Matrix(new_data, backend=\"numpy\")\n",
    "        else:\n",
    "            # Convert torch tensor to numpy, then to Fraction.\n",
    "            np_data = self.data.cpu().numpy()\n",
    "            vec_func = np.vectorize(lambda x: float(x.real))\n",
    "            new_data = vec_func(np_data)\n",
    "            # Although backend remains 'torch', data is now a numpy array.\n",
    "            return Matrix(new_data, backend=\"torch\")\n",
    "\n",
    "    def to_complex(self):\n",
    "        \"\"\"\n",
    "        Converts all values stored in the matrix to complex numbers.\n",
    "        For the numpy backend, returns a new Matrix with a numpy array of complex numbers.\n",
    "        For the torch backend, returns a new Matrix with data converted to a torch complex tensor.\n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            new_data = self.data.astype(complex)\n",
    "            return Matrix(new_data, backend=\"numpy\")\n",
    "        else:\n",
    "            if not torch.is_complex(self.data):\n",
    "                new_data = self.data.to(torch.complex64)\n",
    "                return Matrix(new_data, backend=\"torch\")\n",
    "            return Matrix(self.data, backend=\"torch\")\n",
    "        \n",
    "    def _apply_op(self, other, op):\n",
    "        \"\"\"\n",
    "        Internal helper to apply an element-wise binary operation.\n",
    "        \n",
    "        Args:\n",
    "            other (Matrix or scalar): The other operand.\n",
    "            op (callable): A function that applies the desired operation element-wise.\n",
    "            \n",
    "        Returns:\n",
    "            Matrix: A new Matrix instance with the operation applied.\n",
    "        \"\"\"\n",
    "        other_val = other.data if isinstance(other, Matrix) else other\n",
    "        result = op(self.data, other_val)\n",
    "        return Matrix(result, backend=self._backend)\n",
    "    \n",
    "    def copy(self, *, backend=None, dtype=None, device=None):\n",
    "        \"\"\"\n",
    "        Creates a deep copy of the current matrix with the specified backend and data type.\n",
    "        \n",
    "        Args:\n",
    "            backend (str): The backend for the copied matrix. Default is None.\n",
    "            dtype: Desired data type for the result. Default is None.\n",
    "            device: Device to which the tensor should be moved if applicable. Default is None.\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: A deep copy of the current matrix with the specified parameters.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            if backend is None:\n",
    "                return Matrix(self.data.copy(), backend=self._backend, dtype=dtype, device=device)\n",
    "            else:\n",
    "                return Matrix(self.data.copy(), backend=backend, dtype=dtype, device=device)\n",
    "        else:\n",
    "            if backend is None:\n",
    "                return Matrix(self.data.clone().detach(), backend=self._backend, dtype=dtype, device=device)\n",
    "            else:\n",
    "                return Matrix(self.data.clone().detach(), backend=backend, dtype=dtype, device=device)\n",
    "    \n",
    "    def append(self, to_append, axis=0):\n",
    "        \"\"\"\n",
    "        Append a scalar (broadcasted) or an array to the matrix along the specified axis.\n",
    "        \n",
    "        Args:\n",
    "            to_append: A scalar value or an array-like object (or Matrix) to append.\n",
    "            axis (int): Axis along which to append. For 2D matrices, use axis=0 (append row)\n",
    "                        or axis=1 (append column).\n",
    "                        \n",
    "        Returns:\n",
    "            Matrix: A new Matrix instance with the appended data.\n",
    "        \"\"\"\n",
    "        orig_shape = self.data.shape\n",
    "    \n",
    "        # If to_append is an instance of self, extract its underlying data.\n",
    "        if isinstance(to_append, type(self)):\n",
    "            append_data = to_append.data\n",
    "        else:\n",
    "            # If to_append is a scalar, create an array of appropriate shape.\n",
    "            if np.isscalar(to_append):\n",
    "                if axis == 0:\n",
    "                    new_shape = (1, orig_shape[1])\n",
    "                elif axis == 1:\n",
    "                    new_shape = (orig_shape[0], 1)\n",
    "                else:\n",
    "                    raise ValueError(\"Axis out of bounds. Only axis=0 or axis=1 are supported.\")\n",
    "                if self._is_numpy:\n",
    "                    append_data = np.full(new_shape, to_append, dtype=self.data.dtype)\n",
    "                else:\n",
    "                    append_data = torch.full(new_shape, to_append, dtype=self.data.dtype, device=self.data.device.type)\n",
    "            # Not a scalar\n",
    "            else:\n",
    "                # If the input is a matrix.\n",
    "                if isinstance(to_append, Matrix):\n",
    "                    if self._is_numpy:\n",
    "                        append_data = np.array(to_append.data.copy(), dtype=self.data.dtype)\n",
    "                    else:\n",
    "                        append_data = torch.tensor(to_append.data.clone().detach(), dtype=self.data.dtype, device=self.data.device.type)\n",
    "                    \n",
    "                # Otherwise, assume to_append is array-like.\n",
    "                else:\n",
    "                    if self._is_numpy:\n",
    "                        append_data = np.array(to_append.copy(), dtype=self.data.dtype)\n",
    "                    else:\n",
    "                        append_data = torch.tensor(to_append.clone().detach(), dtype=self.data.dtype, device=self.data.device.type)\n",
    "                    \n",
    "                # Validate dimensions (assuming 2D matrices)\n",
    "                if len(append_data.shape) != len(orig_shape):\n",
    "                    raise ValueError(\"Dimension mismatch: appended data must match dimensions of the matrix.\")\n",
    "                if axis == 0:\n",
    "                    if append_data.shape[1:] != orig_shape[1:]:\n",
    "                        raise ValueError(\"Shape mismatch for axis 0: appended array must have the same number of columns.\")\n",
    "                elif axis == 1:\n",
    "                    if append_data.shape[0] != orig_shape[0]:\n",
    "                        raise ValueError(\"Shape mismatch for axis 1: appended array must have the same number of rows.\")\n",
    "                else:\n",
    "                    raise ValueError(\"Axis out of bounds. Only axis=0 or axis=1 are supported.\")\n",
    "        \n",
    "        # Concatenate along the specified axis using the appropriate backend.\n",
    "        if self._is_numpy:\n",
    "            new_data = np.concatenate((self.data, append_data), axis=axis)\n",
    "        else:\n",
    "            new_data = torch.cat((self.data, append_data), dim=axis)\n",
    "        \n",
    "        return Matrix(new_data, backend=self._backend)\n",
    "\n",
    "    def __add__(self, other):\n",
    "        \"\"\"\n",
    "        Element-wise addition.\n",
    "        \"\"\"\n",
    "        other_val = other.data if isinstance(other, Object) else other\n",
    "        return Matrix(self.data + other_val, backend=self._backend)\n",
    "    \n",
    "    def __radd__(self, other):\n",
    "        \"\"\"\n",
    "        Right-hand element-wise addition.\n",
    "        \"\"\"\n",
    "        return self.__add__(other)\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        \"\"\"\n",
    "        Element-wise subtraction.\n",
    "        \"\"\"\n",
    "        other_val = other.data if isinstance(other, Object) else other\n",
    "        return Matrix(self.data - other_val, backend=self._backend)\n",
    "    \n",
    "    def __rsub__(self, other):\n",
    "        \"\"\"\n",
    "        Right-hand element-wise subtraction.\n",
    "        \"\"\"\n",
    "        other_val = other.data if isinstance(other, Object) else other\n",
    "        return Matrix(other_val - self.data, backend=self._backend)\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        \"\"\"\n",
    "        Element-wise multiplication.\n",
    "        \"\"\"\n",
    "        other_val = other.data if isinstance(other, Object) else other\n",
    "        return Matrix(self.data * other_val, backend=self._backend)\n",
    "    \n",
    "    def __rmul__(self, other):\n",
    "        \"\"\"\n",
    "        Right-hand element-wise multiplication.\n",
    "        \"\"\"\n",
    "        other_val = other.data if isinstance(other, Object) else other\n",
    "        return Matrix(other_val * self.data, backend=self._backend)\n",
    "    \n",
    "    def __truediv__(self, other):\n",
    "        \"\"\"\n",
    "        Element-wise true division.\n",
    "        \"\"\"\n",
    "        other_val = other.data if isinstance(other, Object) else other\n",
    "        return Matrix(self.data / other_val, backend=self._backend)\n",
    "    \n",
    "    def __rtruediv__(self, other):\n",
    "        \"\"\"\n",
    "        Right-hand element-wise true division.\n",
    "        \"\"\"\n",
    "        other_val = other.data if isinstance(other, Object) else other\n",
    "        return Matrix(other_val / self.data, backend=self._backend)\n",
    "    \n",
    "    def __matmul__(self, other):\n",
    "        \"\"\"\n",
    "        Matrix multiplication using the @ operator.\n",
    "        \n",
    "        Args:\n",
    "            other (Matrix or array-like): The matrix to multiply with.\n",
    "            \n",
    "        Returns:\n",
    "            Matrix: The result of the matrix multiplication.\n",
    "        \"\"\"\n",
    "        other_val = other.data if isinstance(other, Object) else other\n",
    "        result = self.data @ other_val\n",
    "        return Matrix(result, backend=self._backend)\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        \"\"\" Equals operator. \"\"\"\n",
    "        if isinstance(other, Object):\n",
    "            if np.prod(self.data.shape) == 1 and np.prod(other.data.shape) == 1:\n",
    "                # Scalar to scalar, output a scalar\n",
    "                return bool(self.flatten().data == other.flatten().data)\n",
    "            else:\n",
    "                return Matrix(self.data == other.data, backend=self._backend)\n",
    "                \n",
    "        elif np.prod(self.data.shape) == 1 and hasattr(other, \"__len__\") == False:\n",
    "            # Scalar to scalar, output a scalar\n",
    "            return bool(self.data == other)\n",
    "        else:\n",
    "            return Matrix(self.data == other, backend=self._backend)\n",
    "    \n",
    "    def __pow__(self, to_power):\n",
    "        \"\"\"Element-wise power.\"\"\"\n",
    "        return self._apply_op(to_power, lambda a, b: a ** b)\n",
    "\n",
    "    def __rpow__(self, other):\n",
    "        \"\"\"Right-hand element-wise power.\"\"\"\n",
    "        return self.__pow__(other)\n",
    "    \n",
    "    def __neg__(self):\n",
    "        \"\"\"\n",
    "        Returns the negation of the matrix.\n",
    "        \"\"\"\n",
    "        return Matrix(-self.data, backend=self._backend) \n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the length of the matrix.\n",
    "        \"\"\"\n",
    "        return len(self.data)\n",
    "    \n",
    "    def unique(self):\n",
    "        \"\"\"\n",
    "        Returns the unique values of the elements that are non-zero.\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: The unique value matrix.\n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.unique(self.data)\n",
    "        else:\n",
    "            result = torch.unique(self.data)\n",
    "        return Matrix(result, backend=self._backend)\n",
    "    \n",
    "    def nonzero(self):\n",
    "        \"\"\"\n",
    "        Returns the indices of the elements that are non-zero.\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: The indices matrix.\n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = self.data.nonzero()\n",
    "        else:\n",
    "            result = self.data.nonzero()\n",
    "        return Matrix(result, backend=self._backend)\n",
    "    \n",
    "    def any(self, axis = None):\n",
    "        \"\"\"\n",
    "        Computes the element-wise logical OR along a specified axis.\n",
    "        \n",
    "        Args: \n",
    "            axis (int or None): Axis along which to apply the `any` operation. If not provided,\n",
    "                                it applies over all elements of the matrix.\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: A new matrix containing the result of the logical OR operation along the specified axis.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.any(self.data, axis=axis)\n",
    "        else:\n",
    "            result = self.data.any(dim=axis)\n",
    "        return Matrix(result, backend=self._backend)\n",
    "    \n",
    "    def all(self, axis = None):\n",
    "        \"\"\"\n",
    "        Computes the element-wise logical AND along a specified axis.\n",
    "    \n",
    "        Args: \n",
    "            axis (int or None): Axis along which to apply the `all` operation. If not provided,\n",
    "                                it applies over all elements of the matrix.\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: A new matrix containing the result of the logical AND operation along the specified axis.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.all(self.data, axis=axis)\n",
    "        else:\n",
    "            result = self.data.all(dim=axis)\n",
    "        return Matrix(result, backend=self._backend)\n",
    "    \n",
    "    def round(self, digits = 0):\n",
    "        \"\"\"\n",
    "        Rounds the data to a specified number of decimal places.\n",
    "    \n",
    "        Args:\n",
    "            digits (int): The number of decimal places to round the data. Default is 0.\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: A new matrix containing the rounded values of the original data.\n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.round(self.data, decimals = digits)\n",
    "        else:\n",
    "            result = torch.round(self.data, decimals = digits)\n",
    "        return Matrix(result, backend=self._backend)\n",
    "    \n",
    "    def mean(self, axis = None):\n",
    "        \"\"\"\n",
    "        Computes the mean of the matrix along a specified axis.\n",
    "    \n",
    "        Args:\n",
    "            axis (Optional[int]): Axis along which to compute the mean. If None, computes the mean across all dimensions.\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: A new instance containing the computed mean values.\n",
    "    \n",
    "        Raises:\n",
    "            AttributeError: If no data attribute exists in the instance.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.mean(self.data, axis=axis)\n",
    "        else:\n",
    "            result = torch.mean(self.data, dim=axis)\n",
    "        return Matrix(result, backend=self._backend)\n",
    "    \n",
    "    def median(self, axis = None):\n",
    "        \"\"\"\n",
    "        Computes the median along a given axis.\n",
    "    \n",
    "        Args:\n",
    "            axis (Optional[int]): Axis along which to compute the median. Default is None, which computes over all dimensions.\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: A new matrix containing the computed medians.\n",
    "    \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.median(self.data, axis=axis)\n",
    "        else:\n",
    "            if axis is None:\n",
    "                result = torch.median(self.data)\n",
    "            else:\n",
    "                result, _ = torch.median(self.data, dim=axis)\n",
    "        return Matrix(result, backend=self._backend)\n",
    "    \n",
    "    def std(self, axis = None):\n",
    "        \"\"\"\n",
    "        Computes the standard deviation of the matrix along a specified axis.\n",
    "        \n",
    "        Args:\n",
    "            axis (Optional[int]): Axis along which to compute the standard deviation. If None, computes across all dimensions.\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: A new instance containing the computed standard deviation values.\n",
    "        \n",
    "        Raises:\n",
    "            AttributeError: If no data attribute exists in the instance.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.std(self.data, axis=axis)\n",
    "        else:\n",
    "            result = torch.std(self.data, dim=axis)\n",
    "        return Matrix(result, backend=self._backend)\n",
    "    \n",
    "    def var(self, axis = None):\n",
    "        \"\"\"\n",
    "        Computes the variance of the matrix along a specified axis.\n",
    "        \n",
    "        Args:\n",
    "            axis (Optional[int]): Axis along which to compute the variance. If None, computes across all dimensions.\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: A new instance containing the computed variance values.\n",
    "        \n",
    "        Raises:\n",
    "            AttributeError: If no data attribute exists in the instance.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.var(self.data, axis=axis)\n",
    "        else:\n",
    "            result = torch.var(self.data, dim=axis)\n",
    "        return Matrix(result, backend=self._backend)\n",
    "    \n",
    "    def min(self, axis = None):\n",
    "        \"\"\"\n",
    "        Computes the minimum of the matrix along a specified axis.\n",
    "        \n",
    "        Args:\n",
    "            axis (Optional[int]): Axis along which to compute the minimum. If None, computes across all dimensions.\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: A new instance containing the computed minimum values.\n",
    "        \n",
    "        Raises:\n",
    "            AttributeError: If no data attribute exists in the instance.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            if axis is None:\n",
    "                result = np.min(self.data)\n",
    "            else:\n",
    "                result = np.min(self.data, axis=axis)\n",
    "        else:\n",
    "            if axis is None:\n",
    "                result = torch.min(self.data)\n",
    "            else:\n",
    "                result, indices = torch.min(self.data, dim=axis)\n",
    "        return Matrix(result, backend=self._backend)\n",
    "    \n",
    "    def max(self, axis = None):\n",
    "        \"\"\"\n",
    "        Computes the maximum of the matrix along a specified axis.\n",
    "        \n",
    "        Args:\n",
    "            axis (Optional[int]): Axis along which to compute the maximum. If None, computes across all dimensions.\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: A new instance containing the computed maximum values.\n",
    "        \n",
    "        Raises:\n",
    "            AttributeError: If no data attribute exists in the instance.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            if axis is None:\n",
    "                result = np.max(self.data)\n",
    "            else:\n",
    "                result = np.max(self.data, axis=axis)\n",
    "        else:\n",
    "            if axis is None:\n",
    "                result = torch.max(self.data)\n",
    "            else:\n",
    "                result, indices = torch.max(self.data, dim=axis)\n",
    "        return Matrix(result, backend=self._backend)\n",
    "    \n",
    "    def clip(self, a_min=None, a_max=None):\n",
    "        \"\"\"\n",
    "        Clips the values of the matrix to a specified range.\n",
    "    \n",
    "        Args: \n",
    "            a_min (float or None): Minimum value for clipping. If None, no minimum is applied.\n",
    "            a_max (float or None): Maximum value for clipping. If None, no maximum is applied.\n",
    "    \n",
    "        Returns:\n",
    "            Matrix: A new instance containing the clipped values of the original data within the specified range.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.clip(self.data, a_min=a_min, a_max=a_max)\n",
    "        else:\n",
    "            result = torch.clip(self.data, min=a_min, max=a_max)\n",
    "        return Matrix(result, backend=self._backend)\n",
    "    \n",
    "    def sum(self, axis = None, keepdims = False):\n",
    "        \"\"\"\n",
    "        Computes the sum of the matrix along a specified axis.\n",
    "        \n",
    "        Args:\n",
    "            axis (Optional[int]): Axis along which to compute the sum. If None, computes across all dimensions.\n",
    "            keepdims (bool): If keeps the dimension or not.\n",
    "            \n",
    "        Returns:\n",
    "            Matrix: A new instance containing the computed sum values.\n",
    "        \n",
    "        Raises:\n",
    "            AttributeError: If no data attribute exists in the instance.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.sum(self.data, axis=axis, keepdims=keepdims)\n",
    "        else:\n",
    "            result = torch.sum(self.data, dim=axis, keepdim=keepdims)\n",
    "        return Matrix(result, backend=self._backend)\n",
    "    \n",
    "    def cumsum(self, axis = None):\n",
    "        \"\"\"\n",
    "        Computes the cumulative sum of the matrix along a specified axis.\n",
    "        \n",
    "        Args:\n",
    "            axis (Optional[int]): Axis along which to compute the cumulative sum. If None, computes across all dimensions.\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: A new instance containing the computed cumulative sum values.\n",
    "        \n",
    "        Raises:\n",
    "            AttributeError: If no data attribute exists in the instance.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.cumsum(self.data, axis=axis)\n",
    "        else:\n",
    "            result = torch.cumsum(self.data, dim=axis)\n",
    "        return Matrix(result, backend=self._backend)\n",
    "    \n",
    "    def prod(self, axis = None, keepdims = False):\n",
    "        \"\"\"\n",
    "        Computes the product of the matrix along a specified axis.\n",
    "        \n",
    "        Args:\n",
    "            axis (Optional[int]): Axis along which to compute the product. If None, computes across all dimensions.\n",
    "            keepdims (bool): If keeps the dimension or not.\n",
    "            \n",
    "        Returns:\n",
    "            Matrix: A new instance containing the computed product values.\n",
    "        \n",
    "        Raises:\n",
    "            AttributeError: If no data attribute exists in the instance.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.prod(self.data, axis=axis, keepdims=keepdims)\n",
    "        else:\n",
    "            result = torch.prod(self.data, dim=axis, keepdim=keepdims)\n",
    "        return Matrix(result, backend=self._backend)\n",
    "    \n",
    "    def cumprod(self, axis = None):\n",
    "        \"\"\"\n",
    "        Computes the cumulative product of the matrix along a specified axis.\n",
    "        \n",
    "        Args:\n",
    "            axis (Optional[int]): Axis along which to compute the cumulative product. If None, computes across all dimensions.\n",
    "    \n",
    "        Returns:\n",
    "            Matrix: A new instance containing the computed cumulative product values.\n",
    "        \n",
    "        Raises:\n",
    "            AttributeError: If no data attribute exists in the instance.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.cumprod(self.data, axis=axis)\n",
    "        else:\n",
    "            result = torch.cumprod(self.data, dim=axis)\n",
    "        return Matrix(result, backend=self._backend)\n",
    "    \n",
    "    def exp(self):\n",
    "        \"\"\"\n",
    "        Computes the element-wise exponential.\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: New matrix with exponential applied.\n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.exp(self.data)\n",
    "        else:\n",
    "            result = torch.exp(self.data)\n",
    "        return Matrix(result, backend=self._backend)\n",
    "    \n",
    "    def sin(self):\n",
    "        \"\"\"\n",
    "        Computes the element-wise sine.\n",
    "        \n",
    "        Args: \n",
    "            None\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: A new matrix containing the sine values of the original data.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.sin(self.data)\n",
    "        else:\n",
    "            result = torch.sin(self.data)\n",
    "        return Matrix(result, backend=self._backend)\n",
    "    \n",
    "    def cos(self):\n",
    "        \"\"\"\n",
    "        Computes the element-wise cosine.\n",
    "        \n",
    "        Args: \n",
    "            None\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: A new matrix containing the cosine values of the original data.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.cos(self.data)\n",
    "        else:\n",
    "            result = torch.cos(self.data)\n",
    "        return Matrix(result, backend=self._backend)\n",
    "    \n",
    "    def tan(self):\n",
    "        \"\"\"\n",
    "        Computes the element-wise tangent.\n",
    "        \n",
    "        Args: \n",
    "            None\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: A new matrix containing the tangent values of the original data.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.tan(self.data)\n",
    "        else:\n",
    "            result = torch.tan(self.data)\n",
    "        return Matrix(result, backend=self._backend)\n",
    "    \n",
    "    def sinh(self):\n",
    "        \"\"\"\n",
    "        Computes the element-wise hyperbolic sine.\n",
    "        \n",
    "        Args: \n",
    "            None\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: A new matrix containing the hyperbolic sine values of the original data.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.sinh(self.data)\n",
    "        else:\n",
    "            result = torch.sinh(self.data)\n",
    "        return Matrix(result, backend=self._backend)\n",
    "    \n",
    "    def cosh(self):\n",
    "        \"\"\"\n",
    "        Computes the element-wise hyperbolic cosine.\n",
    "        \n",
    "        Args: \n",
    "            None\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: A new matrix containing the hyperbolic cosine values of the original data.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.cosh(self.data)\n",
    "        else:\n",
    "            result = torch.cosh(self.data)\n",
    "        return Matrix(result, backend=self._backend)\n",
    "    \n",
    "    def tanh(self):\n",
    "        \"\"\"\n",
    "        Computes the element-wise hyperbolic tangent.\n",
    "        \n",
    "        Args: \n",
    "            None\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: A new matrix containing the hyperbolic tangent values of the original data.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.tanh(self.data)\n",
    "        else:\n",
    "            result = torch.tanh(self.data)\n",
    "        return Matrix(result, backend=self._backend)\n",
    "    \n",
    "    def abs(self):\n",
    "        \"\"\"\n",
    "        Computes the element-wise absolute values.\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: New matrix with absolute values applied.\n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.abs(self.data)\n",
    "        else:\n",
    "            result = torch.abs(self.data)\n",
    "        return Matrix(result, backend=self._backend)\n",
    "    \n",
    "    def relu(self):\n",
    "        \"\"\"\n",
    "        Computes the element-wise ReLU function.\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: New matrix with ReLU applied.\n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.clip(self.data, 0.0)\n",
    "        else:\n",
    "            result = torch.relu(self.data)\n",
    "        return Matrix(result, backend=self._backend)\n",
    "    \n",
    "    def log(self):\n",
    "        \"\"\"\n",
    "        Computes the element-wise natural logarithm.\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: New matrix with logarithm applied.\n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.log(self.data)\n",
    "        else:\n",
    "            result = torch.log(self.data)\n",
    "        return Matrix(result, backend=self._backend)\n",
    "    \n",
    "    def gamma(self):\n",
    "        \"\"\"\n",
    "        Computes the element-wise Gamma function.\n",
    "        \n",
    "        Args: \n",
    "            None\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: A new matrix containing the Gamma function values for each element in the original data.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = sp.special.gamma(self.data)\n",
    "        else:\n",
    "            def torch_gamma(x):\n",
    "                pos = torch.exp(torch.lgamma(x))\n",
    "                neg = torch.pi / (torch.sin(torch.pi * x) * torch.exp(torch.lgamma(1 - x)))\n",
    "                return torch.where(x > 0, pos, neg)\n",
    "            result = torch_gamma(self.data)\n",
    "        return Matrix(result, backend=self._backend) \n",
    "    \n",
    "    def loggamma(self):\n",
    "        \"\"\"\n",
    "        Computes the element-wise natural logarithm of the Gamma function.\n",
    "        \n",
    "        Args: \n",
    "            None\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: A new matrix containing the natural logarithm of the Gamma function values for each element in the original data.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = sp.special.gammaln(self.data)\n",
    "        else:\n",
    "            result = torch.special.gammaln(self.data)\n",
    "        return Matrix(result, backend=self._backend) \n",
    "    \n",
    "    def sigmoid(self):\n",
    "        \"\"\"\n",
    "        Applies the standard sigmoid function element-wise on the input Matrix.\n",
    "        \n",
    "        f(x) = L / (1 + exp(-1*x))\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: A new Matrix with the sigmoid function applied element-wise.\n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = 1.0 / (1.0 + np.exp(-1.0 * self.data ))\n",
    "        else:\n",
    "            result = torch.sigmoid(self.data)\n",
    "        return Matrix(result, backend=self._backend)\n",
    "    \n",
    "    def logistic(self, L=1.0, k=1.0, x0=0.0):\n",
    "        \"\"\"\n",
    "        Applies the logistic (sigmoid) function element-wise on the input Matrix.\n",
    "        \n",
    "        f(x) = L / (1 + exp(-k*(x - x0)))\n",
    "        \n",
    "        Args:\n",
    "            L (float): The curve's maximum value.\n",
    "            k (float): The steepness of the curve.\n",
    "            x0 (float): The x-value of the sigmoid's midpoint.\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: A new Matrix with the logistic function applied element-wise.\n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = L / (1.0 + np.exp(-k * (self.data - x0)))\n",
    "        else:\n",
    "            result = L / (1.0 + torch.exp(-k * (self.data - x0)))\n",
    "        return Matrix(result, backend=self._backend)\n",
    "    \n",
    "    def logistic_inv(self, L=1.0, k=1.0, x0=0.0):\n",
    "        \"\"\"\n",
    "        Applies the inverse of the logistic (sigmoid) function element-wise on the input Matrix.\n",
    "        \n",
    "        f⁻¹(y) = x0 - (1/k)*ln((L - y)/y)\n",
    "        \n",
    "        Args:\n",
    "            L (float): The curve's maximum value used in the logistic function.\n",
    "            k (float): The steepness of the curve used in the logistic function.\n",
    "            x0 (float): The sigmoid's midpoint used in the logistic function.\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: A new Matrix with the inverse logistic function applied element-wise.\n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = x0 - (1/k) * np.log((L - self.data) / self.data)\n",
    "        else:\n",
    "            result = x0 - (1/k) * torch.log((L - self.data) / self.data)\n",
    "        return Matrix(result, backend=self._backend)\n",
    "    \n",
    "    def cholesky(self, upper = False):\n",
    "        \"\"\"\n",
    "        Computes the Cholesky decomposition of a symmetric positive-definite matrix.\n",
    "        L @ U = self.data\n",
    "        returns L if upper = False else U\n",
    "        \n",
    "        Args:\n",
    "            upper (bool): If True, compute the upper triangular factor. Default is False.\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: A new matrix containing the lower triangular factor of the original data if `upper` is False,\n",
    "                    or its transpose if `upper` is True.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.linalg.cholesky(self.data)\n",
    "            if upper == True:\n",
    "                result = result.T\n",
    "        else:\n",
    "            result = torch.cholesky(self.data, upper = upper)\n",
    "        return Matrix(result, backend=self._backend)\n",
    "    \n",
    "    def softmax(self, axis = 1, keepdims:bool | None = True):\n",
    "        \"\"\"\n",
    "        Applies the softmax function along a specified axis.\n",
    "    \n",
    "        Args:\n",
    "            axis (int): Axis along which to apply the softmax. Default is 1.\n",
    "            keepdims (bool or None): Whether to keep the reduced dimensions as axes with size one. \n",
    "                                    If `True`, the shape of the result will be the same as input; otherwise, it will not have these dimensions.\n",
    "    \n",
    "        Returns:\n",
    "            Matrix: A new matrix containing the softmax values along the specified axis.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            if keepdims is not None and axis is not None:\n",
    "                e_x = np.exp(self.data - np.max(self.data, axis=axis, keepdims=keepdims))\n",
    "                result = e_x / e_x.sum(axis=axis, keepdims=keepdims)\n",
    "            else:\n",
    "                e_x = np.exp(self.data - np.max(self.data, axis=axis))\n",
    "                result = e_x / e_x.sum(axis=axis)\n",
    "        else:\n",
    "            result = torch.nn.functional.softmax(self.data, dim=axis)\n",
    "        return Matrix(result, backend=self._backend)\n",
    "\n",
    "    def argmax(self, axis = 1):\n",
    "        \"\"\"\n",
    "         Computes the indices of the maximum values along a specified axis.\n",
    "         Reminder -> Shapre will be shrinked by 1 so that you may need to reshape() it.\n",
    "        \n",
    "         Args:\n",
    "             axis (int | None): Axis along which to compute the argmax. Default is 1.\n",
    "         \n",
    "         Returns:\n",
    "             Matrix: A new matrix containing the indices of the maximum values along the specified axis.\n",
    "         \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.argmax(self.data, axis=axis)\n",
    "        else:\n",
    "            result = torch.argmax(self.data, dim=axis)\n",
    "        return Matrix(result, backend=self._backend)\n",
    "    \n",
    "    def argmin(self, axis = 1):\n",
    "        \"\"\"\n",
    "        Computes the indices of the minimum values along a specified axis.\n",
    "        Reminder -> Shapre will be shrinked by 1 so that you may need to reshape() it.\n",
    "    \n",
    "        Args:\n",
    "            axis (int | None): Axis along which to compute the argmin. Default is 1.\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: A new matrix containing the indices of the minimum values along the specified axis.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.argmin(self.data, axis=axis)\n",
    "        else:\n",
    "            result = torch.argmin(self.data, dim=axis)\n",
    "        return Matrix(result, backend=self._backend)    \n",
    "    \n",
    "    def flatten(self, major = \"row\"):\n",
    "        \"\"\"\n",
    "        Returns the flattened matrix.\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: The flattened matrix.\n",
    "        \"\"\"\n",
    "        if major == \"col\":\n",
    "            x = self.transpose()\n",
    "            return Matrix(x.data.flatten(), backend=self._backend)\n",
    "        elif major == \"row\":\n",
    "            return Matrix(self.data.flatten(), backend=self._backend)\n",
    "        else:\n",
    "            raise ValueError(\"major must be either 'row' or 'column'!\")\n",
    "            \n",
    "    def reverse(self, axis = 0):\n",
    "        \"\"\"\n",
    "        Reverse the flattened matrix.\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: The reversed matrix.\n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            return Matrix(np.flip(self.data, axis=axis), backend=self._backend)\n",
    "        else:\n",
    "            return Matrix(torch.flip(self.data, axis=axis), backend=self._backend)\n",
    "            \n",
    "    def stack(self, *wargs, axis = 0):\n",
    "        \"\"\"\n",
    "        Stack data in sequence on an axis.\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: The stacked matrix.\n",
    "        \"\"\"\n",
    "        data_list = [self.data]\n",
    "        for arg in wargs:\n",
    "            data_list.append(arg.data)\n",
    "        if self._is_numpy:\n",
    "            result = np.stack(data_list, axis=axis)\n",
    "        else:\n",
    "            result = torch.stack(data_list, dim=axis)\n",
    "        return Matrix(result, backend=self._backend)\n",
    "        \n",
    "    def vstack(self, *wargs):\n",
    "        \"\"\"\n",
    "        Stack data in sequence vertically (row wise).\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: The vstacked matrix.\n",
    "        \"\"\"\n",
    "        data_list = [self.data]\n",
    "        for arg in wargs:\n",
    "            data_list.append(arg.data)\n",
    "        if self._is_numpy:\n",
    "            result = np.vstack(data_list)\n",
    "        else:\n",
    "            result = torch.vstack(data_list)\n",
    "        return Matrix(result, backend=self._backend)\n",
    "    \n",
    "    def hstack(self, *wargs):\n",
    "        \"\"\"\n",
    "        Stack data in sequence horizontally (col wise).\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: The hstacked matrix.\n",
    "        \"\"\"\n",
    "        data_list = [self.data]\n",
    "        for arg in wargs:\n",
    "            data_list.append(arg.data)\n",
    "        if self._is_numpy:\n",
    "            result = np.hstack(data_list)\n",
    "        else:\n",
    "            result = torch.hstack(data_list)\n",
    "        return Matrix(result, backend=self._backend)\n",
    "    \n",
    "    def sign(self):\n",
    "        \"\"\"\n",
    "        Computes the element-wise sign of the data. Returns with the same type.\n",
    "    \n",
    "        Args: \n",
    "            None\n",
    "    \n",
    "        Returns:\n",
    "            Matrix: A new matrix containing the sign values (1 for positive, -1 for negative, 0 for zero) of each element in the original data.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.sign(self.data)\n",
    "        else:\n",
    "            result = torch.sign(self.data)\n",
    "        return Matrix(result, backend=self._backend)\n",
    "    \n",
    "    def repeat(self, repeats, axis=None):\n",
    "        \"\"\"\n",
    "        Repeats the matrix elements along a specified axis.\n",
    "    \n",
    "        Args:\n",
    "            repeats (int or tuple[int]): The number of times to repeat each element.\n",
    "            axis (int): Axis along which to repeat the elements. If `None`, repeats over all dimensions.\n",
    "    \n",
    "        Returns:\n",
    "            Matrix: A new matrix with repeated elements.\n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = self.data.repeat(repeats, axis=axis)\n",
    "        else:\n",
    "            if axis is None:\n",
    "                result = torch.repeat_interleave(self.data, repeats)\n",
    "            else:\n",
    "                result = torch.repeat_interleave(self.data, repeats, dim=axis)\n",
    "        return Matrix(result, backend=self._backend)\n",
    "    \n",
    "    def bincount(self, *, weights = None, inttype: type = int):\n",
    "        \"\"\"\n",
    "        Counts the number of occurrences of each value in `data` and optionally returns a weighted count.\n",
    "    \n",
    "        Args:\n",
    "            weights (array_like | Matrix): An array-like object containing weights corresponding to each element in `data`. Default is None.\n",
    "            inttype (type): A type that the data is going to be casted to.\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: A new matrix with the bin counts or weighted bin counts.\n",
    "    \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.bincount(self.astype(inttype).data, weights = weights)\n",
    "        else:\n",
    "            result = torch.bincount(self.astype(inttype).data, weights = weights)\n",
    "        return Matrix(result, backend=self._backend)\n",
    "    \n",
    "    def transpose(self, *axes):\n",
    "        \"\"\"\n",
    "        Returns the transpose of the matrix.\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: The transposed matrix.\n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = self.data.transpose(axes) if axes else self.data.T\n",
    "        else:\n",
    "            result = self.data.permute(*axes) if axes else self.data.permute(*reversed(range(self.data.dim())))\n",
    "        if len(self.shape) > 1:\n",
    "            return Matrix(result, backend=self._backend)\n",
    "        else:\n",
    "            # From a row vector to a column vector\n",
    "            return Matrix(result.reshape([self.shape[0], 1]), backend=self._backend)\n",
    "        \n",
    "    def quantile(self, q: float, axis = None, keepdims = False):\n",
    "        \"\"\"\n",
    "        Computes the specified quantiles along a given axis.\n",
    "    \n",
    "        Args:\n",
    "            q (float): The quantile to compute. Should be between 0 and 1.\n",
    "            axis (Optional[int]): Axis along which to compute the quantile. Default is None, which computes over all dimensions.\n",
    "            keepdims (bool): Whether to keep the reduced axes in the result as singleton dimensions. Default is False.\n",
    "    \n",
    "        Returns:\n",
    "            Matrix: A new matrix containing the computed quantiles.\n",
    "    \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.quantile(self.data, q, axis = axis, keepdims = keepdims)\n",
    "        else:\n",
    "            if axis is None:\n",
    "                result = torch.quantile(self.data, q, keepdims = keepdims)\n",
    "            else:\n",
    "                result = torch.quantile(self.data, q, dim = axis, keepdims = keepdims)\n",
    "        return Matrix(result, backend=self._backend)\n",
    "        \n",
    "    def sort(self, axis: int | None = None):\n",
    "        \"\"\"\n",
    "        Sorts the matrix elements along (the first column of) a specified axis.\n",
    "        If you intend to sort on only one array, use `sort_along` instead,\n",
    "        or if you want to sort along each column of each base dimension, use `sort_along_each_column` instead.\n",
    "        \n",
    "        Args:\n",
    "            axis (int or None): Axis to sort along. If `None`, sorts the entire matrix.\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: A new matrix with sorted elements.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.sort(self.data, axis=axis)\n",
    "        else:\n",
    "            if axis is None:\n",
    "                result, idx = self.data.sort()\n",
    "            else:\n",
    "                result, idx = self.data.sort(dim=axis)\n",
    "        return Matrix(result, backend=self._backend)\n",
    "    \n",
    "    def sort_along(self, axis: tuple = (None, 0)):\n",
    "        \"\"\"\n",
    "        Sort the N-dimensional data along the 1d values on `axis`.\n",
    "        If you intend to sort on the first column of axis `axis`, use `sort` to speed up,\n",
    "        or if you want to sort along each column of each base dimension, use `sort_along_each_column` instead.\n",
    "       \n",
    "        Detail:\n",
    "        Sort the input array x. The axis parameter is a tuple of t\n",
    "        he same length as x.ndim, where each position can be None or an integer. \n",
    "        It is required that exactly one position d in axis_vec is not None,\n",
    "        and the reference position of this dimension d is fixed = axis_vec[d], \n",
    "        but when taking this reference, the index 0 is selected for each dimension \n",
    "        in the global uniform way (i.e., only x[(0,)*d + (fixed,)] is used as the \n",
    "        reference). \n",
    "        The 1D sorting permutation is calculated (using argsort, in ascending order), \n",
    "        and then the global permutation is applied to dimension d+1 \n",
    "        (the subsequent dimension) of x, acting on all data without sorting each\n",
    "        preceding block separately.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "            axis : tuple\n",
    "                The indicator indicating sort which data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            Matrix, sorted copy.\n",
    "\n",
    "        \"\"\"\n",
    "        if len(self.data.shape) != len(axis):\n",
    "            raise ValueError(\"The length of axis must be equal to the number of dimensions of the input array.\")\n",
    "        non_none = [(d, val) for d, val in enumerate(axis) if val is not None]\n",
    "        if len(non_none) != 1:\n",
    "            raise ValueError(\"There must be exactly one non-None element in axis\")\n",
    "        d, fixed = non_none[0]\n",
    "        if d > len(self.data.shape) - 1:\n",
    "            raise ValueError(\"The non-None dimension cannot be greater than dimension.\")\n",
    "        if fixed < 0 or fixed >= self.data.shape[d]:\n",
    "            raise IndexError(f\"Fixed index {fixed} is out of range for dimension {d} (0 to {self.data.shape[d]-1})\")\n",
    "        \n",
    "        # Last dim sort\n",
    "        if d == len(self.data.shape) - 1 and len(self.data.shape) == 2:\n",
    "            # Transpose, and sort the row\n",
    "            return self.transpose().sort_along(axis=(axis[1], axis[0])).transpose()\n",
    "        elif d == len(self.data.shape) - 1 and len(self.data.shape) > 2:\n",
    "            # Transpose the inner two dimensions\n",
    "            tr_axe = list(range(len(self.data.shape))); tmp = tr_axe[-1]; tr_axe[-1] = tr_axe[-2]; tr_axe[-2] = tmp;\n",
    "            axis_new = list(np.repeat(None, len(self.data.shape))); axis_new[-2] = fixed\n",
    "            return self.transpose(*tr_axe).sort_along(axis=axis_new).transpose(*tr_axe)\n",
    "        \n",
    "        # Extract global reference: fixed on dimension d, but all dimensions before d are indexed as 0.\n",
    "        # Construct index tuple: fixed to 0 for dimensions < d, fixed to the dth dimension, and use slice(None) to eliminate the remaining axes.\n",
    "        idx = (0,) * d + (fixed,)\n",
    "        \n",
    "        if self._is_numpy:\n",
    "            # Extract the reference key, which is expected to be 1D and have a length equal to self.data.shape[d+1]\n",
    "            key = np.asarray(self.data[idx])\n",
    "            if key.ndim != 1 or key.shape[0] != self.data.shape[d+1]:\n",
    "                raise ValueError(\"The reference key must be one-dimensional and its length must be the same as the length of the sorting axis.\")\n",
    "            order = np.argsort(key)\n",
    "           \n",
    "            # Construct a global index array for np.take_along_axis: needs to have the same shape as x,\n",
    "            # but order along sorting axis d+1, other dimensions are copied via broadcasting.\n",
    "            order_shape = [1] * len(self.data.shape)\n",
    "            order_shape[d+1] = self.data.shape[d+1]\n",
    "            order_global = order.reshape(order_shape)\n",
    "            order_global = np.broadcast_to(order_global, self.data.shape)\n",
    "            sorted_ = np.take_along_axis(self.data, order_global, axis=d+1)\n",
    "            return Matrix(sorted_, backend=self._backend, dtype=self.dtype, device=self.device)\n",
    "\n",
    "        else:\n",
    "            key = self.data[idx]\n",
    "            # key should be 1D, and its length should be equal to self.data.shape[d+1]\n",
    "            if key.dim() != 1 or key.size(0) != self.data.shape[d+1]:\n",
    "                raise ValueError(\"The reference key must be one-dimensional and its length must be the same as the length of the sorting axis.\")\n",
    "            # Calculate the sort order (ascending)\n",
    "            order = torch.argsort(key, dim=0)\n",
    "            \n",
    "            # Construct a global index tensor with the same shape as self.data, but with order on the sorting axis d+1\n",
    "            order_shape = [1] * len(self.data.shape)\n",
    "            order_shape[d+1] = self.data.shape[d+1]\n",
    "            order_global = order.view(*order_shape).expand(self.data.shape)\n",
    "            \n",
    "            # Use torch.gather to rearrange self.data according to the global index tensor on dim=d+1\n",
    "            sorted_ = torch.gather(self.data, dim=d+1, index=order_global)\n",
    "            return Matrix(sorted_, backend=self._backend, dtype=self.dtype, device=self.device)\n",
    "        \n",
    "    def sort_along_each_column(self, axis: int = 1, on_col: int = 0):\n",
    "        \"\"\"\n",
    "        Sort the N-dimensional data along values on column `on_col` of the axis `axis`.\n",
    "        Note, it will sort EACH `on_col` of the exterior axises.\n",
    "        If you intend to sort on the first column of axis `axis`, use `sort` to speed up.\n",
    "       \n",
    "        Detail:\n",
    "        Instead of sorting itself in d dimensions, use the reference sequence obtained by taking index=i on the d axis of x, and apply the same rearrangement to the d+1 axis (next axis) of x.\n",
    "\n",
    "        For example, for a 2D array, when d=0, i=1,\n",
    "        take the reference sequence = x[1, :], calculate its argsort to get the sorted arrangement, and then rearrange the columns of each row of the entire array according to this arrangement;\n",
    "        For a 3D array, when d=1, i=0,\n",
    "        for each subarray with fixed axis=0, take the reference sequence = subarray[0, :] (that is, the row of axis=1 index 0),\n",
    "        calculate argsort (sort the elements in the reference sequence), and then rearrange all rows in the subarray (all slices of axis=1) on axis=2 according to this arrangement.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "            axis : int\n",
    "                The axis of the column is on. The default is 1.\n",
    "            on_col : int\n",
    "                The index of the column is on. The default is 0.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            Matrix, sorted copy.\n",
    "\n",
    "        \"\"\"\n",
    "        if len(self.data.shape) < 2:\n",
    "            raise ValueError(\"The input data must at least have 2 dimensions. Use `sort` if it is a 1d array.\")\n",
    "        if axis < 0 or axis > len(self.data.shape) - 1:\n",
    "            raise ValueError(\"The parameter d must be positive and smaller than ndim.\")\n",
    "        if axis == len(self.data.shape) - 1 and len(self.data.shape) == 2:\n",
    "            # Transpose, and sort the row\n",
    "            return self.transpose().sort_along_each_column(axis=0, on_col=on_col).transpose()\n",
    "        elif axis == len(self.data.shape) - 1 and len(self.data.shape) > 2:\n",
    "            # Transpose the inner two dimensions\n",
    "            tr_axe = list(range(len(self.data.shape))); tmp = tr_axe[-1]; tr_axe[-1] = tr_axe[-2]; tr_axe[-2] = tmp;\n",
    "            return self.transpose(*tr_axe).sort_along_each_column(axis=axis-1, on_col=on_col).transpose(*tr_axe)\n",
    "            \n",
    "        sorted_axis = axis + 1\n",
    "\n",
    "        if self._is_numpy:\n",
    "            key = np.take(self.data, indices=on_col, axis=axis)\n",
    "            order = np.argsort(key, axis=axis)\n",
    "            order_expanded = np.expand_dims(order, axis=axis)\n",
    "            sorted_ = np.take_along_axis(self.data, order_expanded, axis=sorted_axis)\n",
    "            return Matrix(sorted_, backend=self._backend, dtype=self.dtype, device=self.device)\n",
    "\n",
    "        else:\n",
    "            key = self.data.select(dim=axis, index=on_col)\n",
    "            order = torch.argsort(key, dim=axis)\n",
    "            order_expanded = order.unsqueeze(dim=axis)\n",
    "            expand_shape = list(self.data.shape)\n",
    "            index = order_expanded.expand(*expand_shape)\n",
    "            sorted_ = torch.gather(self.data, dim=sorted_axis, index=index)\n",
    "            return Matrix(sorted_, backend=self._backend, dtype=self.dtype, device=self.device)        \n",
    "    \n",
    "    def _eigen_kernel(self):\n",
    "        \"\"\"\n",
    "        Calculates eigenvalues and eigenvectors using internal libraries (numpy or torch).\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: eigen_values, eigen_vectors: The computed eigenvalues and eigenvectors.\n",
    "        \"\"\"\n",
    "        if self.shape[0] != self.shape[1]:\n",
    "            raise ValueError(\"Eigen decomposition requires a square matrix.\")\n",
    "        if self._is_numpy:\n",
    "            eigen_values, eigen_vectors = np.linalg.eig(self.data)\n",
    "        else:\n",
    "            # Using torch.linalg.eig (available in newer versions of PyTorch)\n",
    "            eigen_values, eigen_vectors = torch.linalg.eig(self.data)\n",
    "        return Matrix(eigen_values, backend=self._backend), Matrix(eigen_vectors, backend=self._backend)\n",
    "\n",
    "    def _jacobi_eigen(self, tol=1e-10, max_iterations=1000):\n",
    "        \"\"\"\n",
    "        Eigen decomposition using the Jacobi rotation method for symmetric matrices.\n",
    "        \n",
    "        Returns:\n",
    "            eigen_values, eigen_vectors: Sorted in descending order.\n",
    "        \"\"\"\n",
    "        A = self.data.astype(float).copy()\n",
    "        n = A.shape[0]\n",
    "        V = np.eye(n)\n",
    "        for iteration in range(max_iterations):\n",
    "            p, q = 0, 1\n",
    "            max_val = 0\n",
    "            for i in range(n):\n",
    "                for j in range(i+1, n):\n",
    "                    if abs(A[i, j]) > max_val:\n",
    "                        max_val = abs(A[i, j])\n",
    "                        p, q = i, j\n",
    "            if max_val < tol:\n",
    "                break\n",
    "            if abs(A[p, p] - A[q, q]) < tol:\n",
    "                theta = np.pi / 4\n",
    "            else:\n",
    "                theta = 0.5 * np.arctan2(2 * A[p, q], A[q, q] - A[p, p])\n",
    "            c = np.cos(theta)\n",
    "            s = np.sin(theta)\n",
    "            app, aqq, apq = A[p, p], A[q, q], A[p, q]\n",
    "            A[p, p] = c**2 * app - 2 * s * c * apq + s**2 * aqq\n",
    "            A[q, q] = s**2 * app + 2 * s * c * apq + c**2 * aqq\n",
    "            A[p, q] = 0.0\n",
    "            A[q, p] = 0.0\n",
    "            for i in range(n):\n",
    "                if i != p and i != q:\n",
    "                    aip, aiq = A[i, p], A[i, q]\n",
    "                    A[i, p] = c * aip - s * aiq\n",
    "                    A[p, i] = A[i, p]\n",
    "                    A[i, q] = s * aip + c * aiq\n",
    "                    A[q, i] = A[i, q]\n",
    "            for i in range(n):\n",
    "                vip, viq = V[i, p], V[i, q]\n",
    "                V[i, p] = c * vip - s * viq\n",
    "                V[i, q] = s * vip + c * viq\n",
    "        eigen_values = np.diag(A)\n",
    "        idx = np.argsort(eigen_values)[::-1]\n",
    "        eigen_values = eigen_values[idx]\n",
    "        eigen_vectors = V[:, idx]\n",
    "        return Matrix(eigen_values, backend=self._backend), Matrix(eigen_vectors, backend=self._backend)\n",
    "\n",
    "    def _qr_eigen_nonsymmetric(self, tol=1e-10, max_iterations=1000):\n",
    "        \"\"\"\n",
    "        Eigen decomposition for non-symmetric matrices using a basic QR algorithm\n",
    "        for eigenvalues and SVD-based extraction for eigenvectors.\n",
    "        \n",
    "        Returns:\n",
    "            eigen_values, eigen_vectors: Sorted in descending order by modulus.\n",
    "        \"\"\"\n",
    "        # Work in complex to capture possible complex eigenvalues.\n",
    "        A_orig = np.array(self.data, dtype=complex)\n",
    "        n = A_orig.shape[0]\n",
    "        A = A_orig.copy()\n",
    "        for _ in range(max_iterations):\n",
    "            Q, R = np.linalg.qr(A)\n",
    "            A = R @ Q\n",
    "            off_diag = A - np.diag(np.diag(A))\n",
    "            if np.linalg.norm(off_diag) < tol:\n",
    "                break\n",
    "        eigen_values = np.diag(A)\n",
    "        # Compute eigenvectors by solving (A_orig - lambda I)v = 0 via SVD.\n",
    "        eigen_vectors = np.empty((n, n), dtype=complex)\n",
    "        for j, lam in enumerate(eigen_values):\n",
    "            B = A_orig - lam * np.eye(n, dtype=complex)\n",
    "            U, S, Vh = np.linalg.svd(B)\n",
    "            v = Vh.conj().T[:, -1]\n",
    "            v = v / np.linalg.norm(v)\n",
    "            eigen_vectors[:, j] = v\n",
    "        # Sort eigenpairs by descending modulus of eigenvalues.\n",
    "        idx = np.argsort(np.abs(eigen_values))[::-1]\n",
    "        eigen_values = eigen_values[idx]\n",
    "        eigen_vectors = eigen_vectors[:, idx]\n",
    "        return Matrix(eigen_values, backend = self._backend), Matrix(eigen_vectors, backend = self._backend)\n",
    "\n",
    "    def eigen(self, method=\"kernel\", symmetric=None, tol=1e-10, max_iterations=1000):\n",
    "        \"\"\"\n",
    "        Unified eigen decomposition method.\n",
    "        \n",
    "        Args:\n",
    "            method (str): \"kernel\" to use internal libraries; \"selfimpl\" to use the self-implemented solver.\n",
    "            symmetric (bool, optional): If known symmetric; if None, determined automatically.\n",
    "            tol (float): Tolerance for convergence (used in self-implementation).\n",
    "            max_iterations (int): Maximum iterations (used in self-implementation).\n",
    "        \n",
    "        Returns:\n",
    "            eigen_values, eigen_vectors: eigenvalues and eigenvectors.\n",
    "        \"\"\"\n",
    "        if self.shape[0] != self.shape[1]:\n",
    "            raise ValueError(\"Eigen decomposition requires a square matrix.\")\n",
    "        # Determine symmetry if not explicitly provided.\n",
    "        if symmetric is None:\n",
    "            if self._is_numpy:\n",
    "                symmetric = np.allclose(self.data, self.data.T, atol=tol)\n",
    "            else:\n",
    "                symmetric = torch.allclose(self.data, self.data.T)\n",
    "                \n",
    "        if method == \"kernel\":\n",
    "            return self._eigen_kernel()\n",
    "        \n",
    "        elif method == \"selfimpl\":\n",
    "            if self._is_numpy:\n",
    "                if symmetric:\n",
    "                    return self._jacobi_eigen(tol, max_iterations)\n",
    "                else:\n",
    "                    return self._qr_eigen_nonsymmetric(tol, max_iterations)\n",
    "            else:\n",
    "                # Use kernel instead\n",
    "                return self._eigen_kernel()\n",
    "        else:\n",
    "            raise ValueError(\"Unknown eigen method. Use 'kernel' or 'selfimpl'.\")\n",
    "\n",
    "    def _check_square(self):\n",
    "        \"\"\"\n",
    "        Internal helper to ensure the matrix is square (required for determinant, inverse, and trace).\n",
    "        \n",
    "        Raises:\n",
    "            ValueError: If the matrix is not square.\n",
    "        \"\"\"\n",
    "        if len(self.shape) != 2 or self.shape[0] != self.shape[1]:\n",
    "            raise ValueError(\"This operation requires a square matrix.\")\n",
    "    \n",
    "    def determinant(self):\n",
    "        \"\"\"\n",
    "        Computes the determinant of a square matrix.\n",
    "        \n",
    "        Returns:\n",
    "            Scalar: The determinant value.\n",
    "            \n",
    "        Raises:\n",
    "            ValueError: If the matrix is not square.\n",
    "        \"\"\"\n",
    "        self._check_square()\n",
    "        if self._is_numpy:\n",
    "            return np.linalg.det(self.data)\n",
    "        else:\n",
    "            return torch.det(self.data)\n",
    "    \n",
    "    def inverse(self):\n",
    "        \"\"\"\n",
    "        Computes the inverse of a square matrix.\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: The inverse matrix.\n",
    "            \n",
    "        Raises:\n",
    "            ValueError: If the matrix is not square.\n",
    "        \"\"\"\n",
    "        self._check_square()\n",
    "        if self._is_numpy:\n",
    "            inv_data = np.linalg.inv(self.data)\n",
    "        else:\n",
    "            inv_data = torch.inverse(self.data)\n",
    "        return Matrix(inv_data, backend=self._backend)\n",
    "    \n",
    "    def trace(self):\n",
    "        \"\"\"\n",
    "        Computes the trace of a square matrix (sum of diagonal elements).\n",
    "        \n",
    "        Returns:\n",
    "            Scalar: The trace value.\n",
    "            \n",
    "        Raises:\n",
    "            ValueError: If the matrix is not square.\n",
    "        \"\"\"\n",
    "        self._check_square()\n",
    "        if self._is_numpy:\n",
    "            return np.trace(self.data)\n",
    "        else:\n",
    "            return torch.trace(self.data)\n",
    "    \n",
    "    def diag(self):\n",
    "        \"\"\"\n",
    "        Computes the diagonal vector of a square matrix.\n",
    "        Or create a diagonal matrix if 1D matrix.\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: The diagonal vector.\n",
    "            \n",
    "        Raises:\n",
    "            ValueError: If the matrix is not square.\n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            return Matrix(np.diag(self.data), backend=self._backend)\n",
    "        else:\n",
    "            return Matrix(torch.diag(self.data), backend=self._backend)\n",
    "    \n",
    "    def dot(self, other):\n",
    "        \"\"\"\n",
    "        Computes the dot product of this matrix with another Matrix.\n",
    "        \n",
    "        Args:\n",
    "            other (Matrix): The other matrix to multiply.\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: A new Matrix instance with the result of the dot product.\n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            return Matrix(np.dot(self.data, other.data), backend=\"numpy\")\n",
    "        else:\n",
    "            return Matrix(torch.matmul(self.data, other.data), backend=\"torch\")\n",
    "    \n",
    "    def inner(self, other):\n",
    "        \"\"\"\n",
    "        Computes the inner product of this matrix with another Matrix.\n",
    "        \n",
    "        Args:\n",
    "            other (Matrix): The other matrix to multiply.\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: A new Matrix instance with the result of the inner product.\n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            return Matrix(np.inner(self.data, other.data), backend=\"numpy\")\n",
    "        else:\n",
    "            return Matrix(torch.inner(self.data, other.data), backend=\"torch\")\n",
    "    \n",
    "    def outer(self, other):\n",
    "        \"\"\"\n",
    "        Computes the outer product of this matrix with another Matrix.\n",
    "        \n",
    "        Args:\n",
    "            other (Matrix): The other matrix to multiply.\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: A new Matrix instance with the result of the outer product.\n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            return Matrix(np.outer(self.data, other.data), backend=\"numpy\")\n",
    "        else:\n",
    "            return Matrix(torch.outer(self.data, other.data), backend=\"torch\")\n",
    "    \n",
    "    def svd(self, full_matrices=True):\n",
    "        \"\"\"\n",
    "        Computes the Singular Value Decomposition (SVD) of the matrix.\n",
    "        \n",
    "        Args:\n",
    "            full_matrices (bool): If True, compute the full SVD; otherwise, compute the reduced SVD.\n",
    "            \n",
    "        Returns:\n",
    "            Tuple[Matrix, Matrix, Matrix]: A tuple containing U, S, and V^T as Matrix objects.\n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            U, S, Vh = np.linalg.svd(self.data, full_matrices=full_matrices)\n",
    "            return Matrix(U, backend=\"numpy\"), Matrix(S, backend=\"numpy\"), Matrix(Vh, backend=\"numpy\")\n",
    "        else:\n",
    "            if hasattr(torch.linalg, 'svd'):\n",
    "                U, S, Vh = torch.linalg.svd(self.data, full_matrices=full_matrices)\n",
    "            else:\n",
    "                U, S, V = torch.svd(self.data, some=not full_matrices)\n",
    "                Vh = V.t()\n",
    "            return Matrix(U, backend=\"torch\"), Matrix(S, backend=\"torch\"), Matrix(Vh, backend=\"torch\")\n",
    "          \n",
    "    def to_zeros(self):\n",
    "        \"\"\"\n",
    "        Converts the Matrix data into a same shape Matrix with 0s.\n",
    "        \n",
    "        Args: \n",
    "            None\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: a same shape Matrix with 0s.\n",
    "        \n",
    "        \"\"\"\n",
    "        x = self.copy()\n",
    "        x[...] = 0\n",
    "        return x\n",
    "    \n",
    "    def to_ones(self):\n",
    "        \"\"\"\n",
    "        Converts the Matrix data into a same shape Matrix with 1s.\n",
    "        \n",
    "        Args: \n",
    "            None\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: a same shape Matrix with 1s.\n",
    "        \n",
    "        \"\"\"\n",
    "        x = self.copy()\n",
    "        x[...] = 1\n",
    "        return x\n",
    "    \n",
    "    def to_ks(self, k: float | int = 0):\n",
    "        \"\"\"\n",
    "        Converts the Matrix data into a same shape Matrix with ks.\n",
    "        \n",
    "        Args: \n",
    "            None\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: a same shape Matrix with 0s.\n",
    "        \n",
    "        \"\"\"\n",
    "        x = self.copy()\n",
    "        x[...] = k\n",
    "        return x        \n",
    "\n",
    "    def to_rands(self):\n",
    "        \"\"\"\n",
    "        Converts the Matrix data into a same shape Matrix with uniform random numbers.\n",
    "        \n",
    "        Args: \n",
    "            None\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: a same shape Matrix with uniform random numbers.\n",
    "        \n",
    "        \"\"\"\n",
    "        return self.rand(self.shape, backend=self._backend, dtype=self.dtype, device=self.device)\n",
    "\n",
    "    def to_list(self):\n",
    "        \"\"\"\n",
    "        Converts the matrix data into a Python list.\n",
    "        \n",
    "        Args: \n",
    "            None\n",
    "        \n",
    "        Returns:\n",
    "            list: A Python list containing the same elements as `self.data`.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            return self.data.tolist()\n",
    "        else:\n",
    "            return self.data.cpu().tolist()\n",
    "        \n",
    "    def to_numpy_array(self):\n",
    "        \"\"\"\n",
    "        Converts the matrix data into a NumPy array.\n",
    "        \n",
    "        Returns: \n",
    "            np.ndarray: The underlying NumPy array of the matrix.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            return self.data\n",
    "        else:\n",
    "            return self.data.detach().cpu().numpy()\n",
    "        \n",
    "    def to_torch_tensor(self, *, dtype=None, device=None):\n",
    "        \"\"\"\n",
    "        Converts the matrix data into a PyTorch tensor.\n",
    "        \n",
    "        Args: \n",
    "            dtype (torch.dtype or None): The desired data type for the resulting tensor. If not provided,\n",
    "                                         uses the current data type of `self.data`.\n",
    "            device (torch.device or None): The target device to which the tensor should be moved.\n",
    "                                           If not provided, it will use the default device.\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: A PyTorch tensor containing the same data as `self.data`.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            return torch.tensor(self.data, dtype=dtype, device=device)\n",
    "        else:\n",
    "            return self.data\n",
    "        \n",
    "    @staticmethod\n",
    "    def equal(x, other, *, equal_nan=False):\n",
    "        \"\"\"\n",
    "        Compare if two Matrix objects have the same shape and elements.\n",
    "        \n",
    "        Args:\n",
    "            x (Matrix): The one matrix to compare.\n",
    "            other (Matrix): The other matrix to compare.\n",
    "        \n",
    "        Returns:\n",
    "           ``True`` if two matrices have the same size and elements, \n",
    "           ``False`` otherwise.\n",
    "        \"\"\"\n",
    "        if x._is_numpy == True and other._is_numpy == True:\n",
    "            return np.array_equal(x, other, equal_nan=equal_nan)\n",
    "        elif  x._is_numpy == False and other._is_numpy == False:\n",
    "            return torch.equal(x, other)\n",
    "        else:\n",
    "            raise ValueError(\"Input `x` and `other` for comparison must have to have the same backend!\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def gather_along(data, axis, index):\n",
    "        \"\"\"\n",
    "        Gather values on an axis with specified index.\n",
    "        \n",
    "        Parameters:\n",
    "            axis: int, the axis number to gather values on.\n",
    "            index: list | array | Matrix, the indices for each row/column/.. to gather values on.\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: gathered elements.\n",
    "        \"\"\"\n",
    "        if data._is_numpy:\n",
    "            result = np.take_along_axis(data.data, indices=index, axis=axis)\n",
    "        else:\n",
    "            result = torch.gather(data.data, dim=axis, index=index.data)\n",
    "        return Matrix(result, backend=data._backend, dtype=data.dtype, device=data.device)\n",
    "    \n",
    "    @staticmethod\n",
    "    def where(condition, backend=\"numpy\", dtype=None, device=None):\n",
    "        \"\"\"\n",
    "        Returns elements depending on `condition`.\n",
    "        \n",
    "        Parameters:\n",
    "            condition : Internal Type (array_like); bool Where True\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: chosen elements.\n",
    "        \"\"\"\n",
    "        if backend == \"numpy\":\n",
    "            result = np.where(condition)\n",
    "        else:\n",
    "            result = torch.where(condition)         \n",
    "            if isinstance(result, tuple):\n",
    "                result = result[0]\n",
    "        return Matrix(result, backend=backend, dtype=dtype, device=device)\n",
    "    \n",
    "    @staticmethod\n",
    "    def where_as(condition, then, other, backend=\"numpy\", dtype=None, device=None):\n",
    "        \"\"\"\n",
    "        Returns elements depending on `condition`.\n",
    "        \n",
    "        Parameters:\n",
    "            condition : Internal Type (array_like); bool Where True\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: if true then applied then to true elements; other to fales elements.\n",
    "        \"\"\"\n",
    "        if backend == \"numpy\":\n",
    "            result = np.where(condition, then, other)\n",
    "        else:\n",
    "            result = torch.where(condition, then, other)         \n",
    "            if isinstance(result, tuple):\n",
    "                result = result[0]\n",
    "        return Matrix(result, backend=backend, dtype=dtype, device=device)\n",
    "    \n",
    "    @staticmethod\n",
    "    def zeros(shape, backend=\"numpy\", dtype=None, device=None):\n",
    "        \"\"\"\n",
    "        Creates a matrix filled with zeros.\n",
    "        \n",
    "        Args:\n",
    "            shape (tuple): The shape of the matrix.\n",
    "            backend (str): The backend (\"numpy\" or \"torch\").\n",
    "            dtype: Desired data type.\n",
    "            device: Data device, \"cpu\" or \"cuda\".\n",
    "            \n",
    "        Returns:\n",
    "            Matrix: A new matrix of zeros.\n",
    "        \"\"\"\n",
    "        if backend == \"numpy\":\n",
    "            data = np.zeros(shape, dtype=dtype)\n",
    "        elif backend == \"torch\":\n",
    "            data = torch.zeros(shape, dtype=dtype, device=device) if dtype else torch.zeros(shape, device=device)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported backend. Choose 'numpy' or 'torch'.\")\n",
    "        return Matrix(data, backend=backend)\n",
    "    \n",
    "    @staticmethod\n",
    "    def ones(shape, backend=\"numpy\", dtype=None, device=None):\n",
    "        \"\"\"\n",
    "        Creates a matrix filled with ones.\n",
    "        \n",
    "        Args:\n",
    "            shape (tuple): The shape of the matrix.\n",
    "            backend (str): The backend (\"numpy\" or \"torch\").\n",
    "            dtype: Desired data type.\n",
    "            device: Data device, \"cpu\" or \"cuda\".\n",
    "            \n",
    "        Returns:\n",
    "            Matrix: A new matrix of ones.\n",
    "        \"\"\"\n",
    "        if backend == \"numpy\":\n",
    "            data = np.ones(shape, dtype=dtype)\n",
    "        elif backend == \"torch\":\n",
    "            data = torch.ones(shape, dtype=dtype, device=device) if dtype else torch.ones(shape, device=device)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported backend. Choose 'numpy' or 'torch'.\")\n",
    "        return Matrix(data, backend=backend)\n",
    "    \n",
    "    @staticmethod\n",
    "    def zeros_like(x, backend=\"numpy\", dtype=None, device=None):\n",
    "        \"\"\"\n",
    "        Creates a matrix of zeros with the same shape and data type as another matrix.\n",
    "        \n",
    "        Args:\n",
    "            x (Matrix): The input matrix.\n",
    "            backend (str): The backend for computation (\"numpy\" or \"torch\"). Default is \"numpy\".\n",
    "            dtype: Desired data type for the result. If not specified, uses the data type from `x`.\n",
    "            device: Data device, \"cpu\" or \"cuda\".\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: A new matrix containing zeros with the same shape and type as `x`.\n",
    "        \n",
    "        Raises:\n",
    "            ValueError: If an unsupported backend is provided.\n",
    "        \n",
    "        \"\"\"\n",
    "        if backend == \"numpy\":\n",
    "            data = np.zeros_like(x.data, dtype=dtype)\n",
    "        elif backend == \"torch\":\n",
    "            data = torch.zeros_like(x.data, dtype=dtype, device=device) if dtype else torch.zeros_like(x.data, device=device)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported backend. Choose 'numpy' or 'torch'.\")\n",
    "        return Matrix(data, backend=backend)\n",
    "    \n",
    "    @staticmethod\n",
    "    def ones_like(x, backend=\"numpy\", dtype=None, device=None):\n",
    "        \"\"\"\n",
    "        Creates a matrix of ones with the same shape and data type as another matrix.\n",
    "        \n",
    "        Args:\n",
    "            x (Matrix): The input matrix.\n",
    "            backend (str): The backend for computation (\"numpy\" or \"torch\"). Default is \"numpy\".\n",
    "            dtype: Desired data type for the result. If not specified, uses the data type from `x`.\n",
    "            device: Data device, \"cpu\" or \"cuda\".\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: A new matrix containing ones with the same shape and type as `x`.\n",
    "        \n",
    "        Raises:\n",
    "            ValueError: If an unsupported backend is provided.\n",
    "        \n",
    "        \"\"\"\n",
    "        if backend == \"numpy\":\n",
    "            data = np.ones_like(x.data, dtype=dtype)\n",
    "        elif backend == \"torch\":\n",
    "            data = torch.ones_like(x.data, dtype=dtype, device=device) if dtype else torch.ones_like(x.data, device=device)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported backend. Choose 'numpy' or 'torch'.\")\n",
    "        return Matrix(data, backend=backend)\n",
    "    \n",
    "    @staticmethod\n",
    "    def identity(n, backend=\"numpy\", dtype=None):\n",
    "        \"\"\"\n",
    "        Creates an identity matrix of size n x n.\n",
    "        \n",
    "        Args:\n",
    "            n (int): The number of rows and columns.\n",
    "            backend (str): The backend (\"numpy\" or \"torch\").\n",
    "            dtype: Desired data type.\n",
    "            \n",
    "        Returns:\n",
    "            Matrix: An identity matrix.\n",
    "        \"\"\"\n",
    "        bk = backend.lower()\n",
    "        if bk == \"numpy\":\n",
    "            data = np.eye(n, dtype=dtype)\n",
    "        elif bk == \"torch\":\n",
    "            if torch is None:\n",
    "                raise ImportError(\"PyTorch is not installed.\")\n",
    "            data = torch.eye(n, dtype=dtype)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported backend. Choose 'numpy' or 'torch'.\")\n",
    "        return Matrix(data, backend=bk)\n",
    "    \n",
    "    @staticmethod\n",
    "    def rand(shape, backend=\"numpy\", dtype=None, device=None):\n",
    "        \"\"\"\n",
    "        Creates a matrix with random values uniformly distributed in [0, 1).\n",
    "        \n",
    "        Args:\n",
    "            shape (tuple): The shape of the matrix.\n",
    "            backend (str): The backend (\"numpy\" or \"torch\").\n",
    "            dtype: Desired data type.\n",
    "            device: Data device, \"cpu\" or \"cuda\".\n",
    "            \n",
    "        Returns:\n",
    "            Matrix: A new matrix with random values.\n",
    "        \"\"\"\n",
    "        bk = backend.lower()\n",
    "        if bk == \"numpy\":\n",
    "            data = np.random.rand(*shape)\n",
    "            if dtype:\n",
    "                data = data.astype(dtype)\n",
    "        elif bk == \"torch\":\n",
    "            if torch is None:\n",
    "                raise ImportError(\"PyTorch is not installed.\")\n",
    "            data = torch.rand(shape, dtype=dtype, device=device) if dtype else torch.rand(shape, device=device)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported backend. Choose 'numpy' or 'torch'.\")\n",
    "        return Matrix(data, backend=bk)\n",
    "\n",
    "    @staticmethod\n",
    "    def least_square(X, Y, backend=\"numpy\", dtype=None):\n",
    "        \"\"\"\n",
    "        Solves the linear system bX = Y using the normal equation approach.\n",
    "        \n",
    "        Args:\n",
    "            X (Matrix): The matrix of features or independent variables.\n",
    "            Y (Matrix): The matrix of observations or dependent variables.\n",
    "            backend (str): The backend (\"numpy\" or \"torch\").\n",
    "            dtype: Desired data type for the result.\n",
    "            \n",
    "        Returns:\n",
    "            Matrix: The least-squares solution b satisfying bX = Y\n",
    "        \"\"\"    \n",
    "        # Check if the matrix Y is with one column or not\n",
    "        if len(Y.shape) == 1:\n",
    "            Y = Y.reshape([Y.shape[0], 1])\n",
    "        if Y.shape[1] != 1:\n",
    "            raise ValueError(\"The input matrix Y must be of 1 column!\")\n",
    "        \n",
    "        # Compute the least-squares solution (X^T@X)^-1@X^T@Y\n",
    "        X_transpose = X.transpose()\n",
    "        b = (X_transpose @ X).inverse() @ X_transpose @ Y\n",
    "        if dtype:\n",
    "            b = b.astype(dtype)\n",
    "            \n",
    "        return b\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`P. Tensor Wrapper Library (self-implemented)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This tensor class is self implemented and open-sourced\n",
    "# Available at https://github.com/dof-studio/MML/\n",
    "# By Nathmath Huang (bh2821)\n",
    "# License: Apache License Version 2.0\n",
    "\n",
    "class Tensor(Object):\n",
    "    \"\"\"\n",
    "    A production-level Tensor class providing a unified interface for common machine learning operations.\n",
    "    This class supports either a numpy.ndarray or a torch.Tensor as its underlying backend. To optimize performance,\n",
    "    the backend string is processed once at initialization, and boolean flags (_is_numpy and _is_torch) are used to \n",
    "    avoid repeated string comparisons.\n",
    "    \n",
    "    The implemented operations include element-wise arithmetic, matrix multiplication, reshaping, reductions (sum, mean, \n",
    "    max, min), and element-wise exponential and logarithmic functions.\n",
    "    \n",
    "    Attributes:\n",
    "        data (np.ndarray or torch.Tensor): Underlying storage for tensor data.\n",
    "        _backend (str): Lowercase string for the backend (\"numpy\" or \"torch\").\n",
    "        _is_numpy (bool): True if using numpy as the backend.\n",
    "        _is_torch (bool): True if using torch as the backend.\n",
    "    \"\"\"\n",
    "    \n",
    "    __attr__ = \"MML.Tensor\"    \n",
    "    \n",
    "    def e(self):\n",
    "        \"\"\"\n",
    "        Returns natural exponent value as a single-value Matrix.\n",
    "        \n",
    "        Returns:\n",
    "            -------\n",
    "            Tensor with 0 shape. exp value.\n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            return Tensor(np.e, backend = self._backend, dtype = self.dtype, device = self.device)\n",
    "        else:\n",
    "            return Tensor(torch.e, backend = self._backend, dtype = self.dtype, device = self.device)\n",
    "    \n",
    "    def pi(self):\n",
    "        \"\"\"\n",
    "        Returns pi value as a single-value Matrix.\n",
    "        \n",
    "        Returns:\n",
    "            -------\n",
    "            Tensor with 0 shape. pi value.\n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            return Tensor(np.pi, backend = self._backend, dtype = self.dtype, device = self.device)\n",
    "        else:\n",
    "            return Tensor(torch.pi, backend = self._backend, dtype = self.dtype, device = self.device)\n",
    "    \n",
    "    def __init__(self, data, backend=\"numpy\", *, dtype=None, device=None):\n",
    "        \"\"\"\n",
    "        Initializes a Tensor instance with the specified backend.\n",
    "        \n",
    "        Args:\n",
    "            data (array-like): Input data to be converted into a tensor.\n",
    "            backend (str): Backend to use (\"numpy\" or \"torch\").\n",
    "            dtype(str): The type of data to be stored in (any type or None).\n",
    "            device (str): Device where the data is stored on (\"cpu\" or \"cuda\", or None).\n",
    "            \n",
    "        Raises:\n",
    "            ValueError: If an unsupported backend is specified.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self._backend = backend.lower()\n",
    "        if self._backend not in (\"numpy\", \"torch\"):\n",
    "            raise ValueError(\"Unsupported backend. Please choose 'numpy' or 'torch'.\")\n",
    "        self._is_numpy = (self._backend == \"numpy\")\n",
    "        self._is_torch = (self._backend == \"torch\")\n",
    "        \n",
    "        # Convert input data to the appropriate tensor type.\n",
    "        if self._is_numpy:\n",
    "            self.data = np.array(data, dtype=dtype)\n",
    "        else:\n",
    "            if torch is None:\n",
    "                raise ImportError(\"PyTorch is not installed but backend 'torch' was requested.\")\n",
    "            self.data = data.to(device, dtype=dtype) if isinstance(data, torch.Tensor) else torch.tensor(data, device=device, dtype=dtype)\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        Returns a string representation showing backend, shape, and data.\n",
    "        \"\"\"\n",
    "        return f\"Tensor(backend={self._backend}, shape={self.shape}, data=\\n{self.data})\"\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "       \"\"\"\n",
    "       Allows subscription using a[i, j]. \n",
    "       If the result is an array, it returns a new Tensor; otherwise, the scalar value.\n",
    "       \"\"\"\n",
    "       result = self.data[key]\n",
    "       if (self._is_numpy and isinstance(result, np.ndarray)) or (self._is_torch and torch is not None and isinstance(result, torch.Tensor)):\n",
    "           return Tensor(result, backend=self._backend)\n",
    "       return result\n",
    "   \n",
    "    def __setitem__(self, key, value):\n",
    "        \"\"\"\n",
    "        Allows assignment using a[i, j] = value.\n",
    "        If the value is a Tensor instance, its underlying data is used.\n",
    "        \"\"\"\n",
    "        if isinstance(value, Tensor):\n",
    "            value = value.data\n",
    "        self.data[key] = value\n",
    "\n",
    "    @property\n",
    "    def shape(self):\n",
    "        \"\"\"\n",
    "        Retrieves the shape of the tensor.\n",
    "        \"\"\"\n",
    "        return self.data.shape\n",
    "\n",
    "    @property\n",
    "    def dtype(self):\n",
    "        \"\"\"\n",
    "        Retrieves the data type of the tensor elements.\n",
    "        \"\"\"\n",
    "        return self.data.dtype\n",
    "        \n",
    "    @property\n",
    "    def device(self):\n",
    "        \"\"\"\n",
    "        Returns the data device of the tensor elements.\n",
    "        \"\"\"\n",
    "        if self._backend == \"numpy\":\n",
    "            return \"cpu\"\n",
    "        else:\n",
    "            return self.data.device.type\n",
    "        \n",
    "    @property\n",
    "    def requires_grad(self):\n",
    "        \"\"\"\n",
    "        Retrieve whether it requires gradients or not\n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            raise NotImplementedError(\"Autograd is not implemented in numpy backend.\")\n",
    "        else:\n",
    "            return self.data.requires_grad\n",
    "        \n",
    "    def zero_grad(self):\n",
    "        \"\"\"\n",
    "        Clear the Pytorch Gradient if any.\n",
    "        \"\"\"\n",
    "        if self._is_numpy == False:\n",
    "            if self.data.grad is not None:\n",
    "                self.data.grad.zero_()\n",
    "        return self\n",
    "    \n",
    "    def reshape(self, shape):\n",
    "        \"\"\"\n",
    "        Converts the tensor into a new shape.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: A new Tensor object with the specified shape.\n",
    "        \"\"\"\n",
    "        return Tensor(self.data.reshape(shape), backend=self._backend)\n",
    "    \n",
    "    def astype(self, dtype):\n",
    "        \"\"\"\n",
    "        Converts the underlying data to the specified type.\n",
    "        \n",
    "        For the numpy backend, it uses np.ndarray.astype.\n",
    "        For the torch backend, it maps the input (which can be a torch.dtype, a string, or a numpy type)\n",
    "        to the corresponding torch dtype and uses tensor.to(dtype=...).\n",
    "        \n",
    "        Args:\n",
    "            dtype: The desired data type. For numpy, any valid numpy dtype is accepted.\n",
    "                   For torch, this can be a torch.dtype, a string (e.g., \"float32\", \"int64\"),\n",
    "                   or a numpy dtype.\n",
    "                   \n",
    "        Returns:\n",
    "            A new Matrix instance with the data converted to the specified type.\n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            new_data = self.data.astype(dtype)\n",
    "            return Tensor(new_data, backend=\"numpy\")\n",
    "        else:\n",
    "            # Map the input dtype to a torch dtype.\n",
    "            torch_dtype = None\n",
    "            if isinstance(dtype, torch.dtype):\n",
    "                torch_dtype = dtype\n",
    "            elif isinstance(dtype, str):\n",
    "                mapping = {\n",
    "                    \"float32\": torch.float32,\n",
    "                    \"float\": torch.float32,\n",
    "                    \"float64\": torch.float64,\n",
    "                    \"double\": torch.float64,\n",
    "                    \"int32\": torch.int32,\n",
    "                    \"int\": torch.int32,\n",
    "                    \"int64\": torch.int64,\n",
    "                    \"long\": torch.int64,\n",
    "                    \"bool\": torch.bool,\n",
    "                    \"complex64\": torch.complex64,\n",
    "                    \"complex128\": torch.complex128\n",
    "                }\n",
    "                if dtype in mapping:\n",
    "                    torch_dtype = mapping[dtype]\n",
    "                else:\n",
    "                    raise ValueError(f\"Unsupported dtype string: {dtype}\")\n",
    "            elif isinstance(dtype, (np.dtype, type)):\n",
    "                np_dtype = np.dtype(dtype)\n",
    "                mapping = {\n",
    "                    np.dtype(\"float32\"): torch.float32,\n",
    "                    np.dtype(\"float64\"): torch.float64,\n",
    "                    np.dtype(\"int32\"): torch.int32,\n",
    "                    np.dtype(\"int64\"): torch.int64,\n",
    "                    np.dtype(\"bool\"): torch.bool,\n",
    "                    np.dtype(\"complex64\"): torch.complex64,\n",
    "                    np.dtype(\"complex128\"): torch.complex128,\n",
    "                }\n",
    "                if np_dtype in mapping:\n",
    "                    torch_dtype = mapping[np_dtype]\n",
    "                else:\n",
    "                    raise ValueError(f\"Unsupported numpy dtype: {np_dtype}\")\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported dtype argument: {dtype}\")\n",
    "            new_data = self.data.to(dtype=torch_dtype)\n",
    "            return Tensor(new_data, backend=\"torch\")\n",
    "    \n",
    "    def to(self, backend, *, dtype = None, device=None):\n",
    "        \"\"\"\n",
    "        Converts the tensor to the specified backend and moves it to the specified device.\n",
    "        \n",
    "        Args:\n",
    "            backend (str): The target backend (\"numpy\" or \"torch\").\n",
    "            dtype (str, optional): The target type (any numpy or torch type nor None for auto inferenence).\n",
    "            device (str, optional): The target device (\"cpu\" or \"cuda\"). This parameter is only applicable when the target or source is torch.\n",
    "            \n",
    "        Returns:\n",
    "            Tensor: A new Tensor object with data in the target backend and on the specified device.\n",
    "        \"\"\"\n",
    "        target = backend.lower()\n",
    "        # If the target backend is the same as the current one.\n",
    "        if target == self._backend:\n",
    "            if self._is_torch:\n",
    "                # If already torch tensor, just move it to the desired device.\n",
    "                return Tensor(self.data.to(device, dtype = dtype), backend=\"torch\")\n",
    "            return Tensor(self.data, backend=self._backend, device=device, dtype=dtype)\n",
    "        \n",
    "        # Convert to numpy if requested.\n",
    "        if target == \"numpy\":\n",
    "            if self._is_torch:\n",
    "                # Move to CPU first (numpy only works on CPU) then convert to numpy.\n",
    "                return Tensor(self.data.cpu().to(dtype = dtype).numpy(), backend=\"numpy\")\n",
    "        \n",
    "        # Convert to torch if requested.\n",
    "        elif target == \"torch\":\n",
    "            if torch is None:\n",
    "                raise ImportError(\"PyTorch is not installed.\")\n",
    "            if self._is_numpy:\n",
    "                # Create a torch tensor from numpy array.\n",
    "                tensor = torch.tensor(self.data, dtype = dtype, device = device)\n",
    "            else:\n",
    "                tensor = self.data\n",
    "            return Tensor(tensor, backend=\"torch\")\n",
    "        \n",
    "        raise ValueError(\"Unsupported backend conversion.\")\n",
    "\n",
    "    def _apply_op(self, other, op):\n",
    "        \"\"\"\n",
    "        Helper method to apply an element-wise binary operation.\n",
    "        \n",
    "        Args:\n",
    "            other (Tensor or scalar): Other operand.\n",
    "            op (callable): Function applying the desired operation element-wise.\n",
    "            \n",
    "        Returns:\n",
    "            Tensor: New Tensor resulting from the operation.\n",
    "        \"\"\"\n",
    "        other_val = other.data if isinstance(other, Tensor) else other\n",
    "        result = op(self.data, other_val)\n",
    "        return Tensor(result, backend=self._backend)\n",
    "    \n",
    "    def copy(self, *, backend=None, dtype=None, device=None):\n",
    "        \"\"\"\n",
    "        Creates a deep copy of the current tensor with the specified backend and data type.\n",
    "        \n",
    "        Args:\n",
    "            backend (str): The backend for the copied matrix. Default is None.\n",
    "            dtype: Desired data type for the result. Default is None.\n",
    "            device: Device to which the tensor should be moved if applicable. Default is None.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: A deep copy of the current matrix with the specified parameters.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            if backend is None:\n",
    "                return Tensor(self.data.copy(), backend=self._backend, dtype=dtype, device=device)\n",
    "            else:\n",
    "                return Tensor(self.data.copy(), backend=backend, dtype=dtype, device=device)\n",
    "        else:\n",
    "            if backend is None:\n",
    "                return Tensor(self.data.clone().detach(), backend=self._backend, dtype=dtype, device=device)\n",
    "            else:\n",
    "                return Tensor(self.data.clone().detach(), backend=backend, dtype=dtype, device=device)\n",
    "    \n",
    "    def clone(self, *, backend=None, dtype=None, device=None):\n",
    "        \"\"\"\n",
    "        Creates a deep copy of the current tensor with the specified backend and data type without detach.\n",
    "        \n",
    "        Args:\n",
    "            backend (str): The backend for the copied matrix. Default is None.\n",
    "            dtype: Desired data type for the result. Default is None.\n",
    "            device: Device to which the tensor should be moved if applicable. Default is None.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: A deep copy of the current matrix with the specified parameters.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            if backend is None:\n",
    "                return Tensor(self.data.copy(), backend=self._backend, dtype=dtype, device=device)\n",
    "            else:\n",
    "                return Tensor(self.data.copy(), backend=backend, dtype=dtype, device=device)\n",
    "        else:\n",
    "            if backend is None:\n",
    "                return Tensor(self.data.clone(), backend=self._backend, dtype=dtype, device=device)\n",
    "            else:\n",
    "                return Tensor(self.data.clone(), backend=backend, dtype=dtype, device=device)\n",
    "            \n",
    "    def clone_detach(self, *, backend=None, dtype=None, device=None):\n",
    "        \"\"\"\n",
    "        Creates a deep copy of the current tensor with the specified backend and data type with detach.\n",
    "        \n",
    "        Args:\n",
    "            backend (str): The backend for the copied matrix. Default is None.\n",
    "            dtype: Desired data type for the result. Default is None.\n",
    "            device: Device to which the tensor should be moved if applicable. Default is None.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: A deep copy of the current matrix with the specified parameters.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            if backend is None:\n",
    "                return Tensor(self.data.copy(), backend=self._backend, dtype=dtype, device=device)\n",
    "            else:\n",
    "                return Tensor(self.data.copy(), backend=backend, dtype=dtype, device=device)\n",
    "        else:\n",
    "            if backend is None:\n",
    "                return Tensor(self.data.clone().detach(), backend=self._backend, dtype=dtype, device=device)\n",
    "            else:\n",
    "                return Tensor(self.data.clone().detach(), backend=backend, dtype=dtype, device=device)\n",
    "    \n",
    "    def append(self, to_append, axis=0):\n",
    "        \"\"\"\n",
    "        Append a scalar (broadcasted) or an array to the matrix along the specified axis.\n",
    "        The tensor is a general n-dimensional tensor, so the appended data must have the same \n",
    "        shape as the original tensor on all axes except the specified axis.\n",
    "    \n",
    "        Args:\n",
    "            to_append: A scalar or an array-like object (or Tensor instance) to append.\n",
    "            axis (int): Axis along which to append. Negative values are supported.\n",
    "    \n",
    "        Returns:\n",
    "            Tensor: A new Tensor instance with the appended data.\n",
    "        \"\"\"\n",
    "        # Get number of dimensions and normalize the axis.\n",
    "        n_dim = len(self.data.shape)\n",
    "        if axis < 0:\n",
    "            axis = axis % n_dim\n",
    "        if axis >= n_dim:\n",
    "            raise ValueError(f\"Axis {axis} out of bounds for array with {n_dim} dimensions.\")\n",
    "    \n",
    "        orig_shape = self.data.shape\n",
    "    \n",
    "        # If to_append is a Tensor instance, extract its data.\n",
    "        if isinstance(to_append, type(self)):\n",
    "            appended_data = to_append.data\n",
    "        else:\n",
    "            # If to_append is a scalar, create an array/tensor with shape matching the original\n",
    "            # on every axis except the specified axis (which will be 1).\n",
    "            if np.isscalar(to_append):\n",
    "                new_shape = tuple(1 if i == axis else orig_shape[i] for i in range(n_dim))\n",
    "                if self._is_numpy:\n",
    "                    appended_data = np.full(new_shape, to_append, dtype=self.data.dtype)\n",
    "                else:\n",
    "                    appended_data = torch.full(new_shape, to_append, dtype=self.data.dtype, device=self.data.device)\n",
    "            elif isinstance(to_append, Tensor):\n",
    "                # Convert to array/tensor using the appropriate backend.\n",
    "                if self._is_numpy:\n",
    "                    appended_data = np.array(to_append.data.copy(), dtype=self.data.dtype)\n",
    "                else:\n",
    "                    appended_data = torch.tensor(to_append.data.clone().detach(), dtype=self.data.dtype, device=self.data.device)\n",
    "            else:\n",
    "                # Convert to array/tensor using the appropriate backend.\n",
    "                if self._is_numpy:\n",
    "                    appended_data = np.array(to_append.copy(), dtype=self.data.dtype)\n",
    "                else:\n",
    "                    appended_data = torch.tensor(to_append.clone().detach(), dtype=self.data.dtype, device=self.data.device)\n",
    "        \n",
    "        # If appended_data has one less dimension, expand it along the specified axis.\n",
    "        if len(appended_data.shape) == n_dim - 1:\n",
    "            if self._is_numpy:\n",
    "                appended_data = np.expand_dims(appended_data, axis=axis)\n",
    "            else:\n",
    "                appended_data = torch.unsqueeze(appended_data, dim=axis)\n",
    "        elif len(appended_data.shape) != n_dim:\n",
    "            raise ValueError(\"Appended data must have either the same number of dimensions as the original Tensor or one less.\")\n",
    "        \n",
    "        # Validate shape compatibility: for all dimensions except the specified axis, sizes must match.\n",
    "        for i in range(n_dim):\n",
    "            if i != axis and appended_data.shape[i] != orig_shape[i]:\n",
    "                raise ValueError(f\"Shape mismatch at dimension {i}: expected {orig_shape[i]}, got {appended_data.shape[i]}.\")\n",
    "        \n",
    "        # Concatenate along the specified axis.\n",
    "        if self._is_numpy:\n",
    "            new_data = np.concatenate((self.data, appended_data), axis=axis)\n",
    "        else:\n",
    "            new_data = torch.cat((self.data, appended_data), dim=axis)\n",
    "        \n",
    "            # Return a new Matrix instance with the updated data.\n",
    "        return Tensor(new_data, backend=self._backend)\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        \"\"\" Equals operator. \"\"\"\n",
    "        if isinstance(other, Object):\n",
    "            if np.prod(self.data.shape) == 1 and np.prod(other.data.shape) == 1:\n",
    "                # Scalar to scalar, output a scalar\n",
    "                return bool(self.flatten().data == other.flatten().data)\n",
    "            else:\n",
    "                return Tensor(self.data == other.data, backend=self._backend)\n",
    "                \n",
    "        elif np.prod(self.data.shape) == 1 and hasattr(other, \"__len__\") == False:\n",
    "            # Scalar to scalar, output a scalar\n",
    "            return bool(self.data == other)\n",
    "        else:\n",
    "            return Tensor(self.data == other, backend=self._backend)\n",
    "\n",
    "    def __add__(self, other):\n",
    "        \"\"\"Element-wise addition.\"\"\"\n",
    "        other_val = other.data if isinstance(other, Object) else other\n",
    "        return Tensor(self.data + other_val, backend=self._backend)\n",
    "\n",
    "    def __radd__(self, other):\n",
    "        \"\"\"Right-hand element-wise addition.\"\"\"\n",
    "        other_val = other.data if isinstance(other, Object) else other\n",
    "        return Tensor(other_val + self.data, backend=self._backend)\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        \"\"\"Element-wise subtraction.\"\"\"\n",
    "        other_val = other.data if isinstance(other, Object) else other\n",
    "        return Tensor(self.data - other_val, backend=self._backend)\n",
    "\n",
    "    def __rsub__(self, other):\n",
    "        \"\"\"Right-hand element-wise subtraction.\"\"\"\n",
    "        other_val = other.data if isinstance(other, Object) else other\n",
    "        return Tensor(other_val - self.data, backend=self._backend)\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        \"\"\"Element-wise multiplication.\"\"\"\n",
    "        other_val = other.data if isinstance(other, Object) else other\n",
    "        return Tensor(self.data * other_val, backend=self._backend)\n",
    "\n",
    "    def __rmul__(self, other):\n",
    "        \"\"\"Right-hand element-wise multiplication.\"\"\"\n",
    "        other_val = other.data if isinstance(other, Object) else other\n",
    "        return Tensor(other_val * self.data, backend=self._backend)\n",
    "\n",
    "    def __truediv__(self, other):\n",
    "        \"\"\"Element-wise true division.\"\"\"\n",
    "        other_val = other.data if isinstance(other, Object) else other\n",
    "        return Tensor(self.data / other_val, backend=self._backend)\n",
    "\n",
    "    def __rtruediv__(self, other):\n",
    "        \"\"\"Right-hand element-wise true division.\"\"\"\n",
    "        other_val = other.data if isinstance(other, Object) else other\n",
    "        return Tensor(other_val / self.data, backend=self._backend)\n",
    "    \n",
    "    def __pow__(self, to_power):\n",
    "        \"\"\"Element-wise power.\"\"\"\n",
    "        return self._apply_op(to_power, lambda a, b: a ** b)\n",
    "\n",
    "    def __rpow__(self, other):\n",
    "        \"\"\"Right-hand element-wise power.\"\"\"\n",
    "        return self.__pow__(other)\n",
    "\n",
    "    def __matmul__(self, other):\n",
    "        \"\"\"\n",
    "        Matrix multiplication using the @ operator.\n",
    "        \n",
    "        Uses the backend's built-in matmul operator.\n",
    "        \"\"\"\n",
    "        other_val = other.data if isinstance(other, Tensor) else other\n",
    "        result = self.data @ other_val\n",
    "        return Tensor(result, backend=self._backend)\n",
    "\n",
    "    def __neg__(self):\n",
    "        \"\"\"Negates the tensor element-wise.\"\"\"\n",
    "        return Tensor(-self.data, backend=self._backend)\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the length of the tensor.\n",
    "        \"\"\"\n",
    "        return len(self.data)\n",
    "    \n",
    "    def requires_grad_(self, requires_grad: bool = False):\n",
    "        \"\"\"\n",
    "        Require gradients to be computed or not. Only support torch.tensor.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: A revised self tensor with gradients opt-in or opt-out.\n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            raise NotImplementedError(\"Autograd is not implemented by numpy backend\")\n",
    "        else:\n",
    "            self.data = self.data.requires_grad_(requires_grad)\n",
    "        return self\n",
    "    \n",
    "    def unique(self):\n",
    "        \"\"\"\n",
    "        Returns the unique values of the elements that are non-zero.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: The unique value tensor.\n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.unique(self.data)\n",
    "        else:\n",
    "            result = torch.unique(self.data)\n",
    "        return Tensor(result, backend=self._backend)\n",
    "    \n",
    "    def nonzero(self):\n",
    "        \"\"\"\n",
    "        Returns the indices of the elements that are non-zero.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: The indices tensor.\n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = self.data.nonzero()\n",
    "        else:\n",
    "            result = self.data.nonzero()\n",
    "        return Tensor(result, backend=self._backend)\n",
    "    \n",
    "    def any(self, axis = None):\n",
    "        \"\"\"\n",
    "        Computes the element-wise logical OR along a specified axis.\n",
    "        \n",
    "        Args: \n",
    "            axis (int or None): Axis along which to apply the `any` operation. If not provided,\n",
    "                                it applies over all elements of the matrix.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: A new Tensor containing the result of the logical OR operation along the specified axis.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.any(self.data, axis=axis)\n",
    "        else:\n",
    "            result = self.data.any(dim=axis)\n",
    "        return Tensor(result, backend=self._backend)\n",
    "    \n",
    "    def all(self, axis = None):\n",
    "        \"\"\"\n",
    "        Computes the element-wise logical AND along a specified axis.\n",
    "    \n",
    "        Args: \n",
    "            axis (int or None): Axis along which to apply the `all` operation. If not provided,\n",
    "                                it applies over all elements of the matrix.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: A new Tensor containing the result of the logical AND operation along the specified axis.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.all(self.data, axis=axis)\n",
    "        else:\n",
    "            result = self.data.all(dim=axis)\n",
    "        return Tensor(result, backend=self._backend)\n",
    "    \n",
    "    def round(self, digits = 0):\n",
    "        \"\"\"\n",
    "        Rounds the data to a specified number of decimal places.\n",
    "    \n",
    "        Args:\n",
    "            digits (int): The number of decimal places to round the data. Default is 0.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: A new Tensor containing the rounded values of the original data.\n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.round(self.data, decimals = digits)\n",
    "        else:\n",
    "            result = torch.round(self.data, decimals = digits)\n",
    "        return Tensor(result, backend=self._backend)\n",
    "    \n",
    "    def mean(self, axis = None):\n",
    "        \"\"\"\n",
    "        Computes the mean of the Tensor along a specified axis.\n",
    "        \n",
    "        Args:\n",
    "            axis (Optional[int]): Axis along which to compute the mean. If None, computes the mean over all dimensions.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: A new instance containing the computed mean values.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.mean(self.data, axis=axis)\n",
    "        else:\n",
    "            result = torch.mean(self.data, dim=axis)\n",
    "        return Tensor(result, backend=self._backend)\n",
    "    \n",
    "    def median(self, axis = None):\n",
    "        \"\"\"\n",
    "        Computes the median along a given axis.\n",
    "    \n",
    "        Args:\n",
    "            axis (Optional[int]): Axis along which to compute the median. Default is None, which computes over all dimensions.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: A new Tensor containing the computed medians.\n",
    "    \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.median(self.data, axis=axis)\n",
    "        else:\n",
    "            if axis is None:\n",
    "                result = torch.median(self.data)\n",
    "            else:\n",
    "                result, _ = torch.median(self.data, dim=axis)\n",
    "        return Tensor(result, backend=self._backend)\n",
    "    \n",
    "    def std(self, axis = None):\n",
    "        \"\"\"\n",
    "        Computes the standard deviation of the Tensor along a specified axis.\n",
    "        \n",
    "        Args:\n",
    "            axis (Optional[int]): Axis along which to compute the standard deviation. If None, computes the mean over all dimensions.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: A new instance containing the computed standard deviation values.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.std(self.data, axis=axis)\n",
    "        else:\n",
    "            result = torch.std(self.data, dim=axis)\n",
    "        return Tensor(result, backend=self._backend)\n",
    "    \n",
    "    def var(self, axis = None):\n",
    "        \"\"\"\n",
    "        Computes the variance of the Tensor along a specified axis.\n",
    "    \n",
    "        Args:\n",
    "            axis (Optional[int]): Axis along which to compute the variance. If None, computes the mean over all dimensions.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: A new instance containing the computed variance values.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.var(self.data, axis=axis)\n",
    "        else:\n",
    "            result = torch.var(self.data, dim=axis)\n",
    "        return Tensor(result, backend=self._backend)\n",
    "    \n",
    "    def min(self, axis = None):\n",
    "        \"\"\"\n",
    "        Computes the minimum of the Tensor along a specified axis.\n",
    "        \n",
    "        Args:\n",
    "            axis (Optional[int]): Axis along which to compute the minimum. If None, computes the mean over all dimensions.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: A new instance containing the computed minimum values.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            if axis is None:\n",
    "                result = np.min(self.data)\n",
    "            else:\n",
    "                result = np.min(self.data, axis=axis)\n",
    "        else:\n",
    "            if axis is None:\n",
    "                result = torch.min(self.data)\n",
    "            else:\n",
    "                result, indices = torch.min(self.data, dim=axis)\n",
    "        return Tensor(result, backend=self._backend)\n",
    "    \n",
    "    def max(self, axis = None):\n",
    "        \"\"\"\n",
    "        Computes the maximum of the Tensor along a specified axis.\n",
    "    \n",
    "        Args:\n",
    "            axis (Optional[int]): Axis along which to compute the maximum. If None, computes the maximum over all dimensions.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: A new instance containing the computed maximum values.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            if axis is None:\n",
    "                result = np.max(self.data)\n",
    "            else:\n",
    "                result = np.max(self.data, axis=axis)\n",
    "        else:\n",
    "            if axis is None:\n",
    "                result = torch.max(self.data)\n",
    "            else:\n",
    "                result, indices = torch.max(self.data, dim=axis)\n",
    "        return Tensor(result, backend=self._backend)\n",
    "    \n",
    "    def clip(self, a_min=None, a_max=None):\n",
    "        \"\"\"\n",
    "        Clips the values of the matrix to a specified range.\n",
    "    \n",
    "        Args: \n",
    "            a_min (float or None): Minimum value for clipping. If None, no minimum is applied.\n",
    "            a_max (float or None): Maximum value for clipping. If None, no maximum is applied.\n",
    "    \n",
    "        Returns:\n",
    "            Tensor: A new instance containing the clipped values of the original data within the specified range.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.clip(self.data, a_min=a_min, a_max=a_max)\n",
    "        else:\n",
    "            result = torch.clip(self.data, min=a_min, max=a_max)\n",
    "        return Tensor(result, backend=self._backend)\n",
    "\n",
    "    def sum(self, axis = None, keepdims = False):\n",
    "        \"\"\"\n",
    "        Computes the sum of the Tensor along a specified axis.\n",
    "        \n",
    "        Args:\n",
    "            axis (Optional[int]): Axis along which to compute the sum. If None, computes across all dimensions.\n",
    "            keepdims (bool): If keeps the dimension or not.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: A new instance containing the computed sum values.\n",
    "        \n",
    "        Raises:\n",
    "            AttributeError: If no data attribute exists in the instance.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.sum(self.data, axis=axis, keepdims=keepdims)\n",
    "        else:\n",
    "            result = torch.sum(self.data, dim=axis, keepdim=keepdims)\n",
    "        return Tensor(result, backend=self._backend)\n",
    "    \n",
    "    def cumsum(self, axis = None):\n",
    "        \"\"\"\n",
    "        Computes the cumulative sum of the Tensor along a specified axis.\n",
    "        \n",
    "        Args:\n",
    "            axis (Optional[int]): Axis along which to compute the cumulative sum. If None, computes across all dimensions.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: A new instance containing the computed cumulative sum values.\n",
    "        \n",
    "        Raises:\n",
    "            AttributeError: If no data attribute exists in the instance.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.cumsum(self.data, axis=axis)\n",
    "        else:\n",
    "            result = torch.cumsum(self.data, dim=axis)\n",
    "        return Tensor(result, backend=self._backend)\n",
    "    \n",
    "    def prod(self, axis = None, keepdims = False):\n",
    "        \"\"\"\n",
    "        Computes the product of the Tensor along a specified axis.\n",
    "        \n",
    "        Args:\n",
    "            axis (Optional[int]): Axis along which to compute the product. If None, computes across all dimensions.\n",
    "            keepdims (bool): If keeps the dimension or not.\n",
    "    \n",
    "        Returns:\n",
    "            Tensor: A new instance containing the computed product values.\n",
    "        \n",
    "        Raises:\n",
    "            AttributeError: If no data attribute exists in the instance.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.prod(self.data, axis=axis, keepdims=keepdims)\n",
    "        else:\n",
    "            result = torch.prod(self.data, dim=axis, keepdim=keepdims)\n",
    "        return Tensor(result, backend=self._backend)\n",
    "    \n",
    "    def cumprod(self, axis = None):\n",
    "        \"\"\"\n",
    "        Computes the cumulative product of the Tensor along a specified axis.\n",
    "        \n",
    "        Args:\n",
    "            axis (Optional[int]): Axis along which to compute the cumulative product. If None, computes across all dimensions.\n",
    "    \n",
    "        Returns:\n",
    "            Tensor: A new instance containing the computed cumulative product values.\n",
    "        \n",
    "        Raises:\n",
    "            AttributeError: If no data attribute exists in the instance.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.cumprod(self.data, axis=axis)\n",
    "        else:\n",
    "            result = torch.cumprod(self.data, dim=axis)\n",
    "        return Tensor(result, backend=self._backend)\n",
    "    \n",
    "    def gamma(self):\n",
    "        \"\"\"\n",
    "        Computes the element-wise Gamma function.\n",
    "        \n",
    "        Args: \n",
    "            None\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: A new Tensor containing the Gamma function values for each element in the original data.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = sp.special.gamma(self.data)\n",
    "        else:\n",
    "            def torch_gamma(x):\n",
    "                pos = torch.exp(torch.lgamma(x))\n",
    "                neg = torch.pi / (torch.sin(torch.pi * x) * torch.exp(torch.lgamma(1 - x)))\n",
    "                return torch.where(x > 0, pos, neg)\n",
    "            result = torch_gamma(self.data)\n",
    "        return Tensor(result, backend=self._backend) \n",
    "    \n",
    "    def loggamma(self):\n",
    "        \"\"\"\n",
    "        Computes the element-wise natural logarithm of the Gamma function.\n",
    "        \n",
    "        Args: \n",
    "            None\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: A new Tensor containing the natural logarithm of the Gamma function values for each element in the original data.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = sp.special.gammaln(self.data)\n",
    "        else:\n",
    "            result = torch.special.gammaln(self.data)\n",
    "        return Tensor(result, backend=self._backend) \n",
    "    \n",
    "    def sigmoid(self):\n",
    "        \"\"\"\n",
    "        Applies the standard sigmoid function element-wise on the input Matrix.\n",
    "        \n",
    "        f(x) = L / (1 + exp(-1*x))\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: A new Tensor with the sigmoid function applied element-wise.\n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = 1.0 / (1.0 + np.exp(-1.0 * self.data ))\n",
    "        else:\n",
    "            result = torch.sigmoid(self.data)\n",
    "        return Tensor(result, backend=self._backend)\n",
    "    \n",
    "    def logistic(self, L=1.0, k=1.0, x0=0.0):\n",
    "        \"\"\"\n",
    "        Applies the logistic (sigmoid) function element-wise on the input Tensor.\n",
    "        \n",
    "        f(x) = L / (1 + exp(-k*(x - x0)))\n",
    "        \n",
    "        Args:\n",
    "            L (float): The curve's maximum value.\n",
    "            k (float): The steepness of the curve.\n",
    "            x0 (float): The x-value of the sigmoid's midpoint.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: A new Tensor with the logistic function applied element-wise.\n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = L / (1.0 + np.exp(-k * (self.data - x0)))\n",
    "        else:\n",
    "            result = L / (1.0 + torch.exp(-k * (self.data - x0)))\n",
    "        return Tensor(result, backend=self._backend)\n",
    "    \n",
    "    def logistic_inv(self, L=1.0, k=1.0, x0=0.0):\n",
    "        \"\"\"\n",
    "        Applies the inverse of the logistic (sigmoid) function element-wise on the input Tensor.\n",
    "        \n",
    "        f⁻¹(y) = x0 - (1/k)*ln((L - y)/y)\n",
    "        \n",
    "        Args:\n",
    "            L (float): The curve's maximum value used in the logistic function.\n",
    "            k (float): The steepness of the curve used in the logistic function.\n",
    "            x0 (float): The sigmoid's midpoint used in the logistic function.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: A new Tensor with the inverse logistic function applied element-wise.\n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = x0 - (1/k) * np.log((L - self.data) / self.data)\n",
    "        else:\n",
    "            result = x0 - (1/k) * torch.log((L - self.data) / self.data)\n",
    "        return Tensor(result, backend=self._backend)\n",
    "    \n",
    "    def cholesky(self, upper = False):\n",
    "        \"\"\"\n",
    "        Computes the Cholesky decomposition of a symmetric positive-definite matrix.\n",
    "        L @ U = self.data\n",
    "        returns L if upper = False else U\n",
    "        \n",
    "        Args:\n",
    "            upper (bool): If True, compute the upper triangular factor. Default is False.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: A new Tensor containing the lower triangular factor of the original data if `upper` is False,\n",
    "                    or its transpose if `upper` is True.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.linalg.cholesky(self.data)\n",
    "            if upper == True:\n",
    "                result = result.T\n",
    "        else:\n",
    "            result = torch.cholesky(self.data, upper = upper)\n",
    "        return Tensor(result, backend=self._backend)\n",
    "    \n",
    "    def exp(self):\n",
    "        \"\"\"\n",
    "        Computes the element-wise exponential.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: New tensor with exponential applied.\n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.exp(self.data)\n",
    "        else:\n",
    "            result = torch.exp(self.data)\n",
    "        return Tensor(result, backend=self._backend)\n",
    "    \n",
    "    def sin(self):\n",
    "        \"\"\"\n",
    "        Computes the element-wise sine.\n",
    "        \n",
    "        Args: \n",
    "            None\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: A new Tensor containing the sine values of the original data.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.sin(self.data)\n",
    "        else:\n",
    "            result = torch.sin(self.data)\n",
    "        return Tensor(result, backend=self._backend)\n",
    "    \n",
    "    def cos(self):\n",
    "        \"\"\"\n",
    "        Computes the element-wise cosine.\n",
    "        \n",
    "        Args: \n",
    "            None\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: A new Tensor containing the cosine values of the original data.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.cos(self.data)\n",
    "        else:\n",
    "            result = torch.cos(self.data)\n",
    "        return Tensor(result, backend=self._backend)\n",
    "    \n",
    "    def tan(self):\n",
    "        \"\"\"\n",
    "        Computes the element-wise tangent.\n",
    "        \n",
    "        Args: \n",
    "            None\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: A new Tensor containing the tangent values of the original data.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.tan(self.data)\n",
    "        else:\n",
    "            result = torch.tan(self.data)\n",
    "        return Tensor(result, backend=self._backend)\n",
    "    \n",
    "    def sinh(self):\n",
    "        \"\"\"\n",
    "        Computes the element-wise hyperbolic sine.\n",
    "        \n",
    "        Args: \n",
    "            None\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: A new Tensor containing the hyperbolic sine values of the original data.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.sinh(self.data)\n",
    "        else:\n",
    "            result = torch.sinh(self.data)\n",
    "        return Tensor(result, backend=self._backend)\n",
    "    \n",
    "    def cosh(self):\n",
    "        \"\"\"\n",
    "        Computes the element-wise hyperbolic cosine.\n",
    "        \n",
    "        Args: \n",
    "            None\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: A new Tensor containing the hyperbolic cosine values of the original data.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.cosh(self.data)\n",
    "        else:\n",
    "            result = torch.cosh(self.data)\n",
    "        return Tensor(result, backend=self._backend)\n",
    "    \n",
    "    def tanh(self):\n",
    "        \"\"\"\n",
    "        Computes the element-wise hyperbolic tangent.\n",
    "        \n",
    "        Args: \n",
    "            None\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: A new Tensor containing the hyperbolic tangent values of the original data.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.tanh(self.data)\n",
    "        else:\n",
    "            result = torch.tanh(self.data)\n",
    "        return Tensor(result, backend=self._backend)\n",
    "    \n",
    "    def abs(self):\n",
    "        \"\"\"\n",
    "        Computes the element-wise absolute values.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: New tensor with absolute values applied.\n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.abs(self.data)\n",
    "        else:\n",
    "            result = torch.abs(self.data)\n",
    "        return Tensor(result, backend=self._backend)\n",
    "    \n",
    "    def log(self):\n",
    "        \"\"\"\n",
    "        Computes the element-wise natural logarithm.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: New tensor with logarithm applied.\n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.log(self.data)\n",
    "        else:\n",
    "            result = torch.log(self.data)\n",
    "        return Tensor(result, backend=self._backend)\n",
    "    \n",
    "    def relu(self):\n",
    "        \"\"\"\n",
    "        Computes the element-wise ReLU function.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: New Tensor with ReLU applied.\n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.clip(self.data, 0.0)\n",
    "        else:\n",
    "            result = torch.relu(self.data)\n",
    "        return Tensor(result, backend=self._backend)\n",
    "    \n",
    "    def softmax(self, axis = 1, keepdims:bool | None = True):\n",
    "        \"\"\"\n",
    "        Applies the softmax function along a specified axis.\n",
    "        Reminder -> Shapre will be shrinked by 1 so that you may need to reshape() it.\n",
    "        \n",
    "        Args:\n",
    "            axis (int): Axis along which to apply the softmax. Default is 1.\n",
    "            keepdims (bool or None): Whether to keep the reduced dimensions as axes with size one. \n",
    "                                    If `True`, the shape of the result will be the same as input; otherwise, it will not have these dimensions.\n",
    "    \n",
    "        Returns:\n",
    "            Tensor: A new Tensor containing the softmax values along the specified axis.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            if keepdims is not None and axis is not None:\n",
    "                e_x = np.exp(self.data - np.max(self.data, axis=axis, keepdims=keepdims))\n",
    "                result = e_x / e_x.sum(axis=axis, keepdims=keepdims)\n",
    "            else:\n",
    "                e_x = np.exp(self.data - np.max(self.data, axis=axis))\n",
    "                result = e_x / e_x.sum(axis=axis)\n",
    "        else:\n",
    "            result = torch.nn.functional.softmax(self.data, dim=axis)\n",
    "        return Tensor(result, backend=self._backend)\n",
    "\n",
    "    def argmax(self, axis = 1):\n",
    "        \"\"\"\n",
    "         Computes the indices of the maximum values along a specified axis.\n",
    "         Reminder -> Shapre will be shrinked by 1 so that you may need to reshape() it.\n",
    "        \n",
    "         Args:\n",
    "             axis (int | None): Axis along which to compute the argmax. Default is 1.\n",
    "         \n",
    "         Returns:\n",
    "             Tensor: A new Tensor containing the indices of the maximum values along the specified axis.\n",
    "         \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.argmax(self.data, axis=axis)\n",
    "        else:\n",
    "            result = torch.argmax(self.data, dim=axis)\n",
    "        return Tensor(result, backend=self._backend)\n",
    "    \n",
    "    def argmin(self, axis = 1):\n",
    "        \"\"\"\n",
    "        Computes the indices of the minimum values along a specified axis.\n",
    "    \n",
    "        Args:\n",
    "            axis (int | None): Axis along which to compute the argmin. Default is 1.\n",
    "        \n",
    "        Returns:\n",
    "           Tensor: A new Tensor containing the indices of the minimum values along the specified axis.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.argmin(self.data, axis=axis)\n",
    "        else:\n",
    "            result = torch.argmin(self.data, dim=axis)\n",
    "        return Tensor(result, backend=self._backend)    \n",
    "    \n",
    "    def flatten(self):\n",
    "        \"\"\"\n",
    "        Returns the flattened tensor.\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: The flattened tensor.\n",
    "        \"\"\"\n",
    "        return Tensor(self.data.flatten(), backend=self._backend)\n",
    "    \n",
    "    def diag(self):\n",
    "        \"\"\"\n",
    "        Computes the diagonal vector of a square matrix.\n",
    "        Or create a diagonal matrix if 1D matrix.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: The diagonal vector.\n",
    "            \n",
    "        Raises:\n",
    "            ValueError: If the Tensor is not square.\n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            return Tensor(np.diag(self.data), backend=self._backend)\n",
    "        else:\n",
    "            return Tensor(torch.diag(self.data), backend=self._backend)\n",
    "    \n",
    "    def reverse(self, axis = 0):\n",
    "        \"\"\"\n",
    "        Reverse the flattened tensor.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: The reversed tensor.\n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            return Tensor(np.flip(self.data, axis=axis), backend=self._backend)\n",
    "        else:\n",
    "            return Tensor(torch.flip(self.data, axis=axis), backend=self._backend)\n",
    "\n",
    "    def stack(self, *wargs, axis = 0):\n",
    "        \"\"\"\n",
    "        Stack data in sequence on an axis.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: The stacked Tensor.\n",
    "        \"\"\"\n",
    "        data_list = [self.data]\n",
    "        for arg in wargs:\n",
    "            data_list.append(arg.data)\n",
    "        if self._is_numpy:\n",
    "            result = np.stack(data_list, axis=axis)\n",
    "        else:\n",
    "            result = torch.stack(data_list, dim=axis)\n",
    "        return Tensor(result, backend=self._backend)\n",
    "\n",
    "    def vstack(self, *wargs):\n",
    "        \"\"\"\n",
    "        Stack data in sequence vertically (row wise).\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: The vstacked tensor.\n",
    "        \"\"\"\n",
    "        data_list = [self.data]\n",
    "        for arg in wargs:\n",
    "            data_list.append(arg.data)\n",
    "        if self._is_numpy:\n",
    "            result = np.vstack(data_list)\n",
    "        else:\n",
    "            result = torch.vstack(data_list)\n",
    "        return Tensor(result, backend=self._backend)\n",
    "    \n",
    "    def hstack(self, *wargs):\n",
    "        \"\"\"\n",
    "        Stack data in sequence horizontally (col wise).\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: The hstacked tensor.\n",
    "        \"\"\"\n",
    "        data_list = [self.data]\n",
    "        for arg in wargs:\n",
    "            data_list.append(arg.data)\n",
    "        if self._is_numpy:\n",
    "            result = np.hstack(data_list)\n",
    "        else:\n",
    "            result = torch.hstack(data_list)\n",
    "        return Tensor(result, backend=self._backend)\n",
    "\n",
    "    def sign(self):\n",
    "        \"\"\"\n",
    "        Computes the element-wise sign of the data. Returns with the same type.\n",
    "    \n",
    "        Args: \n",
    "            None\n",
    "    \n",
    "        Returns:\n",
    "            Tensor: A new Tensor containing the sign values (1 for positive, -1 for negative, 0 for zero) of each element in the original data.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.sign(self.data)\n",
    "        else:\n",
    "            result = torch.sign(self.data)\n",
    "        return Tensor(result, backend=self._backend)\n",
    "\n",
    "    def repeat(self, repeats, axis=None):\n",
    "        \"\"\"\n",
    "        Repeats the Tensor elements along a specified axis.\n",
    "    \n",
    "        Args:\n",
    "            repeats (int or tuple[int]): The number of times to repeat each element.\n",
    "            axis (int): Axis along which to repeat the elements. If `None`, repeats over all dimensions.\n",
    "    \n",
    "        Returns:\n",
    "            Tensor: A new Tensor with repeated elements.\n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = self.data.repeat(repeats, axis=axis)\n",
    "        else:\n",
    "            if axis is None:\n",
    "                result = torch.repeat_interleave(self.data, repeats)\n",
    "            else:\n",
    "                result = torch.repeat_interleave(self.data, repeats, dim=axis)\n",
    "        return Tensor(result, backend=self._backend)\n",
    "    \n",
    "    def bincount(self, *, weights = None, inttype: type = int):\n",
    "        \"\"\"\n",
    "        Counts the number of occurrences of each value in `data` and optionally returns a weighted count.\n",
    "        Values will be forcefully casted to `inttype`\n",
    "    \n",
    "        Args:\n",
    "            weights (array_like | Tensor): An array-like object containing weights corresponding to each element in `data`. Default is None.\n",
    "            inttype (type): A type that the data is going to be casted to.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: A new Tensor with the bin counts or weighted bin counts.\n",
    "    \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.bincount(self.astype(inttype).data, weights = weights)\n",
    "        else:\n",
    "            result = torch.bincount(self.astype(inttype).data, weights = weights)\n",
    "        return Tensor(result, backend=self._backend)\n",
    "    \n",
    "    def transpose(self, *axes):\n",
    "        \"\"\"\n",
    "        Transposes the tensor dimensions. If axes are provided, permutes accordingly; otherwise, reverses dimensions.\n",
    "        \n",
    "        Args:\n",
    "            *axes: Optional permutation of dimensions.\n",
    "            \n",
    "        Returns:\n",
    "            Tensor: New transposed tensor.\n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = self.data.transpose(axes) if axes else self.data.T\n",
    "        else:\n",
    "            result = self.data.permute(*axes) if axes else self.data.permute(*reversed(range(self.data.dim())))\n",
    "        if len(self.shape) > 1:\n",
    "            return Tensor(result, backend=self._backend)\n",
    "        else:\n",
    "            # From a row vector to a column vector\n",
    "            return Tensor(result.reshape([self.shape[0], 1]), backend=self._backend)\n",
    "    \n",
    "    def quantile(self, q: float, axis = None, keepdims = False):\n",
    "        \"\"\"\n",
    "        Computes the specified quantiles along a given axis.\n",
    "    \n",
    "        Args:\n",
    "            q (float): The quantile to compute. Should be between 0 and 1.\n",
    "            axis (Optional[int]): Axis along which to compute the quantile. Default is None, which computes over all dimensions.\n",
    "            keepdims (bool): Whether to keep the reduced axes in the result as singleton dimensions. Default is False.\n",
    "    \n",
    "        Returns:\n",
    "            Tensor: A new Tensor containing the computed quantiles.\n",
    "    \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.quantile(self.data, q, axis = axis, keepdims = keepdims)\n",
    "        else:\n",
    "            if axis is None:\n",
    "                result = torch.quantile(self.data, q, keepdims = keepdims)\n",
    "            else:\n",
    "                result = torch.quantile(self.data, q, dim = axis, keepdims = keepdims)\n",
    "        return Tensor(result, backend=self._backend)\n",
    "    \n",
    "    def sort(self, axis: int | None = None):\n",
    "        \"\"\"\n",
    "        Sorts the tensor elements along (the first column of) a specified axis.\n",
    "        If you intend to sort on only one array, use `sort_along` instead,\n",
    "        or if you want to sort along each column of each base dimension, use `sort_along_each_column` instead.\n",
    "        \n",
    "        Args:\n",
    "            axis (int or None): Axis to sort along. If `None`, sorts the entire matrix.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: A new Tensor with sorted elements.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.sort(self.data, axis=axis)\n",
    "        else:\n",
    "            if axis is None:\n",
    "                result, idx = self.data.sort()\n",
    "            else:\n",
    "                result, idx = self.data.sort(dim=axis)\n",
    "        return Tensor(result, backend=self._backend)\n",
    "    \n",
    "    def sort_along(self, axis: tuple = (None, 0)):\n",
    "        \"\"\"\n",
    "        Sort the N-dimensional data along the 1d values on `axis`.\n",
    "        If you intend to sort on the first column of axis `axis`, use `sort` to speed up,\n",
    "        or if you want to sort along each column of each base dimension, use `sort_along_each_column` instead.\n",
    "       \n",
    "        Detail:\n",
    "        Sort the input array x. The axis parameter is a tuple of t\n",
    "        he same length as x.ndim, where each position can be None or an integer. \n",
    "        It is required that exactly one position d in axis_vec is not None,\n",
    "        and the reference position of this dimension d is fixed = axis_vec[d], \n",
    "        but when taking this reference, the index 0 is selected for each dimension \n",
    "        in the global uniform way (i.e., only x[(0,)*d + (fixed,)] is used as the \n",
    "        reference). \n",
    "        The 1D sorting permutation is calculated (using argsort, in ascending order), \n",
    "        and then the global permutation is applied to dimension d+1 \n",
    "        (the subsequent dimension) of x, acting on all data without sorting each\n",
    "        preceding block separately.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "            axis : tuple\n",
    "                The indicator indicating sort which data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            Tensor, sorted copy.\n",
    "\n",
    "        \"\"\"\n",
    "        if len(self.data.shape) != len(axis):\n",
    "            raise ValueError(\"The length of axis must be equal to the number of dimensions of the input array.\")\n",
    "        non_none = [(d, val) for d, val in enumerate(axis) if val is not None]\n",
    "        if len(non_none) != 1:\n",
    "            raise ValueError(\"There must be exactly one non-None element in axis\")\n",
    "        d, fixed = non_none[0]\n",
    "        if d > len(self.data.shape) - 1:\n",
    "            raise ValueError(\"The non-None dimension cannot be greater than dimension.\")\n",
    "        if fixed < 0 or fixed >= self.data.shape[d]:\n",
    "            raise IndexError(f\"Fixed index {fixed} is out of range for dimension {d} (0 to {self.data.shape[d]-1})\")\n",
    "        \n",
    "        # Last dim sort\n",
    "        if d == len(self.data.shape) - 1 and len(self.data.shape) == 2:\n",
    "            # Transpose, and sort the row\n",
    "            return self.transpose().sort_along(axis=(axis[1], axis[0])).transpose()\n",
    "        elif d == len(self.data.shape) - 1 and len(self.data.shape) > 2:\n",
    "            # Transpose the inner two dimensions\n",
    "            tr_axe = list(range(len(self.data.shape))); tmp = tr_axe[-1]; tr_axe[-1] = tr_axe[-2]; tr_axe[-2] = tmp;\n",
    "            axis_new = list(np.repeat(None, len(self.data.shape))); axis_new[-2] = fixed\n",
    "            return self.transpose(*tr_axe).sort_along(axis=axis_new).transpose(*tr_axe)\n",
    "        \n",
    "        # Extract global reference: fixed on dimension d, but all dimensions before d are indexed as 0.\n",
    "        # Construct index tuple: fixed to 0 for dimensions < d, fixed to the dth dimension, and use slice(None) to eliminate the remaining axes.\n",
    "        idx = (0,) * d + (fixed,)\n",
    "        \n",
    "        if self._is_numpy:\n",
    "            # Extract the reference key, which is expected to be 1D and have a length equal to self.data.shape[d+1]\n",
    "            key = np.asarray(self.data[idx])\n",
    "            if key.ndim != 1 or key.shape[0] != self.data.shape[d+1]:\n",
    "                raise ValueError(\"The reference key must be one-dimensional and its length must be the same as the length of the sorting axis.\")\n",
    "            order = np.argsort(key)\n",
    "           \n",
    "            # Construct a global index array for np.take_along_axis: needs to have the same shape as x,\n",
    "            # but order along sorting axis d+1, other dimensions are copied via broadcasting.\n",
    "            order_shape = [1] * len(self.data.shape)\n",
    "            order_shape[d+1] = self.data.shape[d+1]\n",
    "            order_global = order.reshape(order_shape)\n",
    "            order_global = np.broadcast_to(order_global, self.data.shape)\n",
    "            sorted_ = np.take_along_axis(self.data, order_global, axis=d+1)\n",
    "            return Tensor(sorted_, backend=self._backend, dtype=self.dtype, device=self.device)\n",
    "\n",
    "        else:\n",
    "            key = self.data[idx]\n",
    "            # key should be 1D, and its length should be equal to self.data.shape[d+1]\n",
    "            if key.dim() != 1 or key.size(0) != self.data.shape[d+1]:\n",
    "                raise ValueError(\"The reference key must be one-dimensional and its length must be the same as the length of the sorting axis.\")\n",
    "            # Calculate the sort order (ascending)\n",
    "            order = torch.argsort(key, dim=0)\n",
    "            \n",
    "            # Construct a global index tensor with the same shape as self.data, but with order on the sorting axis d+1\n",
    "            order_shape = [1] * len(self.data.shape)\n",
    "            order_shape[d+1] = self.data.shape[d+1]\n",
    "            order_global = order.view(*order_shape).expand(self.data.shape)\n",
    "            \n",
    "            # Use torch.gather to rearrange self.data according to the global index tensor on dim=d+1\n",
    "            sorted_ = torch.gather(self.data, dim=d+1, index=order_global)\n",
    "            return Tensor(sorted_, backend=self._backend, dtype=self.dtype, device=self.device)\n",
    "        \n",
    "    def sort_along_each_column(self, axis: int = 1, on_col: int = 0):\n",
    "        \"\"\"\n",
    "        Sort the N-dimensional data along values on column `on_col` of the axis `axis`.\n",
    "        Note, it will sort EACH `on_col` of the exterior axises.\n",
    "        If you intend to sort on the first column of axis `axis`, use `sort` to speed up.\n",
    "       \n",
    "        Detail:\n",
    "        Instead of sorting itself in d dimensions, use the reference sequence obtained by taking index=i on the d axis of x, and apply the same rearrangement to the d+1 axis (next axis) of x.\n",
    "\n",
    "        For example, for a 2D array, when d=0, i=1,\n",
    "        take the reference sequence = x[1, :], calculate its argsort to get the sorted arrangement, and then rearrange the columns of each row of the entire array according to this arrangement;\n",
    "        For a 3D array, when d=1, i=0,\n",
    "        for each subarray with fixed axis=0, take the reference sequence = subarray[0, :] (that is, the row of axis=1 index 0),\n",
    "        calculate argsort (sort the elements in the reference sequence), and then rearrange all rows in the subarray (all slices of axis=1) on axis=2 according to this arrangement.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "            axis : int\n",
    "                The axis of the column is on. The default is 1.\n",
    "            on_col : int\n",
    "                The index of the column is on. The default is 0.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            Tensor, sorted copy.\n",
    "\n",
    "        \"\"\"\n",
    "        if len(self.data.shape) < 2:\n",
    "            raise ValueError(\"The input data must at least have 2 dimensions. Use `sort` if it is a 1d array.\")\n",
    "        if axis < 0 or axis > len(self.data.shape) - 1:\n",
    "            raise ValueError(\"The parameter d must be positive and smaller than ndim.\")\n",
    "        if axis == len(self.data.shape) - 1 and len(self.data.shape) == 2:\n",
    "            # Transpose, and sort the row\n",
    "            return self.transpose().sort_along_each_column(axis=0, on_col=on_col).transpose()\n",
    "        elif axis == len(self.data.shape) - 1 and len(self.data.shape) > 2:\n",
    "            # Transpose the inner two dimensions\n",
    "            tr_axe = list(range(len(self.data.shape))); tmp = tr_axe[-1]; tr_axe[-1] = tr_axe[-2]; tr_axe[-2] = tmp;\n",
    "            return self.transpose(*tr_axe).sort_along_each_column(axis=axis-1, on_col=on_col).transpose(*tr_axe)\n",
    "            \n",
    "        sorted_axis = axis + 1\n",
    "\n",
    "        if self._is_numpy:\n",
    "            key = np.take(self.data, indices=on_col, axis=axis)\n",
    "            order = np.argsort(key, axis=axis)\n",
    "            order_expanded = np.expand_dims(order, axis=axis)\n",
    "            sorted_ = np.take_along_axis(self.data, order_expanded, axis=sorted_axis)\n",
    "            return Tensor(sorted_, backend=self._backend, dtype=self.dtype, device=self.device)\n",
    "\n",
    "        else:\n",
    "            key = self.data.select(dim=axis, index=on_col)\n",
    "            order = torch.argsort(key, dim=axis)\n",
    "            order_expanded = order.unsqueeze(dim=axis)\n",
    "            expand_shape = list(self.data.shape)\n",
    "            index = order_expanded.expand(*expand_shape)\n",
    "            sorted_ = torch.gather(self.data, dim=sorted_axis, index=index)\n",
    "            return Tensor(sorted_, backend=self._backend, dtype=self.dtype, device=self.device)        \n",
    "    \n",
    "    def determinant(self):\n",
    "        \"\"\"\n",
    "        Computes the determinant if the tensor is a matrix.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: New tensor of determinant.\n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            return np.linalg.det(self.data)\n",
    "        else:\n",
    "            if hasattr(torch, 'linalg') and hasattr(torch.linalg, 'det'):\n",
    "                return torch.linalg.det(self.data)\n",
    "            else:\n",
    "                return torch.det(self.data)\n",
    "            \n",
    "    def inverse(self):\n",
    "        \"\"\"\n",
    "        Computes the inverse of a tensor.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: New tensor with its inversed stored.\n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            result = np.linalg.inv(self.data)\n",
    "        else:\n",
    "            if hasattr(torch, 'linalg') and hasattr(torch.linalg, 'inv'):\n",
    "                result = torch.linalg.inv(self.data)\n",
    "            else:\n",
    "                result = torch.inverse(self.data)\n",
    "        return Tensor(result, backend=self._backend)\n",
    "\n",
    "    def trace(self):\n",
    "        \"\"\"\n",
    "        Computes the trace of a tensor.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: New tensor with trace stored.\n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            return np.trace(self.data)\n",
    "        else:\n",
    "            return torch.trace(self.data)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def dot(self, other):\n",
    "        \"\"\"\n",
    "        Computes the dot product of this matrix with another Matrix.\n",
    "        \n",
    "        Args:\n",
    "            other (Matrix): The other matrix to multiply.\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: A new Matrix instance with the result of the dot product.\n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            return Tensor(np.dot(self.data, other.data), backend=\"numpy\")\n",
    "        else:\n",
    "            return Tensor(torch.matmul(self.data, other.data), backend=\"torch\")\n",
    "        \n",
    "    def inner(self, other):\n",
    "        \"\"\"\n",
    "        Computes the inner product of this Tensor with another Tensor.\n",
    "        \n",
    "        Args:\n",
    "            other (Tensor): The other Tensor to multiply.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: A new Tensor instance with the result of the inner product.\n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            return Tensor(np.inner(self.data, other.data), backend=\"numpy\")\n",
    "        else:\n",
    "            return Tensor(torch.inner(self.data, other.data), backend=\"torch\")\n",
    "    \n",
    "    def outer(self, other):\n",
    "        \"\"\"\n",
    "        Computes the outer product of this Tensor with another Tensor.\n",
    "        \n",
    "        Args:\n",
    "            other (Tensor): The other Tensor to multiply.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: A new Tensor instance with the result of the outer product.\n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            return Tensor(np.outer(self.data, other.data), backend=\"numpy\")\n",
    "        else:\n",
    "            return Tensor(torch.outer(self.data, other.data), backend=\"torch\")\n",
    "\n",
    "    def svd(self, full_matrices=True):\n",
    "        \"\"\"\n",
    "        Computes the singular value decomposition for a general tensor.\n",
    "        \n",
    "        Returns:\n",
    "            Tensors: New s,v,d tensors in a tuple.\n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            U, s, Vh = np.linalg.svd(self.data, full_matrices=full_matrices)\n",
    "            return Tensor(U, backend=\"numpy\"), Tensor(s, backend=\"numpy\"), Tensor(Vh, backend=\"numpy\")\n",
    "        else:\n",
    "            if hasattr(torch, 'linalg') and hasattr(torch.linalg, 'svd'):\n",
    "                U, s, Vh = torch.linalg.svd(self.data, full_matrices=full_matrices)\n",
    "            else:\n",
    "                U, s, V = torch.svd(self.data, some=not full_matrices)\n",
    "                Vh = V.t()\n",
    "            return Tensor(U, backend=\"torch\"), Tensor(s, backend=\"torch\"), Tensor(Vh, backend=\"torch\")\n",
    " \n",
    "    def to_zeros(self):\n",
    "        \"\"\"\n",
    "        Converts the Tensor data into a same shape Tensor with 0s.\n",
    "        \n",
    "        Args: \n",
    "            None\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: a same shape Tensor with 0s.\n",
    "        \n",
    "        \"\"\"\n",
    "        x = self.copy()\n",
    "        x[...] = 0\n",
    "        return x\n",
    "    \n",
    "    def to_ones(self):\n",
    "        \"\"\"\n",
    "        Converts the Tensor data into a same shape Tensor with 1s.\n",
    "        \n",
    "        Args: \n",
    "            None\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: a same shape Tensor with 0s.\n",
    "        \n",
    "        \"\"\"\n",
    "        x = self.copy()\n",
    "        x[...] = 1\n",
    "        return x\n",
    "    \n",
    "    def to_ks(self, k: float | int = 0):\n",
    "        \"\"\"\n",
    "        Converts the Tensor data into a same shape Tensor with ks.\n",
    "        \n",
    "        Args: \n",
    "            None\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: a same shape Tensor with 1s.\n",
    "        \n",
    "        \"\"\"\n",
    "        x = self.copy()\n",
    "        x[...] = k\n",
    "        return x\n",
    "    \n",
    "    def to_rands(self):\n",
    "        \"\"\"\n",
    "        Converts the Tensor data into a same shape Tensor with uniform random numbers.\n",
    "        \n",
    "        Args: \n",
    "            None\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: a same shape Tensor with uniform random numbers.\n",
    "        \n",
    "        \"\"\"\n",
    "        return self.rand(self.shape, backend=self._backend, dtype=self.dtype, device=self.device)\n",
    "\n",
    "    def to_list(self):\n",
    "        \"\"\"\n",
    "        Converts the Tensor data into a Python list.\n",
    "        \n",
    "        Args: \n",
    "            None\n",
    "        \n",
    "        Returns:\n",
    "            list: A Python list containing the same elements as `self.data`.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            return self.data.tolist()\n",
    "        else:\n",
    "            return self.data.cpu().tolist()\n",
    "        \n",
    "    def to_numpy_array(self):\n",
    "        \"\"\"\n",
    "        Converts the Tensor data into a NumPy array.\n",
    "        \n",
    "        Returns: \n",
    "            np.ndarray: The underlying NumPy array of the matrix.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            return self.data\n",
    "        else:\n",
    "            return self.data.detach().cpu().numpy()\n",
    "        \n",
    "    def to_torch_tensor(self, *, dtype=None, device=None):\n",
    "        \"\"\"\n",
    "        Converts the Tensor data into a PyTorch tensor.\n",
    "        \n",
    "        Args: \n",
    "            dtype (torch.dtype or None): The desired data type for the resulting tensor. If not provided,\n",
    "                                         uses the current data type of `self.data`.\n",
    "            device (torch.device or None): The target device to which the tensor should be moved.\n",
    "                                           If not provided, it will use the default device.\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: A PyTorch tensor containing the same data as `self.data`.\n",
    "        \n",
    "        \"\"\"\n",
    "        if self._is_numpy:\n",
    "            return torch.tensor(self.data, dtype=dtype, device=device)\n",
    "        else:\n",
    "            return self.data\n",
    "        \n",
    "    @staticmethod\n",
    "    def equal(x, other, *, equal_nan=False):\n",
    "        \"\"\"\n",
    "        Compare if two Tensor objects have the same shape and elements.\n",
    "        \n",
    "        Args:\n",
    "            x (Tensor): The one tensor to compare.\n",
    "            other (Tensor): The other tensor to compare.\n",
    "        \n",
    "        Returns:\n",
    "           ``True`` if two tensors have the same size and elements, \n",
    "           ``False`` otherwise.\n",
    "        \"\"\"\n",
    "        if x._is_numpy == True and other._is_numpy == True:\n",
    "            return np.array_equal(x, other, equal_nan=equal_nan)\n",
    "        elif  x._is_numpy == False and other._is_numpy == False:\n",
    "            return torch.equal(x, other)\n",
    "        else:\n",
    "            raise ValueError(\"Input `x` and `other` for comparison must have to have the same backend!\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def gather_along(data, axis, index):\n",
    "        \"\"\"\n",
    "        Gather values on an axis with specified index.\n",
    "        \n",
    "        Parameters:\n",
    "            axis: int, the axis number to gather values on.\n",
    "            index: list | array | Matrix, the indices for each row/column/.. to gather values on.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: gathered elements.\n",
    "        \"\"\"\n",
    "        if data._is_numpy:\n",
    "            result = np.take_along_axis(data.data, indices=index, axis=axis)\n",
    "        else:\n",
    "            result = torch.gather(data.data, dim=axis, index=index.data)\n",
    "        return Tensor(result, backend=data._backend, dtype=data.dtype, device=data.device)\n",
    "    \n",
    "    @staticmethod\n",
    "    def where(condition, backend=\"numpy\", dtype=None, device=None):\n",
    "        \"\"\"\n",
    "        Returns elements depending on `condition`.\n",
    "        \n",
    "        Parameters:\n",
    "            condition : Internal Type (array_like); bool Where True\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: chosen elements.\n",
    "        \"\"\"\n",
    "        if backend == \"numpy\":\n",
    "            result = np.where(condition)\n",
    "        else:\n",
    "            result, = torch.where(condition)\n",
    "            if isinstance(result, tuple):\n",
    "                result = result[0]\n",
    "        return Tensor(result, backend=backend, dtype=dtype, device=device)\n",
    "    \n",
    "    @staticmethod\n",
    "    def where_as(condition, then, other, backend=\"numpy\", dtype=None, device=None):\n",
    "        \"\"\"\n",
    "        Returns elements depending on `condition`.\n",
    "        \n",
    "        Parameters:\n",
    "            condition : Internal Type (array_like); bool Where True\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: if true then applied then to true elements; other to fales elements.\n",
    "        \"\"\"\n",
    "        if backend == \"numpy\":\n",
    "            result = np.where(condition, then, other)\n",
    "        else:\n",
    "            result = torch.where(condition, then, other)         \n",
    "            if isinstance(result, tuple):\n",
    "                result = result[0]\n",
    "        return Tensor(result, backend=backend, dtype=dtype, device=device)\n",
    "        \n",
    "    @staticmethod\n",
    "    def zeros(shape, backend=\"numpy\", dtype=None, device=None):\n",
    "        \"\"\"\n",
    "        Creates a tensor filled with zeros.\n",
    "        \n",
    "        Args:\n",
    "            shape (tuple): Desired shape.\n",
    "            backend (str): Backend (\"numpy\" or \"torch\").\n",
    "            dtype: Desired data type.\n",
    "            device: Data device, \"cpu\" or \"cuda\".\n",
    "            \n",
    "        Returns:\n",
    "            Tensor: New tensor of zeros.\n",
    "        \"\"\"\n",
    "        if backend == \"numpy\":\n",
    "            data = np.zeros(shape, dtype=dtype)\n",
    "        elif backend == \"torch\":\n",
    "            data = torch.zeros(shape, dtype=dtype, device=device) if dtype else torch.zeros(shape, device=device)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported backend. Choose 'numpy' or 'torch'.\")\n",
    "        return Tensor(data, backend=backend)\n",
    "\n",
    "    @staticmethod\n",
    "    def ones(shape, backend=\"numpy\", dtype=None, device=None):\n",
    "        \"\"\"\n",
    "        Creates a tensor filled with ones.\n",
    "        \n",
    "        Args:\n",
    "            shape (tuple): Desired shape.\n",
    "            backend (str): Backend (\"numpy\" or \"torch\").\n",
    "            dtype: Desired data type.\n",
    "            device: Data device, \"cpu\" or \"cuda\".\n",
    "            \n",
    "        Returns:\n",
    "            Tensor: New tensor of ones.\n",
    "        \"\"\"\n",
    "        if backend == \"numpy\":\n",
    "            data = np.ones(shape, dtype=dtype)\n",
    "        elif backend == \"torch\":\n",
    "            data = torch.ones(shape, dtype=dtype, device=device) if dtype else torch.ones(shape, device=device)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported backend. Choose 'numpy' or 'torch'.\")\n",
    "        return Tensor(data, backend=backend)\n",
    "    \n",
    "    @staticmethod\n",
    "    def zeros_like(x, backend=\"numpy\", dtype=None, device=None):\n",
    "        \"\"\"\n",
    "        Creates a matrix of zeros with the same shape and data type as another matrix.\n",
    "        \n",
    "        Args:\n",
    "            x (Tensor): The input matrix.\n",
    "            backend (str): The backend for computation (\"numpy\" or \"torch\"). Default is \"numpy\".\n",
    "            dtype: Desired data type for the result. If not specified, uses the data type from `x`.\n",
    "            device: Data device, \"cpu\" or \"cuda\".\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: A new Tensor containing zeros with the same shape and type as `x`.\n",
    "        \n",
    "        Raises:\n",
    "            ValueError: If an unsupported backend is provided.\n",
    "        \n",
    "        \"\"\"\n",
    "        if backend == \"numpy\":\n",
    "            data = np.zeros_like(x.data, dtype=dtype)\n",
    "        elif backend == \"torch\":\n",
    "            data = torch.zeros_like(x.data, dtype=dtype, device=device) if dtype else torch.zeros_like(x.data, device=device)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported backend. Choose 'numpy' or 'torch'.\")\n",
    "        return Tensor(data, backend=backend)\n",
    "    \n",
    "    @staticmethod\n",
    "    def ones_like(x, backend=\"numpy\", dtype=None, device=None):\n",
    "        \"\"\"\n",
    "        Creates a matrix of ones with the same shape and data type as another matrix.\n",
    "        \n",
    "        Args:\n",
    "            x (Tensor): The input matrix.\n",
    "            backend (str): The backend for computation (\"numpy\" or \"torch\"). Default is \"numpy\".\n",
    "            dtype: Desired data type for the result. If not specified, uses the data type from `x`.\n",
    "            device: Data device, \"cpu\" or \"cuda\".\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: A new Tensor containing ones with the same shape and type as `x`.\n",
    "        \n",
    "        Raises:\n",
    "            ValueError: If an unsupported backend is provided.\n",
    "        \n",
    "        \"\"\"\n",
    "        if backend == \"numpy\":\n",
    "            data = np.ones_like(x.data, dtype=dtype)\n",
    "        elif backend == \"torch\":\n",
    "            data = torch.ones_like(x.data, dtype=dtype, device=device) if dtype else torch.ones_like(x.data, device=device)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported backend. Choose 'numpy' or 'torch'.\")\n",
    "        return Tensor(data, backend=backend)\n",
    "    \n",
    "    @staticmethod\n",
    "    def rand(shape, backend=\"numpy\", dtype=None, device=None):\n",
    "        \"\"\"\n",
    "        Creates a tensor with random values uniformly distributed in [0, 1).\n",
    "        \n",
    "        Args:\n",
    "            shape (tuple): Desired shape.\n",
    "            backend (str): Backend (\"numpy\" or \"torch\").\n",
    "            dtype: Desired data type.\n",
    "            device: Data device, \"cpu\" or \"cuda\".\n",
    "            \n",
    "        Returns:\n",
    "            Tensor: New tensor with random values.\n",
    "        \"\"\"\n",
    "        bk = backend.lower()\n",
    "        if bk == \"numpy\":\n",
    "            data = np.random.rand(*shape)\n",
    "            if dtype:\n",
    "                data = data.astype(dtype)\n",
    "        elif bk == \"torch\":\n",
    "            if torch is None:\n",
    "                raise ImportError(\"PyTorch is not installed.\")\n",
    "            data = torch.rand(shape, dtype=dtype, device=device) if dtype else torch.rand(shape, device=device)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported backend. Choose 'numpy' or 'torch'.\")\n",
    "        return Tensor(data, backend=bk)\n",
    "    \n",
    "    @staticmethod\n",
    "    def identity(n, backend=\"numpy\", dtype=None):\n",
    "        \"\"\"\n",
    "        Creates a tensor with identity property.\n",
    "        \n",
    "        Args:\n",
    "            backend (str): Backend (\"numpy\" or \"torch\").\n",
    "            dtype: Desired data type.\n",
    "            \n",
    "        Returns:\n",
    "            Tensor: New identity tensor.\n",
    "        \"\"\"\n",
    "        bk = backend.lower()\n",
    "        if bk == \"numpy\":\n",
    "            data = np.eye(n, dtype=dtype)\n",
    "        elif bk == \"torch\":\n",
    "            if torch is None:\n",
    "                raise ImportError(\"PyTorch is not installed.\")\n",
    "            data = torch.eye(n, dtype=dtype)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported backend. Choose 'numpy' or 'torch'.\")\n",
    "        return Tensor(data, backend=bk)\n",
    "\n",
    "    def reshape_(self, *shape):\n",
    "        \"\"\"\n",
    "        In-place reshape of the tensor.\n",
    "        \n",
    "        Args:\n",
    "            *shape: New shape dimensions.\n",
    "            \n",
    "        Returns:\n",
    "            self: The reshaped tensor.\n",
    "        \"\"\"\n",
    "        self.data = self.data.reshape(*shape)\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`P. Metrics for Binary and Multi-Class Classification Implementation (self-implemented)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These metrics classes are self implemented and open-sourced\n",
    "# Available at https://github.com/dof-studio/MML/\n",
    "# By Nathmath Huang (bh2821)\n",
    "# License: Apache License Version 2.0\n",
    "\n",
    "# Metrics Base Class\n",
    "class BaseMetrics:\n",
    "    \n",
    "    __attr__ = \"MML.BaseMetrics\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def compute(self):\n",
    "        \"\"\"\n",
    "        Compute the specified metric for the predictions given true data.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Compute is NOT implemented in the base class.\")\n",
    "        \n",
    "    def deriv_1(self):\n",
    "        \"\"\"\n",
    "        Compute the sample-wise 1st order derivatives of metric for the predictions given true data.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Deriv_1 is NOT implemented in the base class.\")\n",
    "        \n",
    "    def deriv_2(self):\n",
    "        \"\"\"\n",
    "        Compute the sample-wise 2nd order derivatives of metric for the predictions given true data.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Deriv_2 is NOT implemented in the base class.\")\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"BaseMetrics(Abstract Class).\"\n",
    "\n",
    "\n",
    "# Metrics for regression\n",
    "class RegressionMetrics(BaseMetrics):\n",
    "    \"\"\"\n",
    "    A class to compute common regression metrics between predicted results and target values.\n",
    "    \n",
    "    Supported metrics:\n",
    "        - MSE (Mean Squared Error)\n",
    "        - RMSE (Root Mean Squared Error) \n",
    "        - MAE (Mean Absolute Error)\n",
    "        - MAPE (Mean Absolute Percentage Error)\n",
    "        - Huber Loss\n",
    "        - Quantile Loss\n",
    "        - WMSE (Weighted Mean Squared Error)\n",
    "        - WRMSE (Weighted Root Mean Squared Error)\n",
    "        - R^2 (R Square)\n",
    "        - Adjusted R^2 (Adjusted R Square)\n",
    "        \n",
    "    Special metrics:\n",
    "        - Negative R^2 (R Square)\n",
    "        - Negative Adjusted R^2 (Adjusted R Square)\n",
    "        \n",
    "    The computations are performed using the underlying tensor operations, maintaining\n",
    "    compatibility with both numpy and torch backends.\n",
    "    \n",
    "    Attributes:\n",
    "        result: Predicted results tensor\n",
    "        target: Target values tensor\n",
    "        metric_type: String specifying which metric to compute ('mse', 'rmse', 'mae', 'mape', 'r2', 'adjusted r2')\n",
    "    \"\"\"\n",
    "    \n",
    "    __attr__ = \"MML.RegressionMetrics\"\n",
    "    \n",
    "    def __init__(self, result: Tensor | Matrix, target: Tensor | Matrix, metric_type: str, k: int | None = None, **kwargs):\n",
    "        \"\"\"\n",
    "        Initializes the RegressionMetrics instance with result and target tensors.\n",
    "        \n",
    "        Args:\n",
    "            result (Tensor | Matrix): Predicted results tensor\n",
    "            target (Tensor | Matrix): Target values tensor\n",
    "            metric_type (str): Metric type to compute ('mse', 'rmse', 'mae', 'mape', 'huber_loss', 'quantile_loss', \n",
    "                              'wmse', 'wrmse', 'r2', 'adjusted r2', 'nr2', 'nadjusted r2')\n",
    "            k (int): Number of predictors (parameters) in the model, only used in Adjusted R2.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Different instances or different backends.\n",
    "        if isinstance(result,  Object) == False or isinstance(target, Object) == False:\n",
    "            raise ValueError(\"Predicted `result` and real `target` should be either `Matrix` or `Tensor` type!\")\n",
    "        if type(result) != type(target):\n",
    "            raise ValueError(\"Predicted `result` and real `target` should have the same type, either Tensor or Matrix!\")\n",
    "        if result._backend != target._backend:\n",
    "            raise ValueError(\"Predicted `result` and real `target` should have the same backend, either numpy or torch!\")\n",
    "        \n",
    "        # Member variables.\n",
    "        self.k = k\n",
    "        self.result = result\n",
    "        self.target = target\n",
    "        self.typeclass = type(result)\n",
    "        self.metric_type = metric_type.lower()\n",
    "        \n",
    "        if not self.result.shape == self.target.shape:\n",
    "            raise ValueError(\"Result and target tensors must have the same shape.\")\n",
    "            \n",
    "    def compute(self, **kwargs) -> Tensor | Matrix:\n",
    "        \"\"\"\n",
    "        Computes the specified regression metric between result and target.\n",
    "        \n",
    "        Args:\n",
    "            **kwargs: Other arguments supported by metrics.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor | Matrix: The computed metric value as a tensor\n",
    "        \"\"\"\n",
    "        if self.metric_type == 'mse':\n",
    "            return self._compute_mse(**kwargs)\n",
    "        elif self.metric_type == 'rmse':\n",
    "            return self._compute_rmse(**kwargs)\n",
    "        elif self.metric_type == 'mae':\n",
    "            return self._compute_mae(**kwargs)\n",
    "        elif self.metric_type == 'mape':\n",
    "            return self._compute_mape(**kwargs)\n",
    "        elif self.metric_type == 'huber_loss':\n",
    "            return self._compute_huber_loss(**kwargs)\n",
    "        elif self.metric_type == 'quantile_loss':\n",
    "            return self._compute_quantile_loss(**kwargs)\n",
    "        elif self.metric_type == 'wmse':\n",
    "            return self._compute_wmse(**kwargs)\n",
    "        elif self.metric_type == 'wrmse':\n",
    "            return self._compute_wrmse(**kwargs)\n",
    "        elif self.metric_type == 'r2':\n",
    "            return self._compute_r2(**kwargs)\n",
    "        elif self.metric_type == 'adjusted r2':\n",
    "            return self._compute_adjusted_r2(**kwargs)\n",
    "        # Special Metrics\n",
    "        elif self.metric_type == 'nr2':\n",
    "            return - self._compute_r2(**kwargs)\n",
    "        elif self.metric_type == 'nadjusted r2':\n",
    "            return - self._compute_adjusted_r2(**kwargs)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported metric type: {self.metric_type}\")\n",
    "    \n",
    "    def deriv_1(self, **kwargs) -> Tensor | Matrix:\n",
    "        \"\"\"\n",
    "        Computes the sample-wise 1st order derivative of the specified regression metric between result and target.\n",
    "        \n",
    "        Args:\n",
    "            **kwargs: Other arguments supported by metrics.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor | Matrix: The computed gradient vector as a matrix or a tensor\n",
    "        \"\"\"\n",
    "        if self.metric_type == 'mse':\n",
    "            return self._deriv_1_mse(**kwargs)\n",
    "        elif self.metric_type == 'rmse':\n",
    "            return self._deriv_1_rmse(**kwargs)\n",
    "        elif self.metric_type == 'mae':\n",
    "            return self._deriv_1_mae(**kwargs)\n",
    "        elif self.metric_type == 'mape':\n",
    "            return self._deriv_1_mape(**kwargs)\n",
    "        elif self.metric_type == 'huber_loss':\n",
    "            return self._deriv_1_huber_loss(**kwargs)\n",
    "        elif self.metric_type == 'quantile_loss':\n",
    "            return self._deriv_1_quantile_loss(**kwargs)\n",
    "        elif self.metric_type == 'wmse':\n",
    "            return self._deriv_1_wmse(**kwargs)\n",
    "        elif self.metric_type == 'wrmse':\n",
    "            return self._deriv_1_wrmse(**kwargs)\n",
    "        elif self.metric_type == 'r2':\n",
    "            raise  ValueError(f\"Metric type: {self.metric_type} cannot compute derivatives.\")\n",
    "        elif self.metric_type == 'adjusted r2':\n",
    "            raise  ValueError(f\"Metric type: {self.metric_type} cannot compute derivatives.\")\n",
    "        # Special Metrics\n",
    "        elif self.metric_type == 'nr2':\n",
    "            raise  ValueError(f\"Metric type: {self.metric_type} cannot compute derivatives.\")\n",
    "        elif self.metric_type == 'nadjusted r2':\n",
    "            raise  ValueError(f\"Metric type: {self.metric_type} cannot compute derivatives.\")\n",
    "        else:\n",
    "            raise  ValueError(f\"Unsupported metric type: {self.metric_type}\")\n",
    "    \n",
    "    def deriv_2(self, **kwargs) -> Tensor | Matrix:\n",
    "        \"\"\"\n",
    "        Computes the sample-wise 2nd order derivative of the specified regression metric between result and target.\n",
    "        \n",
    "        Args:\n",
    "            **kwargs: Other arguments supported by metrics.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor | Matrix: The computed hessian matrix (without cross terms) as a matrix or a tensor\n",
    "        \"\"\"\n",
    "        if self.metric_type == 'mse':\n",
    "            return self._deriv_2_mse(**kwargs)\n",
    "        elif self.metric_type == 'rmse':\n",
    "            return self._deriv_2_rmse(**kwargs)\n",
    "        elif self.metric_type == 'mae':\n",
    "            return self._deriv_2_mae(**kwargs)\n",
    "        elif self.metric_type == 'mape':\n",
    "            return self._deriv_2_mape(**kwargs)\n",
    "        elif self.metric_type == 'huber_loss':\n",
    "            return self._deriv_2_huber_loss(**kwargs)\n",
    "        elif self.metric_type == 'quantile_loss':\n",
    "            return self._deriv_2_quantile_loss(**kwargs)\n",
    "        elif self.metric_type == 'wmse':\n",
    "            return self._deriv_2_wmse(**kwargs)\n",
    "        elif self.metric_type == 'wrmse':\n",
    "            return self._deriv_2_wrmse(**kwargs)\n",
    "        elif self.metric_type == 'r2':\n",
    "            raise  ValueError(f\"Metric type: {self.metric_type} cannot compute derivatives.\")\n",
    "        elif self.metric_type == 'adjusted r2':\n",
    "            raise  ValueError(f\"Metric type: {self.metric_type} cannot compute derivatives.\")\n",
    "        # Special Metrics\n",
    "        elif self.metric_type == 'nr2':\n",
    "            raise  ValueError(f\"Metric type: {self.metric_type} cannot compute derivatives.\")\n",
    "        elif self.metric_type == 'nadjusted r2':\n",
    "            raise  ValueError(f\"Metric type: {self.metric_type} cannot compute derivatives.\")\n",
    "        else:\n",
    "            raise  ValueError(f\"Unsupported metric type: {self.metric_type}\")\n",
    "        \n",
    "    def _compute_mse(self, axis: int | None = None, **kwargs) -> Tensor | Matrix:\n",
    "        \"\"\"\n",
    "        Computes the Mean Squared Error between result and target.\n",
    "        \n",
    "        Args:\n",
    "            axis: None or int, if you intend to get the per-output metrics/derivs, set axis = 0. Else None.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor | Matrix: MSE tensor or matrix\n",
    "        \"\"\"\n",
    "        error = (self.result - self.target)\n",
    "        squared_error = error ** 2\n",
    "        if axis is None:\n",
    "            mean_squared_error = squared_error.sum(axis = axis) / np.array(squared_error.shape).prod()\n",
    "        else:\n",
    "            mean_squared_error = squared_error.sum(axis = axis) / squared_error.shape[axis]\n",
    "        return mean_squared_error\n",
    "    \n",
    "    def _deriv_1_mse(self, axis: int | None = None, **kwargs) -> Tensor | Matrix:\n",
    "        \"\"\"\n",
    "        Computes the first-order derivative of the Mean Squared Error between result and target.\n",
    "        \n",
    "        Args:\n",
    "            axis: None or int, if you intend to get the per-output metrics/derivs, set axis = 0. Else None.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor | Matrix: Derivative of MSE tensor or matrix with respect to result.\n",
    "        \"\"\"\n",
    "        error = self.result - self.target\n",
    "        grad = 2 * error / error.shape[0]\n",
    "        if axis is None:\n",
    "            grad = 2 * error / np.array(error.shape).prod()\n",
    "        else:\n",
    "            grad = 2 * error / error.shape[axis]\n",
    "        return grad\n",
    "    \n",
    "    def _deriv_2_mse(self, axis: int | None = None, **kwargs) -> Tensor | Matrix:\n",
    "        \"\"\"\n",
    "        Computes the second-order derivative (Hessian diagonal) of the Mean Squared Error between result and target.\n",
    "    \n",
    "        Args:\n",
    "            only_diag: bool, if True, only calculate the diagonal vector and return,\n",
    "                             else, return the full hessian matrix.\n",
    "    \n",
    "        Returns:\n",
    "            Tensor | Matrix: Constant Hessian of MSE (2/N) with the same shape as result.\n",
    "        \"\"\"\n",
    "        ones = self.result.copy(); ones[...] = 1;\n",
    "        if axis is None:\n",
    "            return ones * (2.0 / np.array(self.result.shape).prod())\n",
    "        else:\n",
    "            return ones * (2.0 / self.result.shape[axis])\n",
    "    \n",
    "    def _compute_rmse(self, axis: int | None = None, **kwargs) -> Tensor | Matrix:\n",
    "        \"\"\"\n",
    "        Computes the Root Mean Squared Error between result and target.\n",
    "        \n",
    "        Args:\n",
    "            axis: None or int, if you intend to get the per-output metrics/derivs, set axis = 0. Else None.\n",
    "                \n",
    "        Returns:\n",
    "            Tensor | Matrix: RMSE tensor or matrix\n",
    "        \"\"\"\n",
    "        error = (self.result - self.target)\n",
    "        squared_error = error ** 2\n",
    "        if axis is None:\n",
    "            mean_squared_error = squared_error.sum(axis = axis) / np.array(squared_error.shape).prod()\n",
    "        else:\n",
    "            mean_squared_error = squared_error.sum(axis = axis) / squared_error.shape[axis]\n",
    "        rmse = mean_squared_error ** 0.5\n",
    "        return rmse\n",
    "    \n",
    "    def _deriv_1_rmse(self, axis: int | None = None, **kwargs) -> Tensor | Matrix:\n",
    "        \"\"\"\n",
    "        Computes the first-order derivative of the Root Mean Squared Error between result and target.\n",
    "        \n",
    "        Args:\n",
    "            axis: None or int, if you intend to get the per-output metrics/derivs, set axis = 0. Else None.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor | Matrix: Derivative of RMSE tensor or matrix with respect to result.\n",
    "        \"\"\"\n",
    "        error = self.result - self.target\n",
    "        squared_error = error ** 2\n",
    "        if axis is None:\n",
    "            mean_squared_error = squared_error.sum(axis = axis) / np.array(squared_error.shape).prod()\n",
    "            rmse = mean_squared_error ** 0.5\n",
    "            grad = error / (np.array(squared_error.shape).prod() * rmse)\n",
    "        else:\n",
    "            mean_squared_error = squared_error.sum(axis = axis) / squared_error.shape[axis]\n",
    "            rmse = mean_squared_error ** 0.5\n",
    "            grad = error / (error.shape[axis] * rmse)\n",
    "\n",
    "        return grad\n",
    "    \n",
    "    def _deriv_2_rmse(self, axis: int | None = None, **kwargs) -> Tensor | Matrix:\n",
    "        \"\"\"\n",
    "        Computes the second-order derivative of the Root Mean Squared Error between result and target.\n",
    "        \n",
    "        Args:\n",
    "            axis: None or int, if you intend to get the per-output metrics/derivs, set axis = 0. Else None.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor | Matrix: Second-order derivative (Hessian diagonal) of RMSE with respect to result.\n",
    "        \"\"\"\n",
    "        error = self.result - self.target\n",
    "        squared_error = error ** 2\n",
    "        if axis is None:\n",
    "            sum_squared = squared_error.sum(axis = axis)\n",
    "            mean_squared_error = sum_squared / np.array(squared_error.shape).prod()\n",
    "            rmse = mean_squared_error ** 0.5\n",
    "            hessian = (sum_squared - squared_error) / ((np.array(squared_error.shape).prod() ** 2) * (rmse ** 3))\n",
    "        else:\n",
    "            sum_squared = squared_error.sum(axis = axis)\n",
    "            mean_squared_error = sum_squared / squared_error.shape[axis]\n",
    "            rmse = mean_squared_error ** 0.5\n",
    "            hessian = (sum_squared - squared_error) / ((error.shape[axis] ** 2) * (rmse ** 3))\n",
    "        \n",
    "        return hessian\n",
    "    \n",
    "    def _compute_mae(self, axis: int | None = None, **kwargs) -> Tensor | Matrix:\n",
    "        \"\"\"\n",
    "        Computes the Mean Absolute Error between result and target.\n",
    "        \n",
    "        Args:\n",
    "            axis: None or int, if you intend to get the per-output metrics/derivs, set axis = 0. Else None.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor | Matrix: MAE tensor or matrix\n",
    "        \"\"\"\n",
    "        error = (self.result - self.target)\n",
    "        absolute_error = error.abs()\n",
    "        if axis is None:\n",
    "            mean_absolute_error = absolute_error.sum(axis = axis) / np.array(absolute_error.shape).prod()\n",
    "        else:\n",
    "            mean_absolute_error = absolute_error.sum(axis = axis) / absolute_error.shape[axis]\n",
    "        return mean_absolute_error\n",
    "\n",
    "    def _deriv_1_mae(self, axis: int | None = None, **kwargs) -> Tensor | Matrix:\n",
    "        \"\"\"\n",
    "        Computes the first-order derivative of the Mean Absolute Error between result and target.\n",
    "        \n",
    "        Args:\n",
    "            axis: None or int, if you intend to get the per-output metrics/derivs, set axis = 0. Else None.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor | Matrix: Derivative of MAE tensor or matrix with respect to result.\n",
    "        \"\"\"\n",
    "        error = self.result - self.target\n",
    "        grad = error.sign() / error.shape[0]\n",
    "        if axis is None:\n",
    "            grad = error.sign() / np.array(error.shape).prod()\n",
    "        else:\n",
    "            grad = error.sign() / error.shape[axis]\n",
    "        return grad\n",
    "    \n",
    "    def _deriv_2_mae(self, axis: int | None = None, **kwargs) -> Tensor | Matrix:\n",
    "        \"\"\"\n",
    "        Computes the second-order derivative of per-output MAE between result and target.\n",
    "        \n",
    "        Args:\n",
    "            axis: None or int, if you intend to get the per-output metrics/derivs, set axis = 0. Else None.\n",
    "                \n",
    "        Returns:\n",
    "            Tensor | Matrix: Hessian diagonal of MAE (zero), shape (N, D).\n",
    "        \"\"\"\n",
    "        zeros = self.result.copy(); zeros[...] = 0;\n",
    "        return zeros\n",
    "    \n",
    "    def _compute_mape(self, axis: int | None = None, **kwargs) -> Tensor | Matrix:\n",
    "        \"\"\"\n",
    "        Computes the Mean Absolute Percentage Error between result and target.\n",
    "        \n",
    "        Note: Division by zero occurs if target contains zeros. This is handled\n",
    "        gracefully by the underlying tensor operations, but users should ensure \n",
    "        target values are non-zero when using MAPE.\n",
    "        \n",
    "        Args:\n",
    "            axis: None or int, if you intend to get the per-output metrics/derivs, set axis = 0. Else None.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor | Matrix: MAPE tensor or matrix\n",
    "        \"\"\"\n",
    "        error = (self.result - self.target) / self.target\n",
    "        absolute_percentage_error = error.abs()\n",
    "        if axis is None:\n",
    "            mean_absolute_percentage_error = absolute_percentage_error.sum(axis = axis) / np.array(absolute_percentage_error.shape).prod()\n",
    "        else:\n",
    "            mean_absolute_percentage_error = absolute_percentage_error.sum(axis = axis) / absolute_percentage_error.shape[axis]\n",
    "        return mean_absolute_percentage_error\n",
    "\n",
    "    def _deriv_1_mape(self, axis: int | None = None, **kwargs) -> Tensor | Matrix:\n",
    "        \"\"\"\n",
    "        Computes the first-order derivative of the Mean Absolute Percentage Error between result and target.\n",
    "        \n",
    "        Args:\n",
    "            axis: None or int, if you intend to get the per-output metrics/derivs, set axis = 0. Else None.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor | Matrix: Derivative of MAPE tensor or matrix with respect to result.\n",
    "        \"\"\"\n",
    "        # Derivative: sign(ratio) * (1/target) / N\n",
    "        ratio = (self.result - self.target) / self.target\n",
    "        if axis is None:\n",
    "            grad = ratio.sign() / (self.target * np.array(ratio.shape).prod())\n",
    "        else:\n",
    "            grad = ratio.sign() / (self.target * ratio.shape[axis])\n",
    "        return grad\n",
    "    \n",
    "    def _deriv_2_mape(self, axis: int | None = None, **kwargs) -> Tensor | Matrix:\n",
    "        \"\"\"\n",
    "        Computes the second-order derivative of per-output MAPE between result and target.\n",
    "        \n",
    "        Args:\n",
    "            axis: None or int, if you intend to get the per-output metrics/derivs, set axis = 0. Else None.\n",
    "                \n",
    "        Returns:\n",
    "            Tensor | Matrix: Hessian diagonal of MAPE (zero), shape (N, D).\n",
    "        \"\"\"\n",
    "        zeros = self.result.copy(); zeros[...] = 0;\n",
    "        return zeros\n",
    "    \n",
    "    def _compute_huber_loss(self, axis: int | None = None, delta: float = 1.0, **kwargs) -> Tensor | Matrix:\n",
    "        \"\"\"\n",
    "        Computes the Huber Loss between result and target.\n",
    "        \n",
    "        Args:\n",
    "            axis: None or int, if you intend to get the per-output metrics/derivs, set axis = 0. Else None.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor | Matrix: Huber Loss tensor or matrix\n",
    "        \"\"\"\n",
    "        error = self.result - self.target\n",
    "        abs_error = error.abs()\n",
    "        # The mask is in an INTERNAL format (np/torch)\n",
    "        small_mask = abs_error.data <= delta\n",
    "        \n",
    "        # The squared region: 0.5 * e^2\n",
    "        sq_loss = 0.5 * (error ** 2)\n",
    "        # The linear region: delta * (|e| - 0.5 * delta)\n",
    "        lin_loss = delta * (abs_error - 0.5 * delta)\n",
    "        \n",
    "        # Huber loss is a combinition of mse and mae\n",
    "        if self.result._is_numpy:\n",
    "            huber = np.where(small_mask, sq_loss.data, lin_loss.data)\n",
    "        else:\n",
    "            huber = torch.where(small_mask, sq_loss.data, lin_loss.data)\n",
    "        \n",
    "        if axis is None:\n",
    "            return type(self.result)(huber, backend=self.result._backend, dtype=self.result.dtype, device=self.result.device).sum(axis = axis) / np.array(error.shape).prod()\n",
    "        else:\n",
    "            return type(self.result)(huber, backend=self.result._backend, dtype=self.result.dtype, device=self.result.device).sum(axis = axis) / error.shape[axis]\n",
    "    \n",
    "    def _deriv_1_huber_loss(self, axis: int | None = None, delta: float = 1.0, **kwargs) -> Tensor | Matrix:\n",
    "        \"\"\"\n",
    "        Computes the first-order derivative of the Huber Loss between result and target.\n",
    "        \n",
    "        Args:\n",
    "            axis: None or int, if you intend to get the per-output metrics/derivs, set axis = 0. Else None.\n",
    "            delta: float, the Huber threshold parameter.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor | Matrix: Derivative of Huber Loss tensor or matrix with respect to result.\n",
    "        \"\"\"\n",
    "        error = self.result - self.target\n",
    "        abs_error = error.abs()\n",
    "        # The mask is in an INTERNAL format (np/torch)\n",
    "        small_mask = abs_error.data <= delta\n",
    "        \n",
    "        if self.result._is_numpy:\n",
    "            grad_elt = np.where(small_mask, error.data, delta * error.sign().data)\n",
    "        else:\n",
    "            grad_elt = torch.where(small_mask, error.data, delta * error.sign().data)\n",
    "        \n",
    "        if axis is None:\n",
    "            return type(self.result)(grad_elt, backend=self.result._backend, dtype=self.result.dtype, device=self.result.device) / np.array(error.shape).prod()\n",
    "        else:\n",
    "            return type(self.result)(grad_elt, backend=self.result._backend, dtype=self.result.dtype, device=self.result.device) / error.shape[axis]\n",
    "\n",
    "    def _deriv_2_huber_loss(self, axis: int | None = None, delta: float = 1.0, **kwargs) -> Tensor | Matrix:\n",
    "        \"\"\"\n",
    "        Computes the second-order derivative (Hessian diagonal) of the Huber Loss between result and target.\n",
    "\n",
    "        Args:\n",
    "            axis: None or int, if you intend to get per-output metrics/derivs, set axis = 0. Else None.\n",
    "            delta: float, the Huber threshold parameter.\n",
    "            \n",
    "        Returns:\n",
    "            Tensor | Matrix: Second-order derivative of Huber Loss wrt result, shape like result.\n",
    "        \"\"\"\n",
    "        error = self.result - self.target\n",
    "        abs_error = error.abs()\n",
    "        # The mask is in an INTERNAL format (np/torch)\n",
    "        small_mask = abs_error.data <= delta\n",
    "\n",
    "        if self.result._is_numpy:\n",
    "            hess_elt = small_mask.astype(float)\n",
    "        else:\n",
    "            # torch.where on a boolean mask: 1.0 where small, else 0.0\n",
    "            one = error.ones(error.shape, backend=self.result._backend).to(backend=self.result._backend, dtype=self.result.dtype, device=self.result.device)\n",
    "            zero = error.zeros(error.shape, backend=self.result._backend).to(backend=self.result._backend, dtype=self.result.dtype, device=self.result.device)\n",
    "            hess_elt = torch.where(small_mask, one.data, zero.data)\n",
    "\n",
    "        if axis is None:\n",
    "            return type(self.result)(hess_elt, backend=self.result._backend, dtype=self.result.dtype, device=self.result.device) / np.array(error.shape).prod()\n",
    "        else:\n",
    "            return type(self.result)(hess_elt, backend=self.result._backend, dtype=self.result.dtype, device=self.result.device) / error.shape[axis]\n",
    "\n",
    "    def _compute_quantile_loss(self, axis: int | None = None, q: float = 0.5, **kwargs) -> Tensor | Matrix:\n",
    "        \"\"\"\n",
    "        Computes the Quantile Loss between result and target.\n",
    "        \n",
    "        Args:\n",
    "            axis: None or int, if you intend to get the per-output metrics/derivs, set axis = 0. Else None.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor | Matrix: Quantile Loss tensor or matrix\n",
    "        \"\"\"\n",
    "        error = self.result - self.target\n",
    "        if self.result._is_numpy:\n",
    "            loss = np.where(error.data >= 0, q * error.data, (q - 1) * error.data)\n",
    "        else:\n",
    "            loss = torch.where(error.data >= 0, q * error.data, (q - 1) * error.data)\n",
    "        \n",
    "        if axis is None:\n",
    "            return type(self.result)(loss, backend=self.result._backend, dtype=self.result.dtype, device=self.result.device).sum(axis = axis) / np.array(error.shape).prod()\n",
    "        else:\n",
    "            return type(self.result)(loss, backend=self.result._backend, dtype=self.result.dtype, device=self.result.device).sum(axis = axis) / error.shape[axis]\n",
    "\n",
    "    def _deriv_1_quantile_loss(self, axis: int | None = None, q: float = 0.5, **kwargs) -> Tensor | Matrix:\n",
    "        \"\"\"\n",
    "        Computes the first-order derivative of the Quantile Loss between result and target.\n",
    "        \n",
    "        Args:\n",
    "            axis: None or int, if you intend to get per-output metrics/derivs, set axis = 0. Else None.\n",
    "            q: float in (0,1), the quantile level.\n",
    "            \n",
    "        Returns:\n",
    "            Tensor | Matrix: Derivative of Quantile Loss tensor or matrix with respect to result.\n",
    "        \"\"\"\n",
    "        error = self.result - self.target\n",
    "        if self.result._is_numpy:\n",
    "            grad_elt = np.where(error.data >= 0, q, q - 1)\n",
    "        else:\n",
    "            grad_elt = torch.where(error.data >= 0, q, q - 1)\n",
    "\n",
    "        if axis is None:\n",
    "            return type(self.result)(grad_elt, backend=self.result._backend, dtype=self.result.dtype, device=self.result.device) / np.array(error.shape).prod()\n",
    "        else:\n",
    "            return type(self.result)(grad_elt, backend=self.result._backend, dtype=self.result.dtype, device=self.result.device) / error.shape[axis]\n",
    "\n",
    "    def _deriv_2_quantile_loss(self, axis: int | None = None, q: float = 0.5, **kwargs) -> Tensor | Matrix:\n",
    "        \"\"\"\n",
    "        Computes the second-order derivative (Hessian diagonal) of the Quantile Loss between result and target.\n",
    "\n",
    "        Args:\n",
    "            axis: None or int, if you intend to get per-output metrics/derivs, set axis = 0. Else None.\n",
    "            q: float in (0,1), the quantile level.\n",
    "            \n",
    "        Returns:\n",
    "            Tensor | Matrix: Second-order derivative of Quantile Loss wrt result, shape like result (all zeros).\n",
    "        \"\"\"\n",
    "        error = self.result - self.target\n",
    "        \n",
    "        if axis is None:\n",
    "            return error.zeros_like(error).to(backend=self.result._backend, dtype=self.result.dtype, device=self.result.device) / np.array(error.shape).prod()\n",
    "        else:\n",
    "            return error.zeros_like(error).to(backend=self.result._backend, dtype=self.result.dtype, device=self.result.device) / error.shape[axis]\n",
    "\n",
    "    def _compute_wmse(self, axis: int | None = None, weights: Matrix | Tensor | None = None, **kwargs) -> Tensor | Matrix:\n",
    "        \"\"\"\n",
    "        Computes the Weighted Mean Squared Error between result and target.\n",
    "        \n",
    "        Args:\n",
    "            axis: None or int, if you intend to get the per-output metrics/derivs, set axis = 0. Else None.\n",
    "            weights: Matrix or Tensor or None, if None, fail to normal mse.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor | Matrix: MSE tensor or matrix\n",
    "        \"\"\"\n",
    "        if weights is None:\n",
    "            return self._compute_mse(axis = axis, **kwargs)\n",
    "        \n",
    "        error = (self.result - self.target)\n",
    "        squared_error = error ** 2\n",
    "        if axis is None:\n",
    "            mean_squared_error = (weights * squared_error).sum(axis = axis) / np.array(squared_error.shape).prod()\n",
    "        else:\n",
    "            mean_squared_error = (weights * squared_error).sum(axis = axis) / squared_error.shape[axis]\n",
    "        return mean_squared_error\n",
    "    \n",
    "    def _deriv_1_wmse(self, axis: int | None = None, weights: Matrix | Tensor | None = None, **kwargs) -> Tensor | Matrix:\n",
    "        \"\"\"\n",
    "        Computes the first-order derivative of the Weighted Mean Squared Error between result and target.\n",
    "        \n",
    "        Args:\n",
    "            axis: None or int, if you intend to get the per-output metrics/derivs, set axis = 0. Else None.\n",
    "            weights: Matrix or Tensor or None, if None, fail to normal mse.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor | Matrix: Derivative of MSE tensor or matrix with respect to result.\n",
    "        \"\"\"\n",
    "        if weights is None:\n",
    "            return self._deriv_1_mse(axis = axis, **kwargs)\n",
    "        \n",
    "        error = self.result - self.target\n",
    "        grad = 2 * error / error.shape[0]\n",
    "        if axis is None:\n",
    "            grad = 2 * weights * error / np.array(error.shape).prod()\n",
    "        else:\n",
    "            grad = 2 * weights * error / error.shape[axis]\n",
    "        return grad\n",
    "    \n",
    "    def _deriv_2_wmse(self, axis: int | None = None, weights: Matrix | Tensor | None = None, **kwargs) -> Tensor | Matrix:\n",
    "        \"\"\"\n",
    "        Computes the second-order derivative (Hessian diagonal) of the Weighted Mean Squared Error between result and target.\n",
    "    \n",
    "        Args:\n",
    "            axis: None or int, if you intend to get the per-output metrics/derivs, set axis = 0. Else None.\n",
    "            weights: Matrix or Tensor or None, if None, fail to normal mse.\n",
    "            \n",
    "        Returns:\n",
    "            Tensor | Matrix: Constant Hessian of MSE (2/N) with the same shape as result.\n",
    "        \"\"\"\n",
    "        if weights is None:\n",
    "            return self._deriv_2_mse(axis = axis, **kwargs)\n",
    "        \n",
    "        ones = self.result.copy(); ones[...] = 1;\n",
    "        if axis is None:\n",
    "            return ones * (2.0 * weights / np.array(self.result.shape).prod())\n",
    "        else:\n",
    "            return ones * (2.0 * weights / self.result.shape[axis])\n",
    "    \n",
    "    def _compute_wrmse(self, axis: int | None = None, weights: Matrix | Tensor | None = None, **kwargs) -> Tensor | Matrix:\n",
    "        \"\"\"\n",
    "        Computes the Weighted Root Mean Squared Error between result and target.\n",
    "        \n",
    "        Args:\n",
    "            axis: None or int, if you intend to get the per-output metrics/derivs, set axis = 0. Else None.\n",
    "            weights: Matrix or Tensor or None, if None, fail to normal rmse.\n",
    "            \n",
    "        Returns:\n",
    "            Tensor | Matrix: RMSE tensor or matrix\n",
    "        \"\"\"\n",
    "        if weights is None:\n",
    "            return self._compute_rmse(axis = axis, **kwargs)\n",
    "        \n",
    "        error = (self.result - self.target)\n",
    "        squared_error = weights * error ** 2\n",
    "        if axis is None:\n",
    "            mean_squared_error = squared_error.sum(axis = axis) / np.array(squared_error.shape).prod()\n",
    "        else:\n",
    "            mean_squared_error = squared_error.sum(axis = axis) / squared_error.shape[axis]\n",
    "        rmse = mean_squared_error ** 0.5\n",
    "        return rmse\n",
    "    \n",
    "    def _deriv_1_wrmse(self, axis: int | None = None, weights: Matrix | Tensor | None = None, **kwargs) -> Tensor | Matrix:\n",
    "        \"\"\"\n",
    "        Computes the first-order derivative of the Weighted Root Mean Squared Error between result and target.\n",
    "        \n",
    "        Args:\n",
    "            axis: None or int, if you intend to get the per-output metrics/derivs, set axis = 0. Else None.\n",
    "            weights: Matrix or Tensor or None, if None, fail to normal rmse.\n",
    "            \n",
    "        Returns:\n",
    "            Tensor | Matrix: Derivative of RMSE tensor or matrix with respect to result.\n",
    "        \"\"\"\n",
    "        if weights is None:\n",
    "            return self._deriv_1_rmse(axis = axis, **kwargs)\n",
    "        \n",
    "        error = self.result - self.target\n",
    "        squared_error = weights * error ** 2\n",
    "        if axis is None:\n",
    "            mean_squared_error = squared_error.sum(axis = axis) / np.array(squared_error.shape).prod()\n",
    "            rmse = mean_squared_error ** 0.5\n",
    "            grad = weights * error / (np.array(squared_error.shape).prod() * rmse)\n",
    "        else:\n",
    "            mean_squared_error = squared_error.sum(axis = axis) / squared_error.shape[axis]\n",
    "            rmse = mean_squared_error ** 0.5\n",
    "            grad = weights * error / (error.shape[axis] * rmse)\n",
    "\n",
    "        return grad\n",
    "    \n",
    "    def _deriv_2_wrmse(self, axis: int | None = None, weights: Matrix | Tensor | None = None, **kwargs) -> Tensor | Matrix:\n",
    "        \"\"\"\n",
    "        Computes the second-order derivative of the Weighted Root Mean Squared Error between result and target.\n",
    "        \n",
    "        Args:\n",
    "            axis: None or int, if you intend to get the per-output metrics/derivs, set axis = 0. Else None.\n",
    "            weights: Matrix or Tensor or None, if None, fail to normal rmse.\n",
    "            \n",
    "        Returns:\n",
    "            Tensor | Matrix: Second-order derivative (Hessian diagonal) of RMSE with respect to result.\n",
    "        \"\"\"\n",
    "        if weights is None:\n",
    "            return self._deriv_2_rmse(axis = axis, **kwargs)\n",
    "        \n",
    "        error = self.result - self.target\n",
    "        squared_error = weights * error ** 2\n",
    "        if axis is None:\n",
    "            sum_squared = squared_error.sum(axis = axis)\n",
    "            mean_squared_error = sum_squared / np.array(squared_error.shape).prod()\n",
    "            rmse = mean_squared_error ** 0.5\n",
    "            N = np.array(squared_error.shape).prod()\n",
    "            hessian = weights / (N * rmse) - (weights **2 * error ** 2) / ((N ** 2) * (rmse ** 3))\n",
    "        else:\n",
    "            sum_squared = squared_error.sum(axis = axis)\n",
    "            mean_squared_error = sum_squared / squared_error.shape[axis]\n",
    "            rmse = mean_squared_error ** 0.5\n",
    "            N = error.shape[axis] \n",
    "            hessian = weights / (N * rmse) - (weights **2 * error ** 2) / ((N ** 2) * (rmse ** 3))\n",
    "        \n",
    "        return hessian\n",
    "    \n",
    "    def _compute_r2(self, **kwargs) -> Tensor | Matrix:\n",
    "        \"\"\"\n",
    "        Computes the coefficient of determination R^2 between result and target.\n",
    "        \n",
    "        Args:\n",
    "            axis: None or int, if you intend to get the per-output metrics/derivs, set axis = 0. Else None.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor | Matrix: R^2 value.\n",
    "        \"\"\"\n",
    "        # Compute the residual sum of squares (SS_res)\n",
    "        error = self.result - self.target\n",
    "        ss_res = (error ** 2.0).sum()\n",
    "        \n",
    "        # Compute the total sum of squares (SS_tot)\n",
    "        target_mean = self.target.sum() / self.target.shape[0]\n",
    "        total_error = self.target - target_mean\n",
    "        ss_tot = (total_error ** 2.0).sum()\n",
    "        \n",
    "        # Calculate R^2 = 1 - (SS_res / SS_tot)\n",
    "        r2 = 1 - (ss_res / ss_tot)\n",
    "        return r2\n",
    "    \n",
    "    def _compute_adjusted_r2(self, **kwargs) -> Tensor | Matrix:\n",
    "        \"\"\"\n",
    "        Computes the adjusted R^2 value.\n",
    "        \n",
    "        Args:\n",
    "            axis: None or int, if you intend to get the per-output metrics/derivs, set axis = 0. Else None.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor | Matrix: Adjusted R^2 value.\n",
    "        \"\"\"\n",
    "        # If self.k is None, badly initialized.\n",
    "        if self.k is None or isinstance(self.k, int) == False:\n",
    "            raise ValueError(\"You must specify a valid `k` as the number of parameters in the model before calculating Adjusted R^2.\")\n",
    "        \n",
    "        # Compute R^2 using the previously defined method.\n",
    "        r2 = self._compute_r2()\n",
    "        \n",
    "        # Determine the number of observations (be the size along the first dimension)\n",
    "        n = self.target.shape[0]\n",
    "        \n",
    "        # Calculate adjusted R^2 using: 1 - (1-R^2)*((n-1)/(n-p-1))\n",
    "        adjusted_r2 = 1 - ((1 - r2) * ((n - 1) /  (n - self.k - 1)))\n",
    "        return adjusted_r2\n",
    "    \n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        String representation of the RegressionMetrics instance.\n",
    "        \"\"\"\n",
    "        return f\"RegressionMetrics(metric_type={self.metric_type}, shape={self.result.shape})\"\n",
    "\n",
    "\n",
    "# Metrics for classfication (base)\n",
    "class ClassificationMetrics(BaseMetrics):\n",
    "\n",
    "    __attr__ = \"MML.ClassificationMetrics\"    \n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"ClassificationMetrics(Abstract Class).\"\n",
    "\n",
    "\n",
    "# Metrics for binary classification\n",
    "class BinaryClassificationMetrics(ClassificationMetrics):\n",
    "    \"\"\"\n",
    "    A class to compute common binary classification metrics between predicted results and target values.\n",
    "    \n",
    "    Supported metrics include:\n",
    "        - accuracy\n",
    "        - precision\n",
    "        - recall (sensitivity) [TPR]\n",
    "        - f1 score\n",
    "        - specificity [TNR]\n",
    "        - auc_roc\n",
    "        - confusion_matrix\n",
    "        - tpr (True Positive Rate)\n",
    "        - tnr (True Negative Rate)\n",
    "        - fpr (False Positive Rate)\n",
    "        - fnr (False Negative Rate)\n",
    "        - logloss (continuous)\n",
    "    \n",
    "    The computations are performed using the underlying tensor operations. It is assumed that both \n",
    "    the result and target are of the same type (Tensor or Matrix) and support similar operations.\n",
    "    \n",
    "    Attributes:\n",
    "        result: Predicted results tensor or matrix (can be continuous scores or binary labels).\n",
    "        target: Target binary values tensor or matrix.\n",
    "        metric_type: A string specifying which metric to compute ('accuracy', 'precision', 'recall',\n",
    "                     'f1', 'specificity', 'auc_roc', 'confusion_matrix').\n",
    "        threshold: A float value used to convert continuous scores into binary predictions (default 0.5).\n",
    "    \"\"\"\n",
    "    \n",
    "    __attr__ = \"MML.BinaryClassificationMetrics\"   \n",
    "    \n",
    "    def __init__(self, result: Tensor | Matrix, target: Tensor | Matrix, metric_type: str = \"accuracy\", threshold: float = 0.5, **kwargs):\n",
    "        \"\"\"\n",
    "        Initializes the BinaryClassificationMetrics instance with result and target tensors.\n",
    "        \n",
    "        Args:\n",
    "            result (Tensor | Matrix): Predicted results tensor\n",
    "            target (Tensor | Matrix): Target values tensor\n",
    "            metric_type (str): Metric type to compute ('accuracy', 'precision', 'recall', 'f1', 'specificity',\n",
    "                               'auc_roc', 'confusion_matrix', 'tpr', 'tnr', 'fpr', 'fnr', 'logloss')\n",
    "            threshold (float): a threshold for considering which one to be the positive samples and negative samples.\n",
    "                               In normal tasks, it is recommended to be 0.5. But adjusting this may change the metrics.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Different instances or different backends.\n",
    "        if isinstance(result, Object) == False or isinstance(target, Object) == False:\n",
    "            raise ValueError(\"Predicted `result` and real `target` should be either `Matrix` or `Tensor` type!\")\n",
    "        if type(result) != type(target):\n",
    "            raise ValueError(\"Predicted `result` and real `target` should have the same type, either Tensor or Matrix!\")\n",
    "        if result._backend != target._backend:\n",
    "            raise ValueError(\"Predicted `result` and real `target` should have the same backend, either numpy or torch!\")\n",
    "        \n",
    "        # Data Members.\n",
    "        self.result = result\n",
    "        self.target = target\n",
    "        self.metric_type = metric_type.lower()\n",
    "        self.threshold = threshold\n",
    "        \n",
    "        # Use the type of result as the typeclass.\n",
    "        self.typeclass = type(result)\n",
    "        \n",
    "        if not self.result.shape == self.target.shape:\n",
    "            raise ValueError(\"Result and target tensors must have the same shape.\")\n",
    "\n",
    "    def compute(self, **kwargs) -> Matrix | Tensor:\n",
    "        \"\"\"\n",
    "        Computes the specified metric for a given model or data.\n",
    "    \n",
    "        Args:\n",
    "            **kwargs: Other arguments supported by metrics.\n",
    "            \n",
    "        Returns:\n",
    "            Matrix | Tensor: The computed metric value. The result is always returned as a Matrix or Tensor object,\n",
    "                                      even if the computation yields a scalar.                            \n",
    "        \n",
    "        Raises:\n",
    "            ValueError: If an unsupported metric type is provided.\n",
    "            \n",
    "        \"\"\"\n",
    "        # Note, all results are stored in a Matrix | Tensor even it is a scalar.\n",
    "        if self.metric_type == 'accuracy':\n",
    "            return self._compute_accuracy(**kwargs)\n",
    "        elif self.metric_type == 'precision':\n",
    "            return self._compute_precision(**kwargs)\n",
    "        elif self.metric_type in ('recall', 'sensitivity', 'tpr'):\n",
    "            return self._compute_recall(**kwargs)\n",
    "        elif self.metric_type in ('f1', 'f1 score'):\n",
    "            return self._compute_f1(**kwargs)\n",
    "        elif self.metric_type in ('specificity', 'tnr'):\n",
    "            return self._compute_specificity(**kwargs)\n",
    "        elif self.metric_type == 'fpr':\n",
    "            return self._compute_fpr(**kwargs)\n",
    "        elif self.metric_type == 'fnr':\n",
    "            return self._compute_fnr(**kwargs)\n",
    "        elif self.metric_type == 'auc_roc':\n",
    "            return self._compute_auc_roc(**kwargs)\n",
    "        elif self.metric_type == 'confusion_matrix':\n",
    "            return self._compute_confusion_matrix(**kwargs)\n",
    "        elif self.metric_type in ('logloss', 'log-loss', 'entropy', 'cross-entropy'):\n",
    "            return self._compute_logloss(**kwargs)\n",
    "        # Implemented by Nathmath Huang.\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported metric type: {self.metric_type}\")\n",
    "    \n",
    "    def deriv_1(self, **kwargs) -> Tensor | Matrix:\n",
    "        \"\"\"\n",
    "        Computes the sample-wise 1st order derivative for a given model or data.\n",
    "        \n",
    "        Args:\n",
    "            **kwargs: Other arguments supported by metrics.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor | Matrix: The computed metric value as a tensor\n",
    "        \"\"\"\n",
    "        # Note, all results are stored in a Matrix | Tensor even it is a scalar.\n",
    "        if self.metric_type in ('logloss', 'log-loss', 'entropy', 'cross-entropy'):\n",
    "            return self._deriv_1_logloss(**kwargs)\n",
    "        elif self.metric_type == 'accuracy':\n",
    "            raise  ValueError(f\"Metric type: {self.metric_type} cannot compute derivatives.\")\n",
    "        elif self.metric_type == 'precision':\n",
    "            raise  ValueError(f\"Metric type: {self.metric_type} cannot compute derivatives.\")\n",
    "        elif self.metric_type in ('recall', 'sensitivity', 'tpr'):\n",
    "            raise  ValueError(f\"Metric type: {self.metric_type} cannot compute derivatives.\")\n",
    "        elif self.metric_type in ('f1', 'f1 score'):\n",
    "            raise  ValueError(f\"Metric type: {self.metric_type} cannot compute derivatives.\")\n",
    "        elif self.metric_type in ('specificity', 'tnr'):\n",
    "            raise  ValueError(f\"Metric type: {self.metric_type} cannot compute derivatives.\")\n",
    "        elif self.metric_type == 'fpr':\n",
    "            raise  ValueError(f\"Metric type: {self.metric_type} cannot compute derivatives.\")\n",
    "        elif self.metric_type == 'fnr':\n",
    "            raise  ValueError(f\"Metric type: {self.metric_type} cannot compute derivatives.\")\n",
    "        elif self.metric_type == 'auc_roc':\n",
    "            raise  ValueError(f\"Metric type: {self.metric_type} cannot compute derivatives.\")\n",
    "        elif self.metric_type == 'confusion_matrix':\n",
    "            raise  ValueError(f\"Metric type: {self.metric_type} cannot compute derivatives.\")\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported metric type: {self.metric_type}\")\n",
    "    \n",
    "    def deriv_2(self, **kwargs) -> Tensor | Matrix:\n",
    "        \"\"\"\n",
    "        Computes the sample-wise 2nd order derivative for a given model or data.\n",
    "        \n",
    "        Args:\n",
    "            **kwargs: Other arguments supported by metrics.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor | Matrix: The computed metric value as a tensor\n",
    "        \"\"\"\n",
    "        # Note, all results are stored in a Matrix | Tensor even it is a scalar.\n",
    "        if self.metric_type in ('logloss', 'log-loss', 'entropy', 'cross-entropy'):\n",
    "            return self._deriv_2_logloss(**kwargs)\n",
    "        elif self.metric_type == 'accuracy':\n",
    "            raise  ValueError(f\"Metric type: {self.metric_type} cannot compute derivatives.\")\n",
    "        elif self.metric_type == 'precision':\n",
    "            raise  ValueError(f\"Metric type: {self.metric_type} cannot compute derivatives.\")\n",
    "        elif self.metric_type in ('recall', 'sensitivity', 'tpr'):\n",
    "            raise  ValueError(f\"Metric type: {self.metric_type} cannot compute derivatives.\")\n",
    "        elif self.metric_type in ('f1', 'f1 score'):\n",
    "            raise  ValueError(f\"Metric type: {self.metric_type} cannot compute derivatives.\")\n",
    "        elif self.metric_type in ('specificity', 'tnr'):\n",
    "            raise  ValueError(f\"Metric type: {self.metric_type} cannot compute derivatives.\")\n",
    "        elif self.metric_type == 'fpr':\n",
    "            raise  ValueError(f\"Metric type: {self.metric_type} cannot compute derivatives.\")\n",
    "        elif self.metric_type == 'fnr':\n",
    "            raise  ValueError(f\"Metric type: {self.metric_type} cannot compute derivatives.\")\n",
    "        elif self.metric_type == 'auc_roc':\n",
    "            raise  ValueError(f\"Metric type: {self.metric_type} cannot compute derivatives.\")\n",
    "        elif self.metric_type == 'confusion_matrix':\n",
    "            raise  ValueError(f\"Metric type: {self.metric_type} cannot compute derivatives.\")\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported metric type: {self.metric_type}\")\n",
    "            \n",
    "    def _binarize(self, y_real_or_pred: Matrix | Tensor) -> Matrix | Tensor:\n",
    "        \"\"\"\n",
    "        Binarizes continuous prediction scores by applying a threshold.\n",
    "    \n",
    "        Args:\n",
    "            y_real_or_pred (Matrix | Tensor): the y values to be binarized.\n",
    "    \n",
    "        Returns:\n",
    "            Matrix | Tensor: A matrix or tensor containing binary predictions (True/False values).\n",
    "    \n",
    "        \"\"\"\n",
    "        # Return the results in a Matrix or Tensor of Booleans\n",
    "        return self.typeclass(y_real_or_pred.data >= self.threshold, backend = y_real_or_pred._backend, device = y_real_or_pred.device)\n",
    "    \n",
    "    def _compute_confusion_counts(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Computes the counts of true positives (TP), true negatives (TN), \n",
    "                     false positives (FP) and false negatives (FN) using binarized predictions.\n",
    "    \n",
    "        Args:\n",
    "            None\n",
    "    \n",
    "        Returns:\n",
    "            tuple: A tuple containing four elements, each representing TP, TN, FP, and FN respectively.\n",
    "                  Each element is a matrix or tensor of the same type as self.target.\n",
    "    \n",
    "        \"\"\"\n",
    "        pred = self._binarize(self.result)  # Full of Booleans.\n",
    "        real = self._binarize(self.target)  # Full of Booleans.\n",
    "\n",
    "        TP = ((pred.data == True) & (real.data == True)).sum()\n",
    "        TN = ((pred.data == False) & (real.data == False)).sum()\n",
    "        FP = ((pred.data == True) & (real.data == False)).sum()\n",
    "        FN = ((pred.data == False) & (real.data == True)).sum()\n",
    "        return (self.typeclass(TP, backend=self.target._backend, dtype=self.target.dtype, device=self.target.device), \n",
    "                self.typeclass(TN, backend=self.target._backend, dtype=self.target.dtype, device=self.target.device),\n",
    "                self.typeclass(FP, backend=self.target._backend, dtype=self.target.dtype, device=self.target.device),\n",
    "                self.typeclass(FN, backend=self.target._backend, dtype=self.target.dtype, device=self.target.device)\n",
    "                )\n",
    "\n",
    "    def _compute_accuracy(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Computes accuracy = (TP + TN) / total.\n",
    "        \n",
    "        Args:\n",
    "            None\n",
    "    \n",
    "        Returns:\n",
    "            Matrix | Tensor: The computed accuracy value.\n",
    "        \n",
    "        \"\"\"\n",
    "        # Always return a Matrix | Tensor as the class input.\n",
    "        TP, TN, FP, FN = self._compute_confusion_counts()\n",
    "        total = TP + TN + FP + FN\n",
    "        return (TP + TN) / total\n",
    "\n",
    "    def _compute_precision(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Computes precision = TP / (TP + FP).\n",
    "    \n",
    "        Args:\n",
    "            None\n",
    "    \n",
    "        Returns:\n",
    "            Matrix | Tensor: The computed precision value.\n",
    "        \n",
    "        \"\"\"\n",
    "        # Always return a Matrix | Tensor as the class input.\n",
    "        TP, _, FP, _ = self._compute_confusion_counts()\n",
    "        denom = TP + FP\n",
    "        if bool(denom.data == 0) == True:\n",
    "            return self.typeclass(0, backend=self.target._backend, dtype=self.target.dtype, device=self.target.device)\n",
    "        return TP / denom\n",
    "\n",
    "    def _compute_recall(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Computes recall (sensitivity) = TP / (TP + FN).\n",
    "        \n",
    "        Args:\n",
    "            None\n",
    "    \n",
    "        Returns:\n",
    "            Matrix | Tensor: The computed recall value.\n",
    "        \n",
    "        \"\"\"\n",
    "        # Always return a Matrix | Tensor as the class input.\n",
    "        TP, _, _, FN = self._compute_confusion_counts()\n",
    "        denom = TP + FN\n",
    "        if bool(denom.data == 0) == True:\n",
    "            return self.typeclass(0, backend=self.target._backend, dtype=self.target.dtype, device=self.target.device)\n",
    "        return TP / denom\n",
    "\n",
    "    def _compute_f1(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Computes the F1 score as the harmonic mean of precision and recall.\n",
    "        \n",
    "        Args:\n",
    "            None\n",
    "    \n",
    "        Returns:\n",
    "            Matrix | Tensor: The computed f1 score value.\n",
    "        \n",
    "        \"\"\"\n",
    "        # Always return a Matrix | Tensor as the class input.\n",
    "        TP, TN, FP, FN = self._compute_confusion_counts()\n",
    "        denom = 2 * TP + FP + FN\n",
    "        if bool(denom.data == 0) == True:\n",
    "            return self.typeclass(0, backend=self.target._backend, dtype=self.target.dtype, device=self.target.device)\n",
    "        return 2 * TP / denom\n",
    "\n",
    "    def _compute_specificity(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Computes specificity = TN / (TN + FP).\n",
    "        \n",
    "        Args:\n",
    "            None\n",
    "    \n",
    "        Returns:\n",
    "            Matrix | Tensor: The computed specificity value.\n",
    "        \"\"\"\n",
    "        # Always return a Matrix | Tensor as the class input.\n",
    "        _, TN, FP, _ = self._compute_confusion_counts()\n",
    "        denom = TN + FP\n",
    "        if bool(denom.data == 0) == True:\n",
    "            return self.typeclass(0, backend=self.target._backend, dtype=self.target.dtype, device=self.target.device)\n",
    "        return TN / denom\n",
    "\n",
    "    def _compute_tpr(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Computes recall (TPR) = TP / (TP + FN).\n",
    "        \n",
    "        Args:\n",
    "            None\n",
    "    \n",
    "        Returns:\n",
    "            Matrix | Tensor: The computed TPR value.\n",
    "        \n",
    "        \"\"\"\n",
    "        # Always return a Matrix | Tensor as the class input.\n",
    "        TP, _, _, FN = self._compute_confusion_counts()\n",
    "        denom = TP + FN\n",
    "        if bool(denom.data == 0) == True:\n",
    "            return self.typeclass(0, backend=self.target._backend, dtype=self.target.dtype, device=self.target.device)\n",
    "        return TP / denom\n",
    "\n",
    "    def _compute_tnr(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Computes specificity (TNR) = TN / (TN + FP).\n",
    "        \n",
    "        Args:\n",
    "            None\n",
    "    \n",
    "        Returns:\n",
    "            Matrix | Tensor: The computed TNR value.\n",
    "        \"\"\"\n",
    "        # Always return a Matrix | Tensor as the class input.\n",
    "        _, TN, FP, _ = self._compute_confusion_counts()\n",
    "        denom = TN + FP\n",
    "        if bool(denom.data == 0) == True:\n",
    "            return self.typeclass(0, backend=self.target._backend, dtype=self.target.dtype, device=self.target.device)\n",
    "        return TN / denom\n",
    "\n",
    "    def _compute_fpr(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Computes FPR = FP / (FP + TN).\n",
    "        \n",
    "        Args:\n",
    "            None\n",
    "    \n",
    "        Returns:\n",
    "            Matrix | Tensor: The computed TPR value.\n",
    "        \n",
    "        \"\"\"\n",
    "        # Always return a Matrix | Tensor as the class input.\n",
    "        _, TN, FP, _ = self._compute_confusion_counts()\n",
    "        denom = FP + TN\n",
    "        if bool(denom.data == 0) == True:\n",
    "            return self.typeclass(0, backend=self.target._backend, dtype=self.target.dtype, device=self.target.device)\n",
    "        return FP / denom\n",
    "\n",
    "    def _compute_fnr(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Computes FNR = FN / (TP + FN).\n",
    "        \n",
    "        Args:\n",
    "            None\n",
    "    \n",
    "        Returns:\n",
    "            Matrix | Tensor: The computed FNR value.\n",
    "        \n",
    "        \"\"\"\n",
    "        TP, _, _, FN = self._compute_confusion_counts()\n",
    "        denom = TP + FN\n",
    "        if bool(denom.data == 0) == True:\n",
    "            return self.typeclass(0, backend=self.target._backend, dtype=self.target.dtype, device=self.target.device)\n",
    "        return FN / denom\n",
    "\n",
    "    def _compute_auc_roc(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Computes the area under the ROC curve (AUC-ROC) using the trapezoidal rule.\n",
    "        This method assumes that self.result contains continuous prediction scores.\n",
    "        \n",
    "        Args:\n",
    "            None\n",
    "    \n",
    "        Returns:\n",
    "            Matrix | Tensor: The computed auc_roc area.\n",
    "        \"\"\"\n",
    "        # Always return a Matrix | Tensor as the class input.\n",
    "        scores = self.result.data\n",
    "        labels = self.target.data\n",
    "        \n",
    "        # Sort indices based on scores in descending order.\n",
    "        sorted_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)\n",
    "        sorted_labels = [labels[i] for i in sorted_indices]\n",
    "        P = sum(labels)\n",
    "        N = len(labels) - P\n",
    "        if P == 0 or N == 0:\n",
    "            return self.typeclass(0.0, backend=self.target._backend, dtype=self.target.dtype, device=self.target.device)\n",
    "\n",
    "        tpr, fpr = [], []\n",
    "        tp = 0\n",
    "        fp = 0\n",
    "        for label in sorted_labels:\n",
    "            if label == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fp += 1\n",
    "            tpr.append(tp / P)\n",
    "            fpr.append(fp / N)\n",
    "        \n",
    "        auc = 0.0\n",
    "        prev_fpr = 0.0\n",
    "        prev_tpr = 0.0\n",
    "        for current_fpr, current_tpr in zip(fpr, tpr):\n",
    "            auc += (current_fpr - prev_fpr) * (current_tpr + prev_tpr) / 2.0\n",
    "            prev_fpr = current_fpr\n",
    "            prev_tpr = current_tpr\n",
    "        return self.typeclass(auc, backend=self.target._backend, dtype=self.target.dtype, device=self.target.device)\n",
    "\n",
    "    def _compute_logloss(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Computes the binary classification log loss between predicted and actual values.\n",
    "    \n",
    "        Args: \n",
    "            None\n",
    "            \n",
    "        Returns:\n",
    "            Matrix | Tensor: The computed logloss using this formula:\n",
    "                logloss = - (y_true * log(y_pred) + (1 - y_true) * log(1 - y_pred))\n",
    "        \"\"\"\n",
    "        epsilon = 1e-15\n",
    "        preds = self.result.to(self.result._backend, dtype=float)\n",
    "        labels = self.target.to(self.result._backend, dtype=float)\n",
    "        clipped_preds = preds.clip(epsilon, 1 - epsilon)\n",
    "        losses = -(labels * clipped_preds.log() + (1 - labels) * (1 - clipped_preds).log())\n",
    "        return losses.mean()\n",
    "\n",
    "    def _deriv_1_logloss(self, **kwargs) -> Tensor | Matrix:\n",
    "        \"\"\"\n",
    "        Computes the first-order derivative of the binary classification log loss between predicted and actual values.\n",
    "\n",
    "        Args: \n",
    "            None\n",
    "            \n",
    "        Returns:\n",
    "            Matrix | Tensor: Derivative of logloss with respect to the predictions.\n",
    "        \"\"\"\n",
    "        epsilon = 1e-15\n",
    "        preds = self.result.to(self.result._backend, dtype=float)\n",
    "        labels = self.target.to(self.result._backend, dtype=float)\n",
    "        clipped = preds.clip(epsilon, 1 - epsilon)\n",
    "        grad = ((1 - labels) / (1 - clipped) - labels / clipped) / clipped.shape[0]\n",
    "        return grad\n",
    "    \n",
    "    def _deriv_2_logloss(self, **kwargs) -> Tensor | Matrix:\n",
    "        \"\"\"\n",
    "        Computes the second-order derivative of the binary classification log loss between predicted and actual values.\n",
    "\n",
    "        Args: \n",
    "            None\n",
    "            \n",
    "        Returns:\n",
    "            Matrix | Tensor: Second-order derivative (Hessian diagonal) of logloss with respect to the predictions.\n",
    "        \"\"\"\n",
    "        epsilon = 1e-15\n",
    "        preds = self.result.to(self.result._backend, dtype=float)\n",
    "        labels = self.target.to(self.result._backend, dtype=float)\n",
    "        clipped = preds.clip(epsilon, 1 - epsilon)\n",
    "        hess = ((1 - labels) / (1 - clipped) ** 2 + labels / clipped ** 2) / clipped.shape[0]\n",
    "        return hess\n",
    "\n",
    "    def _compute_confusion_matrix(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Computes the confusion matrix as a 2x2 tensor or matrix with the format:\n",
    "          [[TP, FP],\n",
    "           [FN, TN]]\n",
    "            \n",
    "        Args:\n",
    "            None\n",
    "    \n",
    "        Returns:\n",
    "            Matrix | Tensor: The computed confusion matrix, with shape 2,2.\n",
    "        \"\"\"\n",
    "        TP, TN, FP, FN = self._compute_confusion_counts()\n",
    "        return self.typeclass(\n",
    "            [TP.data, FP.data,\n",
    "             FN.data, TN.data], \n",
    "            backend=self.target._backend, dtype=self.target.dtype, device=self.target.device).reshape([2,2])\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"BinaryClassificationMetrics(metric_type={self.metric_type}, shape={self.result.shape})\"\n",
    "\n",
    "\n",
    "# Metrics for multi-class classification\n",
    "class MultiClassificationMetrics(ClassificationMetrics):\n",
    "    \"\"\"\n",
    "    A class to compute common multi-class classification metrics between predicted results and target values.\n",
    "    \n",
    "    Supported metrics include:\n",
    "        - accuracy\n",
    "        - precision        (macro-average computed either in one-vs-rest (OVR) or one-vs-one (OVO) mode)\n",
    "        - recall           (macro-average computed either in OVR or OVO mode)\n",
    "        - f1 score         (macro-average computed either in OVR or OVO mode)\n",
    "        - logloss          (cross-entropy loss, continuous)\n",
    "        - confusion_matrix (of shape [n_classes, n_classes])\n",
    "        \n",
    "    The class is designed to support two scenarios:\n",
    "        1. Multi-target: where predictions are provided as a 1D vector of labels\n",
    "           (e.g. 0, 1, 2, 3, ...) and the target is also a vector.\n",
    "        2. One-hot: where the target (and optionally predictions) are provided as a\n",
    "           one-hot encoded matrix of shape [n_samples, n_classes].\n",
    "    \n",
    "    When computing precision, recall, and f1-score, the user can specify\n",
    "    whether the aggregation should be based on one-vs-rest (default) or one-vs-one.\n",
    "    \n",
    "    Attributes:\n",
    "        result (Tensor | Matrix): Predicted results. Can be either a 1D vector (labels) \n",
    "                                   or a 2D matrix (probabilities / one-hot scores). \n",
    "        target (Tensor | Matrix): True labels. Must be in a format compatible with result\n",
    "                                   (either both 1D or both 2D, or convertible between them).\n",
    "        metric_type (str): Which metric to compute (\"accuracy\", \"precision\", \"recall\",\n",
    "                           \"f1\", \"logloss\", \"confusion_matrix\").\n",
    "        mode (str): For metrics that require binary decomposition (\"precision\",\n",
    "                    \"recall\", \"f1\"), the aggregation mode: either \"ovr\" (one-vs-rest) or \"ovo\" (one-vs-one).\n",
    "    \"\"\"\n",
    "    \n",
    "    __attr__ = \"MML.MultiClassificationMetrics\"   \n",
    "    \n",
    "    def __init__(self, result: Tensor | Matrix, target: Tensor | Matrix, metric_type: str = \"accuracy\", n_classes: int = None, mode: str = \"ovr\", **kwargs):\n",
    "        \"\"\"\n",
    "        Initializes the MultiClassificationMetrics instance with result and target tensors.\n",
    "        \n",
    "        Args:\n",
    "            result (Tensor | Matrix): Predicted results tensor\n",
    "            target (Tensor | Matrix): Target values tensor\n",
    "            metric_type (str): Metric type to compute ('accuracy', 'precision', 'recall', 'f1', 'confusion_matrix', 'logloss')\n",
    "            n_classes (int): Number of Classes\n",
    "            mode (str): `ovr` or `ovo`, one versus remaining or one versus one.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Check type compatibility\n",
    "        if isinstance(result, Object) == False or isinstance(target,  Object) == False:\n",
    "            raise ValueError(\"Predicted `result` and real `target` should be either `Matrix` or `Tensor` type!\")\n",
    "        if type(result) != type(target):\n",
    "            raise ValueError(\"Predicted `result` and real `target` should have the same type, either Tensor or Matrix!\")\n",
    "        if result._backend != target._backend:\n",
    "            raise ValueError(\"Predicted `result` and real `target` should have the same backend, either numpy or torch!\")\n",
    "\n",
    "        # Data Members.        \n",
    "        self.result = result\n",
    "        self.target = target\n",
    "        self.metric_type = metric_type.lower()\n",
    "        self.mode = mode.lower()\n",
    "        self.typeclass = type(result)\n",
    "        \n",
    "        # Determine classification format and number of classes.\n",
    "        # If given, then okay, or infer.\n",
    "        # If one of the inputs is two-dimensional, we assume the second dimension is the number of classes.\n",
    "        if n_classes is not None:\n",
    "            self.n_classes = n_classes\n",
    "        else:\n",
    "            if len(result.shape) == 2:\n",
    "                if result.shape[1] == 1:\n",
    "                    # Check if it is indeed a binary problem\n",
    "                    if len(result.flatten().bincount().unique()) == 2:\n",
    "                        self.n_classes = 2\n",
    "                    else:\n",
    "                        self.n_classes = len(result.flatten().bincount().unique())\n",
    "                        # Not safe, but okay.\n",
    "                else:\n",
    "                    self.n_classes = result.shape[1]\n",
    "            elif len(target.shape) == 2:\n",
    "                if target.shape[1] == 1:\n",
    "                    # Check if it is indeed a binary problem\n",
    "                    if len(target.flatten().unique()) == 2:\n",
    "                        self.n_classes = 2\n",
    "                    else:\n",
    "                        self.n_classes = len(target.flatten().unique())\n",
    "                        # Not safe, but okay.\n",
    "                else:\n",
    "                    self.n_classes = target.shape[1]\n",
    "            else:\n",
    "                # Error. The result dimension is not 2?!!\n",
    "                raise ValueError(\"The input `result` and `target` do not have a 2-dimension shape. Make sure it is a multi-classification problem. Set n_classes or resize the Matrix | Tensor if you only have one row.\")\n",
    "        \n",
    "    def compute(self, eps: float = 1e-15, floattype: type = float, **kwargs) -> Matrix | Tensor:\n",
    "        \"\"\"\n",
    "        Computes the specified multi-class metric.\n",
    "        \n",
    "        Supported metric_type values (case insensitive):\n",
    "            - 'accuracy'\n",
    "            - 'precision'\n",
    "            - 'recall'\n",
    "            - 'f1'\n",
    "            - 'logloss'\n",
    "            - 'confusion_matrix'\n",
    "        \n",
    "        For precision, recall and f1, the results are computed according to the specified mode (ovr or ovo).\n",
    "        \n",
    "        Args:\n",
    "            eps: float, clip value to ensure 0/0 cases.\n",
    "            floattype: type, the internal precision of calculation.\n",
    "            **kwargs: Other arguments supported by metrics.\n",
    "    \n",
    "        Returns:\n",
    "            Matrix | Tensor: The computed metric value. The result is always returned as a Matrix or Tensor object,\n",
    "                                      even if the computation yields a scalar.\n",
    "        \n",
    "        Raises:\n",
    "            ValueError: If an unsupported metric type or mode type is provided.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Note, all results are stored in a Matrix | Tensor even it is a scalar.\n",
    "        # Accuracy\n",
    "        if self.metric_type == 'accuracy':\n",
    "            return self._compute_accuracy(floattype=floattype, **kwargs)\n",
    "        # Precision\n",
    "        elif self.metric_type == 'precision':\n",
    "            if self.mode == 'ovr':\n",
    "                return self._compute_precision_ovr(eps=eps, floattype=floattype, **kwargs)\n",
    "            elif self.mode == 'ovo':\n",
    "                return self._compute_precision_ovo(eps=eps, floattype=floattype, **kwargs)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported mode for precision: {self.mode}. Use `ovo` or `ovr`.\")\n",
    "        # Recall\n",
    "        elif self.metric_type in ('recall', 'sensitivity', 'tpr'):\n",
    "            if self.mode == 'ovr':\n",
    "                return self._compute_recall_ovr(eps=eps, floattype=floattype, **kwargs)\n",
    "            elif self.mode == 'ovo':\n",
    "                return self._compute_recall_ovo(eps=eps, floattype=floattype, **kwargs)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported mode for recall: {self.mode}. Use `ovo` or `ovr`.\")\n",
    "        # F1 Score\n",
    "        elif self.metric_type in ('f1', 'f1 score'):\n",
    "            if self.mode == 'ovr':\n",
    "                return self._compute_f1_ovr(eps=eps, floattype=floattype, **kwargs)\n",
    "            elif self.mode == 'ovo':\n",
    "                return self._compute_f1_ovo(eps=eps, floattype=floattype, **kwargs)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported mode for f1: {self.mode}. Use `ovo` or `ovr`.\")\n",
    "        # Cross entropy/logloss\n",
    "        elif self.metric_type in ('logloss', 'log-loss', 'entropy', 'cross-entropy'):\n",
    "            return self._compute_logloss(eps=eps, floattype=floattype, **kwargs)\n",
    "        # Confusion matrix\n",
    "        elif self.metric_type == 'confusion_matrix':\n",
    "            return self._compute_confusion_matrix(**kwargs)\n",
    "        # Implemented by Nathmath Huang.\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported metric type: {self.metric_type}\")\n",
    "    \n",
    "    def deriv_1(self, eps: float = 1e-15, floattype: type = float, **kwargs) -> Matrix | Tensor:\n",
    "        \"\"\"\n",
    "        Computes the specified multi-class element-wise 1st order derivative.\n",
    "        \n",
    "        Supported metric_type values (case insensitive):\n",
    "            - 'logloss'\n",
    "        \n",
    "        Args:\n",
    "            eps: float, clip value to ensure 0/0 cases.\n",
    "            floattype: type, the internal precision of calculation.\n",
    "            **kwargs: Other arguments supported by metrics.\n",
    "    \n",
    "        Returns:\n",
    "            Matrix | Tensor: The computed metric value. The result is always returned as a Matrix or Tensor object,\n",
    "                                      even if the computation yields a scalar.\n",
    "        \n",
    "        Raises:\n",
    "            ValueError: If an unsupported metric type or mode type is provided.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Note, only \"logloss\" supports derivatives.\n",
    "        # Cross entropy/logloss\n",
    "        if self.metric_type in ('logloss', 'log-loss', 'entropy', 'cross-entropy'):\n",
    "            return self._deriv_1_logloss(eps=eps, floattype=floattype, **kwargs)\n",
    "        # Accuracy\n",
    "        elif self.metric_type == 'accuracy':\n",
    "            raise  ValueError(f\"Metric type: {self.metric_type} cannot compute derivatives.\")\n",
    "        # Precision\n",
    "        elif self.metric_type == 'precision':\n",
    "            raise  ValueError(f\"Metric type: {self.metric_type} cannot compute derivatives.\")\n",
    "        # Recall\n",
    "        elif self.metric_type in ('recall', 'sensitivity', 'tpr'):\n",
    "            raise  ValueError(f\"Metric type: {self.metric_type} cannot compute derivatives.\")\n",
    "        # F1 Score\n",
    "        elif self.metric_type in ('f1', 'f1 score'):\n",
    "            raise  ValueError(f\"Metric type: {self.metric_type} cannot compute derivatives.\")\n",
    "\n",
    "        # Confusion matrix\n",
    "        elif self.metric_type == 'confusion_matrix':\n",
    "            raise  ValueError(f\"Metric type: {self.metric_type} cannot compute derivatives.\")\n",
    "        \n",
    "        # Implemented by Nathmath Huang.\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported metric type: {self.metric_type}\")\n",
    "    \n",
    "    def deriv_2(self, eps: float = 1e-15, floattype: type = float, **kwargs) -> Tensor | Matrix:\n",
    "        \"\"\"\n",
    "        Computes the sample-wise 2nd order derivative for a given model or data.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor | Matrix: The computed metric value as a tensor\n",
    "        \"\"\"\n",
    "        # Note, only \"logloss\" supports derivatives.\n",
    "        # Cross entropy/logloss\n",
    "        if self.metric_type in ('logloss', 'log-loss', 'entropy', 'cross-entropy'):\n",
    "            return self._deriv_2_logloss(eps=eps, floattype=floattype, **kwargs)\n",
    "        # Accuracy\n",
    "        elif self.metric_type == 'accuracy':\n",
    "            raise  ValueError(f\"Metric type: {self.metric_type} cannot compute derivatives.\")\n",
    "        # Precision\n",
    "        elif self.metric_type == 'precision':\n",
    "            raise  ValueError(f\"Metric type: {self.metric_type} cannot compute derivatives.\")\n",
    "        # Recall\n",
    "        elif self.metric_type in ('recall', 'sensitivity', 'tpr'):\n",
    "            raise  ValueError(f\"Metric type: {self.metric_type} cannot compute derivatives.\")\n",
    "        # F1 Score\n",
    "        elif self.metric_type in ('f1', 'f1 score'):\n",
    "            raise  ValueError(f\"Metric type: {self.metric_type} cannot compute derivatives.\")\n",
    "\n",
    "        # Confusion matrix\n",
    "        elif self.metric_type == 'confusion_matrix':\n",
    "            raise  ValueError(f\"Metric type: {self.metric_type} cannot compute derivatives.\")\n",
    "        \n",
    "        # Implemented by Nathmath Huang.\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported metric type: {self.metric_type}\")\n",
    "            \n",
    "    def _to_labels(self, x: Tensor | Matrix, *, apply_softmax:bool = False) -> Tensor | Matrix:\n",
    "        \"\"\"\n",
    "        Converts predictions or targets into label vectors.\n",
    "        \n",
    "        If x has more than one column (i.e. one-hot or probability matrix), it returns\n",
    "        the index of the maximum value along axis 1. Otherwise, x is assumed already to be a vector.\n",
    "             \n",
    "        Args:\n",
    "            x: Matrix | Tensor: The one-hot or probability matrix.\n",
    "            apply_softmax: bool, whether to apply softmax before calculating argmax or not.\n",
    "    \n",
    "        Returns:\n",
    "            Matrix | Tensor: The converted Tensor or Matrix in (n_samples, 1) shape.\n",
    "        \"\"\"\n",
    "        # Wide-table: prob or one-hot\n",
    "        if len(x.shape) > 1 and x.shape[1] > 1:\n",
    "            # Always keep the dim.\n",
    "            return x.argmax(axis=1).reshape([-1, 1]) if apply_softmax == False else x.softmax(axis=1).argmax(axis=1).reshape([-1, 1])\n",
    "        # Narrow table\n",
    "        else:    \n",
    "            return x\n",
    "\n",
    "    def _to_onehot(self, x: Tensor | Matrix, *, binarize = False) -> Tensor | Matrix:\n",
    "        \"\"\"\n",
    "        Converts a label vector into a one-hot encoded matrix of shape [n_samples, n_classes].\n",
    "        If x is already a matrix with the correct number of columns, it is returned unaltered.\n",
    "        If x is binary probability input and binarize is False, then will return the probablistic one-hot.\n",
    "                     \n",
    "        Args:\n",
    "            x: Matrix | Tensor: The label-encoded matrix.\n",
    "    \n",
    "        Returns:\n",
    "            Matrix | Tensor: The converted one-hot Tensor or Matrix in (n_samples, n_classes) shape.\n",
    "        \"\"\"\n",
    "        if len(x.shape) == 2 and x.shape[1] == self.n_classes:\n",
    "            return x\n",
    "        \n",
    "        # If binary case, then create a probabilistic one-hot to reduce information loss\n",
    "        if self.n_classes == 2 and binarize == False:\n",
    "            onehot_data = type(x).zeros([x.shape[0], 2], backend=x._backend)\n",
    "            onehot_data[:, 1] = x.flatten()\n",
    "            onehot_data[:, 0] = 1.0 - onehot_data[:, 1]\n",
    "            return onehot_data.to(backend=x._backend, device=x.device, dtype=x.dtype)\n",
    "        \n",
    "        # Else, do the round\n",
    "        else:\n",
    "            # Create one-hot by comparing each element with a range vector.\n",
    "            range_vec = self.typeclass(np.arange(self.n_classes), backend=x._backend, device=x.device)\n",
    "            # Reshape x to [n_samples, 1] if necessary\n",
    "            x_reshaped = x.reshape([x.shape[0], 1])\n",
    "        \n",
    "            # Broadcast the comparison: each entry becomes True if equal to the class index.\n",
    "            onehot_data = x_reshaped.astype(self.result.dtype).round() == range_vec\n",
    "            # The above one produces a boolean array -> like True, False, True, ...\n",
    "            #                                                False, True, False, ...\n",
    "            return onehot_data.to(backend=x._backend, device=x.device, dtype=float)\n",
    "\n",
    "    def _compute_accuracy(self, *, floattype: type = float, **kwargs):\n",
    "        \"\"\"\n",
    "        Computes accuracy = (# correct predictions) / (# total samples).\n",
    "                             \n",
    "        Args:\n",
    "            floattype: type, the internal precision of calculation\n",
    "    \n",
    "        Returns:\n",
    "            Matrix | Tensor: The computed accuracy value.\n",
    "        \"\"\"\n",
    "        pred_labels = self._to_labels(self.result)\n",
    "        true_labels = self._to_labels(self.target)\n",
    "        correct = pred_labels == true_labels\n",
    "        total = true_labels.shape[0]\n",
    "        return correct.sum().to(correct._backend, dtype=floattype, device=correct.device) / total\n",
    "    \n",
    "    def _compute_confusion_matrix(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Computes the multi-class confusion matrix of shape [n_classes, n_classes],\n",
    "        where rows correspond to true labels and columns to predicted labels.\n",
    "        \n",
    "        Args:\n",
    "            None\n",
    "    \n",
    "        Returns:\n",
    "            Matrix | Tensor: The computed confusion matrix with shape [n_classes, n_classes].\n",
    "        \"\"\"\n",
    "        # Convert both predictions and targets to label vectors\n",
    "        pred_labels = self._to_labels(self.result)\n",
    "        true_labels = self._to_labels(self.target)\n",
    "        \n",
    "        # Convert them into one-hot matrices of shape [n_samples, n_classes]\n",
    "        pred_onehot = self._to_onehot(pred_labels)\n",
    "        true_onehot = self._to_onehot(true_labels)\n",
    "        \n",
    "        # Compute the confusion matrix as: (true_onehot)^T dot (pred_onehot)\n",
    "        conf_matrix = true_onehot.transpose().dot(pred_onehot)\n",
    "        return conf_matrix\n",
    "\n",
    "    def _compute_logloss(self, *, eps: float = 1e-15, floattype: type = float, **kwargs):\n",
    "        \"\"\"\n",
    "        Computes the cross-entropy loss (logloss) for multi-class classification.\n",
    "        \n",
    "        Assumes that `result` is a probability matrix of shape [n_samples, n_classes].\n",
    "        The loss is computed as:\n",
    "            logloss = - 1/N * sum_over_samples [ sum_over_classes (y_true * log(y_pred)) ]\n",
    "            \n",
    "        Args:\n",
    "            eps: float, clip value to ensure predictions are not going to be log(0)\n",
    "            floattype: type, the internal precision of calculation\n",
    "    \n",
    "        Returns:\n",
    "            Matrix | Tensor: The computed log loss value.\n",
    "        \"\"\"\n",
    "        if len(self.result.shape) != 2:\n",
    "            raise ValueError(\"Logloss metric requires probability predictions with shape [n_samples, n_classes].\")\n",
    "        \n",
    "        # Ensure predictions are floating point and clip values to avoid log(0)\n",
    "        preds = self.result.to(self.result._backend, dtype=floattype, device=self.result.device).clip(eps, 1 - eps)\n",
    "\n",
    "        # Compute elementwise: y_true * log(y_pred), then sum over classes (axis=1) then average over samples.\n",
    "        true_onehot = self._to_onehot(self._to_labels(self.target))\n",
    "        losses = -(true_onehot * preds.log()).sum(axis=1)\n",
    "        return losses.mean()\n",
    "    \n",
    "    def _deriv_1_logloss(self, *, eps: float = 1e-15, floattype: type = float, **kwargs) -> Tensor | Matrix:\n",
    "        \"\"\"\n",
    "        Computes the first-order derivative of the cross-entropy loss (logloss) for multi-class classification.\n",
    "        \n",
    "        Assumes that `result` is a probability matrix of shape [n_samples, n_classes].\n",
    "        \n",
    "        Args:\n",
    "            eps: float, clip value to ensure predictions are not going to be log(0)\n",
    "            floattype: type, the internal precision of calculation\n",
    "    \n",
    "        Returns:\n",
    "            Matrix | Tensor: Derivative of the logloss with respect to the predictions.\n",
    "        \"\"\"\n",
    "        if len(self.result.shape) != 2:\n",
    "            raise ValueError(\"Logloss metric requires probability predictions with shape [n_samples, n_classes].\")\n",
    "        \n",
    "        # Ensure predictions are floating point and clip values to avoid log(0)\n",
    "        preds = self.result.to(self.result._backend, dtype=floattype, device=self.result.device).clip(eps, 1 - eps)\n",
    "\n",
    "        # Convert to one-hot labels\n",
    "        true_onehot = self._to_onehot(self._to_labels(self.target))\n",
    "\n",
    "        # ∂L/∂p = -y/p \n",
    "        grad = -(true_onehot / preds) / preds.shape[0]\n",
    "        return grad\n",
    "    \n",
    "    def _deriv_2_logloss(self, *, eps: float = 1e-15, floattype: type = float, **kwargs) -> Tensor | Matrix:\n",
    "        \"\"\"\n",
    "        Computes the second-order derivative of the cross-entropy loss (logloss) for multi-class classification.\n",
    "        \n",
    "        Assumes that `result` is a probability matrix of shape [n_samples, n_classes].\n",
    "        \n",
    "        Args:\n",
    "            eps: float, clip value to ensure predictions are not going to be log(0)\n",
    "            floattype: type, the internal precision of calculation\n",
    "    \n",
    "        Returns:\n",
    "            Matrix | Tensor: Second-order derivative (Hessian diagonal) of the logloss with respect to the predictions.\n",
    "        \"\"\"\n",
    "        if len(self.result.shape) != 2:\n",
    "            raise ValueError(\"Logloss metric requires probability predictions with shape [n_samples, n_classes].\")\n",
    "        \n",
    "        # Ensure predictions are floating point and clip values to avoid log(0)\n",
    "        preds = self.result.to(self.result._backend, dtype=floattype, device=self.result.device).clip(eps, 1 - eps)\n",
    "\n",
    "        # Convert to one-hot labels\n",
    "        true_onehot = self._to_onehot(self._to_labels(self.target))\n",
    "        \n",
    "        hess = (true_onehot / preds ** 2) / preds.shape[0]\n",
    "        return hess\n",
    "\n",
    "    # OVR (One-Vs-Remaining) implementations for precision, recall and f1\n",
    "\n",
    "    def _compute_precision_ovr(self, *, eps: float = 1e-15, floattype: type = float, **kwargs):\n",
    "        \"\"\"\n",
    "        Computes macro-average precision using a one-vs-rest approach.\n",
    "        \n",
    "        For each class c:\n",
    "            precision[c] = TP[c] / (TP[c] + FP[c])\n",
    "        and the final metric is the mean over classes.\n",
    "        \n",
    "        Args:\n",
    "            eps: float, clip value to ensure 0/0 cases.\n",
    "            floattype: type, the internal precision of calculation.\n",
    "    \n",
    "        Returns:\n",
    "            Matrix | Tensor: The computed precision value.\n",
    "        \"\"\"\n",
    "        pred_labels = self._to_labels(self.result)\n",
    "        true_labels = self._to_labels(self.target)\n",
    "        pred_onehot = self._to_onehot(pred_labels.astype(self.result.dtype))\n",
    "        true_onehot = self._to_onehot(true_labels.astype(self.target.dtype))\n",
    "        \n",
    "        # True positives: elementwise multiplication then sum over samples (axis=0)\n",
    "        TP = (true_onehot * pred_onehot).sum(axis=0)\n",
    "       \n",
    "        # False positives: predicted positive but not truly positive.\n",
    "        FP = ((self.typeclass.ones_like(true_onehot, backend=true_onehot._backend) - true_onehot) * pred_onehot).sum(axis=0)\n",
    "        precision_per_class = TP / (TP + FP + floattype(eps))\n",
    "        return precision_per_class.mean()\n",
    "\n",
    "    def _compute_recall_ovr(self, *, eps: float = 1e-15, floattype: type = float, **kwargs):\n",
    "        \"\"\"\n",
    "        Computes macro-average recall (sensitivity) using a one-vs-rest approach.\n",
    "        \n",
    "        For each class c:\n",
    "            recall[c] = TP[c] / (TP[c] + FN[c])\n",
    "        and the final metric is the mean over classes.\n",
    "        \n",
    "        Args:\n",
    "            eps: float, clip value to ensure 0/0 cases.\n",
    "            floattype: type, the internal precision of calculation.\n",
    "    \n",
    "        Returns:\n",
    "            Matrix | Tensor: The computed recall value.\n",
    "        \"\"\"\n",
    "        pred_labels = self._to_labels(self.result)\n",
    "        true_labels = self._to_labels(self.target)\n",
    "        pred_onehot = self._to_onehot(pred_labels.astype(self.result.dtype))\n",
    "        true_onehot = self._to_onehot(true_labels.astype(self.target.dtype))\n",
    "        \n",
    "        # True positives: elementwise multiplication then sum over samples (axis=0)\n",
    "        TP = (true_onehot * pred_onehot).sum(axis=0)\n",
    "        \n",
    "        # False negatives: predicted negative but not trully negative.\n",
    "        FN = (true_onehot * (self.typeclass.ones_like(pred_onehot, backend=pred_onehot._backend) - pred_onehot)).sum(axis=0)\n",
    "        recall_per_class = TP / (TP + FN + floattype(eps))\n",
    "        return recall_per_class.mean()\n",
    "\n",
    "    def _compute_f1_ovr(self, *, eps: float = 1e-15, floattype: type = float, **kwargs):\n",
    "        \"\"\"\n",
    "        Computes macro-average F1-score in one-vs-rest mode.\n",
    "        F1 per class is computed as:\n",
    "            F1[c] = 2 * precision[c] * recall[c] / (precision[c] + recall[c])\n",
    "        The final score is the average over classes.\n",
    "        \n",
    "        Args:\n",
    "            eps: float, clip value to ensure 0/0 cases.\n",
    "            floattype: type, the internal precision of calculation.\n",
    "    \n",
    "        Returns:\n",
    "            Matrix | Tensor: The computed recall value.\n",
    "        \"\"\"\n",
    "        # Compute per-class precision and recall in OVR mode.\n",
    "        pred_labels = self._to_labels(self.result)\n",
    "        true_labels = self._to_labels(self.target)\n",
    "        pred_onehot = self._to_onehot(pred_labels.astype(self.result.dtype))\n",
    "        true_onehot = self._to_onehot(true_labels.astype(self.target.dtype))\n",
    "        \n",
    "        TP = (true_onehot * pred_onehot).sum(axis=0)\n",
    "        FP = ((self.typeclass.ones_like(true_onehot, backend=true_onehot._backend) - true_onehot) * pred_onehot).sum(axis=0)\n",
    "        FN = (true_onehot * (self.typeclass.ones_like(pred_onehot, backend=pred_onehot._backend) - pred_onehot)).sum(axis=0)\n",
    "        \n",
    "        precision_per_class = TP / (TP + FP + floattype(eps))\n",
    "        recall_per_class = TP / (TP + FN + floattype(eps))\n",
    "        f1_per_class = (2 * precision_per_class * recall_per_class) / (precision_per_class + recall_per_class + floattype(eps))\n",
    "        return f1_per_class.mean()\n",
    "\n",
    "    # OVO (One-Vs-One) implementations for precision, recall and f1\n",
    "    #\n",
    "    # These computations use the full confusion matrix. For every pair of different classes\n",
    "    # (i, j), we define binary precision and recall:\n",
    "    #   For class i in pair (i,j):\n",
    "    #       precision_i = M[i,i] / (M[i,i] + M[j,i] + eps)\n",
    "    #       recall_i = M[i,i] / (M[i,i] + M[i,j] + eps)\n",
    "    #   Similarly for class j.\n",
    "    # The final OVO metric is computed as the average over all the binary evaluations.\n",
    "    \n",
    "    def _compute_precision_ovo(self, *, eps: float = 1e-15, floattype: type = float, **kwargs):\n",
    "        \"\"\"\n",
    "        Computes macro-average precision using a ovo approach.\n",
    "        \n",
    "        Args:\n",
    "            eps: float, clip value to ensure 0/0 cases.\n",
    "            floattype: type, the internal precision of calculation.\n",
    "    \n",
    "        Returns:\n",
    "            Matrix | Tensor: The computed precision value.\n",
    "        \"\"\"\n",
    "        conf_matrix = self._compute_confusion_matrix()  # shape: [n_classes, n_classes]\n",
    "        \n",
    "        # Create index matrices using broadcasting.\n",
    "        idx = self.typeclass(np.arange(self.n_classes), backend=conf_matrix._backend, device=conf_matrix.device)\n",
    "        I = idx.reshape([self.n_classes, 1]).repeat(self.n_classes, axis=1)\n",
    "        J = idx.reshape([1, self.n_classes]).repeat(self.n_classes, axis=0)\n",
    "        mask = I.data < J.data  # boolean mask selecting one instance per unordered pair\n",
    "                                # Internal type\n",
    "        \n",
    "        # Extract diagonal elements as a vector.\n",
    "        diag = conf_matrix.diag()  # shape [n_classes]\n",
    "        \n",
    "        # Expand diagonals for broadcasting.\n",
    "        diag_i = diag.reshape([self.n_classes, 1]).repeat(self.n_classes, axis=1)  # each row: diag[i]\n",
    "        diag_j = diag.reshape([1, self.n_classes]).repeat(self.n_classes, axis=0)  # each column: diag[j]\n",
    "        \n",
    "        # For a given pair (i, j):\n",
    "        # precision for class i:\n",
    "        p_i_matrix = diag_i / (diag_i + conf_matrix.transpose())  \n",
    "        \n",
    "        # We need M[j, i] for p_i. In our matrix, conf_matrix[j,i] is given by\n",
    "        # conf_matrix.transpose()[i,j]. Thus, we use:\n",
    "        p_i_matrix = diag_i / (diag_i + conf_matrix.transpose() + floattype(eps))\n",
    "        \n",
    "        # And precision for class j:\n",
    "        p_j_matrix = diag_j / (diag_j + conf_matrix + floattype(eps))\n",
    "        \n",
    "        # Now select only entries for pairs where I < J.\n",
    "        p_i_vals = p_i_matrix[mask]\n",
    "        p_j_vals = p_j_matrix[mask]\n",
    "        \n",
    "        # Concatenate and compute the mean.\n",
    "        all_precisions = p_i_vals.append(p_j_vals, axis=0)\n",
    "        return all_precisions.mean()\n",
    "\n",
    "    def _compute_recall_ovo(self, *, eps: float = 1e-15, floattype: type = float, **kwargs):\n",
    "        \"\"\"\n",
    "        Computes macro-average recall using a ovo approach.\n",
    "        \n",
    "        Args:\n",
    "            eps: float, clip value to ensure 0/0 cases.\n",
    "            floattype: type, the internal precision of calculation.\n",
    "    \n",
    "        Returns:\n",
    "            Matrix | Tensor: The computed recall value.\n",
    "        \"\"\"\n",
    "        conf_matrix = self._compute_confusion_matrix()  # shape: [n_classes, n_classes]\n",
    "        \n",
    "        # Create index matrices using broadcasting.\n",
    "        idx = self.typeclass(np.arange(self.n_classes), backend=conf_matrix._backend, device=conf_matrix.device)\n",
    "        I = idx.reshape([self.n_classes, 1]).repeat(self.n_classes, axis=1)\n",
    "        J = idx.reshape([1, self.n_classes]).repeat(self.n_classes, axis=0)\n",
    "        mask = I.data < J.data  # boolean mask selecting one instance per unordered pair\n",
    "                                # Internal type\n",
    "        \n",
    "        # Extract diagonal elements as a vector.\n",
    "        diag = conf_matrix.diag()\n",
    "        diag_i = diag.reshape([self.n_classes, 1]).repeat(self.n_classes, axis=1)\n",
    "        diag_j = diag.reshape([1, self.n_classes]).repeat(self.n_classes, axis=0)\n",
    "\n",
    "        # For recall in a pair (i, j):\n",
    "        # recall for class i:\n",
    "        r_i_matrix = diag_i / (diag_i + conf_matrix +  floattype(eps))\n",
    "        # and recall for class j:\n",
    "        r_j_matrix = diag_j / (diag_j + conf_matrix.transpose() + floattype(eps))\n",
    "        \n",
    "        # Now select only entries for pairs where I < J.\n",
    "        r_i_vals = r_i_matrix[mask]\n",
    "        r_j_vals = r_j_matrix[mask]\n",
    "        \n",
    "        # Concatenate and compute the mean.\n",
    "        all_recalls = r_i_vals.append(r_j_vals, axis=0)\n",
    "        return all_recalls.mean()\n",
    "\n",
    "    def _compute_f1_ovo(self, *, eps: float = 1e-15, floattype: type = float, **kwargs):\n",
    "        \"\"\"\n",
    "        Computes macro-average f1-score using a ovo approach.\n",
    "        \n",
    "        Args:\n",
    "            eps: float, clip value to ensure 0/0 cases.\n",
    "            floattype: type, the internal precision of calculation.\n",
    "    \n",
    "        Returns:\n",
    "            Matrix | Tensor: The computed f1-score.\n",
    "        \"\"\"\n",
    "        \n",
    "        # First compute the binary precisions and recalls from OVO.\n",
    "        precision_ovo = self._compute_precision_ovo(eps=eps, floattype=floattype)\n",
    "        recall_ovo = self._compute_recall_ovo(eps=eps, floattype=floattype)\n",
    "        f1_ovo = (2 * precision_ovo * recall_ovo) / (precision_ovo + recall_ovo + floattype(eps))\n",
    "        return f1_ovo\n",
    "\n",
    "    def __repr__(self):\n",
    "        return (f\"MultiClassificationMetrics(metric_type={self.metric_type}, mode={self.mode}, \"\n",
    "                f\"n_classes={self.n_classes}, result_shape={self.result.shape})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`P. BaseML Classes for all algorithms (self-implemented)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These base classes are self implemented and open-sourced\n",
    "# Available at https://github.com/dof-studio/MML/\n",
    "# By Nathmath Huang (bh2821)\n",
    "# License: Apache License Version 2.0\n",
    "\n",
    "# Machine Learning Algorithm Base Class\n",
    "class MLBase:\n",
    "    \"\"\"\n",
    "    Base class that provides common traits for machine learning tasks,\n",
    "    including data splitting methods.\n",
    "    \"\"\"\n",
    "    \n",
    "    __attr__ = \"MML.MLBase\"\n",
    "    \n",
    "    def _random_state_next(self, attr: str = \"random_state\") -> int | None:\n",
    "        \"\"\"\n",
    "        Advances the random state for a given attribute and returns it.\n",
    "        If assigned as None, then return None without doing anything.\n",
    "        \n",
    "        Args:\n",
    "            attr (str): The name of the attribute to retrieve and advance. Default is 'random_state'.\n",
    "        \n",
    "        Returns:\n",
    "            int | None: The next value of the random state or None if no such state exists.\n",
    "        \n",
    "        Raises:\n",
    "            AttributeError: If the specified attribute does not exist in the object.\n",
    "        \n",
    "        \"\"\"\n",
    "        # Retrieve the random state atrribute\n",
    "        if getattr(self, attr) is None:\n",
    "            return None  # Nonetype cannot be advanced\n",
    "        else:\n",
    "            random_state = getattr(self, attr)\n",
    "        \n",
    "        # If existing random_state_count, retrieve the count, else create it\n",
    "        try:\n",
    "            if getattr(self, attr + \"_count\") is None:\n",
    "                setattr(self, attr + \"_count\", 0)\n",
    "        except AttributeError as e:\n",
    "            setattr(self, attr + \"_count\", 0)\n",
    "        \n",
    "        # If existing random_state_offset, retrieve the offset, else create it\n",
    "        try:\n",
    "            if getattr(self, attr + \"_offset\") is None:\n",
    "                setattr(self, attr + \"_offset\", 57119)\n",
    "        except AttributeError as e:\n",
    "            setattr(self, attr + \"_offset\", 57119)\n",
    "        random_state_offset = getattr(self, attr + \"_offset\")\n",
    "        \n",
    "        # Next the random state and return it\n",
    "        random_state += random_state_offset\n",
    "        setattr(self, attr + \"_count\", getattr(self, attr + \"_count\") + 1)\n",
    "        \n",
    "        return random_state\n",
    "    \n",
    "    @staticmethod\n",
    "    def train_test_split(X: Matrix | Tensor, y: Matrix | Tensor, test_size=0.2, random_state=None):\n",
    "        \"\"\"\n",
    "        Splits the input data into training and testing sets.\n",
    "        \n",
    "        Args:\n",
    "            X (Matrix | Tensor): The feature matrix.\n",
    "            y (Matrix | Tensor): The target vector.\n",
    "            test_size (float): Proportion of samples to include in the test split.\n",
    "            random_state (int or None): Seed for reproducible random number generation. Default is None.\n",
    "        \n",
    "        Returns:\n",
    "            tuple[Matrix | Tensor, Matrix | Tensor]: A tuple containing four elements: \n",
    "                - X_train: Training feature matrix.\n",
    "                - X_test: Testing feature matrix.\n",
    "                - y_train: Training target vector.\n",
    "                - y_test: Testing target vector.\n",
    "        \n",
    "        Raises:\n",
    "            TypeError: If 'X' and 'y' are not of the same type (Matrix or Tensor).\n",
    "        \n",
    "        \"\"\"\n",
    "        if X.__attr__ != y.__attr__:\n",
    "            raise TypeError(\"Input 'X' and 'y' should have the same type Matrix or Tensor!\")\n",
    "        if random_state is not None:\n",
    "            np.random.seed(random_state)\n",
    "        n_samples = X.shape[0]\n",
    "        indices = np.random.permutation(n_samples)\n",
    "        test_count = int(round(n_samples * test_size))\n",
    "        train_idx = indices[test_count:]\n",
    "        test_idx = indices[:test_count]\n",
    "        return X[train_idx], X[test_idx], y[train_idx], y[test_idx]\n",
    "\n",
    "    @staticmethod\n",
    "    def train_test_split_for_timeseries(X: Matrix | Tensor, y: Matrix | Tensor, test_size=0.2):\n",
    "        \"\"\"\n",
    "        Splits time series data into training and testing sets.\n",
    "        \n",
    "        Args:\n",
    "            X (Matrix | Tensor): The feature matrix of the time series.\n",
    "            y (Matrix | Tensor): The target vector or dependent variable of the time series.\n",
    "            test_size (float): Proportion of the dataset to include in the test split. Default is 0.2.\n",
    "        \n",
    "        Returns:\n",
    "            tuple[Matrix, Matrix]: A tuple containing two matrices: \n",
    "                                   - X_train: Training feature matrix\n",
    "                                   - X_test: Testing feature matrix\n",
    "                                   - y_train: Training target vector\n",
    "                                   - y_test: Testing target vector\n",
    "        \n",
    "        Raises:\n",
    "            TypeError: If 'X' and 'y' are not of the same type (Matrix or Tensor).\n",
    "        \n",
    "        \"\"\"\n",
    "        if X.__attr__ != y.__attr__:\n",
    "            raise TypeError(\"Input 'X' and 'y' should have the same type Matrix or Tensor!\")\n",
    "        n_samples = X.shape[0]\n",
    "        test_count = int(round(n_samples * test_size))\n",
    "        # For time series the split is sequential: training data comes first.\n",
    "        train_idx = slice(0, n_samples - test_count)\n",
    "        test_idx = slice(n_samples - test_count, n_samples)\n",
    "        return X[train_idx], X[test_idx], y[train_idx], y[test_idx]\n",
    "\n",
    "    @staticmethod\n",
    "    def train_test_split_binarydata_siid(X: Matrix | Tensor, y: Matrix | Tensor, test_size=0.2, random_state=None):\n",
    "        \"\"\"\n",
    "        Splits the input data into training and testing sets ensuring that the percentage of \n",
    "        positives and negatives in the target vector y are similar in both sets, as if they\n",
    "        are similar to iid distributed in the train and the test set.\n",
    "        \n",
    "        Args:\n",
    "            X (Matrix | Tensor): The feature matrix.\n",
    "            y (Matrix | Tensor): The binary target vector.\n",
    "            test_size (float): Proportion of samples to include in the test split.\n",
    "            random_state (int or None): Seed for reproducible random number generation. Default is None.\n",
    "            \n",
    "        Returns:\n",
    "            tuple: A tuple containing four elements:\n",
    "                - X_train: Training feature matrix.\n",
    "                - X_test: Testing feature matrix.\n",
    "                - y_train: Training target vector.\n",
    "                - y_test: Testing target vector.\n",
    "        \n",
    "        Raises:\n",
    "            TypeError: If 'X' and 'y' are not of the same type (Matrix or Tensor).\n",
    "            ValueError: If y does not contain binary labels (0 and 1).\n",
    "        \"\"\"\n",
    "        # Ensure both X and y are of the same type.\n",
    "        if X.__attr__ != y.__attr__:\n",
    "            raise TypeError(\"Input 'X' and 'y' should have the same type Matrix or Tensor!\")\n",
    "        \n",
    "        if random_state is not None:\n",
    "            np.random.seed(random_state)\n",
    "        \n",
    "        # Verify y is binary.\n",
    "        unique_labels = y.unique().to(\"numpy\")\n",
    "        if len(unique_labels.data) != 2:\n",
    "            raise ValueError(f\"Target vector y must be binary (contain 2 kinds of labels) while it contains {len(unique_labels)} kinds.\")\n",
    "        \n",
    "        # Get indices for each class.\n",
    "        idx0 = np.where(y.flatten().to(\"numpy\").data == unique_labels.data[0])[0]\n",
    "        idx1 = np.where(y.flatten().to(\"numpy\").data == unique_labels.data[1])[0]\n",
    "        \n",
    "        # Shuffle indices for each class.\n",
    "        idx0 = np.random.permutation(idx0)\n",
    "        idx1 = np.random.permutation(idx1)\n",
    "        \n",
    "        # Determine the number of test samples per class.\n",
    "        n_test_0 = int(round(len(idx0) * test_size))\n",
    "        n_test_1 = int(round(len(idx1) * test_size))\n",
    "        \n",
    "        # Split indices for each class.\n",
    "        test_idx = np.concatenate((idx0[:n_test_0], idx1[:n_test_1]))\n",
    "        train_idx = np.concatenate((idx0[n_test_0:], idx1[n_test_1:]))\n",
    "        \n",
    "        # Shuffle the final indices.\n",
    "        train_idx = np.random.permutation(train_idx)\n",
    "        test_idx = np.random.permutation(test_idx)\n",
    "        \n",
    "        return X[train_idx], X[test_idx], y[train_idx], y[test_idx]\n",
    "\n",
    "    @staticmethod\n",
    "    def k_fold(X: Matrix | Tensor, y: Matrix | Tensor, n_splits=5, random_state=None) -> List:\n",
    "        \"\"\"\n",
    "        Splits the data into `n_splits` folds for cross-validation.\n",
    "        \n",
    "        Args:\n",
    "            X (Matrix | Tensor): The feature matrix.\n",
    "            y (Matrix | Tensor): The target vector.\n",
    "            n_splits (int): Number of splits to make. Default is 5.\n",
    "            random_state (Optional[int]): Seed value for reproducible randomness. Default is None.\n",
    "        \n",
    "        Returns:\n",
    "            List: A list where each element contains a tuple with the training and test indices for `X` and `y`.\n",
    "                  List[ (X[train_idx], X[test_idx], y[train_idx], y[test_idx]) ]\n",
    "        \n",
    "        Raises:\n",
    "            TypeError: If 'X' and 'y' are not of the same type, either Matrix or Tensor.\n",
    "        \n",
    "        \"\"\"        \n",
    "        if X.__attr__ != y.__attr__:\n",
    "            raise TypeError(\"Input 'X' and 'y' should have the same type Matrix or Tensor!\")\n",
    "        if random_state is not None:\n",
    "            np.random.seed(random_state)\n",
    "        n_samples = X.shape[0]\n",
    "        indices = np.random.permutation(n_samples)\n",
    "        fold_size = n_samples // n_splits\n",
    "        folds = []\n",
    "        for i in range(n_splits):\n",
    "            start = i * fold_size\n",
    "            # Make sure the last fold takes all remaining samples\n",
    "            end = (i + 1) * fold_size if i < n_splits - 1 else n_samples\n",
    "            test_idx = indices[start:end]\n",
    "            train_idx = np.concatenate((indices[:start], indices[end:]))\n",
    "            folds.append((X[train_idx], X[test_idx], y[train_idx], y[test_idx]))\n",
    "        return folds\n",
    "\n",
    "    @staticmethod\n",
    "    def make_rolling_window(X: Matrix | Tensor, y: Matrix | Tensor, window_size=10) -> List[Matrix | Tensor]:\n",
    "        \"\"\"\n",
    "        Make the data into rolling window 3D data and make the 1st dimension as saples.\n",
    "        \n",
    "        Args:\n",
    "            X (Matrix | Tensor): The feature matrix.\n",
    "            y (Matrix | Tensor): The target vector.\n",
    "            window_size (int): The number of historical window size (axis = 0)\n",
    "\n",
    "        Returns:\n",
    "            List: A list where each element is for processed `X` and `y`.\n",
    "                  List[X, y], where y is using the LAST row of target in the sliced piece.\n",
    "        \n",
    "        Raises:\n",
    "            TypeError: If 'X' and 'y' are not of the same type, either Matrix or Tensor.\n",
    "        \n",
    "        \"\"\"   \n",
    "        if X.__attr__ != y.__attr__:\n",
    "            raise TypeError(\"Input 'X' and 'y' should have the same type Matrix or Tensor!\")\n",
    "            \n",
    "        T, D = X.shape\n",
    "        if window_size > T:\n",
    "            raise ValueError(\"Window size cannot exceed number of 1st dimensions\")\n",
    "        \n",
    "        # build each window by slicing and then stack into a 3D tensor\n",
    "        X_list = [X[i : i + window_size] for i in range(T - window_size + 1)]\n",
    "        # shape: (T - window_size + 1, window_size, D)\n",
    "        X_new = X_list[0].stack(*X_list[1:], axis = 0)\n",
    "        \n",
    "        # Create renewed y\n",
    "        y_new = y[(window_size-1):]\n",
    "        \n",
    "        return X_new, y_new\n",
    "                \n",
    "    @staticmethod\n",
    "    def save(instance, filepath:str):\n",
    "        \"\"\"\n",
    "        Save the model object into a file to your disk.\n",
    "        \n",
    "        Args:\n",
    "            instance: a MLBase derived object\n",
    "            filepath: str, the destination file path to save.\n",
    "        \"\"\"\n",
    "        save({\"__attr__\" : instance.__attr__, \"data\": instance}, filepath, kompress=lzma, protocol=5)\n",
    "        \n",
    "    def load(self, filepath:str):\n",
    "        \"\"\"\n",
    "        Load the model object from a file from your disk.\n",
    "        Return the loaded model instead of evaluating to self.\n",
    "        \n",
    "        Args:\n",
    "            filepath: str, the destination file path to load.\n",
    "        \"\"\"\n",
    "        rawobj = load(filepath, kompress=lzma)\n",
    "        if isinstance(rawobj, dict) == False:\n",
    "            raise ValueError(f\"The file input is NOT a valid {self.__attr__} model.\")\n",
    "        if rawobj.get(\"__attr__\", \"\") != self.__attr__:\n",
    "            raise ValueError(f\"The file input is NOT a valid {self.__attr__} model.\")\n",
    "        return rawobj[\"data\"]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"MLBase(Machine Learning Abstract Base Class).\"\n",
    "\n",
    "\n",
    "# Base Class for Regression Models\n",
    "class Regression(MLBase):\n",
    "    \"\"\"\n",
    "    Base regression model that provides common traits for regression tasks.\n",
    "    \"\"\"\n",
    "    \n",
    "    __attr__ = \"MML.Regression\"\n",
    "    \n",
    "    def fit(self, X: Matrix | Tensor, y: Matrix | Tensor):\n",
    "        \"\"\"\n",
    "        Fits a regression model to the given data.\n",
    "        \n",
    "        Args:\n",
    "            X (Matrix | Tensor): The feature matrix.\n",
    "            y (Matrix | Tensor): The target vector.\n",
    "        \n",
    "        Raises:\n",
    "            TypeError: If 'X' and 'y' are not of the same type, either Matrix or Tensor.\n",
    "            NotImplementedError: If the specific regression model does not implement a fit method.\n",
    "        \n",
    "        \"\"\"\n",
    "        if X.__attr__ != y.__attr__:\n",
    "            raise TypeError(\"Input 'X' and 'y' should have the same type Matrix or Tensor!\")\n",
    "        raise NotImplementedError(\"Regression model must implement fit method.\")\n",
    "\n",
    "    def predict(self, X: Matrix | Tensor):\n",
    "        \"\"\"\n",
    "        Predicts target values for the given feature matrix `X`.\n",
    "        \n",
    "        Args:\n",
    "            X (Matrix | Tensor): The feature matrix.\n",
    "        \n",
    "        Raises:\n",
    "            NotImplementedError: If the specific regression model does not implement a predict method.\n",
    "        \n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Regression model must implement predict method.\")\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"Regression(Regression Abstract Base Class).\"\n",
    "\n",
    "\n",
    "# Base Class for Classification Models\n",
    "class Classification(MLBase):\n",
    "    \"\"\"\n",
    "    Base classification model that provides common traits for classification tasks.\n",
    "    \"\"\"\n",
    "    \n",
    "    __attr__ = \"MML.Classification\"\n",
    "    \n",
    "    def fit(self, X: Matrix | Tensor, y: Matrix | Tensor):\n",
    "        \"\"\"\n",
    "        Fits a classification model to the provided feature matrix `X` and target vector `y`.\n",
    "        \n",
    "        Args:\n",
    "            X (Matrix | Tensor): The feature matrix.\n",
    "            y (Matrix | Tensor): The target vector.\n",
    "        \n",
    "        Raises:\n",
    "            TypeError: If 'X' and 'y' are not of the same type, either Matrix or Tensor.\n",
    "            NotImplementedError: If a derived class has not implemented the `fit` method for classification models.\n",
    "        \n",
    "        \"\"\"\n",
    "        if X.__attr__ != y.__attr__:\n",
    "            raise TypeError(\"Input 'X' and 'y' should have the same type Matrix or Tensor!\")\n",
    "        raise NotImplementedError(\"Classification model must implement fit method.\")\n",
    "\n",
    "    def predict(self, X: Matrix | Tensor):\n",
    "        \"\"\"\n",
    "        Predicts the target values for a given set of features.\n",
    "        \n",
    "        Args:\n",
    "            X (Matrix | Tensor): The feature matrix or tensor to make predictions on.\n",
    "        \n",
    "        Returns:\n",
    "            Matrix: A matrix containing the predicted target values.\n",
    "        \n",
    "        Raises:\n",
    "            NotImplementedError: This method should be implemented by subclasses as it is abstract in the current model class.\n",
    "        \n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Classification model must implement predict method.\")\n",
    "\n",
    "    @staticmethod\n",
    "    def _to_binary_prob(x: Tensor | Matrix) -> Tensor | Matrix:\n",
    "        \"\"\"\n",
    "        Converts one-hot predictions or targets into binary probability.\n",
    "        \n",
    "        If x has more than one column (two, must be), it returns\n",
    "        probabilities of entries to be 1.\n",
    "             \n",
    "        Args:\n",
    "            x: Matrix | Tensor: The one-hot or probability matrix.\n",
    "\n",
    "        Returns:\n",
    "            Matrix | Tensor: The converted Tensor or Matrix in (n_samples, 1) shape.\n",
    "        \"\"\"\n",
    "        # Wide-table: prob or one-hot\n",
    "        if len(x.shape) > 1 and x.shape[1] == 2:\n",
    "            # Always keep the dim.\n",
    "            return x[:,1].reshape([-1, 1])\n",
    "        # Already only one column\n",
    "        elif len(x.shape) > 1 and x.shape[1] == 1:\n",
    "            return x\n",
    "        # Unknown cases\n",
    "        else:\n",
    "            raise ValueError(\"When converting to binary_probability from one-hot probabilities, the input dimension must be (n_samples, 2) or aleady been (n_samples, 1)\")\n",
    "\n",
    "    @staticmethod\n",
    "    def _to_labels(x: Tensor | Matrix, *, apply_softmax:bool = False) -> Tensor | Matrix:\n",
    "        \"\"\"\n",
    "        Converts predictions or targets into label vectors.\n",
    "        \n",
    "        If x has more than one column (i.e. one-hot or probability matrix), it returns\n",
    "        the index of the maximum value along axis 1. Otherwise, x is assumed already to be a vector.\n",
    "             \n",
    "        Args:\n",
    "            x: Matrix | Tensor: The one-hot or probability matrix.\n",
    "            apply_softmax: bool, whether to apply softmax before calculating argmax or not.\n",
    "    \n",
    "        Returns:\n",
    "            Matrix | Tensor: The converted Tensor or Matrix in (n_samples, 1) shape.\n",
    "        \"\"\"\n",
    "        # Wide-table: prob or one-hot\n",
    "        if len(x.shape) > 1 and x.shape[1] > 1:\n",
    "            # Always keep the dim.\n",
    "            return x.argmax(axis=1).reshape([-1, 1]) if apply_softmax == False else x.softmax(axis=1).argmax(axis=1).reshape([-1, 1])\n",
    "        # Narrow table\n",
    "        else:    \n",
    "            return x\n",
    "\n",
    "    @staticmethod\n",
    "    def _to_onehot(x: Tensor | Matrix, n_classes: int, *, binarize = False) -> Tensor | Matrix:\n",
    "        \"\"\"\n",
    "        Converts a label vector into a one-hot encoded matrix of shape [n_samples, n_classes].\n",
    "        If x is already a matrix with the correct number of columns, it is returned unaltered.\n",
    "        If x is binary probability input and binarize is False, then will return the probablistic one-hot.\n",
    "                     \n",
    "        Args:\n",
    "            x: Matrix | Tensor: The label-encoded matrix.\n",
    "    \n",
    "        Returns:\n",
    "            Matrix | Tensor: The converted one-hot Tensor or Matrix in (n_samples, n_classes) shape.\n",
    "        \"\"\"\n",
    "        if len(x.shape) == 2 and x.shape[1] == n_classes:\n",
    "            return x\n",
    "        \n",
    "        # If binary case, then create a probabilistic one-hot to reduce information loss\n",
    "        if n_classes == 2 and binarize == False:\n",
    "            onehot_data = type(x).zeros([x.shape[0], 2], backend=x._backend)\n",
    "            onehot_data[:, 1] = x.flatten()\n",
    "            onehot_data[:, 0] = 1.0 - onehot_data[:, 1]\n",
    "            return onehot_data.to(backend=x._backend, device=x.device, dtype=x.dtype)\n",
    "        \n",
    "        # Else, do the round\n",
    "        else:\n",
    "            # Create one-hot by comparing each element with a range vector.\n",
    "            range_vec = type(x)(np.arange(n_classes), backend=x._backend, device=x.device)\n",
    "            # Reshape x to [n_samples, 1] if necessary\n",
    "            x_reshaped = x.reshape([x.shape[0], 1])\n",
    "            \n",
    "            # Broadcast the comparison: each entry becomes True if equal to the class index.\n",
    "            onehot_data = x_reshaped.astype(float).round() == range_vec\n",
    "            # The above one produces a boolean array -> like True, False, True, ...\n",
    "            #                                                False, True, False, ...\n",
    "            return onehot_data.to(backend=x._backend, device=x.device, dtype=float)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"Classification(Regression Abstract Base Class).\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`P. Data Scaler (self-implemented)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This scaling class is self implemented and open-sourced\n",
    "# Available at https://github.com/dof-studio/MML/\n",
    "# By Nathmath Huang (bh2821)\n",
    "# License: Apache License Version 2.0\n",
    "\n",
    "# Implementation of Data Scaler\n",
    "class Scaling:\n",
    "    \"\"\"\n",
    "    Scale class that fits on a Matrix and can perform either centralization (subtracting the mean)\n",
    "    or min-max scaling (scaling features to the [0, 1] range), with the ability to reverse the operation.\n",
    "    \"\"\"\n",
    "    \n",
    "    __attr__ = \"MML.Scaling\"\n",
    "    \n",
    "    def __init__(self, method=\"centralize\", *, robust_p = 0.25):\n",
    "        '''\n",
    "        Args:\n",
    "            `method` can be:\n",
    "                \"centralize\": only subtract the mean\n",
    "                \"normalize\": subtract the mean and standardize the variance to 1\n",
    "                \"minmax\": keep the data with in the range of [0, 1]\n",
    "                \"robust\": compute median and interquartile range to reduce the effect of outliers.\n",
    "            `robust_p` the lower percentile [0,1] of the percentile estimate. 0.25 means 25% and 75%\n",
    "        '''\n",
    "        self.method = method\n",
    "        self.params = {}\n",
    "        \n",
    "        # Method specific parameters\n",
    "        self.robust_p = robust_p if robust_p < 0.5 else 1 - robust_p\n",
    "\n",
    "    def fit(self, X: Matrix | Tensor, axis = 0):\n",
    "        \"\"\"\n",
    "        Fits the scaling parameters to the data.\n",
    "    \n",
    "        Args:\n",
    "            X (Matrix | Tensor): The input matrix or tensor for fitting.\n",
    "            axis (int): Axis along which to compute the mean and standard deviation. Default is 0.\n",
    "    \n",
    "        Returns:\n",
    "            self: The fitted instance of the class, allowing method chaining.\n",
    "    \n",
    "        Raises:\n",
    "            ValueError: If an unsupported scaling method is provided.\n",
    "    \n",
    "        \"\"\"\n",
    "        type_X = type(X)\n",
    "        if self.method == \"centralize\":\n",
    "            # Just demean the data to 0 mean\n",
    "            if X._is_numpy:\n",
    "                mean_val = np.mean(X.data, axis=axis)\n",
    "            else:\n",
    "                mean_val = torch.mean(X.data, dim=axis)\n",
    "            self.params['mean'] = type_X(mean_val, backend=X._backend, device=X.device, dtype=X.dtype)\n",
    "        \n",
    "        elif self.method == \"normalize\":\n",
    "            # Normalize the data with 0 mean and std of 1\n",
    "            if X._is_numpy:\n",
    "                mean_val = np.mean(X.data, axis=axis)\n",
    "                stdev_val = np.std(X.data, axis=axis)\n",
    "            else:\n",
    "                mean_val = torch.mean(X.data, dim=axis)\n",
    "                stdev_val = torch.std(X.data, dim=axis)\n",
    "            self.params['mean'] = type_X(mean_val, backend=X._backend, device=X.device, dtype=X.dtype)\n",
    "            self.params['std'] = type_X(stdev_val, backend=X._backend, device=X.device, dtype=X.dtype)\n",
    "        \n",
    "        elif self.method == \"minmax\":\n",
    "            # Minmax to make data in a range of [0,1]\n",
    "            if X._is_numpy:\n",
    "                min_val = np.min(X.data, axis=axis)\n",
    "                max_val = np.max(X.data, axis=axis)\n",
    "            else:\n",
    "                min_val = torch.min(X.data, dim=axis).values\n",
    "                max_val = torch.max(X.data, dim=axis).values\n",
    "            self.params['min'] = type_X(min_val, backend=X._backend, device=X.device, dtype=X.dtype)\n",
    "            self.params['max'] = type_X(max_val, backend=X._backend, device=X.device, dtype=X.dtype)\n",
    "        \n",
    "        elif self.method == \"robust\":\n",
    "            # Compute median and interquartile range to reduce the effect of outliers.\n",
    "            if X._is_numpy:\n",
    "                median_val = np.median(X.data, axis=axis)\n",
    "                q1 = np.percentile(X.data, int(self.robust_p * 100), axis=axis)\n",
    "                q3 = np.percentile(X.data, 100 - int(self.robust_p * 100), axis=axis)\n",
    "                iqr_val = q3 - q1\n",
    "            else:\n",
    "                median_val = torch.median(X.data, dim=axis).values\n",
    "                q1 = torch.quantile(X.data, self.robust_p, dim=axis)\n",
    "                q3 = torch.quantile(X.data, 1 - self.robust_p, dim=axis)\n",
    "                iqr_val = q3 - q1\n",
    "            self.params['p'] = self.robust_p\n",
    "            self.params['median'] = type_X(median_val, backend=X._backend, device=X.device, dtype=X.dtype)\n",
    "            self.params['iqr'] = type_X(iqr_val, backend=X._backend, device=X.device, dtype=X.dtype)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported scaling method. Choose 'centralize', 'normalize', 'minmax', or 'robust'.\")\n",
    "        return self\n",
    "\n",
    "    def transform(self, X: Matrix | Tensor):\n",
    "        \"\"\"\n",
    "        Transforms the input matrix using the fitted parameters.\n",
    "        \n",
    "        Args:\n",
    "            X (Matrix | Tensor): The input matrix for transformation.\n",
    "        \n",
    "        Returns:\n",
    "            Matrix | Tensor: The transformed matrix or tensor.\n",
    "        \n",
    "        Raises:\n",
    "            InterruptedError: If no scaling parameters have been fitted yet.\n",
    "            ValueError: If an unsupported scaling method is provided.\n",
    "        \n",
    "        \"\"\"\n",
    "        if len(self.params) == 0:\n",
    "            raise InterruptedError(\"You should call `fit` before doing any transformation\")\n",
    "        if self.method == \"centralize\":\n",
    "            return (X - self.params['mean'])\n",
    "        elif self.method == \"normalize\":\n",
    "            return (X - self.params['mean']) / self.params['std']\n",
    "        elif self.method == \"minmax\":\n",
    "            range_matrix = self.params['max'] - self.params['min']\n",
    "            return (X - self.params['min']) / range_matrix\n",
    "        elif self.method == \"robust\":\n",
    "            return (X - self.params['median']) / self.params['iqr']\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported scaling method. Choose 'centralize', 'normalize', 'minmax', or 'robust'.\")\n",
    "\n",
    "    def inverse_transform(self, X: Matrix | Tensor):\n",
    "        \"\"\"\n",
    "        Inverses the transformation applied during fitting.\n",
    "        \n",
    "        Args:\n",
    "            X (Matrix | Tensor): The transformed matrix for inversion.\n",
    "        \n",
    "        Returns:\n",
    "            Matrix | Tensor: The original matrix or tensor before scaling.\n",
    "        \n",
    "        Raises:\n",
    "            InterruptedError: If no scaling parameters have been fitted yet.\n",
    "            ValueError: If an unsupported scaling method is provided. \n",
    "        \n",
    "        \"\"\"\n",
    "        if len(self.params) == 0:\n",
    "            raise InterruptedError(\"You should call `fit` before doing any transformation\")\n",
    "        if self.method == \"centralize\":\n",
    "            # Inverse centralization: add the mean back.\n",
    "            return X + self.params['mean']\n",
    "        if self.method == \"normalize\":\n",
    "            # Inverse centralization: multiply the std and add the mean back.\n",
    "            return X * self.params['std'] + self.params['mean']\n",
    "        elif self.method == \"minmax\":\n",
    "            # Inverse minmax scaling: X*(max - min) + min\n",
    "            range_matrix = self.params['max'] - self.params['min']\n",
    "            return X * range_matrix + self.params['min']\n",
    "        elif self.method == \"robust\":\n",
    "            # Inverse robust scaling: X*(iqr) + median\n",
    "            return X * self.params['iqr'] + self.params['median']\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported scaling method. Choose 'centralize', 'normalize', 'minmax', or 'robust'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`P. Threadpool and Mutex Wrapper (self-implemented, for future purpose, what if one day GIL is deprecated)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These threading interface is self implemented and open-sourced\n",
    "# Available at https://github.com/dof-studio/MML/\n",
    "# By Nathmath Huang (bh2821)\n",
    "# License: Apache License Version 2.0\n",
    "\n",
    "import concurrent.futures\n",
    "import uuid\n",
    "import threading\n",
    "from typing import Any\n",
    "\n",
    "# A threadpool worker class\n",
    "class ThreadPool:\n",
    "    \"\"\"\n",
    "    A simple thread pool for executing functions in separate threads.\n",
    "    Each submitted task returns a unique id, and you can wait until a task finishes or stop all tasks.\n",
    "    \"\"\"\n",
    "    def __init__(self, max_workers=4):\n",
    "        \"\"\"\n",
    "        Initialize the thread pool.\n",
    "        \n",
    "        Parameters:\n",
    "            max_workers (int): Maximum number of worker threads (default: system default).\n",
    "        \"\"\"\n",
    "        self.executor = concurrent.futures.ThreadPoolExecutor(max_workers=max_workers)\n",
    "        self.tasks = {}  # Mapping from task id to Future\n",
    "        self.lock = threading.Lock()\n",
    "\n",
    "    # Execute something with an assigned task number returned\n",
    "    def execute(self, func, *args, **kwargs) -> Any:\n",
    "        \"\"\"\n",
    "        Submit a function to be executed in a separate thread.\n",
    "        \n",
    "        Parameters:\n",
    "            func (callable): The function to execute.\n",
    "            *args: Positional arguments for the function.\n",
    "            **kwargs: Keyword arguments for the function.\n",
    "            \n",
    "        Returns:\n",
    "            str: A unique task id representing the submitted task.\n",
    "        \"\"\"\n",
    "        task_id = str(uuid.uuid4())\n",
    "        future = self.executor.submit(func, *args, **kwargs)\n",
    "        with self.lock:\n",
    "            self.tasks[task_id] = future\n",
    "        return task_id\n",
    "\n",
    "    # Coresively stop all tasks\n",
    "    def stopall(self):\n",
    "        \"\"\"\n",
    "        Attempt to cancel all tasks that haven't started.\n",
    "        Note that tasks already running may not be cancelled.\n",
    "        Clears the internal task registry.\n",
    "        \"\"\"\n",
    "        with self.lock:\n",
    "            for task_id, future in list(self.tasks.items()):\n",
    "                future.cancel()\n",
    "            self.tasks.clear()\n",
    "            \n",
    "    # Wait for a certain task\n",
    "    def waituntil(self, task_id: Any):\n",
    "        \"\"\"\n",
    "        Block until the task corresponding to the given id has finished.\n",
    "        \n",
    "        Parameters:\n",
    "            task_id (str): The unique id of the task.\n",
    "        \n",
    "        Returns:\n",
    "            The result of the task, if it completed successfully.\n",
    "        \n",
    "        Raises:\n",
    "            ValueError: If the task id is not found.\n",
    "        \"\"\"\n",
    "        with self.lock:\n",
    "            future = self.tasks.get(task_id)\n",
    "        if future is None:\n",
    "            raise ValueError(f\"Task with id {task_id} not found.\")\n",
    "        return future.result()  # Blocks until the task completes\n",
    "    \n",
    "    # Normally shut down\n",
    "    def shutdown(self, wait=True):\n",
    "        \"\"\"\n",
    "        Shutdown the thread pool.\n",
    "        \n",
    "        Parameters:\n",
    "            wait (bool): If True, block until all running tasks are finished.\n",
    "        \"\"\"\n",
    "        self.executor.shutdown(wait=wait)\n",
    "\n",
    "# A Pythonic/STL mutex comptible wrapper\n",
    "class Mutex:\n",
    "    \"\"\"\n",
    "    A thin wrapper around :class:`threading.Lock` that mimics the interface\n",
    "    of C++ `std::mutex` while feeling Pythonic.\n",
    "\n",
    "    It supports the three canonical methods—``lock``, ``try_lock``, and\n",
    "    ``unlock``—plus context‑manager helpers so you can use the ``with``‑statement\n",
    "    for automatic acquisition / release.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> m = Mutex()\n",
    "    >>> m.lock()          # block until the mutex is free\n",
    "    >>> m.unlock()        # release it again\n",
    "    >>> m.try_lock()      # returns True or False\n",
    "    >>> with m:           # RAII style\n",
    "    ...     critical()\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"\n",
    "        Create an unlocked mutex.\n",
    "        \"\"\"\n",
    "        self._lock: threading.Lock = threading.Lock()\n",
    "\n",
    "    # C++ std::mutex::lock()\n",
    "    def lock(self) -> None:\n",
    "        \"\"\"\n",
    "        Block the calling thread until the mutex is acquired.\n",
    "        \"\"\"\n",
    "        self._lock.acquire()\n",
    "\n",
    "    # C++ std::mutex::try_lock()\n",
    "    def try_lock(self) -> bool:\n",
    "        \"\"\"\n",
    "        Attempt to acquire the mutex without blocking.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        bool\n",
    "            ``True`` if the lock was acquired, ``False`` otherwise.\n",
    "        \"\"\"\n",
    "        return self._lock.acquire(blocking=False)\n",
    "\n",
    "    # C++ std::mutex::unlock()\n",
    "    def unlock(self) -> None:\n",
    "        \"\"\"\n",
    "        Release the mutex.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        Only the thread that currently owns the lock may call this.\n",
    "        \"\"\"\n",
    "        self._lock.release()\n",
    "\n",
    "    def __enter__(self) -> \"Mutex\":\n",
    "        \"\"\"\n",
    "        Enter a ``with``‑block by locking the mutex.\n",
    "        \"\"\"\n",
    "        self.lock()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type: Any, exc: Any, tb: Any) -> None:\n",
    "        \"\"\"\n",
    "        Exit a ``with``‑block by unlocking the mutex—even if an exception\n",
    "        was raised inside the block.\n",
    "        \"\"\"\n",
    "        self.unlock()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`P. Save and Load Interface for Saving a Model (self-implemented, using pickle and lzma)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These save/load interface is self implemented and open-sourced\n",
    "# Available at https://github.com/dof-studio/MML/\n",
    "# By Nathmath Huang (bh2821)\n",
    "# License: Apache License Version 2.0\n",
    "\n",
    "import pickle\n",
    "from typing import Any\n",
    "\n",
    "# Kompress can be:\n",
    "#  gzip\n",
    "#  lzma\n",
    "\n",
    "def save(obj: Any, filename: str, *, kompress: Any = None, protocol: int | None = None, **kwargs):\n",
    "    \"\"\"\n",
    "    Save a Python object to a file using pickle.\n",
    "    Directly save without wrapping.\n",
    "    \"\"\"\n",
    "    # Uncompress\n",
    "    if kompress is None:\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(obj, f, protocol = protocol)\n",
    "    # Compress\n",
    "    else:\n",
    "        with kompress.open(filename, 'wb', **kwargs) as f:\n",
    "            pickle.dump(obj, f, protocol = protocol)\n",
    "\n",
    "def load(filename: str, *, kompress: Any = None) -> Any:\n",
    "    \"\"\"\n",
    "    Load a Python object from a pickle file.\n",
    "    Generally loading. Try to unwrap if possible\n",
    "    \n",
    "    Exception:\n",
    "        Throw a ValueError when in the dumping mode and failed to\n",
    "        pass the hash test.\n",
    "    \"\"\"\n",
    "    # Uncompress\n",
    "    if kompress is None:\n",
    "        with open(filename, 'rb') as f:\n",
    "            obj = pickle.load(f)\n",
    "    else:\n",
    "        with kompress.open(filename, 'rb') as f:\n",
    "            obj = pickle.load(f)\n",
    "    \n",
    "    # No need to unwrap\n",
    "    if isinstance(obj, dict) == False:\n",
    "        return obj\n",
    "    elif isinstance(obj, dict) == True and obj.get(\"~attr~\", None) is None:\n",
    "        return obj\n",
    "    \n",
    "    # Need to unwrap\n",
    "    if isinstance(obj, dict) == True and obj.get(\"~attr~\", None) == \"~dump~\":\n",
    "        if obj.get(\"~hash~\", None) is None:\n",
    "            raise ValueError(\"Corrupted dumpped file. Hash attribute has Nonetype.\")\n",
    "        elif isinstance(obj.get(\"~hash~\", None), str) == False:\n",
    "            raise ValueError(\"Corrupted dumpped file. Hash attribute has Non-string type.\")\n",
    "        if obj.get(\"data\", None) is None:\n",
    "            raise ValueError(\"Corrupted dumpped file. Data attribute is Nonetype.\")\n",
    "        if obj.get(\"~hash~\", None) != str(hash(obj.get(\"data\"))):\n",
    "            raise ValueError(\"Corrupted dumpped file. Data hash mismatched.\")\n",
    "        return obj[\"data\"] \n",
    "    \n",
    "    else:\n",
    "        return obj\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`P. Basic Neural Network Components (self-implemented)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These basic Neural Network Components are self implemented and open-sourced\n",
    "# Available at https://github.com/dof-studio/MML/\n",
    "# By Nathmath Huang (bh2821)\n",
    "# License: Apache License Version 2.0\n",
    "\n",
    "# Deep Neural Network Abstract Module Base Class\n",
    "class nn_Base(Regression, Classification):\n",
    "    \n",
    "    __attr__ = \"MML.nn_Base\"    \n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def zero_grad(self):\n",
    "        \"\"\"\n",
    "        Set all gradients of all parameters to zero.\n",
    "        It will clear the gradients accumulated and restore when a new batch starts.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"zero_grad() is not implemented in nn_Base\")\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Set module to training mode.\n",
    "        It will affect dropouts and enable gradients calculation.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"train() is not implemented in nn_Base\")\n",
    "\n",
    "    def eval(self):\n",
    "        \"\"\"\n",
    "        Set module to evaluation mode.\n",
    "        It will disable dropouts and disable gradients calculation.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"eval() is not implemented in nn_Base\")\n",
    "\n",
    "    def parameters(self):\n",
    "        \"\"\"\n",
    "        Return an iterator of all Parameters in this module (includes children)\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"parameters() is not implemented in nn_Base\")\n",
    "        \n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Perform a forward propagation to calculate the loss.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"forward() is not implemented in nn_Base\")\n",
    "        \n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Perform a backward propagation to compute gradients for updating weights.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"backward() is not implemented in nn_Base\")\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return \"nn_Base(Deep Neural Network Abstract Module Base Class).\"\n",
    "\n",
    "\n",
    "# A Deep Neural Network Trainable Parameter Class\n",
    "class nn_Parameter(MLBase):\n",
    "    \"\"\"\n",
    "    A trainable parameter base data structure with gradient storage.\n",
    "    Contains data and manually implemented gradients, while you can use pytorch autograd techniques.\n",
    "    Optionally, you may set `autograd` = True to enable torch Autograd functionality instead of manual grads computation.\n",
    "    \"\"\"\n",
    "    \n",
    "    __attr__ = \"MML.nn_Parameter\"    \n",
    "    \n",
    "    def __init__(self, \n",
    "                 data: Tensor, \n",
    "                 requires_grad: bool = True, \n",
    "                 *, \n",
    "                 device: str | None = None, \n",
    "                 dtype: str | None = None, \n",
    "                 autograd: bool = False, \n",
    "                 **kwargs): \n",
    "        \"\"\"\n",
    "        Create a wrapped Neural Network Parameter Container, including gradients.\n",
    "        \n",
    "        Parameters:\n",
    "            --------\n",
    "            data: Tensor, The initial value for the parameter as a Tensor object.\n",
    "            requires_grad: bool, A flag indicating whether gradients should be tracked for this parameter. Defaults to True.\n",
    "            Optional:\n",
    "                device: str | None, The device where the tensor should reside (e.g., \"cpu\", \"cuda\"). If None, uses the default device. Defaults to None.\n",
    "                dtype: str | None, The data type of the tensor (e.g., \"float32\", \"float64\", or type like torch.float32). If None, uses the data type of the input `data`. Defaults to None.\n",
    "                autograd: bool, A flag indicating whether to use PyTorch's autograd functionality for gradient computation.  If True, manual gradient tracking is disabled. Defaults to None.\n",
    "        \n",
    "        Raises:\n",
    "            --------\n",
    "            ValueError: If the input `data` is not a Tensor object.\n",
    "    \n",
    "        Attributes:\n",
    "            --------\n",
    "            self.autograd: bool, Indicates whether PyTorch's autograd is enabled.\n",
    "            self.requires_grad: bool, A flag indicating whether gradients are tracked for this parameter.\n",
    "            self.data: Tensor, The parameter data as a Tensor object, cloned and potentially moved to the specified device/dtype.\n",
    "            self.grad: Tensor | None, The manually accumulated gradient (reserved for future evaluation); set to None initially.\n",
    "        \"\"\"\n",
    "        \n",
    "        # MLBase is for save/load purposes.\n",
    "        super().__init__()\n",
    "        \n",
    "        # Record if it uses pytorch's autograd\n",
    "        self.autograd = autograd\n",
    "        \n",
    "        # Record if it uses gradients (either autograd or manually calculated)\n",
    "        self.requires_grad = requires_grad\n",
    "        \n",
    "        # Initialize the parameter with a tensor (ensure float dtype and device placement)\n",
    "        if not isinstance(data, Tensor):\n",
    "            raise ValueError(\"Input data MUST be a MML.Tensor! Please convert by calling Tensor(data, backend='torch')\")\n",
    "        # Parameter Data - a Tensor Object\n",
    "        self.data = data if dtype is None and device is None else data.to(backend=data._backend, dtype=dtype, device=device)\n",
    "        # Gradient manually accumulated during backprop\n",
    "        # If uses autograd, then self.data.grad will record it\n",
    "        self.grad = self.data.to_zeros() if requires_grad == True and autograd == False else None \n",
    "        \n",
    "        # If uses pytorch autograd, then enable if requires grad\n",
    "        if autograd == True and requires_grad == True:\n",
    "            self.data.requires_grad_(True)\n",
    "       \n",
    "    def zero_grad(self):\n",
    "        \"\"\"\n",
    "        Reset the gradient to zero.\n",
    "        \n",
    "        Returns:\n",
    "            -------\n",
    "            self\n",
    "        \"\"\"\n",
    "        if self.requires_grad == True:\n",
    "            # Use pytorch's autograd, then directly set to 0\n",
    "            if self.autograd == True:\n",
    "                self.data.data.grad.zero_()\n",
    "            # Use manually calculated grads, then manually set to 0\n",
    "            else:\n",
    "                if self.grad is not None:\n",
    "                    self.grad[...] = 0\n",
    "                else:\n",
    "                    self.grad = self.data.to_zeros()\n",
    "        return self\n",
    "\n",
    "    def requires_grad_(self, requires_grad: bool = True):\n",
    "        \"\"\"\n",
    "        Set the attributes of `requires_grid` and enable/disable autograd if used.\n",
    "        \n",
    "        Returns:\n",
    "            -------\n",
    "            self\n",
    "        \"\"\"\n",
    "        \n",
    "        # If status conflicts, then create/disable grad\n",
    "        if self.requires_grad != requires_grad:\n",
    "            \n",
    "            # Set the attribute of requiring grads or not\n",
    "            self.requires_grad = requires_grad\n",
    "            \n",
    "            # If uses pytorch autograd, then enable if requires grad\n",
    "            if self.autograd == True:\n",
    "                self.data.requires_grad_(requires_grad)\n",
    "            else:\n",
    "                if requires_grad == True:\n",
    "                    self.zero_grad()\n",
    "                else:\n",
    "                    self.grad = None\n",
    "\n",
    "    def to(self, device: str | None = None):\n",
    "        \"\"\"\n",
    "        Move the parameters and gradients to the specified device.\n",
    "        \n",
    "        Parameters:\n",
    "            --------\n",
    "            device: str | None, The device where the tensor should reside (e.g., \"cpu\", \"cuda\"). If None, do nothing.\n",
    "        \"\"\"\n",
    "        if device is not None:\n",
    "            self.data = self.data.to(backend = self.data._backend, device = device)\n",
    "            if self.grad is not None:\n",
    "                self.grad = self.grad.to(backend = self.grad._backend, device = device)\n",
    "        return self\n",
    "\n",
    "    def copy(self):\n",
    "        \"\"\"\n",
    "        Create a deepcopy of the parameters and gradiets.\n",
    "        \"\"\"\n",
    "        return deepcopy(self)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"nn_Parameter(Deep Neural Network Trainable Parameter Class).\" + \"\\nData: \" + self.data.__repr__() + \"\\nGrad: \" + (self.grad.__repr__() if self.grad is not None else \"\")\n",
    "\n",
    "\n",
    "# A Deep Neural Network Interface Module Base Class\n",
    "class nn_BaseModule(nn_Base):\n",
    "    \"\"\"\n",
    "    A even base class for Neural Network Modules.\n",
    "    \"\"\"\n",
    "    \n",
    "    __attr__ = \"MML.nn_BaseModule\"    \n",
    "    \n",
    "    def __setattr__(self, name: str, value: Any):\n",
    "        \"\"\"\n",
    "        Override setattr to register Parameters and Modules.\n",
    "        \n",
    "        Registers parameters (nn_Parameter) and modules (nn_Module) under the `._parameters` and `._modules` dictionaries \n",
    "        when appropriate. Delegates attribute assignment to the base class's `__setattr__` method for standard attributes.\n",
    "        \n",
    "        Args:\n",
    "            name: str, The name of the attribute being set.\n",
    "            value: Any, The value to assign to the attribute.\n",
    "            \n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        # Register a parameter if is nn_Parameter\n",
    "        if isinstance(value, nn_Parameter):\n",
    "            self._parameters[name] = value\n",
    "            \n",
    "        # Register a submodule if nn_Module\n",
    "        elif isinstance(value, nn_BaseModule):\n",
    "            self._modules[name] = value\n",
    "            \n",
    "        # Copy if it is a Object based variable\n",
    "        elif isinstance(value, Object):\n",
    "            object.__setattr__(self, name, value)\n",
    "        \n",
    "        # In all cases, set the attribute normally as an attribute\n",
    "        object.__setattr__(self, name, value)\n",
    "    \n",
    "    def __call__(self, *inputs):\n",
    "        \"\"\"\n",
    "        Delegates to the forward method to perform the forward pass of the module.\n",
    "        \n",
    "        This method allows a Module instance to be called like a function, passing inputs to the forward() method.\n",
    "        It delegates attribute assignment and functionality to the base class's `__setattr__` and `forward()` methods.\n",
    "        \n",
    "        Args:\n",
    "            *inputs: list, Variable number of input tensors or values to pass to the forward method.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: The output tensor resulting from the forward pass.\n",
    "        \n",
    "        Raises:\n",
    "            NotImplementedError: If the forward() method is not implemented in the subclass.\n",
    "        \"\"\"\n",
    "        # Allows Module instance to be called like a function to perform forward pass\n",
    "        return self.forward(*inputs)\n",
    "    \n",
    "    def __init__(self, module_name: str = \"Default_Module_Container\"):\n",
    "        \"\"\"\n",
    "        Initializes a base neural network module with basic structural components.\n",
    "\n",
    "        This constructor sets up essential properties for a module, including its name,\n",
    "        training mode flag, and containers for parameters and submodules. It follows a\n",
    "        convention similar to PyTorch modules, where modules are organized hierarchically\n",
    "        with parameter tracking and submodule management.\n",
    "\n",
    "        Parameters:\n",
    "            --------\n",
    "            module_name: str, The name of the module instance. Defaults to \"Default_Module_Container\".\n",
    "\n",
    "        Attributes:\n",
    "            self.name: The name of the module instance, set dynamically via __setattr__.\n",
    "            self.training: A flag indicating whether the module is in training mode. Defaults to True.\n",
    "            self._parameter: A dictionary container for all parameters of this module.\n",
    "            self._modules: A dictionary container for all submodule instances nested within this module.\n",
    "        \"\"\"\n",
    "        \n",
    "        # A default module at least contains:\n",
    "        # 1. self.name, str, the name of this module instance\n",
    "        # 2. self.training, bool, whether the module is in training or evaluation mode\n",
    "        # 3. self.accumulate, bool, whether the module is accumulating gradients\n",
    "        # 4. self._parameters, container, all parameters of THIS module\n",
    "        # 5. self._modules, container, all instances of SUB modules\n",
    "        \n",
    "        # Module name, indicating the name of the module, a string\n",
    "        self.__setattr__(\"name\", module_name)\n",
    "        \n",
    "        # Training flag, indicating the model is training or not\n",
    "        self.__setattr__(\"training\", False)\n",
    "        \n",
    "        # Accumulating flag, indicating the model is accumulating gradients or not\n",
    "        self.__setattr__(\"accumulate\", False)\n",
    "        # You can only set this to True by calling accumulate_grad before doing forward\n",
    "        # to accumulate gradients when training.\n",
    "        \n",
    "        # Initialize internal containers for parameters\n",
    "        self.__setattr__(\"_parameters\", {})\n",
    "        self.__setattr__(\"_modules\", {})\n",
    "\n",
    "    def forward(self, *inputs):\n",
    "        \"\"\"\n",
    "        Override this method in subclasses to define forward pass.\n",
    "        A forward pass is the way that the neural network passes the inputs\n",
    "        through parameters and generates the output.\n",
    "        \n",
    "        Raises:\n",
    "            NotImplementedError: You should implement your own forward pass. Calling the base will raise this error.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"forward() is not implemented in the base module. You should define the architecture of your neural network manually.\")\n",
    "\n",
    "    def parameters(self):\n",
    "        \"\"\"\n",
    "        Returns an iterator (or list) of all Parameters in this module, including those from child modules.\n",
    "\n",
    "        This method recursively collects all `Parameter` instances from the current module and its submodules,\n",
    "        following a pattern similar to PyTorch's `parameters()` method. It aggregates parameters from both direct\n",
    "        parameters (`self._parameters`) and nested modules (`self._modules`).\n",
    "\n",
    "        Returns:\n",
    "            list: An iterator of all `Parameter` objects in this module and its submodules.\n",
    "        \"\"\"\n",
    "        params = []\n",
    "        \n",
    "        # Own parameters\n",
    "        for param in self._parameters.values():\n",
    "            params.append(param)\n",
    "            \n",
    "        # Parameters of submodules\n",
    "        for module in self._modules.values():\n",
    "            params.extend(module.parameters())\n",
    "        return params\n",
    "\n",
    "    def zero_grad(self):\n",
    "        \"\"\"\n",
    "        Resets the gradients of all parameters in this module and its submodules to zero.\n",
    "\n",
    "        This method iterates through all `Parameter` objects in the module (including those from child modules)\n",
    "        and calls `zero_grad()` on each, effectively clearing the gradient buffers. It follows the same pattern\n",
    "        as PyTorch's `zero_grad()` method for efficiency and consistency with standard neural network training workflows.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        for param in self.parameters():\n",
    "            param.zero_grad()\n",
    "\n",
    "    def accumulate_grad(self):\n",
    "        \"\"\"\n",
    "        Set module to gradient accumulate mode. \n",
    "\n",
    "        This method sets the `accumulate` flag to True, ensures gradients are accumulated in backward passing instead of being set to 0.\n",
    "        You should first call `train` to turn the module into training model. Otherwise it will raise a RuntimeError.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "            \n",
    "        Raises:\n",
    "            RuntimeError: if calling accumulate_grad in non-training mode.\n",
    "        \"\"\"\n",
    "        # If called in non-training mode, raise RuntimeError\n",
    "        if self.training == False:\n",
    "            raise RuntimeError(f\"Calling accumulate_grad() on Module {self.name} to accumulate gradients, but without turning training mode on. Please call .train() first.\")\n",
    "        \n",
    "        # Set the status to accumulating\n",
    "        self.accumulate = True\n",
    "        \n",
    "        # For other modules, set to accumulate mode\n",
    "        for module in self._modules.values():\n",
    "            module.accumulate_grad()\n",
    "        \n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Set module to training mode (affects dropout, and enables gradients).\n",
    "\n",
    "        This method sets the `training` flag to True, ensures all direct parameters require gradients, and recursively applies \n",
    "        the training mode to all submodules. It is typically used at the beginning of a training loop to activate behaviors \n",
    "        specific to training, such as dropout or batch normalization.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        # Set the status to training\n",
    "        self.training = True\n",
    "        \n",
    "        # For this module, set all parameters to requires_grad = True\n",
    "        for param in self._parameters.values():\n",
    "            param.requires_grad_(True)\n",
    "        \n",
    "        # For other modules, set to train modes\n",
    "        for module in self._modules.values():\n",
    "            module.train()\n",
    "\n",
    "    def eval(self):\n",
    "        \"\"\"\n",
    "        Set module to evaluation mode (disable dropout and gradients).\n",
    "        \n",
    "        This method sets the `training` flag to False, which disables behaviors specific to training (e.g., dropout). \n",
    "        It also explicitly sets all direct parameters to require gradients (`requires_grad=True`) and recursively applies \n",
    "        evaluation mode to all submodules.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        # Set the status to evalutating\n",
    "        self.training = False\n",
    "        \n",
    "        # For this module, set all parameters to requires_grad = False\n",
    "        for param in self._parameters.values():\n",
    "            param.requires_grad_(False)\n",
    "        \n",
    "        # For other modules, set to evaluation mode\n",
    "        for module in self._modules.values():\n",
    "            module.eval()\n",
    "\n",
    "    def to(self, device: str | None = None):\n",
    "        \"\"\"\n",
    "        Moves all parameters and submodules to the specified device.\n",
    "\n",
    "        This method relocates the module's parameters and nested submodules to the given device (e.g., 'cpu', 'cuda'). \n",
    "        If `device` is None, it uses the default device. This is essential for moving models between devices during training or inference.\n",
    "\n",
    "        Args:\n",
    "            device (str | None): The target device (e.g., \"cpu\", \"cuda\"). If None, the default device is used.\n",
    "\n",
    "        Returns:\n",
    "            self\n",
    "        \"\"\"\n",
    "        # Set device directly\n",
    "        if device == self.device:\n",
    "            return self\n",
    "        else:\n",
    "            self.device = device\n",
    "        \n",
    "        # For this module, set every parameters to device\n",
    "        for param in self._parameters.values():\n",
    "            param.to(device)\n",
    "       \n",
    "        # For other modules, set every parameters to device\n",
    "        for module in self._modules.values():\n",
    "            module.to(device)\n",
    "        return self\n",
    "\n",
    "    def backward(self, grad_output: Tensor | None):\n",
    "        \"\"\"\n",
    "        Backpropagate through the module (feedforward).\n",
    "        \n",
    "        If you need gradients with respect to internal states like GRU or LSTM states,\n",
    "        please manually override this method to capture the gradients.\n",
    "        By default, we only capture the 1st gradient which is gradient with respect to inputs.\n",
    "\n",
    "        This method performs gradient propagation through the module's submodules in reverse order of their addition.\n",
    "        ** By default, if the module contains submodules, propagate grad through them in reverse order.\n",
    "        ** Leaf modules (layers) should override this to implement their own backward logic.\n",
    "        ** Leaf modules (layers) should also be compatible to pytorch's autograd and manual calculation.\n",
    "\n",
    "        Args:\n",
    "            grad_output (Tensor): The gradient tensor resulting from the output of the module, used as input for backpropagation..\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The propagated gradient after processing through all submodules.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If `grad_output` is not a valid MML.Tensor object.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Note. This will be override by Layers to implement the actual logic of \n",
    "        #       gradient calculation and backpropagation.\n",
    "        # For a normal non-leaf module, we assume it is a container containing NO\n",
    "        #       parameter and just sub-modules. So we invoke backward of submodules.\n",
    "        \n",
    "        # If autograd, then the true backward is performed by LOSS function\n",
    "        # Although it gets soemthing here into the module, it just for compatibility.\n",
    "        # We don't need to processs it anymore.\n",
    "        if self.autograd == True:\n",
    "            return None\n",
    "        \n",
    "        # Type check, it must be a Tensor object if non-autograd mode\n",
    "        if isinstance(grad_output, Tensor) == False:\n",
    "            raise ValueError(f\"Output gradient must be in a MML `Tensor` format but you have {type(grad_output)}\")\n",
    "            \n",
    "        # Propagate gradient through submodules in `reverse` order of addition\n",
    "        for module in reversed(list(self._modules.values())):\n",
    "            # For GNU or layers with hidden spaces, backward may\n",
    "            # return more than 1 element, but the 1st is ensured to be \n",
    "            # grad wrt to inputs.\n",
    "            # In the general feed-forward case, we only keep the 1st gradient.\n",
    "            # If you need other terms, override this method to achive it.\n",
    "            grad_output = module.backward(grad_output)\n",
    "            if isinstance(grad_output, tuple):\n",
    "                grad_output = grad_output[0]\n",
    "            \n",
    "        return grad_output\n",
    "    \n",
    "    def copy(self):\n",
    "        \"\"\"\n",
    "        Create a deepcopy of the parameters and gradiets.\n",
    "        \"\"\"\n",
    "        return deepcopy(self)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"nn_BaseModule(Deep Neural Network Base Module Class \\nConstruct Your Own Network by Creating Children of this Module.).\"\n",
    "\n",
    "\n",
    "# A Deep Neural Network Interface Module Class\n",
    "# Construct Your Own Network by Creating Children of this Module\n",
    "class nn_Module(nn_BaseModule):\n",
    "    \n",
    "    \"\"\"\n",
    "    Base interface for all neural network modules, with standard keyword arguments.\n",
    "    \n",
    "    All neural network modules implemented by users should inherit this class\n",
    "    and utilize the forward() to define the architecture of their networks.\n",
    "    By interacting with an optimizer and loss function, you can update\n",
    "    the weights stored in Parameters and train your neural network.\n",
    "    \"\"\"\n",
    "    \n",
    "    __attr__ = \"MML.nn_Module\"    \n",
    "\n",
    "    def __init__(self, \n",
    "                 *,\n",
    "                 module_name: str = \"Default_Module_Container\",\n",
    "                 backend: Literal[\"torch\", \"numpy\"] = \"torch\",\n",
    "                 dtype: type | str = None,\n",
    "                 device: str | None = None,\n",
    "                 autograd: bool = False,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        Initializes a neural network module interface with basic structural components.\n",
    "\n",
    "        This constructor sets up essential properties for a module, including its name,\n",
    "        training mode flag, and containers for parameters and submodules. It follows a\n",
    "        convention similar to PyTorch modules, where modules are organized hierarchically\n",
    "        with parameter tracking and submodule management.\n",
    "\n",
    "        Parameters:\n",
    "            --------\n",
    "            module_name: str, The name of the module instance. Defaults to \"Default_Module_Container\".\n",
    "            backend: Literal[\"torch\", \"numpy\"], The computational backend to use. Defaults to \"torch\".\n",
    "            dtype: type, The data type for the tensor values. Defaults to None (auto detection). \n",
    "                    For PyTorch, this corresponds to torch.dtype; for NumPy, it corresponds to np.dtype.\n",
    "            device: str | None, The target device (e.g., \"cpu\", \"cuda\") where the layer's parameters will be placed. \n",
    "                    If None, uses the default device. Defaults to None (auto detection).\n",
    "            autograd: bool, A flag indicating whether to use PyTorch's autograd for gradient computation. \n",
    "                    If True, manual gradient tracking is disabled. Defaults to False.\n",
    "\n",
    "        Attributes:\n",
    "            self.name: The name of the module instance, set dynamically via __setattr__.\n",
    "            self.training: A flag indicating whether the module is in training mode. Defaults to True.\n",
    "            self._parameter: A dictionary container for all parameters of this module.\n",
    "            self._modules: A dictionary container for all submodule instances nested within this module.\n",
    "            self.backend: Literal[\"torch\", \"numpy\"], The computational backend used by the layer.\n",
    "            self.dtype: type, The data type of the tensor values.\n",
    "            self.device: str | None, The device where the parameters are placed.\n",
    "            self.autograd: bool, Whether PyTorch's autograd is enabled for this layer.\n",
    "        \"\"\"\n",
    "        \n",
    "        # A default module at least contains:\n",
    "        # 1. self.name, str, the name of this module instance\n",
    "        # 2. self.training, bool, whether the module is in training or evaluation mode\n",
    "        # 3. self.accumulate, bool, whether the module is accumulating gradients\n",
    "        # 4. self._parameters, container, all parameters of THIS module\n",
    "        # 5. self._modules, container, all instances of SUB modules\n",
    "        # 6. self.backend, str, saying which backend Tensor will by default use\n",
    "        # 7. self.dtype, type, saying the type the data will be stored\n",
    "        # 8. self.device, str, saying the device where the data is stored on\n",
    "        # 9. self.autograd, bool, whether the modules use pytorch autograd or not\n",
    "        \n",
    "        # Module name, internal containers, calling base init to initialize\n",
    "        super().__init__(module_name = module_name)\n",
    "        \n",
    "        # Process the default types\n",
    "        if backend not in (\"numpy\", \"torch\"):\n",
    "            raise ValueError(f\"In creating a module {module_name}, an unsupported backend is passed in. Use 'numpy' or 'torch' only.\")\n",
    "        if backend == \"numpy\":\n",
    "            dtype = np.float32 if dtype is None else dtype\n",
    "            device = \"cpu\" if device is None else device\n",
    "        elif backend == \"torch\":\n",
    "            dtype = torch.float32 if dtype is None else dtype\n",
    "            device = \"cpu\" if device is None else device\n",
    "        \n",
    "        # Record the backend, dtype, device, autograd traits\n",
    "        self.__setattr__(\"backend\", backend)\n",
    "        self.__setattr__(\"dtype\", dtype)\n",
    "        self.__setattr__(\"device\", device)\n",
    "        self.__setattr__(\"autograd\", autograd)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"nn_Module(name = {self.name}).\"\n",
    "\n",
    "\n",
    "# Alias for nn_Module\n",
    "Module = nn_Module\n",
    "\n",
    "\n",
    "# Implementation of Dense Layer (Fully Connected Layer)\n",
    "class nn_Layer_Dense(nn_Module):\n",
    "    \"\"\"\n",
    "    Dense Layer (Fully-Connected Layer) Implementation\n",
    "    \n",
    "    This class serves as the foundation for implementing fully connected (dense)\n",
    "    neural network layers. It contains weight and bias in nn_Parameter containers\n",
    "    and ready to perform forward() and backward() pass to perform MLP tasks.    \n",
    "    \"\"\"\n",
    "    \n",
    "    __attr__ = \"MML.nn_Layer_Dense\"    \n",
    "    \n",
    "    def __init__(self, \n",
    "                 in_features: int = 1, \n",
    "                 out_features: int = 1, \n",
    "                 has_bias: str = True,\n",
    "                 init_scale: float = 0.01,\n",
    "                 *,\n",
    "                 module_name: str = \"nn_Layer_Dense\", \n",
    "                 backend: Literal[\"torch\", \"numpy\"] = \"torch\",\n",
    "                 dtype: type | str = None,\n",
    "                 device: str | None = None,\n",
    "                 autograd: bool = False,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        A fully connected layer: y = x @ W + b.\n",
    "        \n",
    "        This class implements a dense (fully connected) neural network layer, which performs a linear transformation\n",
    "        on input data followed by an optional bias addition. It is designed to be compatible with both PyTorch and NumPy backends,\n",
    "        supporting automatic gradient computation via autograd or manual gradient tracking.\n",
    "\n",
    "        Parameters:\n",
    "            in_features: int, The number of input features for this layer. Defaults to 1.\n",
    "            out_features: int, The number of output features for this layer. Defaults to 1.\n",
    "            has_bias: str, A flag indicating whether to include a bias term. Valid values are \"True\" or \"False\".\n",
    "                    If set to \"True\", the layer includes an additive bias term (b). Defaults to \"True\".\n",
    "            module_name: str, The name of the module instance. Defaults to \"nn_Layer_Dense\".\n",
    "            backend: Literal[\"torch\", \"numpy\"], The computational backend to use. Defaults to \"torch\".\n",
    "            dtype: type, The data type for the tensor values. Defaults to None (auto detection). \n",
    "                    For PyTorch, this corresponds to torch.dtype; for NumPy, it corresponds to np.dtype.\n",
    "            device: str | None, The target device (e.g., \"cpu\", \"cuda\") where the layer's parameters will be placed. \n",
    "                    If None, uses the default device. Defaults to None (auto detection).\n",
    "            autograd: bool, A flag indicating whether to use PyTorch's autograd for gradient computation. \n",
    "                    If True, manual gradient tracking is disabled. Defaults to False.\n",
    "\n",
    "        Attributes:\n",
    "            self.in_features: int, The number of input features for this layer.\n",
    "            self.out_features: int, The number of output features for this layer.\n",
    "            self.has_bias: str, A flag indicating whether a bias term is included (\"True\" or \"False\").\n",
    "            self.init_scale: float, A floatting number indicating the maximum value of initial random weights.\n",
    "            self.backend: Literal[\"torch\", \"numpy\"], The computational backend used by the layer.\n",
    "            self.dtype: type, The data type of the tensor values.\n",
    "            self.device: str | None, The device where the parameters are placed.\n",
    "            self.autograd: bool, Whether PyTorch's autograd is enabled for this layer.\n",
    "            self._parameters.weight: nn_Parameter, The weight parameter matrix (shape: [in_features, out_features]).\n",
    "            self._parameters.bias: nn_Parameter | optional, The optional bias vector (shape: [out_features]) if has_bias is True.\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__(module_name = module_name, backend = backend, dtype = dtype, device = device, autograd = autograd)\n",
    "        \n",
    "        # Shape Notation\n",
    "        # Input: (in_features)\n",
    "        # Output: (out_features)\n",
    "        # self._parameters[\"weight\"]: (in_features, out_features)\n",
    "        # self._parameters[\"bias\"]: (out_features)\n",
    "        \n",
    "        # Record shapes etc\n",
    "        self.__setattr__(\"in_features\", in_features)\n",
    "        self.__setattr__(\"out_features\", out_features)\n",
    "        self.__setattr__(\"has_bias\", has_bias)\n",
    "        self.__setattr__(\"init_scale\", init_scale)\n",
    "        \n",
    "        # Initialize weight and bias parameters\n",
    "        self.__setattr__(\"weight\", nn_Parameter(\n",
    "            Tensor.rand([in_features, out_features], backend=backend, dtype=dtype, device=device) * init_scale,\n",
    "            requires_grad = True,\n",
    "            dtype = None,\n",
    "            device = None,\n",
    "            autograd = autograd)\n",
    "            )\n",
    "        \n",
    "        if has_bias == True:\n",
    "            # If uses bias, then set the bias\n",
    "            self.__setattr__(\"bias\", nn_Parameter(\n",
    "                Tensor.zeros([out_features], backend=backend, dtype=dtype, device=device),\n",
    "                requires_grad = True,\n",
    "                dtype = None,\n",
    "                device = None,\n",
    "                autograd = autograd)\n",
    "                )\n",
    "            \n",
    "        return\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Compute the layer output: out = x @ W + b and return the out.\n",
    "\n",
    "        This method performs the forward computation for a dense neural network layer,\n",
    "        computing the linear transformation `out = x @ W + b`, where `W` is the weight matrix\n",
    "        and `b` is the bias vector. The input tensor `x` is processed through this operation,\n",
    "        and the result is returned as the output of the layer.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape (batch_size, in_features) to be transformed by the layer.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output tensor of shape (batch_size, out_features) after applying the dense transformation.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the input `x` is not a valid MML.Tensor object.\n",
    "\n",
    "        Attributes:\n",
    "            self.input (Tensor): The input tensor saved for use in backward propagation.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Type check, x must be an instance of Tensor\n",
    "        if isinstance(x, Tensor) == False:\n",
    "            raise ValueError(f\"In performing forward(), input `x` must be in a MML `Tensor` format but you have {type(x)}\")\n",
    "        \n",
    "        # Save input for backward\n",
    "        self.__setattr__(\"input\", x)\n",
    "        \n",
    "        # Perform forward pass x @ W + b\n",
    "        out = x @ self._parameters[\"weight\"].data\n",
    "        if self.has_bias == True:\n",
    "            out += self._parameters[\"bias\"].data\n",
    "        return out\n",
    "\n",
    "    def backward(self, grad_output: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Backward pass: compute gradients for weight, bias, and input.\n",
    "\n",
    "        This method performs the gradient computation for a dense layer during backpropagation. \n",
    "        It calculates the gradients of the loss with respect to the weights, biases, and input tensor,\n",
    "        based on the provided `grad_output` (gradient from the next layer). The implementation\n",
    "        supports both PyTorch autograd and manual gradient calculation modes.\n",
    "\n",
    "        Args:\n",
    "            grad_output (Tensor): Gradient tensor resulting from the output of the layer, used as input for backpropagation.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Gradient with respect to the input tensor, for recursive backward calculations in previous layers.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If `grad_output` is not a valid MML.Tensor object..\n",
    "        \"\"\"\n",
    "        \n",
    "        # If use autograd, pass; if manual mode, then calculate\n",
    "        if self.autograd == True:\n",
    "            return None\n",
    "        \n",
    "        # Type check, grad_output must be an instance of Tensor\n",
    "        if isinstance(grad_output, Tensor) == False:\n",
    "            raise ValueError(f\"In performing backward(), `grad_output` must be in a MML `Tensor` format but you have {type(grad_output)}\")\n",
    "        \n",
    "        # Gradient wrt. weight: X^T * grad_output\n",
    "        self._parameters[\"weight\"].grad = self.input.transpose() @ grad_output\n",
    "        \n",
    "        # Gradient wrt. bias: sum grad_output over batch dimension\n",
    "        if self.has_bias == True:\n",
    "            # Sum over the samples to get the gradients\n",
    "            self._parameters[\"bias\"].grad = grad_output.sum(axis = 0)\n",
    "        \n",
    "        # Gradient wrt. input: grad_output * W^T\n",
    "        grad_input = grad_output @ self._parameters[\"weight\"].data.transpose()\n",
    "        \n",
    "        # Return the gradient with respect to input for recursive backward calculation\n",
    "        return grad_input\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"nn_Layer_Dense(shape: ({self.in_features}, {self.out_features}) with{'out' if self.has_bias == False else ''} bias).\"\n",
    "    \n",
    "    \n",
    "# Alias for nn_Layer_Dense\n",
    "Dense = nn_Layer_Dense\n",
    "\n",
    "# Implementation of Dropout Layer (Masked Layer)\n",
    "class nn_Layer_Dropout(nn_Module):\n",
    "    \"\"\"\n",
    "    Dropout Layer (Masked Layer) Implementation\n",
    "    \n",
    "    Dropout layer that zeros out inputs with probability p during training, and \n",
    "    it will not mask anything in non-training mode.\n",
    "    Dropout layer can be used to improve anti-over-fitting capabilities of your model.\n",
    "    And it is compatible for any kind of Tensors with any shape (not only a 2D).\n",
    "    It does not have any learnable parameters.\n",
    "    \"\"\"\n",
    "    \n",
    "    __attr__ = \"MML.nn_Layer_Dropout\"  \n",
    "    \n",
    "    def __init__(self, \n",
    "                 p: float = 0.1,\n",
    "                 *,\n",
    "                 module_name: str = \"nn_Layer_Dropout\", \n",
    "                 backend: Literal[\"torch\", \"numpy\"] = \"torch\",\n",
    "                 dtype: type | str = None,\n",
    "                 device: str | None = None,\n",
    "                 autograd: bool = False,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        A training active dropout layer.\n",
    "        \n",
    "        This class implements a dropout neural network layer, which performs randomly dropout when training and \n",
    "        do nothing in evaluation process. Dropping out is controlled by a dropout rate which is typically ranging from\n",
    "        0 to 1 and common values are [0.1, 0.4].\n",
    "\n",
    "        Parameters:\n",
    "            p: float, The ratio of dropout when training the neural network. By default, it is 0.1.\n",
    "            module_name: str, The name of the module instance. Defaults to \"nn_Layer_Dense\".\n",
    "            backend: Literal[\"torch\", \"numpy\"], The computational backend to use. Defaults to \"torch\".\n",
    "            dtype: type, The data type for the tensor values. Defaults to None (auto detection). \n",
    "                    For PyTorch, this corresponds to torch.dtype; for NumPy, it corresponds to np.dtype.\n",
    "            device: str | None, The target device (e.g., \"cpu\", \"cuda\") where the layer's parameters will be placed. \n",
    "                    If None, uses the default device. Defaults to None (auto detection).\n",
    "            autograd: bool, A flag indicating whether to use PyTorch's autograd for gradient computation. \n",
    "                    If True, manual gradient tracking is disabled. Defaults to False.\n",
    "\n",
    "        Attributes:\n",
    "            self.dropout_p: float, the dropout rate specified and used in training.\n",
    "            self.dtype: type, The data type of the tensor values.\n",
    "            self.device: str | None, The device where the parameters are placed.\n",
    "            self.autograd: bool, Whether PyTorch's autograd is enabled for this layer.\n",
    "            self._parameters.weight: nn_Parameter, The weight parameter matrix (shape: [in_features, out_features]).\n",
    "            self._parameters.bias: nn_Parameter | optional, The optional bias vector (shape: [out_features]) if has_bias is True.\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__(module_name = module_name, backend = backend, dtype = dtype, device = device, autograd = autograd)\n",
    "        \n",
    "        # Record the dropout rate as a non-trainable parameter\n",
    "        self.__setattr__(\"dropout_p\", p)\n",
    "        \n",
    "        # Record an empty tuple showing the shape of the mask\n",
    "        self.__setattr__(\"shape\", ())\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Apply dropout during forward pass.\n",
    "\n",
    "        This method implements the forward computation of a dropout layer, which randomly sets elements of the input tensor to zero\n",
    "        with probability `p` during training. In evaluation mode, it returns the input unchanged. The dropout mask is stored for use\n",
    "        in backpropagation during training.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input tensor to apply dropout to. Shape should match the expected dimensions for the layer.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output tensor after applying dropout. During training, this tensor has elements randomly zeroed out and scaled.\n",
    "                   In evaluation mode, it returns the input tensor unchanged.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the input `x` is not a valid MML.Tensor object.\n",
    "        \"\"\"\n",
    "\n",
    "        # Type check, x must be an instance of Tensor\n",
    "        if isinstance(x, Tensor) == False:\n",
    "            raise ValueError(f\"In performing forward(), input `x` must be in a MML `Tensor` format but you have {type(x)}\")\n",
    "        \n",
    "        # Mask it in training mode\n",
    "        if self.training == True:\n",
    "            \n",
    "            # Save the shape of the mask\n",
    "            self.shape = x.shape\n",
    "            \n",
    "            # Create a dropout mask: 1 with probability (1-p), 0 with probability p\n",
    "            mask = Tensor.rand(x.shape, backend=self.backend, dtype=self.dtype, device=self.device)\n",
    "            mask.data = mask.data >= self.dropout_p\n",
    "            mask.astype(self.dtype)\n",
    "            \n",
    "            # Scale mask by 1 / (1-p) to keep expectation the same\n",
    "            mask = mask / (1 - self.dropout_p)\n",
    "            \n",
    "            # Save the mask as an attribute (non-parameter attribute)\n",
    "            self.__setattr__(\"mask\", mask)\n",
    "            \n",
    "            return x * mask\n",
    "        \n",
    "        # Do nothing in evaluation mode\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    def backward(self, grad_output: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Backward pass: compute gradients for weight, bias, and input.\n",
    "\n",
    "        This method performs the gradient computation for a dense layer during backpropagation. \n",
    "        It calculates the gradients of the loss with respect to the weights, biases, and input tensor,\n",
    "        based on the provided `grad_output` (gradient from the next layer). The implementation\n",
    "        supports both PyTorch autograd and manual gradient calculation modes.\n",
    "\n",
    "        Args:\n",
    "            grad_output (Tensor): Gradient tensor resulting from the output of the layer, used as input for backpropagation.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Gradient with respect to the input tensor, for recursive backward calculations in previous layers.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If `grad_output` is not a valid MML.Tensor object..\n",
    "        \"\"\"\n",
    "        \n",
    "        # If use autograd, pass; if manual mode, then calculate\n",
    "        if self.autograd == True:\n",
    "            return None\n",
    "        \n",
    "        # Type check, grad_output must be an instance of Tensor\n",
    "        if isinstance(grad_output, Tensor) == False:\n",
    "            raise ValueError(f\"In performing backward(), `grad_output` must be in a MML `Tensor` format but you have {type(grad_output)}\")\n",
    "        \n",
    "        # If training, apply the same mask to the gradient\n",
    "        if self.training:\n",
    "            return grad_output * self.mask\n",
    "        \n",
    "        # Else, return identity\n",
    "        else:\n",
    "            return grad_output\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"nn_Layer_Dropout(shape: {self.shape} with probability {round(self.dropout_p)}).\"\n",
    "    \n",
    "    \n",
    "# Alias for nn_Layer_Dropout\n",
    "Dropout = nn_Layer_Dropout\n",
    "\n",
    "# Implementation of Flatten Layer (Make Tensor Flatten)\n",
    "class nn_Layer_Flatten(nn_Module):\n",
    "    \"\"\"\n",
    "    Flatten Layer Implementation\n",
    "    \n",
    "    Flatten Layer just turn the input Tensor into a flatten 2D tensor (batch_size, features),\n",
    "    very suitable for transforming high dimensional data into low dimension ones and then apply\n",
    "    Dense and Dropout layers. It does not have any learnable parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    __attr__ = \"MML.nn_Layer_Flatten\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 *,\n",
    "                 module_name: str = \"nn_Layer_Flatten\", \n",
    "                 backend: Literal[\"torch\", \"numpy\"] = \"torch\",\n",
    "                 dtype: type | str = None,\n",
    "                 device: str | None = None,\n",
    "                 autograd: bool = False,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        A training active dropout layer.\n",
    "        \n",
    "        This class implements a dropout neural network layer, which performs randomly dropout when training and \n",
    "        do nothing in evaluation process. Dropping out is controlled by a dropout rate which is typically ranging from\n",
    "        0 to 1 and common values are [0.1, 0.4].\n",
    "\n",
    "        Parameters:\n",
    "            module_name: str, The name of the module instance. Defaults to \"nn_Layer_Dense\".\n",
    "            backend: Literal[\"torch\", \"numpy\"], The computational backend to use. Defaults to \"torch\".\n",
    "            dtype: type, The data type for the tensor values. Defaults to None (auto detection). \n",
    "                    For PyTorch, this corresponds to torch.dtype; for NumPy, it corresponds to np.dtype.\n",
    "            device: str | None, The target device (e.g., \"cpu\", \"cuda\") where the layer's parameters will be placed. \n",
    "                    If None, uses the default device. Defaults to None (auto detection).\n",
    "            autograd: bool, A flag indicating whether to use PyTorch's autograd for gradient computation. \n",
    "                    If True, manual gradient tracking is disabled. Defaults to False.\n",
    "\n",
    "        Attributes:\n",
    "            self.dtype: type, The data type of the tensor values.\n",
    "            self.device: str | None, The device where the parameters are placed.\n",
    "            self.autograd: bool, Whether PyTorch's autograd is enabled for this layer.\n",
    "            self._parameters.weight: nn_Parameter, The weight parameter matrix (shape: [in_features, out_features]).\n",
    "            self._parameters.bias: nn_Parameter | optional, The optional bias vector (shape: [out_features]) if has_bias is True.\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__(module_name = module_name, backend = backend, dtype = dtype, device = device, autograd = autograd)\n",
    "        \n",
    "        # Record an empty tuple showing the original shape of the input\n",
    "        self.__setattr__(\"original_shape\", ())\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Reshape the input tensor into a 2D format (batch_size, features).\n",
    "        If the input is high dimensional, except the 1st dimension, which is batch_size,\n",
    "        any other dimensions will be flatten into a flatten tensor.\n",
    "        The output of this Layer will always be a 2D Tensor.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input tensor with arbitrary dimensions. The first dimension represents the batch size.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output tensor reshaped into 2D format (batch_size, total_features).\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the input `x` is not a valid MML.Tensor object.\n",
    "        \"\"\"\n",
    "        # Type check, x must be an instance of Tensor\n",
    "        if isinstance(x, Tensor) == False:\n",
    "            raise ValueError(f\"In performing forward(), input `x` must be in a MML `Tensor` format but you have {type(x)}\")\n",
    "        \n",
    "        # Save the original input shape for backward pass\n",
    "        self.original_shape = x.shape\n",
    "\n",
    "        # Flatten all dimensions except the batch dimension (dim 0)\n",
    "        out = x.reshape([x.shape[0], -1])\n",
    "        # Reshape to (batch_size, total_features)\n",
    "        return out\n",
    "\n",
    "    def backward(self, grad_output: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Backward pass: compute gradients for weight, bias, and input.\n",
    "\n",
    "        This method performs the gradient computation for a dense layer during backpropagation. \n",
    "        It calculates the gradients of the loss with respect to the weights, biases, and input tensor,\n",
    "        based on the provided `grad_output` (gradient from the next layer). The implementation\n",
    "        supports both PyTorch autograd and manual gradient calculation modes.\n",
    "\n",
    "        Args:\n",
    "            grad_output (Tensor): Gradient tensor resulting from the output of the layer, used as input for backpropagation.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Gradient with respect to the input tensor, for recursive backward calculations in previous layers.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If `grad_output` is not a valid MML.Tensor object..\n",
    "        \"\"\"\n",
    "        # If use autograd, pass; if manual mode, then calculate\n",
    "        if self.autograd == True:\n",
    "            return None\n",
    "        \n",
    "        # Type check, grad_output must be an instance of Tensor\n",
    "        if isinstance(grad_output, Tensor) == False:\n",
    "            raise ValueError(f\"In performing backward(), `grad_output` must be in a MML `Tensor` format but you have {type(grad_output)}\")\n",
    "\n",
    "        # Reshape gradient back to original shape\n",
    "        return grad_output.reshape(self.original_shape)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"nn_Layer_Flatten(with original shape {self.original_shape}).\"\n",
    "\n",
    "\n",
    "# Alias for nn_Layer_Flatten\n",
    "Flatten = nn_Layer_Flatten\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These Activation Functions are self implemented and open-sourced\n",
    "# Available at https://github.com/dof-studio/MML/\n",
    "# By Nathmath Huang (bh2821)\n",
    "# License: Apache License Version 2.0\n",
    "\n",
    "\n",
    "# Implementation of ReLU Activation\n",
    "class nn_Activation_ReLU(nn_Module):\n",
    "    \"\"\"\n",
    "    ReLU activation function.\n",
    "    \n",
    "    The Rectified Linear Unit (ReLU) is a widely used activation function \n",
    "    defined by the formula: f(x) = \\max(0, x). This function outputs the \n",
    "    input value if it is positive, and zero otherwise. ReLU is celebrated for its \n",
    "    computational efficiency and ability to mitigate vanishing gradient problems \n",
    "    during backpropagation, making it a cornerstone in modern deep learning architectures.\n",
    "    \n",
    "    Formula: f(x) = max(0, x)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    __attr__ = \"MML.nn_Activation_ReLU\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 *,\n",
    "                 module_name: str = \"nn_Activation_ReLU\", \n",
    "                 backend: Literal[\"torch\", \"numpy\"] = \"torch\",\n",
    "                 dtype: type | str = None,\n",
    "                 device: str | None = None,\n",
    "                 autograd: bool = False,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        An ReLU activation function.\n",
    "\n",
    "        Parameters:\n",
    "            module_name: str, The name of the module instance.\n",
    "            backend: Literal[\"torch\", \"numpy\"], The computational backend to use. Defaults to \"torch\".\n",
    "            dtype: type, The data type for the tensor values. Defaults to None (auto detection). \n",
    "                    For PyTorch, this corresponds to torch.dtype; for NumPy, it corresponds to np.dtype.\n",
    "            device: str | None, The target device (e.g., \"cpu\", \"cuda\") where the layer's parameters will be placed. \n",
    "                    If None, uses the default device. Defaults to None (auto detection).\n",
    "            autograd: bool, A flag indicating whether to use PyTorch's autograd for gradient computation. \n",
    "                    If True, manual gradient tracking is disabled. Defaults to False.\n",
    "\n",
    "        Attributes:\n",
    "            self.dtype: type, The data type of the tensor values.\n",
    "            self.device: str | None, The device where the parameters are placed.\n",
    "            self.autograd: bool, Whether PyTorch's autograd is enabled for this layer.\n",
    "            self._parameters.weight: nn_Parameter, The weight parameter matrix (shape: [in_features, out_features]).\n",
    "            self._parameters.bias: nn_Parameter | optional, The optional bias vector (shape: [out_features]) if has_bias is True.\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__(module_name = module_name, backend = backend, dtype = dtype, device = device, autograd = autograd)\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Apply the Rectified Linear Unit (ReLU) activation function to the input tensor.\n",
    "\n",
    "        This method computes the element-wise ReLU activation, which outputs the input if it is positive,\n",
    "        and zero otherwise. It is a fundamental non-linearity in neural networks, enabling the model\n",
    "        to learn complex patterns by introducing non-linearities. The input is saved for backward propagation\n",
    "        to compute gradients during training.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of any shape. The ReLU operation is applied element-wise.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output tensor after applying the ReLU activation, with the same shape as the input.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the input `x` is not a valid MML.Tensor object.\n",
    "        \"\"\"\n",
    "                \n",
    "        # Type check, x must be an instance of Tensor\n",
    "        if isinstance(x, Tensor) == False:\n",
    "            raise ValueError(f\"In performing forward(), input `x` must be in a MML `Tensor` format but you have {type(x)}\")\n",
    "        \n",
    "        # Save input for backward\n",
    "        self.__setattr__(\"input\", x)\n",
    "\n",
    "        # Apply ReLU to the input data\n",
    "        return Tensor.where_as(x.data > 0, x.data, 0, backend=self.backend, dtype=self.dtype, device=self.device)\n",
    "\n",
    "    def backward(self, grad_output: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Backward pass: compute gradients for weight, bias, and input.\n",
    "\n",
    "        This method performs the gradient computation for a module during backpropagation. \n",
    "        It calculates the gradients of the loss with respect to the weights, biases, and input tensor,\n",
    "        based on the provided `grad_output` (gradient from the next layer). The implementation\n",
    "        supports both PyTorch autograd and manual gradient calculation modes.\n",
    "\n",
    "        Args:\n",
    "            grad_output (Tensor): Gradient tensor resulting from the output of the layer, used as input for backpropagation.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Gradient with respect to the input tensor, for recursive backward calculations in previous layers.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If `grad_output` is not a valid MML.Tensor object..\n",
    "        \"\"\"\n",
    "        \n",
    "        # If use autograd, pass; if manual mode, then calculate\n",
    "        if self.autograd == True:\n",
    "            return None\n",
    "        \n",
    "        # Type check, grad_output must be an instance of Tensor\n",
    "        if isinstance(grad_output, Tensor) == False:\n",
    "            raise ValueError(f\"In performing backward(), `grad_output` must be in a MML `Tensor` format but you have {type(grad_output)}\")\n",
    "        \n",
    "        # Pass gradient only where input was positive\n",
    "        grad_input = Tensor.where_as(self.input.data <= 0, 0, grad_output.data, backend=self.backend, dtype=self.dtype, device=self.device)\n",
    "        return grad_input\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"nn_Activation_ReLU(ReLU Activation Function).\"\n",
    "    \n",
    "\n",
    "# Alias for nn_Activation_ReLU\n",
    "ReLU = nn_Activation_ReLU\n",
    "\n",
    "\n",
    "# Implementation of Leaky ReLU Activation\n",
    "class nn_Activation_LeakyReLU(nn_Module):\n",
    "    \"\"\"\n",
    "    Leaky ReLU activation with a small slope for negative inputs.\n",
    "    \n",
    "    The Leaky Rectified Linear Unit (Leaky ReLU) is a variant of the ReLU \n",
    "    activation function that allows a small, non-zero gradient when the input \n",
    "    is negative. This helps mitigate the \"dying ReLU\" problem where neurons \n",
    "    become inactive and cease to learn. The function is defined as:\n",
    "    \n",
    "    Formula: f(x) = max(0, x, α*x), where α is a small positive slope (typically 0.01).\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    __attr__ = \"MML.nn_Activation_LeakyReLU\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 leaky_slope: float = 0.01,\n",
    "                 *,\n",
    "                 module_name: str = \"nn_Activation_ReLU\", \n",
    "                 backend: Literal[\"torch\", \"numpy\"] = \"torch\",\n",
    "                 dtype: type | str = None,\n",
    "                 device: str | None = None,\n",
    "                 autograd: bool = False,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        A Leaky ReLU activation function.\n",
    "\n",
    "        Parameters:\n",
    "            leaky_slope: float, The slope of negative values.\n",
    "            module_name: str, The name of the module instance.\n",
    "            backend: Literal[\"torch\", \"numpy\"], The computational backend to use. Defaults to \"torch\".\n",
    "            dtype: type, The data type for the tensor values. Defaults to None (auto detection). \n",
    "                    For PyTorch, this corresponds to torch.dtype; for NumPy, it corresponds to np.dtype.\n",
    "            device: str | None, The target device (e.g., \"cpu\", \"cuda\") where the layer's parameters will be placed. \n",
    "                    If None, uses the default device. Defaults to None (auto detection).\n",
    "            autograd: bool, A flag indicating whether to use PyTorch's autograd for gradient computation. \n",
    "                    If True, manual gradient tracking is disabled. Defaults to False.\n",
    "\n",
    "        Attributes:\n",
    "            self.leaky_slope: float, The slope applied to negative values.\n",
    "            self.dtype: type, The data type of the tensor values.\n",
    "            self.device: str | None, The device where the parameters are placed.\n",
    "            self.autograd: bool, Whether PyTorch's autograd is enabled for this layer.\n",
    "            self._parameters.weight: nn_Parameter, The weight parameter matrix (shape: [in_features, out_features]).\n",
    "            self._parameters.bias: nn_Parameter | optional, The optional bias vector (shape: [out_features]) if has_bias is True.\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__(module_name = module_name, backend = backend, dtype = dtype, device = device, autograd = autograd)\n",
    "    \n",
    "        # Record the leaky slope as a non-Parameter attribute\n",
    "        self.__setattr__(\"leaky_slope\", leaky_slope)\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Apply the Leaky Rectified Linear Unit (Leaky ReLU) activation function to the input tensor.\n",
    "\n",
    "        This method computes the element-wise Leaky ReLU activation, which outputs the input if it is positive,\n",
    "        and a small negative slope multiplied by the input otherwise. This variant of ReLU mitigates the \"dying ReLU\"\n",
    "        problem by allowing a controlled negative slope, improving gradient flow for negative inputs. The input is saved\n",
    "        for backward propagation to compute gradients during training.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of any shape. The Leaky ReLU operation is applied element-wise.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output tensor after applying the Leaky ReLU activation, with the same shape as the input.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the input `x` is not a valid MML.Tensor object.\n",
    "        \"\"\"\n",
    "                \n",
    "        # Type check, x must be an instance of Tensor\n",
    "        if isinstance(x, Tensor) == False:\n",
    "            raise ValueError(f\"In performing forward(), input `x` must be in a MML `Tensor` format but you have {type(x)}\")\n",
    "        \n",
    "        # Save input for backward\n",
    "        self.__setattr__(\"input\", x)\n",
    "\n",
    "        # Apply Leaky ReLU to the input data\n",
    "        return Tensor.where_as(x.data > 0, x.data, x.data * self.leaky_slope, backend=self.backend, dtype=self.dtype, device=self.device)\n",
    "\n",
    "    def backward(self, grad_output: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Backward pass: compute gradients for weight, bias, and input.\n",
    "\n",
    "        This method performs the gradient computation for a module during backpropagation. \n",
    "        It calculates the gradients of the loss with respect to the weights, biases, and input tensor,\n",
    "        based on the provided `grad_output` (gradient from the next layer). The implementation\n",
    "        supports both PyTorch autograd and manual gradient calculation modes.\n",
    "\n",
    "        Args:\n",
    "            grad_output (Tensor): Gradient tensor resulting from the output of the layer, used as input for backpropagation.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Gradient with respect to the input tensor, for recursive backward calculations in previous layers.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If `grad_output` is not a valid MML.Tensor object..\n",
    "        \"\"\"\n",
    "        \n",
    "        # If use autograd, pass; if manual mode, then calculate\n",
    "        if self.autograd == True:\n",
    "            return None\n",
    "        \n",
    "        # Type check, grad_output must be an instance of Tensor\n",
    "        if isinstance(grad_output, Tensor) == False:\n",
    "            raise ValueError(f\"In performing backward(), `grad_output` must be in a MML `Tensor` format but you have {type(grad_output)}\")\n",
    "        \n",
    "        # Pass gradient only where input was positive\n",
    "        grad_input = Tensor.where_as(self.input.data <= 0, grad_output.data * self.leaky_slope, grad_output.data, backend=self.backend, dtype=self.dtype, device=self.device)\n",
    "        return grad_input\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"nn_Activation_LeakyReLU(Leaky ReLU Activation Function with alpha = {self.leaky_slope}).\"\n",
    "    \n",
    "      \n",
    "# Alias for nn_Activation_LeakyReLU\n",
    "LeakyReLU = nn_Activation_LeakyReLU \n",
    "\n",
    "\n",
    "# Implementation of Sigmoid Activation\n",
    "class nn_Activation_Sigmoid(nn_Module):\n",
    "    \"\"\"\n",
    "    Sigmoid activation function.\n",
    "    \n",
    "    The Sigmoid function maps input values to a range between 0 and 1, \n",
    "    making it suitable for binary classification tasks. It is defined by the \n",
    "    formula: f(x) = 1 / (1 + e^(-x)). However, it suffers from vanishing gradient \n",
    "    issues in deep networks due to its saturation regions near ±1.\n",
    "    \n",
    "    Formula: f(x) = \\frac{1}{1 + e^{-x}}\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    __attr__ = \"MML.nn_Activation_Sigmoid\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 *,\n",
    "                 module_name: str = \"nn_Activation_Sigmoid\", \n",
    "                 backend: Literal[\"torch\", \"numpy\"] = \"torch\",\n",
    "                 dtype: type | str = None,\n",
    "                 device: str | None = None,\n",
    "                 autograd: bool = False,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        An Sigmoid activation function.\n",
    "\n",
    "        Parameters:\n",
    "            module_name: str, The name of the module instance.\n",
    "            backend: Literal[\"torch\", \"numpy\"], The computational backend to use. Defaults to \"torch\".\n",
    "            dtype: type, The data type for the tensor values. Defaults to None (auto detection). \n",
    "                    For PyTorch, this corresponds to torch.dtype; for NumPy, it corresponds to np.dtype.\n",
    "            device: str | None, The target device (e.g., \"cpu\", \"cuda\") where the layer's parameters will be placed. \n",
    "                    If None, uses the default device. Defaults to None (auto detection).\n",
    "            autograd: bool, A flag indicating whether to use PyTorch's autograd for gradient computation. \n",
    "                    If True, manual gradient tracking is disabled. Defaults to False.\n",
    "\n",
    "        Attributes:\n",
    "            self.dtype: type, The data type of the tensor values.\n",
    "            self.device: str | None, The device where the parameters are placed.\n",
    "            self.autograd: bool, Whether PyTorch's autograd is enabled for this layer.\n",
    "            self._parameters.weight: nn_Parameter, The weight parameter matrix (shape: [in_features, out_features]).\n",
    "            self._parameters.bias: nn_Parameter | optional, The optional bias vector (shape: [out_features]) if has_bias is True.\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__(module_name = module_name, backend = backend, dtype = dtype, device = device, autograd = autograd)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Apply the Sigmoid activation function to the input tensor.\n",
    "\n",
    "        This method computes the element-wise Sigmoid activation, which maps input values\n",
    "        to the range (0, 1). The Sigmoid function is widely used in neural networks for\n",
    "        binary classification tasks due to its smooth, differentiable nature. The output\n",
    "        is saved for use during backward propagation to compute gradients.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of any shape. The Sigmoid operation is applied element-wise.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output tensor after applying the Sigmoid activation, with the same shape as the input.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the input `x` is not a valid MML.Tensor object.\n",
    "\n",
    "        \"\"\"\n",
    "                \n",
    "        # Type check, x must be an instance of Tensor\n",
    "        if isinstance(x, Tensor) == False:\n",
    "            raise ValueError(f\"In performing forward(), input `x` must be in a MML `Tensor` format but you have {type(x)}\")\n",
    "        \n",
    "        # Perform a sigmoid function on the input\n",
    "        output = x.sigmoid()\n",
    "        \n",
    "        # Save output for backward\n",
    "        self.__setattr__(\"output\", output)\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def backward(self, grad_output: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Backward pass: compute gradients for weight, bias, and input.\n",
    "\n",
    "        This method performs the gradient computation for a module during backpropagation. \n",
    "        It calculates the gradients of the loss with respect to the weights, biases, and input tensor,\n",
    "        based on the provided `grad_output` (gradient from the next layer). The implementation\n",
    "        supports both PyTorch autograd and manual gradient calculation modes.\n",
    "\n",
    "        Args:\n",
    "            grad_output (Tensor): Gradient tensor resulting from the output of the layer, used as input for backpropagation.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Gradient with respect to the input tensor, for recursive backward calculations in previous layers.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If `grad_output` is not a valid MML.Tensor object..\n",
    "        \"\"\"\n",
    "        \n",
    "        # If use autograd, pass; if manual mode, then calculate\n",
    "        if self.autograd == True:\n",
    "            return None\n",
    "        \n",
    "        # Type check, grad_output must be an instance of Tensor\n",
    "        if isinstance(grad_output, Tensor) == False:\n",
    "            raise ValueError(f\"In performing backward(), `grad_output` must be in a MML `Tensor` format but you have {type(grad_output)}\")\n",
    "        \n",
    "        # grad = grad_output * sigmoid(x) * (1 - sigmoid(x))\n",
    "        grad_input = grad_output * self.output * (1 - self.output)\n",
    "        return grad_input\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"nn_Activation_Sigmoid(Sigmoid Activation Function).\"\n",
    "    \n",
    "    \n",
    "# Alias for nn_Activation_Sigmoid\n",
    "Sigmoid = nn_Activation_Sigmoid\n",
    "\n",
    "\n",
    "# Implementation of Tanh Activation\n",
    "class nn_Activation_Tanh(nn_Module):\n",
    "    \"\"\"\n",
    "    Tanh activation function.\n",
    "    \n",
    "    The hyperbolic tangent (tanh) function maps input values to a range between -1 and 1,\n",
    "    making it suitable for scenarios requiring symmetric output distribution. It is defined as:\n",
    "    \n",
    "    Formula: f(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}} = \\tanh(x)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    __attr__ = \"MML.nn_Activation_Tanh\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 *,\n",
    "                 module_name: str = \"nn_Activation_Sigmoid\", \n",
    "                 backend: Literal[\"torch\", \"numpy\"] = \"torch\",\n",
    "                 dtype: type | str = None,\n",
    "                 device: str | None = None,\n",
    "                 autograd: bool = False,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        An Sigmoid activation function.\n",
    "\n",
    "        Parameters:\n",
    "            module_name: str, The name of the module instance.\n",
    "            backend: Literal[\"torch\", \"numpy\"], The computational backend to use. Defaults to \"torch\".\n",
    "            dtype: type, The data type for the tensor values. Defaults to None (auto detection). \n",
    "                    For PyTorch, this corresponds to torch.dtype; for NumPy, it corresponds to np.dtype.\n",
    "            device: str | None, The target device (e.g., \"cpu\", \"cuda\") where the layer's parameters will be placed. \n",
    "                    If None, uses the default device. Defaults to None (auto detection).\n",
    "            autograd: bool, A flag indicating whether to use PyTorch's autograd for gradient computation. \n",
    "                    If True, manual gradient tracking is disabled. Defaults to False.\n",
    "\n",
    "        Attributes:\n",
    "            self.dtype: type, The data type of the tensor values.\n",
    "            self.device: str | None, The device where the parameters are placed.\n",
    "            self.autograd: bool, Whether PyTorch's autograd is enabled for this layer.\n",
    "            self._parameters.weight: nn_Parameter, The weight parameter matrix (shape: [in_features, out_features]).\n",
    "            self._parameters.bias: nn_Parameter | optional, The optional bias vector (shape: [out_features]) if has_bias is True.\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__(module_name = module_name, backend = backend, dtype = dtype, device = device, autograd = autograd)\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Apply the Tangent Hyperbolic activation function to the input tensor.\n",
    "\n",
    "        This method computes the element-wise hyperbolic tangent (tanh) activation,\n",
    "        which maps input values to the range (-1, 1). The tanh function is smooth and\n",
    "        differentiable everywhere, making it suitable for neural network layers that\n",
    "        require non-linear transformations. The output is saved for use in backward\n",
    "        propagation to compute gradients during training.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of any shape. The tanh operation is applied element-wise.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output tensor after applying the tanh activation, with the same shape as the input.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the input `x` is not a valid MML.Tensor object.\n",
    "        \"\"\"\n",
    "                \n",
    "        # Type check, x must be an instance of Tensor\n",
    "        if isinstance(x, Tensor) == False:\n",
    "            raise ValueError(f\"In performing forward(), input `x` must be in a MML `Tensor` format but you have {type(x)}\")\n",
    "        \n",
    "        # Perform a tanh function on the input\n",
    "        output = x.tanh()\n",
    "        \n",
    "        # Save output for backward\n",
    "        self.__setattr__(\"output\", output)\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def backward(self, grad_output: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Backward pass: compute gradients for weight, bias, and input.\n",
    "\n",
    "        This method performs the gradient computation for a module during backpropagation. \n",
    "        It calculates the gradients of the loss with respect to the weights, biases, and input tensor,\n",
    "        based on the provided `grad_output` (gradient from the next layer). The implementation\n",
    "        supports both PyTorch autograd and manual gradient calculation modes.\n",
    "\n",
    "        Args:\n",
    "            grad_output (Tensor): Gradient tensor resulting from the output of the layer, used as input for backpropagation.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Gradient with respect to the input tensor, for recursive backward calculations in previous layers.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If `grad_output` is not a valid MML.Tensor object..\n",
    "        \"\"\"\n",
    "        \n",
    "        # If use autograd, pass; if manual mode, then calculate\n",
    "        if self.autograd == True:\n",
    "            return None\n",
    "        \n",
    "        # Type check, grad_output must be an instance of Tensor\n",
    "        if isinstance(grad_output, Tensor) == False:\n",
    "            raise ValueError(f\"In performing backward(), `grad_output` must be in a MML `Tensor` format but you have {type(grad_output)}\")\n",
    "        \n",
    "        # grad = grad_output * (1 - tanh(x)^2)\n",
    "        grad_input = grad_output * (1 - self.output ** 2)\n",
    "        return grad_input\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"nn_Activation_Tanh(Tanh Activation Function).\"\n",
    "   \n",
    "    \n",
    "# Alias for nn_Activation_Tanh\n",
    "Tanh = nn_Activation_Tanh\n",
    "\n",
    "   \n",
    "# Implementation of Softmax Activation\n",
    "class nn_Activation_Softmax(nn_Module):\n",
    "    \"\"\"\n",
    "    Softmax activation function.\n",
    "    \n",
    "    The Softmax function converts raw scores (logits) into probabilities \n",
    "    that sum to 1, making it suitable for multi-class classification tasks. \n",
    "    It generalizes the sigmoid function to multiple classes by applying the \n",
    "    formula: f(x_i) = exp(x_i) / sum_j(exp(x_j)), where x_i is the input score \n",
    "    for class i. This ensures the output represents a probability distribution \n",
    "    over the classes.\n",
    "    \n",
    "    Formula: f(x_i) = \\frac{e^{x_i}}{\\sum_{j} e^{x_j}}\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    __attr__ = \"MML.nn_Activation_Softmax\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 dim: int = 1,\n",
    "                 *,\n",
    "                 module_name: str = \"nn_Activation_Sigmoid\", \n",
    "                 backend: Literal[\"torch\", \"numpy\"] = \"torch\",\n",
    "                 dtype: type | str = None,\n",
    "                 device: str | None = None,\n",
    "                 autograd: bool = False,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        An Sigmoid activation function.\n",
    "\n",
    "        Parameters:\n",
    "            dim: int, The dimension to apply softmax on. Defaults to 1.\n",
    "            module_name: str, The name of the module instance.\n",
    "            backend: Literal[\"torch\", \"numpy\"], The computational backend to use. Defaults to \"torch\".\n",
    "            dtype: type, The data type for the tensor values. Defaults to None (auto detection). \n",
    "                    For PyTorch, this corresponds to torch.dtype; for NumPy, it corresponds to np.dtype.\n",
    "            device: str | None, The target device (e.g., \"cpu\", \"cuda\") where the layer's parameters will be placed. \n",
    "                    If None, uses the default device. Defaults to None (auto detection).\n",
    "            autograd: bool, A flag indicating whether to use PyTorch's autograd for gradient computation. \n",
    "                    If True, manual gradient tracking is disabled. Defaults to False.\n",
    "\n",
    "        Attributes:\n",
    "            self.softmax_dim: int, The dimension to apply softmax on.\n",
    "            self.dtype: type, The data type of the tensor values.\n",
    "            self.device: str | None, The device where the parameters are placed.\n",
    "            self.autograd: bool, Whether PyTorch's autograd is enabled for this layer.\n",
    "            self._parameters.weight: nn_Parameter, The weight parameter matrix (shape: [in_features, out_features]).\n",
    "            self._parameters.bias: nn_Parameter | optional, The optional bias vector (shape: [out_features]) if has_bias is True.\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__(module_name = module_name, backend = backend, dtype = dtype, device = device, autograd = autograd)\n",
    "    \n",
    "        # Record the softmax dimension as a non-Parameter attribute\n",
    "        self.__setattr__(\"softmax_dim\", dim)\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Apply the Softmax activation function to the input tensor.\n",
    "\n",
    "        This method applies the softmax function along the specified axis (softmax_dim)\n",
    "        to convert logits into probabilities. The output is a Tensor with the same shape\n",
    "        as the input, but with values normalized to the range [0, 1] along the specified axis.\n",
    "        This operation is commonly used in classification tasks to produce probability distributions.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input tensor containing logits (unnormalized log probabilities).\n",
    "            softmax_dim (int): The axis along which to apply the softmax function. \n",
    "                              For example, for a batch of images, this could be the channel dimension.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output tensor with probabilities computed via softmax along the specified axis.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the input `x` is not a valid MML.Tensor object.\n",
    "\n",
    "        \"\"\"\n",
    "                \n",
    "        # Type check, x must be an instance of Tensor\n",
    "        if isinstance(x, Tensor) == False:\n",
    "            raise ValueError(f\"In performing forward(), input `x` must be in a MML `Tensor` format but you have {type(x)}\")\n",
    "        \n",
    "        # Perform a softmax function on the input\n",
    "        output = x.softmax(axis = self.softmax_dim, keepdims = True)\n",
    "        \n",
    "        # Save output for backward\n",
    "        self.__setattr__(\"output\", output)\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def backward(self, grad_output: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Backward pass: compute gradients for weight, bias, and input.\n",
    "\n",
    "        This method performs the gradient computation for a module during backpropagation. \n",
    "        It calculates the gradients of the loss with respect to the weights, biases, and input tensor,\n",
    "        based on the provided `grad_output` (gradient from the next layer). The implementation\n",
    "        supports both PyTorch autograd and manual gradient calculation modes.\n",
    "\n",
    "        Args:\n",
    "            grad_output (Tensor): Gradient tensor resulting from the output of the layer, used as input for backpropagation.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Gradient with respect to the input tensor, for recursive backward calculations in previous layers.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If `grad_output` is not a valid MML.Tensor object..\n",
    "        \"\"\"\n",
    "        \n",
    "        # If use autograd, pass; if manual mode, then calculate\n",
    "        if self.autograd == True:\n",
    "            return None\n",
    "        \n",
    "        # Type check, grad_output must be an instance of Tensor\n",
    "        if isinstance(grad_output, Tensor) == False:\n",
    "            raise ValueError(f\"In performing backward(), `grad_output` must be in a MML `Tensor` format but you have {type(grad_output)}\")\n",
    "        \n",
    "        # Compute gradient w.r.t input using Jacobian: grad_input = y * (grad_out - (grad_out * y).sum_along_dim)\n",
    "        grad_sum = (grad_output * self.output).sum(axis=self.softmax_dim, keepdims=True) \n",
    "        grad_input = self.output * (grad_output - grad_sum)\n",
    "        return grad_input\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"nn_Activation_Softmax(Softmax Activation Function).\"\n",
    "   \n",
    "    \n",
    "# Alias for nn_Activation_Softmax\n",
    "Softmax = nn_Activation_Softmax\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These Loss Functions are self implemented and open-sourced\n",
    "# Available at https://github.com/dof-studio/MML/\n",
    "# By Nathmath Huang (bh2821)\n",
    "# License: Apache License Version 2.0\n",
    "\n",
    "# Implementation of Base Lose Class\n",
    "class nn_Loss_BaseLoss(nn_Module):\n",
    "\n",
    "    __attr__ = \"MML.nn_Loss_BaseLoss\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 *,\n",
    "                 module_name: str = \"nn_Loss_Base\",\n",
    "                 backend: Literal[\"torch\", \"numpy\"] = \"torch\",\n",
    "                 dtype: type | str = None,\n",
    "                 device: str | None = None,\n",
    "                 autograd: bool = False,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        An abstract Loss Implemetation.\n",
    "\n",
    "        Parameters:\n",
    "            module_name: str, The name of the module instance.\n",
    "            backend: Literal[\"torch\", \"numpy\"], The computational backend to use. Defaults to \"torch\".\n",
    "            dtype: type, The data type for the tensor values. Defaults to None (auto detection). \n",
    "                    For PyTorch, this corresponds to torch.dtype; for NumPy, it corresponds to np.dtype.\n",
    "            device: str | None, The target device (e.g., \"cpu\", \"cuda\") where the layer's parameters will be placed. \n",
    "                    If None, uses the default device. Defaults to None (auto detection).\n",
    "            autograd: bool, A flag indicating whether to use PyTorch's autograd for gradient computation. \n",
    "                    If True, manual gradient tracking is disabled. Defaults to False.\n",
    "\n",
    "        Attributes:\n",
    "            self.dtype: type, The data type of the tensor values.\n",
    "            self.device: str | None, The device where the parameters are placed.\n",
    "            self.autograd: bool, Whether PyTorch's autograd is enabled for this layer.\n",
    "            self._parameters.weight: nn_Parameter, The weight parameter matrix (shape: [in_features, out_features]).\n",
    "            self._parameters.bias: nn_Parameter | optional, The optional bias vector (shape: [out_features]) if has_bias is True.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__(module_name=module_name, backend=backend, dtype=dtype, device=device, autograd=autograd)\n",
    "\n",
    "    def forward(self, pred: Tensor, target: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass, to calculate the loss.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"forward() is not implemented in the base loss class\")\n",
    "        \n",
    "    def backward(self, grad_output: Tensor | None = None) -> Tensor | None:\n",
    "        \"\"\"\n",
    "        Backward pass, to calculate the chained gradient with respect to parameters and return \n",
    "        the gradients with respect to inputs.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"backward() is not implemented in the base loss class\")\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return \"nn_Loss_BaseLoss(Abstract Loss Class).\"   \n",
    "\n",
    "\n",
    "# Implementation of Mean Square Error Loss\n",
    "class nn_Loss_MSE(nn_Loss_BaseLoss):\n",
    "    \"\"\"\n",
    "    Mean Squared Error Loss.\n",
    "\n",
    "    The Mean Squared Error (MSE) loss function quantifies the average squared difference \n",
    "    between predicted values and true values. It is widely used in regression tasks \n",
    "    and is defined as: \n",
    "\n",
    "    Formula: L = \\frac{1}{n} \\sum_{i=1}^{n} (y_{true,i} - y_{pred,i})^2\n",
    "\n",
    "    Where:\n",
    "        - $ n $ is the number of samples\n",
    "        - $ y_{true,i} $ is the true value for sample i\n",
    "        - $ y_{pred,i} $ is the predicted value for sample i\n",
    "\n",
    "    MSE penalizes larger errors more heavily due to squaring, making it sensitive \n",
    "    to outliers. It is differentiable and computationally efficient, but not suitable \n",
    "    for classification tasks where probabilistic outputs are required.\n",
    "\n",
    "    Formula: L = \\frac{1}{n} \\sum_{i=1}^{n} (y_{true,i} - y_{pred,i})^2\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    __attr__ = \"MML.nn_Loss_MSE\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 *,\n",
    "                 module_name: str = \"nn_Loss_MSE\",\n",
    "                 backend: Literal[\"torch\", \"numpy\"] = \"torch\",\n",
    "                 dtype: type | str = None,\n",
    "                 device: str | None = None,\n",
    "                 autograd: bool = False,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        A Mean Squared Error Loss Function.\n",
    "\n",
    "        Parameters:\n",
    "            module_name: str, The name of the module instance.\n",
    "            backend: Literal[\"torch\", \"numpy\"], The computational backend to use. Defaults to \"torch\".\n",
    "            dtype: type, The data type for the tensor values. Defaults to None (auto detection). \n",
    "                    For PyTorch, this corresponds to torch.dtype; for NumPy, it corresponds to np.dtype.\n",
    "            device: str | None, The target device (e.g., \"cpu\", \"cuda\") where the layer's parameters will be placed. \n",
    "                    If None, uses the default device. Defaults to None (auto detection).\n",
    "            autograd: bool, A flag indicating whether to use PyTorch's autograd for gradient computation. \n",
    "                    If True, manual gradient tracking is disabled. Defaults to False.\n",
    "\n",
    "        Attributes:\n",
    "            self.dtype: type, The data type of the tensor values.\n",
    "            self.device: str | None, The device where the parameters are placed.\n",
    "            self.autograd: bool, Whether PyTorch's autograd is enabled for this layer.\n",
    "            self._parameters.weight: nn_Parameter, The weight parameter matrix (shape: [in_features, out_features]).\n",
    "            self._parameters.bias: nn_Parameter | optional, The optional bias vector (shape: [out_features]) if has_bias is True.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__(module_name=module_name, backend=backend, dtype=dtype, device=device, autograd=autograd)\n",
    "\n",
    "    def forward(self, pred: Tensor, target: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Apply the Mean Squared Error (MSE) Loss function to evaluate the values predicted by the network.\n",
    "\n",
    "        This method computes the average squared difference between predicted values (`pred`) and actual values (`target`), \n",
    "        which is a common loss function for regression tasks. The output is a scalar value representing the loss, \n",
    "        averaged over all elements in the input tensors.\n",
    "\n",
    "        Args:\n",
    "            pred (Tensor): Predicted tensor containing model outputs.\n",
    "            target (Tensor): Target tensor containing ground truth values.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Scalar tensor representing the computed MSE loss.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If `pred` or `target` is not a valid MML.Tensor object, \n",
    "                        or if `pred` and `target` do not have the same shape.\n",
    "\n",
    "        Attributes:\n",
    "            self.numel (scalar): Total number of elements in the input tensors (product of tensor shapes).\n",
    "            self.mse (Tensor): Saved MSE Computed fpr backward computation.\n",
    "            self.pred (Tensor): Saved predicted tensor for backward computation.\n",
    "            self.target (Tensor): Saved target tensor for backward computation.\n",
    "            self.loss ([Tensor]): Save the computed loss for backpropagation uses.\n",
    "        \"\"\"\n",
    "\n",
    "        # Type check, pred and target must be an instance of Tensor\n",
    "        if isinstance(pred, Tensor) == False or isinstance(target, Tensor) == False:\n",
    "            raise ValueError(f\"In performing forward(), input `pred` or `target` must be in a MML `Tensor` format but you have {type(pred)} and {type(target)}\")\n",
    "\n",
    "        # Shape check, pred and target must have the same shape\n",
    "        if pred.shape != target.shape:\n",
    "            raise ValueError(f\"In performing forward(), input `pred` and `target` must have the same shape, but you have {pred.shape} and {target.shape}\")\n",
    "\n",
    "        # Compute the MSE loss\n",
    "        mse = ((pred - target) ** 2).mean()\n",
    "\n",
    "        # Save the pred, input, mse, and total number of elements for backward\n",
    "        self.__setattr__(\"numel\", np.array(pred.shape).prod())\n",
    "        self.__setattr__(\"mse\", mse)\n",
    "        self.__setattr__(\"pred\", pred)\n",
    "        self.__setattr__(\"target\", target)\n",
    "        self.__setattr__(\"loss\", [mse])\n",
    "\n",
    "        return mse\n",
    "\n",
    "    def backward(self, grad_output: Tensor | None = None) -> Tensor | None:\n",
    "        \"\"\"\n",
    "        Backward pass: compute gradients for weight, bias, and input.\n",
    "\n",
    "        This method performs the gradient computation for a module during backpropagation. \n",
    "        It calculates the gradients of the loss with respect to the weights, biases, and input tensor,\n",
    "        based on the provided `grad_output` (gradient from the next layer). The implementation\n",
    "        supports both PyTorch autograd and manual gradient calculation modes.\n",
    "\n",
    "        Args:\n",
    "            None: Since it is the first in calculating backward.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Gradient with respect to the input tensor, for recursive backward calculations in previous layers.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # If use autograd, pass; if manual mode, then calculate\n",
    "        if self.autograd == True:\n",
    "            self.loss[0].data.backward()\n",
    "            return None\n",
    "\n",
    "        # If grad_output is None (by default), assign it to 1\n",
    "        if grad_output is None:\n",
    "            grad_output = Tensor(1.0, backend=self.backend, dtype=self.dtype, device=self.device)\n",
    "        \n",
    "        # Else, it must be a scalar.\n",
    "        else:\n",
    "            if isinstance(grad_output, Tensor) == False:\n",
    "                raise ValueError(f\"In performing backward(), input `grad_output` must be in a MML `Tensor` format but you have {type(grad_output)}\")\n",
    "            if len(grad_output.shape) != 0:\n",
    "                raise ValueError(\"In performing backward(), input `grad_output` must be in a MML `Tensor` with a scalar stored in\")\n",
    "\n",
    "        # dL/dpred = 2*(pred - target) / N (N = number of elements)\n",
    "        grad_input = 2 * (self.pred - self.target) / self.numel\n",
    "        return grad_input * grad_output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"nn_Loss_MSE(Mean Square Error Loss).\"\n",
    "\n",
    "\n",
    "# Alias for nn_Loss_MSE\n",
    "MSE = nn_Loss_MSE\n",
    "\n",
    "\n",
    "# Implementation of Root Mean Square Error Loss\n",
    "class nn_Loss_RMSE(nn_Loss_BaseLoss):\n",
    "    \"\"\"\n",
    "    Root Mean Squared Error Loss.\n",
    "\n",
    "    The Root Mean Squared Error (RMSE) is the square root of the Mean Squared Error (MSE), \n",
    "    providing a measure of the magnitude of errors in the same units as the target variable. \n",
    "    It is widely used for evaluating regression models and is defined as:\n",
    "\n",
    "    Formula: L = \\sqrt{ \\frac{1}{n} \\sum_{i=1}^{n} (y_{true,i} - y_{pred,i})^2 }\n",
    "\n",
    "    Where:\n",
    "        - $ n $ is the number of samples\n",
    "        - $ y_{true,i} $ is the true value for sample i\n",
    "        - $ y_{pred,i} $ is the predicted value for sample i\n",
    "\n",
    "    RMSE addresses the interpretability limitation of MSE by scaling the error metric to the same units \n",
    "    as the target variable. It retains the sensitivity to outliers from squaring but offers a more intuitive \n",
    "    interpretation compared to MSE. Like MSE, it is differentiable and computationally efficient.\n",
    "\n",
    "    Formula: L = \\sqrt{ \\frac{1}{n} \\sum_{i=1}^{n} (y_{true,i} - y_{pred,i})^2 }\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    __attr__ = \"MML.nn_Loss_RMSE\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 *,\n",
    "                 module_name: str = \"nn_Loss_RMSE\",\n",
    "                 backend: Literal[\"torch\", \"numpy\"] = \"torch\",\n",
    "                 dtype: type | str = None,\n",
    "                 device: str | None = None,\n",
    "                 autograd: bool = False,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        A Root Mean Squared Error Loss Function.\n",
    "\n",
    "        Parameters:\n",
    "            module_name: str, The name of the module instance. Defaults to \"nn_Loss_RMSE\".\n",
    "            backend: Literal[\"torch\", \"numpy\"], The computational backend to use. Defaults to \"torch\".\n",
    "            dtype: type, The data type for the tensor values. Defaults to None (auto detection). \n",
    "                    For PyTorch, this corresponds to torch.dtype; for NumPy, it corresponds to np.dtype.\n",
    "            device: str | None, The target device (e.g., \"cpu\", \"cuda\") where the layer's parameters will be placed. \n",
    "                    If None, uses the default device. Defaults to None (auto detection).\n",
    "            autograd: bool, A flag indicating whether to use PyTorch's autograd for gradient computation. \n",
    "                    If True, manual gradient tracking is disabled. Defaults to False.\n",
    "\n",
    "        Attributes:\n",
    "            self.dtype: type, The data type of the tensor values.\n",
    "            self.device: str | None, The device where the parameters are placed.\n",
    "            self.autograd: bool, Whether PyTorch's autograd is enabled for this layer.\n",
    "            self._parameters.weight: nn_Parameter, The weight parameter matrix (shape: [in_features, out_features]).\n",
    "            self._parameters.bias: nn_Parameter | optional, The optional bias vector (shape: [out_features]) if has_bias is True.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__(module_name=module_name, backend=backend, dtype=dtype, device=device, autograd=autograd)\n",
    "\n",
    "    def forward(self, pred: Tensor, target: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Compute the Root Mean Squared Error (RMSE) between predictions and targets.\n",
    "\n",
    "        This method evaluates the square root of the average squared difference \n",
    "        between predicted values (`pred`) and actual values (`target`), providing \n",
    "        a loss measure in the same units as the original data.\n",
    "\n",
    "        Args:\n",
    "            pred (Tensor): Predicted tensor containing model outputs.\n",
    "            target (Tensor): Target tensor containing ground truth values.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Scalar tensor representing the computed MSE loss.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If `pred` or `target` is not a valid MML.Tensor object, \n",
    "                        or if `pred` and `target` do not have the same shape.\n",
    "\n",
    "        Attributes:\n",
    "            self.numel (scalar): Total number of elements in the input tensors (product of tensor shapes).\n",
    "            self.mse (Tensor): Saved MSE Computed fpr backward computation.\n",
    "            self.rmse (Tensor): Saved RMSE value for use in gradient calculation.\n",
    "            self.pred (Tensor): Saved predicted tensor for backward computation.\n",
    "            self.target (Tensor): Saved target tensor for backward computation.\n",
    "            self.loss (Tensor): Save the computed loss for backpropagation uses.\n",
    "        \"\"\"\n",
    "\n",
    "        # Type check, pred and target must be an instance of Tensor\n",
    "        if isinstance(pred, Tensor) == False or isinstance(target, Tensor) == False:\n",
    "            raise ValueError(f\"In performing forward(), input `pred` or `target` must be in a MML `Tensor` format but you have {type(pred)} and {type(target)}\")\n",
    "\n",
    "        # Shape check, pred and target must have the same shape\n",
    "        if pred.shape != target.shape:\n",
    "            raise ValueError(f\"In performing forward(), input `pred` and `target` must have the same shape, but you have {pred.shape} and {target.shape}\")\n",
    "\n",
    "        # Compute MSE and RMSE in Tensor\n",
    "        mse = ((pred - target) ** 2).mean()\n",
    "        rmse = mse ** 0.5\n",
    "\n",
    "        # Save the pred, input, mse, rmse, and total number of elements for backward\n",
    "        self.__setattr__(\"numel\", np.array(pred.shape).prod())\n",
    "        self.__setattr__(\"mse\", mse)\n",
    "        self.__setattr__(\"rmse\", rmse)\n",
    "        self.__setattr__(\"pred\", pred)\n",
    "        self.__setattr__(\"target\", target)\n",
    "        self.__setattr__(\"loss\", [rmse])\n",
    "\n",
    "        return rmse\n",
    "\n",
    "    def backward(self, grad_output: Tensor | None = None) -> Tensor | None:\n",
    "        \"\"\"\n",
    "        Backward pass: compute gradients for weight, bias, and input.\n",
    "\n",
    "        This method performs the gradient computation for a module during backpropagation. \n",
    "        It calculates the gradients of the loss with respect to the weights, biases, and input tensor,\n",
    "        based on the provided `grad_output` (gradient from the next layer). The implementation\n",
    "        supports both PyTorch autograd and manual gradient calculation modes.\n",
    "\n",
    "        Args:\n",
    "            None: Since it is the first in calculating backward.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Gradient with respect to the input tensor, for recursive backward calculations in previous layers.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # If use autograd, pass; if manual mode, then calculate\n",
    "        if self.autograd == True:\n",
    "            self.loss[0].data.backward()\n",
    "            return None\n",
    "        \n",
    "        # If grad_output is None (by default), assign it to 1\n",
    "        if grad_output is None:\n",
    "            grad_output = Tensor(1.0, backend=self.backend, dtype=self.dtype, device=self.device)\n",
    "        \n",
    "        # Else, it must be a scalar.\n",
    "        else:\n",
    "            if isinstance(grad_output, Tensor) == False:\n",
    "                raise ValueError(f\"In performing backward(), input `grad_output` must be in a MML `Tensor` format but you have {type(grad_output)}\")\n",
    "            if len(grad_output.shape) != 0:\n",
    "                raise ValueError(\"In performing backward(), input `grad_output` must be in a MML `Tensor` with a scalar stored in\")\n",
    "\n",
    "        # dL/dpred = (pred - target) / (N * RMSE)\n",
    "        grad_input = (self.pred - self.target) / (self.rmse * self.numel)\n",
    "        return grad_input * grad_output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"nn_Loss_RMSE(Root Mean Square Error Loss).\"\n",
    "\n",
    "\n",
    "# Alias for nn_Loss_RMSE\n",
    "RMSE = nn_Loss_RMSE\n",
    "\n",
    "\n",
    "# Implementation of Mean Absolute Error Loss\n",
    "class nn_Loss_MAE(nn_Loss_BaseLoss):\n",
    "    \"\"\"\n",
    "    Mean Absolute Error Loss.\n",
    "\n",
    "    The Mean Absolute Error (MAE) loss function quantifies the average absolute \n",
    "    difference between predicted values and true values. It is widely used in \n",
    "    regression tasks and is defined as: \n",
    "\n",
    "    Formula: L = \\frac{1}{n} \\sum_{i=1}^{n} |y_{true,i} - y_{pred,i}|\n",
    "\n",
    "    Where:\n",
    "        - $ n $ is the number of samples\n",
    "        - $ y_{true,i} $ is the true value for sample i\n",
    "        - $ y_{pred,i} $ is the predicted value for sample i\n",
    "\n",
    "    MAE is less sensitive to outliers compared to Mean Squared Error (MSE), as it \n",
    "    uses absolute differences rather than squared differences. However, it is not \n",
    "    differentiable at zero, which can affect gradient-based optimization methods. \n",
    "    It provides an interpretable measure of error in the same units as the target \n",
    "    variable.\n",
    "\n",
    "    Formula: L = \\frac{1}{n} \\sum_{i=1}^{n} |y_{true,i} - y_{pred,i}|\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    __attr__ = \"MML.nn_Loss_MAE\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 *,\n",
    "                 module_name: str = \"nn_Loss_MAE\",\n",
    "                 backend: Literal[\"torch\", \"numpy\"] = \"torch\",\n",
    "                 dtype: type | str = None,\n",
    "                 device: str | None = None,\n",
    "                 autograd: bool = False,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        A Mean Absolute Error Loss Function.\n",
    "\n",
    "        Parameters:\n",
    "            module_name: str, The name of the module instance. Defaults to \"nn_Loss_MAE\".\n",
    "            backend: Literal[\"torch\", \"numpy\"], The computational backend to use. Defaults to \"torch\".\n",
    "            dtype: type, The data type for the tensor values. Defaults to None (auto detection). \n",
    "                    For PyTorch, this corresponds to torch.dtype; for NumPy, it corresponds to np.dtype.\n",
    "            device: str | None, The target device (e.g., \"cpu\", \"cuda\") where the layer's parameters will be placed. \n",
    "                    If None, uses the default device. Defaults to None (auto detection).\n",
    "            autograd: bool, A flag indicating whether to use PyTorch's autograd for gradient computation. \n",
    "                    If True, manual gradient tracking is disabled. Defaults to False.\n",
    "\n",
    "        Attributes:\n",
    "            self.dtype: type, The data type of the tensor values.\n",
    "            self.device: str | None, The device where the parameters are placed.\n",
    "            self.autograd: bool, Whether PyTorch's autograd is enabled for this layer.\n",
    "            self._parameters.weight: nn_Parameter, The weight parameter matrix (shape: [in_features, out_features]).\n",
    "            self._parameters.bias: nn_Parameter | optional, The optional bias vector (shape: [out_features]) if has_bias is True.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__(module_name=module_name, backend=backend, dtype=dtype, device=device, autograd=autograd)\n",
    "\n",
    "    def forward(self, pred: Tensor, target: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Compute the Mean Absolute Error (MAE) between predictions and targets.\n",
    "\n",
    "        This method evaluates the average absolute difference \n",
    "        between predicted values (`pred`) and actual values (`target`), \n",
    "        offering a robust loss measure less sensitive to outliers.\n",
    "\n",
    "        Args:\n",
    "            pred (Tensor): Predicted tensor containing model outputs.\n",
    "            target (Tensor): Target tensor containing ground truth values.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Scalar tensor representing the computed MSE loss.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If `pred` or `target` is not a valid MML.Tensor object, \n",
    "                        or if `pred` and `target` do not have the same shape.\n",
    "\n",
    "        Attributes:\n",
    "            self.numel (scalar): Total number of elements in the input tensors (product of tensor shapes).\n",
    "            self.mae (Tensor): Saved Mean Absolute Error Tensor value for reference.\n",
    "            self.pred (Tensor): Saved predicted tensor for backward computation.\n",
    "            self.target (Tensor): Saved target tensor for backward computation.\n",
    "            self.loss (Tensor): Save the computed loss for backpropagation uses.\n",
    "        \"\"\"\n",
    "\n",
    "        # Type check, pred and target must be an instance of Tensor\n",
    "        if isinstance(pred, Tensor) == False or isinstance(target, Tensor) == False:\n",
    "            raise ValueError(f\"In performing forward(), input `pred` or `target` must be in a MML `Tensor` format but you have {type(pred)} and {type(target)}\")\n",
    "\n",
    "        # Shape check, pred and target must have the same shape\n",
    "        if pred.shape != target.shape:\n",
    "            raise ValueError(f\"In performing forward(), input `pred` and `target` must have the same shape, but you have {pred.shape} and {target.shape}\")\n",
    "\n",
    "        # Compute MAE in Tensor\n",
    "        abs_diff = (pred - target).abs()\n",
    "        mae = abs_diff.mean()\n",
    "\n",
    "        # Save the pred, input, mse, rmse, and total number of elements for backward\n",
    "        self.__setattr__(\"numel\", np.array(pred.shape).prod())\n",
    "        self.__setattr__(\"mae\", mae)\n",
    "        self.__setattr__(\"pred\", pred)\n",
    "        self.__setattr__(\"target\", target)\n",
    "        self.__setattr__(\"loss\", [mae])\n",
    "\n",
    "        return mae\n",
    "\n",
    "    def backward(self, grad_output: Tensor | None = None) -> Tensor | None:\n",
    "        \"\"\"\n",
    "        Backward pass: compute gradients for weight, bias, and input.\n",
    "\n",
    "        This method performs the gradient computation for a module during backpropagation. \n",
    "        It calculates the gradients of the loss with respect to the weights, biases, and input tensor,\n",
    "        based on the provided `grad_output` (gradient from the next layer). The implementation\n",
    "        supports both PyTorch autograd and manual gradient calculation modes.\n",
    "\n",
    "        Args:\n",
    "            None: Since it is the first in calculating backward.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Gradient with respect to the input tensor, for recursive backward calculations in previous layers.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # If use autograd, pass; if manual mode, then calculate\n",
    "        if self.autograd == True:\n",
    "            self.loss[0].data.backward()\n",
    "            return None\n",
    "\n",
    "        # If grad_output is None (by default), assign it to 1\n",
    "        if grad_output is None:\n",
    "            grad_output = Tensor(1.0, backend=self.backend, dtype=self.dtype, device=self.device)\n",
    "        \n",
    "        # Else, it must be a scalar.\n",
    "        else:\n",
    "            if isinstance(grad_output, Tensor) == False:\n",
    "                raise ValueError(f\"In performing backward(), input `grad_output` must be in a MML `Tensor` format but you have {type(grad_output)}\")\n",
    "            if len(grad_output.shape) != 0:\n",
    "                raise ValueError(\"In performing backward(), input `grad_output` must be in a MML `Tensor` with a scalar stored in\")\n",
    "\n",
    "        # dL/dpred = (pred - target).sign() / N\n",
    "        grad_input = (self.pred - self.target).sign() / self.numel\n",
    "        return grad_input * grad_output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"nn_Loss_MAE(Mean Absolute Error Loss).\"\n",
    "\n",
    "\n",
    "# Alias for nn_Loss_MAE\n",
    "MAE = nn_Loss_MAE\n",
    "\n",
    "\n",
    "# Implementation of Binary Cross Entropy Loss\n",
    "class nn_Loss_BinaryCrossEntropy(nn_Loss_BaseLoss):\n",
    "    \"\"\"\n",
    "    Binary Cross-Entropy for predictions in [0,1] and binary targets.\n",
    "\n",
    "    The Binary Cross-Entropy loss measures the difference between predicted \n",
    "    probabilities (in [0,1]) and true binary labels (0 or 1). It is widely used \n",
    "    in binary classification tasks and is defined as: \n",
    "\n",
    "    Formula: L = -\\frac{1}{n} \\sum_{i=1}^{n} [y_{true,i} \\log(p_i) + (1 - y_{true,i}) \\log(1 - p_i)]\n",
    "\n",
    "    Where:\n",
    "        - $ n $ is the number of samples\n",
    "        - $ y_{true,i} $ is the true binary label for sample i (0 or 1)\n",
    "        - $ p_i $ is the predicted probability for sample i (in [0,1])\n",
    "\n",
    "    This loss function penalizes incorrect predictions more heavily when the model \n",
    "    is confident but wrong. It is differentiable and suitable for optimization via \n",
    "    gradient-based methods. However, it requires care to avoid numerical instability \n",
    "    (e.g., adding a small epsilon to probabilities near 0 or 1).\n",
    "\n",
    "    Formula: L = -\\frac{1}{n} \\sum_{i=1}^{n} [y_{true,i} \\log(p_i) + (1 - y_{true,i}) \\log(1 - p_i)]\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    __attr__ = \"MML.nn_Loss_BinaryCrossEntropy\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 eps: float = 1e-16,\n",
    "                 *,\n",
    "                 module_name: str = \"nn_Loss_BinaryCrossEntropy\",\n",
    "                 backend: Literal[\"torch\", \"numpy\"] = \"torch\",\n",
    "                 dtype: type | str = None,\n",
    "                 device: str | None = None,\n",
    "                 autograd: bool = False,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        A Binary Cross Entropy Loss Function for binary classification.\n",
    "\n",
    "        Parameters:\n",
    "            eps: float, The epsilon amount applied to clip() to avoid log(0).\n",
    "            module_name: str, The name of the module instance. Defaults to \"nn_Loss_BinaryCrossEntropy\".\n",
    "            backend: Literal[\"torch\", \"numpy\"], The computational backend to use. Defaults to \"torch\".\n",
    "            dtype: type, The data type for the tensor values. Defaults to None (auto detection). \n",
    "                    For PyTorch, this corresponds to torch.dtype; for NumPy, it corresponds to np.dtype.\n",
    "            device: str | None, The target device (e.g., \"cpu\", \"cuda\") where the layer's parameters will be placed. \n",
    "                    If None, uses the default device. Defaults to None (auto detection).\n",
    "            autograd: bool, A flag indicating whether to use PyTorch's autograd for gradient computation. \n",
    "                    If True, manual gradient tracking is disabled. Defaults to False.\n",
    "\n",
    "        Attributes:\n",
    "            self.eps: float, The epsilon value applied.\n",
    "            self.dtype: type, The data type of the tensor values.\n",
    "            self.device: str | None, The device where the parameters are placed.\n",
    "            self.autograd: bool, Whether PyTorch's autograd is enabled for this layer.\n",
    "            self._parameters.weight: nn_Parameter, The weight parameter matrix (shape: [in_features, out_features]).\n",
    "            self._parameters.bias: nn_Parameter | optional, The optional bias vector (shape: [out_features]) if has_bias is True.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__(module_name=module_name, backend=backend, dtype=dtype, device=device, autograd=autograd)\n",
    "\n",
    "        # Record the eps value\n",
    "        self.__setattr__(\"eps\", eps)\n",
    "\n",
    "    def forward(self, pred: Tensor, target: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Compute the Binary Cross Entropy between predictions and targets.\n",
    "\n",
    "        Args:\n",
    "            pred (Tensor): Predicted tensor containing model outputs.\n",
    "            target (Tensor): Target tensor containing ground truth values.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Scalar tensor representing the computed MSE loss.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If `pred` or `target` is not a valid MML.Tensor object, \n",
    "                        or if `pred` and `target` do not have the same shape.\n",
    "\n",
    "        Attributes:\n",
    "            self.n_classes (scalar): The number of classes to be classified.\n",
    "            self.numel (scalar): Total number of elements in the input tensors (product of tensor shapes).\n",
    "            self.pred (Tensor): Saved predicted tensor for backward computation.\n",
    "            self.target (Tensor): Saved target tensor for backward computation.\n",
    "            self.loss (Tensor): Save the computed loss for backpropagation uses.\n",
    "        \"\"\"\n",
    "\n",
    "        # Type check, pred and target must be an instance of Tensor\n",
    "        if isinstance(pred, Tensor) == False or isinstance(target, Tensor) == False:\n",
    "            raise ValueError(f\"In performing forward(), input `pred` or `target` must be in a MML `Tensor` format but you have {type(pred)} and {type(target)}\")\n",
    "\n",
    "        # Shape check, pred and target must have the same shape\n",
    "        if pred.shape != target.shape:\n",
    "            raise ValueError(f\"In performing forward(), input `pred` and `target` must have the same shape, but you have {pred.shape} and {target.shape}\")\n",
    "\n",
    "        # n_classes check, must only have 1 dimension (DOES NOT SUPPORT ONE-HOT)\n",
    "        if len(pred.shape) == 2:\n",
    "            if pred.shape[1] != 1:\n",
    "                raise ValueError(f\"In performing forward(), input `pred` and `target` must be 1 dimensional or 2 dimension with the 2nd one be 1, but you have {pred.shape} and {target.shape}\")\n",
    "\n",
    "        # Compute clipped predictions to avoid log0\n",
    "        clipped_pred = pred.clip(self.eps, 1-self.eps)\n",
    "\n",
    "        # Compute binary cross-entropy loss\n",
    "        loss = - (target * clipped_pred.log() +\n",
    "                  (1 - target) * (1 - clipped_pred).log())\n",
    "        loss = loss.mean()\n",
    "\n",
    "        # Save the pred, input etc for backward\n",
    "        self.__setattr__(\"n_classes\", 1)\n",
    "        self.__setattr__(\"numel\", np.array(clipped_pred.shape).prod())\n",
    "        self.__setattr__(\"pred\", clipped_pred)\n",
    "        self.__setattr__(\"target\", target)\n",
    "        self.__setattr__(\"loss\", [loss])\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def backward(self, grad_output: Tensor | None = None) -> Tensor | None:\n",
    "        \"\"\"\n",
    "        Backward pass: compute gradients for weight, bias, and input.\n",
    "\n",
    "        This method performs the gradient computation for a module during backpropagation. \n",
    "        It calculates the gradients of the loss with respect to the weights, biases, and input tensor,\n",
    "        based on the provided `grad_output` (gradient from the next layer). The implementation\n",
    "        supports both PyTorch autograd and manual gradient calculation modes.\n",
    "\n",
    "        Args:\n",
    "            None: Since it is the first in calculating backward.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Gradient with respect to the input tensor, for recursive backward calculations in previous layers.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # If use autograd, pass; if manual mode, then calculate\n",
    "        if self.autograd == True:\n",
    "            self.loss[0].data.backward()\n",
    "            return None\n",
    "        \n",
    "        # If grad_output is None (by default), assign it to 1\n",
    "        if grad_output is None:\n",
    "            grad_output = Tensor(1.0, backend=self.backend, dtype=self.dtype, device=self.device)\n",
    "        \n",
    "        # Else, it must be a scalar.\n",
    "        else:\n",
    "            if isinstance(grad_output, Tensor) == False:\n",
    "                raise ValueError(f\"In performing backward(), input `grad_output` must be in a MML `Tensor` format but you have {type(grad_output)}\")\n",
    "            if len(grad_output.shape) != 0:\n",
    "                raise ValueError(\"In performing backward(), input `grad_output` must be in a MML `Tensor` with a scalar stored in\")\n",
    "\n",
    "        # dL/dpred = -(target/pred - (1-target)/(1-pred)) / N\n",
    "        grad_input = - (self.target / self.pred) + \\\n",
    "            ((1 - self.target) / (1 - self.pred))\n",
    "        grad_input = grad_input / self.numel\n",
    "        return grad_input * grad_output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"nn_Loss_BinaryCrossEntropy(Binary Cross Entropy Loss).\"\n",
    "\n",
    "\n",
    "# Alias for nn_Loss_BinaryCrossEntropy\n",
    "BinaryCrossEntropy = nn_Loss_BinaryCrossEntropy\n",
    "\n",
    "\n",
    "# Implementation of Multi Cross Entropy Loss\n",
    "class nn_Loss_MultiCrossEntropy(nn_Loss_BaseLoss):\n",
    "    \"\"\"\n",
    "    Multi Cross-Entropy for predictions in one_hot and probabilities targets.\n",
    "\n",
    "    The Multi-Class Cross-Entropy loss measures the difference between predicted \n",
    "    probability distributions (for multiple classes) and true one-hot encoded labels. \n",
    "    It is widely used in multi-class classification tasks and is defined as: \n",
    "\n",
    "    Formula: L = -\\frac{1}{n} \\sum_{i=1}^{n} \\sum_{c=1}^{C} y_{true,i,c} \\log(p_{i,c})\n",
    "\n",
    "    Where:\n",
    "        - $ n $ is the number of samples\n",
    "        - $ C $ is the number of classes\n",
    "        - $ y_{true,i,c} $ is the one-hot encoded true label for sample i (1 if class c is correct, 0 otherwise)\n",
    "        - $ p_{i,c} $ is the predicted probability for sample i belonging to class c\n",
    "\n",
    "    This loss function penalizes incorrect predictions by measuring the discrepancy between \n",
    "    the true distribution (one-hot) and the predicted distribution. It is differentiable and \n",
    "    suitable for optimization via gradient-based methods. However, numerical stability \n",
    "    must be ensured (e.g., adding a small epsilon to probabilities near 0 or 1) to avoid log(0).\n",
    "\n",
    "    Formula: L = -\\frac{1}{n} \\sum_{i=1}^{n} \\sum_{c=1}^{C} y_{true,i,c} \\log(p_{i,c})\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    __attr__ = \"MML.nn_Loss_MultiCrossEntropy\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 eps: float = 1e-16,\n",
    "                 raw_logits: bool = True,\n",
    "                 *,\n",
    "                 module_name: str = \"nn_Loss_MultiCrossEntropy\",\n",
    "                 backend: Literal[\"torch\", \"numpy\"] = \"torch\",\n",
    "                 dtype: type | str = None,\n",
    "                 device: str | None = None,\n",
    "                 autograd: bool = False,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        A Binary Cross Entropy Loss Function for multi classification.\n",
    "\n",
    "        Parameters:\n",
    "            eps: float, The epsilon amount applied to clip() to avoid log(0).\n",
    "            raw_logits: bool, If True, then not applied softmax, or applied softmax.\n",
    "            module_name: str, The name of the module instance. Defaults to \"nn_Loss_MultiCrossEntropy\".\n",
    "            backend: Literal[\"torch\", \"numpy\"], The computational backend to use. Defaults to \"torch\".\n",
    "            dtype: type, The data type for the tensor values. Defaults to None (auto detection). \n",
    "                    For PyTorch, this corresponds to torch.dtype; for NumPy, it corresponds to np.dtype.\n",
    "            device: str | None, The target device (e.g., \"cpu\", \"cuda\") where the layer's parameters will be placed. \n",
    "                    If None, uses the default device. Defaults to None (auto detection).\n",
    "            autograd: bool, A flag indicating whether to use PyTorch's autograd for gradient computation. \n",
    "                    If True, manual gradient tracking is disabled. Defaults to False.\n",
    "\n",
    "        Attributes:\n",
    "            self.eps: float, The epsilon value applied.\n",
    "            self.raw_logits: bool, Whether raw logits or not (not applied softmax or not).\n",
    "            self.dtype: type, The data type of the tensor values.\n",
    "            self.device: str | None, The device where the parameters are placed.\n",
    "            self.autograd: bool, Whether PyTorch's autograd is enabled for this layer.\n",
    "            self._parameters.weight: nn_Parameter, The weight parameter matrix (shape: [in_features, out_features]).\n",
    "            self._parameters.bias: nn_Parameter | optional, The optional bias vector (shape: [out_features]) if has_bias is True.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__(module_name=module_name, backend=backend, dtype=dtype, device=device, autograd=autograd)\n",
    "\n",
    "        # Record the eps value\n",
    "        self.__setattr__(\"eps\", eps)\n",
    "\n",
    "        # Record the status whether it is raw logits\n",
    "        self.__setattr__(\"raw_logits\", raw_logits)\n",
    "\n",
    "    def forward(self, pred: Tensor, target: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Compute the Multi-class Cross Entropy between predictions and targets.\n",
    "\n",
    "        Args:\n",
    "            pred (Tensor): Predicted tensor containing model outputs.\n",
    "            target (Tensor): Target tensor containing ground truth values.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Scalar tensor representing the computed MSE loss.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If `pred` or `target` is not a valid MML.Tensor object, \n",
    "                        or if `pred` and `target` do not have the same shape.\n",
    "\n",
    "        Attributes:\n",
    "            self.n_classes (scalar): The number of classes to be classified.\n",
    "            self.n_samples (scalar): Total number of samples in the data.\n",
    "            self.pred (Tensor): Saved predicted tensor for backward computation.\n",
    "            self.pred_logprobs (Tensor): Saved log probabilities tensor for backward computation.\n",
    "            self.target (Tensor): Saved target tensor for backward computation.\n",
    "            self.target_multiclass (Tensor): Save target but in multiclass form for backward computation.\n",
    "            self.loss (Tensor): Save the computed loss for backpropagation uses.\n",
    "        \"\"\"\n",
    "\n",
    "        # Type check, pred and target must be an instance of Tensor\n",
    "        if isinstance(pred, Tensor) == False or isinstance(target, Tensor) == False:\n",
    "            raise ValueError(f\"In performing forward(), input `pred` or `target` must be in a MML `Tensor` format but you have {type(pred)} and {type(target)}\")\n",
    "\n",
    "        # Shape check, pred and target must have the same shape\n",
    "        if pred.shape != target.shape:\n",
    "            raise ValueError(f\"In performing forward(), input `pred` and `target` must have the same shape, but you have {pred.shape} and {target.shape}\")\n",
    "\n",
    "        # n_classes check, must have 2 dim and greater than 1 2nd dim\n",
    "        if len(pred.shape) != 2:\n",
    "            raise ValueError(f\"In performing forward(), input `pred` and `target` must be 2 dimensional, but you have {pred.shape} and {target.shape}\")\n",
    "        if pred.shape[1] <= 1:\n",
    "            raise ValueError(f\"In performing forward(), input `pred` and `target` must have greater than 1 outputs in Multi Cross Entropy, but you have {pred.shape} and {target.shape}\")\n",
    "\n",
    "        # Input shape: (N, C) where C = number of classes\n",
    "        # Target shape: (N, C) with one-hot encoded\n",
    "        if self.raw_logits == True:\n",
    "            # Compute log-softmax for numerical stability (log probabilities)\n",
    "            pred_log_probs = pred.softmax(axis=1).clip(self.eps).log()\n",
    "        else:\n",
    "            # Input is probabilities; take log\n",
    "            pred_log_probs = pred.clip(self.eps).log()\n",
    "\n",
    "        # Turn the true one_hot result into a multi-class result (N, 1)\n",
    "        target_multiclass = self._to_labels(target)\n",
    "\n",
    "        # Gather the log probs along axis 1 by true indices\n",
    "        gathered_log_probs = pred_log_probs.gather_along(\n",
    "            pred_log_probs, axis=1, index=target_multiclass)\n",
    "\n",
    "        # Compute negative log-likelihood loss for each sample and take mean\n",
    "        losses = -gathered_log_probs\n",
    "        loss = losses.mean()\n",
    "\n",
    "        # Save the pred, input etc for backward\n",
    "        self.__setattr__(\"n_classes\", pred.shape[1])\n",
    "        self.__setattr__(\"n_samples\", pred.shape[0])\n",
    "        self.__setattr__(\"pred\", pred)\n",
    "        self.__setattr__(\"pred_logprobs\", pred_log_probs)\n",
    "        self.__setattr__(\"target\", target)\n",
    "        self.__setattr__(\"target_multiclass\", target_multiclass)\n",
    "        self.__setattr__(\"loss\", [loss])\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def backward(self, grad_output: Tensor | None = None) -> Tensor | None:\n",
    "        \"\"\"\n",
    "        Backward pass: compute gradients for weight, bias, and input.\n",
    "\n",
    "        This method performs the gradient computation for a module during backpropagation. \n",
    "        It calculates the gradients of the loss with respect to the weights, biases, and input tensor,\n",
    "        based on the provided `grad_output` (gradient from the next layer). The implementation\n",
    "        supports both PyTorch autograd and manual gradient calculation modes.\n",
    "\n",
    "        Args:\n",
    "            None: Since it is the first in calculating backward.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Gradient with respect to the input tensor, for recursive backward calculations in previous layers.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # If use autograd, pass; if manual mode, then calculate\n",
    "        if self.autograd == True:\n",
    "            self.loss[0].data.backward()\n",
    "            return None\n",
    "        \n",
    "        # If grad_output is None (by default), assign it to 1\n",
    "        if grad_output is None:\n",
    "            grad_output = Tensor(1.0, backend=self.backend, dtype=self.dtype, device=self.device)\n",
    "        \n",
    "        # Else, it must be a scalar.\n",
    "        else:\n",
    "            if isinstance(grad_output, Tensor) == False:\n",
    "                raise ValueError(f\"In performing backward(), input `grad_output` must be in a MML `Tensor` format but you have {type(grad_output)}\")\n",
    "            if len(grad_output.shape) != 0:\n",
    "                raise ValueError(\"In performing backward(), input `grad_output` must be in a MML `Tensor` with a scalar stored in\")\n",
    "\n",
    "        # Initialize grad_input with same shape as predictions\n",
    "        grad_input = self.pred_logprobs.to_zeros()\n",
    "\n",
    "        # Calculate the backward gradients\n",
    "        if self.raw_logits == True:\n",
    "            # grad = (softmax_prob - one_hot(target)) / N\n",
    "            grad_input = self.pred_logprobs.exp() - self.target\n",
    "            grad_input /= self.n_samples\n",
    "        else:\n",
    "            # For probabilities: grad = -1/p_target for target class, 0 for others, divided by N\n",
    "            grad_input = -self.target / (self.pred_logprobs.exp() + self.eps)\n",
    "            grad_input /= self.n_samples\n",
    "\n",
    "        return grad_input * grad_output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"nn_Loss_MultiCrossEntropy(Multi Cross Entropy Loss, with n_classes = {self.n_classes}).\"\n",
    "\n",
    "\n",
    "# Alias for nn_Loss_BinaryCrossEntropy\n",
    "MultiCrossEntropy = nn_Loss_MultiCrossEntropy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These Optimizers are self implemented and open-sourced\n",
    "# Available at https://github.com/dof-studio/MML/\n",
    "# By Nathmath Huang (bh2821)\n",
    "# License: Apache License Version 2.0\n",
    "\n",
    "# Base Class for All nn Optimizers\n",
    "class nn_Optm_BaseOptimizer(nn_Base):\n",
    "    \"\"\"\n",
    "    Base optimizer class.\n",
    "    \n",
    "    Any inherited optimizer takes all of the parameters as a reference and\n",
    "    use a method connected with gradients to update trainable parameters.\n",
    "    A typical __init__ at least requires a list/dict of parameters or a nn_Module.\n",
    "    \"\"\"\n",
    "    \n",
    "    __attr__ = \"MML.nn_Optm_BaseOptimizer\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 params: List[nn_Parameter] | Dict[Any, nn_Parameter] | nn_Module, \n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize an optimizer. Call this and pass in parameters as\n",
    "        a list/dict of nn_Parameter or a nn_Module which contains all parameters and submodules.\n",
    "\n",
    "        Parameters:\n",
    "            params: List[nn_Parameter] | Dict[Any, nn_Parameter] | nn_Module, if directly gives a list of parameters,\n",
    "                    which we believe generates by .parameters(), we record them as a list;\n",
    "                    If directly gives an nn_Module, we accepts and record all of the parameters as a list;\n",
    "\n",
    "        Attributes:\n",
    "            self.params: a List of nn_Parameters which may have gradients and needs to be updated.\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # Record the unfiltered list of parameters\n",
    "        if isinstance(params, list):\n",
    "            self.params = params\n",
    "        elif isinstance(params, dict):\n",
    "            self.params = params.values()\n",
    "        elif isinstance(params, nn_Module):\n",
    "            self.params = params.parameters()\n",
    "        else:\n",
    "            raise ValueError(f\"`params` for an optimizer can either be a list/dict of nn_Parameter or a nn_module, but you have {type(params)}\")\n",
    "            \n",
    "        # Filter all parameters having gradients\n",
    "        self.params = [p for p in self.params if p.requires_grad == True]\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"\n",
    "        Update all trainable parameters in one step (override in subclasses).\n",
    "        \n",
    "        Returns:\n",
    "            self\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"step() is not implemented in the base optimizer class\")\n",
    "\n",
    "    def zero_grad(self):\n",
    "        \"\"\"\n",
    "        Reset gradients of all parameters to zero.\n",
    "        \n",
    "        Returns:\n",
    "            self\n",
    "        \"\"\"\n",
    "        for p in self.params:\n",
    "            p.zero_grad()\n",
    "        return self\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"nn_Optm_BaseOptimizer(with {len(self.params)} trainable parameters).\"\n",
    "\n",
    "\n",
    "# Implementation of Stochastic Gradient Descent Optimzier\n",
    "class nn_Optm_SGD(nn_Optm_BaseOptimizer):\n",
    "    \"\"\"\n",
    "    Stochastic Gradient Descent optimizer with optional momentum.\n",
    "    \n",
    "    This optimizer uses stochastic gradient descent (SGD) to update model parameters \n",
    "    by minimizing a loss function. It supports optional momentum for faster convergence \n",
    "    and better stability in optimization.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    __attr__ = \"MML.nn_Optm_SGD\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 params: List[nn_Parameter] | Dict[Any, nn_Parameter] | nn_Module, \n",
    "                 *,\n",
    "                 lr: float = 0.01, \n",
    "                 momentum: float = 0.9,\n",
    "                 **kwargs):\n",
    "        \n",
    "        \"\"\"\n",
    "        Initialize a SGD optimizer with momentum. Call this and pass in parameters as\n",
    "        a list/dict of nn_Parameter or a nn_Module which contains all parameters and submodules.\n",
    "\n",
    "        Parameters:\n",
    "            params: List[nn_Parameter] | Dict[Any, nn_Parameter] | nn_Module, if directly gives a list of parameters,\n",
    "                    which we believe generates by .parameters(), we record them as a list;\n",
    "                    If directly gives an nn_Module, we accepts and record all of the parameters as a list;\n",
    "            lr: float, learning rate, the step size functioned on the gradients to update parameters;\n",
    "            momentum: float, momentum factor for acceleration in convergence.\n",
    "\n",
    "        Attributes:\n",
    "            self.params: a List of nn_Parameters which may have gradients and needs to be updated.\n",
    "            self.n: int, the total number of steps performed.\n",
    "            self.lr: float, the learning rate as a plain float.\n",
    "            self.momentum: float, the momentum rate as a plain float.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Call the base initialization to initialize all trainable parameters\n",
    "        super().__init__(params, **kwargs)\n",
    "        \n",
    "        # Record the SGD parameters\n",
    "        self.n = 0\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        \n",
    "        # Initialize velocity buffers if momentum is used\n",
    "        if momentum != 0:\n",
    "            self.velocities = [p.data.to_zeros() for p in self.params]\n",
    "        else:\n",
    "            self.velocities = None\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"\n",
    "        Update all trainable parameters in one step by SGD and momentum method.\n",
    "        \n",
    "        Returns:\n",
    "            self\n",
    "        \"\"\"\n",
    "        # For compatibility, n adds at the begining\n",
    "        self.n += 1\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Iterate over trainable parameters\n",
    "            for i, p in enumerate(self.params):\n",
    "    \n",
    "                # If uses momentum method\n",
    "                if self.momentum != 0:\n",
    "                    # Momentum update: v = momentum * v - lr * grad and update parameter: w = w + v\n",
    "                    if p.autograd == False:\n",
    "                        self.velocities[i] = self.velocities[i] * self.momentum - self.lr * p.grad\n",
    "                        p.data += self.velocities[i]\n",
    "                    else:\n",
    "                        self.velocities[i].data = self.velocities[i].data * self.momentum - self.lr * p.data.data.grad\n",
    "                        p.data.data.data += self.velocities[i].data\n",
    "                        \n",
    "                # Use plain vanilla SGD\n",
    "                else:\n",
    "                    # Vanilla SGD: w = w - lr * grad\n",
    "                    if p.autograd == False:\n",
    "                        p.data -= self.lr * p.grad\n",
    "                    else:\n",
    "                        p.data.data.data -= self.lr * p.data.data.grad\n",
    "                        \n",
    "        return self\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"nn_Optm_SGD(with {len(self.params)} trainable parameters).\"\n",
    "\n",
    "\n",
    "# Alias for nn_Optm_SGD\n",
    "SGD = nn_Optm_SGD\n",
    "\n",
    "\n",
    "# Implementation of Adaptive Momentum Optimzier\n",
    "class nn_Optm_Adam(nn_Optm_BaseOptimizer):\n",
    "    \"\"\"\n",
    "    Adaptive Momentum Estimate (Adam) Optimizer.\n",
    "\n",
    "    Adam is an optimization algorithm that combines the advantages of both RMSProp and SGD. \n",
    "    It maintains two moving averages: one for the gradient (momentum) and another for the square \n",
    "    of the gradient, which are updated over time. The learning rate is adaptively adjusted based \n",
    "    on these estimates to achieve faster convergence and better stability.\n",
    "    \"\"\"\n",
    "    \n",
    "    __attr__ = \"MML.nn_Optm_Adam\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 params: List[nn_Parameter] | Dict[Any, nn_Parameter] | nn_Module, \n",
    "                 *,\n",
    "                 lr: float = 0.001, \n",
    "                 beta1: float = 0.9, \n",
    "                 beta2: float = 0.999, \n",
    "                 eps: float = 1e-16,\n",
    "                 **kwargs):\n",
    "        \n",
    "        \"\"\"\n",
    "        Initialize an Adam optimizer with momentum. Call this and pass in parameters as\n",
    "        a list/dict of nn_Parameter or a nn_Module which contains all parameters and submodules.\n",
    "\n",
    "        Parameters:\n",
    "            params: List[nn_Parameter] | Dict[Any, nn_Parameter] | nn_Module, if directly gives a list of parameters,\n",
    "                    which we believe generates by .parameters(), we record them as a list;\n",
    "                    If directly gives an nn_Module, we accepts and record all of the parameters as a list;\n",
    "            lr: float, learning rate, the step size functioned on the gradients to update parameters;\n",
    "            beta1: float, momentum term, the decay rate for the first moment estimate.\n",
    "            beta2: float, adaptive scaling, it is the decay rate for the second moment estimate, 0.999 for 1000 steps averaging.\n",
    "\n",
    "        Attributes:\n",
    "            self.params: a List of nn_Parameters which may have gradients and needs to be updated.\n",
    "            self.n: int, the total number of steps performed.\n",
    "            self.lr: float, the learning rate as a plain float.\n",
    "            self.beta1: float, the momentum factor stored in plain float.\n",
    "            self.beta2: float, the adaptive scaling term stored in plain float.\n",
    "            self.eps: float, epsilon to avoid dividing by 0.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Call the base initialization to initialize all trainable parameters\n",
    "        super().__init__(params, **kwargs)\n",
    "        \n",
    "        # Record the Adam parameters\n",
    "        self.n = 0\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.eps = eps\n",
    "        \n",
    "        # Initialize first and second moment estimates for each param\n",
    "        self.m = [p.data.to_zeros() for p in self.params]  # first moment\n",
    "        self.v = [p.data.to_zeros() for p in self.params]  # second moment\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"\n",
    "        Update all trainable parameters in one step by Adaptive Momentum Estimate (Adam) method.\n",
    "        \n",
    "        Returns:\n",
    "            self\n",
    "        \"\"\"\n",
    "        # For compatibility, n adds at the begining\n",
    "        self.n += 1\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Iterate over trainable parameters\n",
    "            for i, p in enumerate(self.params):\n",
    "    \n",
    "                if p.autograd == False:\n",
    "                    # Update biased first moment: m = beta1*m + (1-beta1)*grad\n",
    "                    self.m[i] = self.m[i] * self.beta1 + (1.0 - self.beta1) * p.grad\n",
    "                    # Update biased second moment: v = beta2*v + (1-beta2)*grad^2\n",
    "                    self.v[i] = self.v[i] * self.beta2 + (1.0 - self.beta2) * (p.grad ** 2)            \n",
    "    \n",
    "                    # Compute bias-corrected moments\n",
    "                    m_hat = self.m[i] / (1.0 - (self.beta1 ** self.n))\n",
    "                    v_hat = self.v[i] / (1.0 - (self.beta2 ** self.n))\n",
    "                    \n",
    "                    # Update parameter: w = w - lr * m_hat / (sqrt(v_hat) + eps)\n",
    "                    p.data -= self.lr * m_hat / (v_hat ** 0.5 + self.eps)\n",
    "                \n",
    "                else:\n",
    "                    # Update biased first moment: m = beta1*m + (1-beta1)*grad\n",
    "                    self.m[i].data = self.m[i].data * self.beta1 + (1.0 - self.beta1) * p.data.data.grad\n",
    "                    # Update biased second moment: v = beta2*v + (1-beta2)*grad^2\n",
    "                    self.v[i].data = self.v[i].data * self.beta2 + (1.0 - self.beta2) * (p.data.data.grad ** 2)            \n",
    "    \n",
    "                    # Compute bias-corrected moments\n",
    "                    m_hat = self.m[i] / (1.0 - (self.beta1 ** self.n))\n",
    "                    v_hat = self.v[i] / (1.0 - (self.beta2 ** self.n))\n",
    "                    \n",
    "                    # Update parameter: w = w - lr * m_hat / (sqrt(v_hat) + eps)\n",
    "                    p.data.data.data -= self.lr * m_hat.data / (v_hat.data ** 0.5 + self.eps)\n",
    "                        \n",
    "        return self\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"nn_Optm_Adam(with {len(self.params)} trainable parameters, beta1 = {self.beta1}, beta2 = {self.beta2}).\"\n",
    "\n",
    "\n",
    "# Alias for nn_Optm_Adam\n",
    "Adam = nn_Optm_Adam\n",
    "\n",
    "\n",
    "# Implementation of Adaptive Momentum Optimzier with Weight Decay\n",
    "class nn_Optm_AdamW(nn_Optm_BaseOptimizer):\n",
    "    \"\"\"\n",
    "    Adaptive Momentum Estimate (AdamW) Optimizer with Weight Decay.\n",
    "\n",
    "    AdamW combines the Adam optimizer with a weight decay regularization term to prevent \n",
    "    overfitting and improve convergence. It applies weight decay to all parameters after \n",
    "    the gradient is computed, ensuring that large weights are penalized in the loss function. \n",
    "    The optimizer uses the same momentum terms as Adam but applies the weight decay during \n",
    "    the update step.\n",
    "\n",
    "    Formula:\n",
    "        For each parameter `p`:\n",
    "        1. Compute gradient `g` and update momentum terms `v`.\n",
    "        2. Apply weight decay: `p.data -= lr * (g + beta_2 * v)`.\n",
    "        3. Update parameters using Adam's updates: `p.data -= lr * (g + beta_1 * v)`.\n",
    "    \"\"\"    \n",
    "    \n",
    "    __attr__ = \"MML.nn_Optm_AdamW\"\n",
    "    \n",
    "    def __init__(self,                 \n",
    "                 params: List[nn_Parameter] | Dict[Any, nn_Parameter] | nn_Module, \n",
    "                 *, \n",
    "                 lr: float = 0.001, \n",
    "                 beta1: float = 0.9, \n",
    "                 beta2: float = 0.999, \n",
    "                 eps: float = 1e-16,\n",
    "                 weight_decay: float = 0.01,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize an AdamW optimizer with weight decay. Call this and pass in parameters as\n",
    "        a list/dict of nn_Parameter or a nn_Module which contains all parameters and submodules.\n",
    "\n",
    "        Parameters:\n",
    "            params: List[nn_Parameter] | Dict[Any, nn_Parameter] | nn_Module, if directly gives a list of parameters,\n",
    "                    which we believe generates by .parameters(), we record them as a list;\n",
    "                    If directly gives an nn_Module, we accepts and record all of the parameters as a list;\n",
    "            lr: float, learning rate, the step size functioned on the gradients to update parameters;\n",
    "            beta1: float, momentum term, the decay rate for the first moment estimate.\n",
    "            beta2: float, adaptive scaling, it is the decay rate for the second moment estimate, 0.999 for 1000 steps averaging;\n",
    "            weight_decay: float, L2 regularization coefficient (decoupled).\n",
    "\n",
    "        Attributes:\n",
    "            self.params: a List of nn_Parameters which may have gradients and needs to be updated.\n",
    "            self.n: int, the total number of steps performed.\n",
    "            self.lr: float, the learning rate as a plain float.\n",
    "            self.beta1: float, the momentum factor stored in plain float.\n",
    "            self.beta2: float, the adaptive scaling term stored in plain float.\n",
    "            self.eps: float, epsilon to avoid dividing by 0.\n",
    "            self.weight_decay: float, weight decay parameter.\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        # Call the base initialization to initialize all trainable parameters\n",
    "        super().__init__(params, **kwargs)\n",
    "        \n",
    "        # Record the AdamW parameters\n",
    "        self.n = 0\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.eps = eps\n",
    "        self.weight_decay = weight_decay\n",
    "        \n",
    "        # Initialize first and second moment estimates for each param\n",
    "        self.m = [p.data.to_zeros() for p in self.params]  # first moment\n",
    "        self.v = [p.data.to_zeros() for p in self.params]  # second moment\n",
    "        \n",
    "    def step(self):\n",
    "        \"\"\"\n",
    "        Update all trainable parameters in one step by Adaptive Momentum Estimate with Weight Decay (AdamW) method.\n",
    "        \n",
    "        Returns:\n",
    "            self\n",
    "        \"\"\"\n",
    "        # For compatibility, n adds at the begining\n",
    "        self.n += 1\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Iterate over trainable parameters\n",
    "            for i, p in enumerate(self.params):\n",
    "    \n",
    "                if p.autograd == False:\n",
    "                    # Update biased first moment: m = beta1*m + (1-beta1)*grad\n",
    "                    self.m[i] = self.m[i] * self.beta1 + (1.0 - self.beta1) * p.grad\n",
    "                    # Update biased second moment: v = beta2*v + (1-beta2)*grad^2\n",
    "                    self.v[i] = self.v[i] * self.beta2 + (1.0 - self.beta2) * (p.grad ** 2)            \n",
    "    \n",
    "                    # Compute bias-corrected moments\n",
    "                    m_hat = self.m[i] / (1.0 - (self.beta1 ** self.n))\n",
    "                    v_hat = self.v[i] / (1.0 - (self.beta2 ** self.n))\n",
    "                    \n",
    "                    # Apply decoupled weight decay\n",
    "                    p.data *= (1.0 - self.lr * self.weight_decay)\n",
    "                    \n",
    "                    # Update parameter: w = w - lr * m_hat / (sqrt(v_hat) + eps)\n",
    "                    p.data -= self.lr * m_hat / (v_hat ** 0.5 + self.eps)\n",
    "                \n",
    "                else:\n",
    "                    # Update biased first moment: m = beta1*m + (1-beta1)*grad\n",
    "                    self.m[i].data = self.m[i].data * self.beta1 + (1.0 - self.beta1) * p.data.data.grad\n",
    "                    # Update biased second moment: v = beta2*v + (1-beta2)*grad^2\n",
    "                    self.v[i].data = self.v[i].data * self.beta2 + (1.0 - self.beta2) * (p.data.data.grad ** 2)            \n",
    "    \n",
    "                    # Compute bias-corrected moments\n",
    "                    m_hat = self.m[i] / (1.0 - (self.beta1 ** self.n))\n",
    "                    v_hat = self.v[i] / (1.0 - (self.beta2 ** self.n))\n",
    "                    \n",
    "                    # Apply decoupled weight decay\n",
    "                    p.data.data.data *= (1.0 - self.lr * self.weight_decay)\n",
    "                    \n",
    "                    # Update parameter: w = w - lr * m_hat / (sqrt(v_hat) + eps)\n",
    "                    p.data.data.data -= self.lr * m_hat.data / (v_hat.data ** 0.5 + self.eps)\n",
    "                        \n",
    "        return self\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"nn_Optm_AdamW(with {len(self.params)} trainable parameters, beta1 = {self.beta1}, beta2 = {self.beta2}, weight_decay = {self.weight_decay}).\"\n",
    "\n",
    "\n",
    "# Alias for nn_Optm_AdamW\n",
    "AdamW = nn_Optm_AdamW\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`1. Implementation of RNN (self-implemented)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This RNN and Stacked RNN layers are self implemented and open-sourced\n",
    "# Available at https://github.com/dof-studio/MML/\n",
    "# By Nathmath Huang (bh2821)\n",
    "# License: Apache License Version 2.0\n",
    "\n",
    "# Implementation of a Vanilla RNN Cell using TBPTT\n",
    "class nn_Layer_RNNCell(nn_Module):\n",
    "    \"\"\"\n",
    "    RNN Cell Implementation\n",
    "    \n",
    "    This class implements a basic Recurrent Neural Network (RNN) cell, \n",
    "    designed to process sequential data by maintaining and updating a hidden state \n",
    "    at each time step. It contains weight matrices and bias vectors stored \n",
    "    in nn_Parameter containers, enabling the computation of forward passes through \n",
    "    sequences while supporting gradient-based learning via backward passes. \n",
    "        \n",
    "    Note, TBPTT should be implemented externally with segmentation and carrying (manually reset_hidden).\n",
    "    \"\"\"\n",
    "\n",
    "    __attr__ = \"MML.nn_Layer_RNNCell\"   \n",
    "\n",
    "    def __init__(self, \n",
    "                 in_features: int = 1, \n",
    "                 hid_features: int = 1, \n",
    "                 has_bias: str = True,\n",
    "                 init_scale: float = 0.1,\n",
    "                 actv: Literal['tanh', 'relu'] = 'tanh', \n",
    "                 tbptt_steps: int | None = None,\n",
    "                 return_sequences: bool = False, \n",
    "                 return_state: bool = False,\n",
    "                 accumulate_hidden: bool = False,\n",
    "                 gradient_clipping: float = 5.0,\n",
    "                 *,\n",
    "                 module_name: str = \"nn_Layer_RNNCell\", \n",
    "                 backend: Literal[\"torch\", \"numpy\"] = \"torch\",\n",
    "                 dtype: type | str = None,\n",
    "                 device: str | None = None,\n",
    "                 autograd: bool = False,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        An RNN (Recurrent Neural Network) cell processes sequences by maintaining\n",
    "            a hidden state that is updated at each time step.\n",
    "            \n",
    "        Structure: h_t = \\tanh(W_{ih} x_t + b_{ih} + W_{hh} h_{t-1} + b_{hh}) \\tag{1}\n",
    "        \n",
    "        Parameters:\n",
    "            in_features: int, The number of input features for this layer. Defaults to 1.\n",
    "            hid_features: int, The number of hidden features for this layer. Defaults to 1.\n",
    "            has_bias: str, A flag indicating whether to include a bias term. Valid values are \"True\" or \"False\".\n",
    "                    If set to \"True\", the layer includes an additive bias term (b). Defaults to \"True\".\n",
    "            actv: str, activation type, must be in 'tanh' and 'relu'. Defaults to 'tanh'.\n",
    "            tbptt_steps: int | None, the number of steps performing Truncated BPTT (TBPTT), \n",
    "                    if None, then use true BPTT. Defaults: None.\n",
    "                    Note, TBPTT is not fully implemented. Please use BPTT instead.\n",
    "            return_sequences: bool, should the layers return only the final hidden state, or the full sequence \n",
    "                    of hidden states as well. Defaults to False.\n",
    "            return_state: bool, should the layers return output and final hidden state. Defaults to False.\n",
    "            accumulate_hidden: bool, should the network accumulate hidden state (when processing segments) or clear it before next forward.\n",
    "            gradient_clipping: float, The factor of performing gradient clipping to avoid gradient expolosion. Defaults to 5.0.\n",
    "            module_name: str, The name of the module instance. Defaults to \"nn_Layer_RNNCell\".\n",
    "            backend: Literal[\"torch\", \"numpy\"], The computational backend to use. Defaults to \"torch\".\n",
    "            dtype: type, The data type for the tensor values. Defaults to None (auto detection). \n",
    "                    For PyTorch, this corresponds to torch.dtype; for NumPy, it corresponds to np.dtype.\n",
    "            device: str | None, The target device (e.g., \"cpu\", \"cuda\") where the layer's parameters will be placed. \n",
    "                    If None, uses the default device. Defaults to None (auto detection).\n",
    "            autograd: bool, A flag indicating whether to use PyTorch's autograd for gradient computation. \n",
    "                    If True, manual gradient tracking is disabled. Defaults to False.\n",
    "\n",
    "        Attributes:\n",
    "            self.in_features: int, The number of input features for this layer.\n",
    "            self.hid_features: int, The number of output features for this layer.\n",
    "            self.has_bias: str, A flag indicating whether a bias term is included (\"True\" or \"False\").\n",
    "            self.init_scale: float, A floatting number indicating the maximum value of initial random weights.\n",
    "            self.actv: Literal[\"tanh\", \"relu\"], the activation type, either \"tanh\" or \"relu\".\n",
    "            self.tbptt_steps: int | None, the number of TBPTT size, or left None.\n",
    "            self.return_sequences: bool, indicator of whether returns full sequence of hidden or not.\n",
    "            self.return_state: bool, indicator of whether returns final state or not.\n",
    "            self.accumulate_hidden: bool, indicator of whether accumulate states or reinitialize when next forward.\n",
    "            self.gradient_clipping: float, The threshold for gradient clipping.\n",
    "            self.backend: Literal[\"torch\", \"numpy\"], The computational backend used by the layer.\n",
    "            self.dtype: type, The data type of the tensor values.\n",
    "            self.device: str | None, The device where the parameters are placed.\n",
    "            self.autograd: bool, Whether PyTorch's autograd is enabled for this layer.\n",
    "            self._parameters.weight_ih: nn_Parameter, input weights\n",
    "            self._parameters.bias_ih: nn_Parameter | None, input bias\n",
    "            self._parameters.weight_hh: nn_Parameter, hidden weights\n",
    "            self._parameters.bias_hh: nn_Parameter | None, hidden bias\n",
    "            self._cache: dict of list of tensors, for backward propagation\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__(module_name = module_name, backend = backend, dtype = dtype, device = device, autograd = autograd)\n",
    "        \n",
    "        # If actv is not in 'tanh' or 'relu', raise ValueError\n",
    "        if actv not in {'tanh', 'relu'}:\n",
    "            raise ValueError(f\"In initializing an RNNCell, you have to ensure `actv` in 'relu' or 'tanh', but you have {actv}\")\n",
    "        \n",
    "        # Record shapes etc\n",
    "        self.__setattr__(\"in_features\", in_features)\n",
    "        self.__setattr__(\"hid_features\", hid_features)\n",
    "        self.__setattr__(\"has_bias\", has_bias)\n",
    "        self.__setattr__(\"init_scale\", init_scale)\n",
    "        self.__setattr__(\"actv\", actv)\n",
    "        self.__setattr__(\"tbptt_steps\", tbptt_steps)\n",
    "        self.__setattr__(\"return_sequences\", return_sequences)\n",
    "        self.__setattr__(\"return_state\", return_state)\n",
    "        self.__setattr__(\"accumulate_hidden\", accumulate_hidden)\n",
    "        self.__setattr__(\"gradient_clipping\", gradient_clipping)\n",
    "        \n",
    "        # Cache: to store values needed for backward\n",
    "        self.__setattr__(\"_cache\", {})\n",
    "        \n",
    "        # Initialize weight and bias parameters\n",
    "        \n",
    "        # weight_ih: hidden_size * input_size\n",
    "        # weight_hh: hidden_size * hidden_size\n",
    "        self.__setattr__(\"weight_ih\", nn_Parameter(\n",
    "            Tensor.rand([hid_features, in_features], backend=backend, dtype=dtype, device=device) * init_scale,\n",
    "            requires_grad = True,\n",
    "            dtype = None,\n",
    "            device = None,\n",
    "            autograd = autograd)\n",
    "            )\n",
    "        self.__setattr__(\"weight_hh\", nn_Parameter(\n",
    "            Tensor.rand([hid_features, hid_features], backend=backend, dtype=dtype, device=device) * init_scale,\n",
    "            requires_grad = True,\n",
    "            dtype = None,\n",
    "            device = None,\n",
    "            autograd = autograd)\n",
    "            )\n",
    "        \n",
    "        if has_bias == True:\n",
    "            # If uses bias, then set the bias\n",
    "            self.__setattr__(\"bias_ih\", nn_Parameter(\n",
    "                Tensor.zeros([hid_features], backend=backend, dtype=dtype, device=device),\n",
    "                requires_grad = True,\n",
    "                dtype = None,\n",
    "                device = None,\n",
    "                autograd = autograd)\n",
    "                )\n",
    "            self.__setattr__(\"bias_hh\", nn_Parameter(\n",
    "                Tensor.zeros([hid_features], backend=backend, dtype=dtype, device=device),\n",
    "                requires_grad = True,\n",
    "                dtype = None,\n",
    "                device = None,\n",
    "                autograd = autograd)\n",
    "                )\n",
    "\n",
    "    def reset_hidden(self) -> None:\n",
    "        \"\"\"\n",
    "        Reset the stored hidden state if done TBPTT.\n",
    "        If invoked, when calling forward() next time, hiddden state will be starting from 0s.\n",
    "        \"\"\"\n",
    "        if self._cache.get('hidden_final', None) is not None:\n",
    "            self._cache.pop('hidden_final')\n",
    "\n",
    "    def forward(self, x: Tensor, h0: Tensor | None = None) -> Tensor| Tuple[Tensor, Tensor]:\n",
    "        \"\"\"\n",
    "        Compute the layer output: h_t = \\tanh(W_{ih} x_t + b_{ih} + W_{hh} h_{t-1} + b_{hh}) \\tag{1} and return it.\n",
    "        You can safely choose NOT to return hidden states since we have memorized them if you do not pass.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape (batch_size, seq_len, input_size)) to be transformed by the layer.\n",
    "            h0 (Tensor | None): initial hidden state, Tensor of shape (batch, hidden_size) or left None.\n",
    "                If left None, the net will automatically use the previous final hidden state stored in the cache.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output tensor of shape (batch_size, hid_features) after applying the RNN transformation.\n",
    "            or \n",
    "            A tuple of Tensors of 2 having outputs and the hidden state.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the input `x` is not a valid MML.Tensor object.\n",
    "            ValueError: If the input `h0` is given but not equal to the (batch_size, hid_features).\n",
    "\n",
    "        Attributes:\n",
    "            self.input (Tensor): The input tensor `x` saved for use in backward propagation.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Type check, x must be an instance of Tensor\n",
    "        if isinstance(x, Tensor) == False:\n",
    "            raise ValueError(f\"In performing forward(), input `x` must be in a MML `Tensor` format but you have {type(x)}\")\n",
    "        \n",
    "        # If h0 is given, it must have (batch_size, hid_features) size\n",
    "        if h0 is not None:\n",
    "            if isinstance(h0, Tensor) == False:\n",
    "                raise ValueError(f\"In performing forward(), input `h0` must be in a MML `Tensor` format but you have {type(h0)}\")\n",
    "            if h0.shape != (x.shape[0], self.hid_features):\n",
    "                raise ValueError(f\"In performing forward(), if given an input `h0`, it must equal to (batch_size, hidden_size) but you have {h0.shape}\")\n",
    "        # Else if it has been stored previously, reuse it\n",
    "        else:\n",
    "            if self._cache.get('hidden_final', None) is not None and self.accumulate_hidden == True:\n",
    "                h0 = self._cache['hidden_final']\n",
    "        \n",
    "        # Save input for backward\n",
    "        self.__setattr__(\"input\", x)\n",
    "        \n",
    "        # Rrcord the critical shapes\n",
    "        batch_size, seq_len, *_ = x.shape\n",
    "        \n",
    "        # Initialize hidden state\n",
    "        if h0 is None:\n",
    "            h = Tensor.zeros((batch_size, self.hid_features), backend=self.backend, dtype=self.dtype, device=self.device)\n",
    "        else:\n",
    "            h = h0\n",
    "            \n",
    "        # Cache lists for backward (to compute gradients)\n",
    "        window = self.tbptt_steps if (self.tbptt_steps is not None and self.tbptt_steps > 0) else seq_len\n",
    "        x_cache  = deque(maxlen=window)\n",
    "        h_lin_cache = deque(maxlen=window)\n",
    "        h_post_cache = deque(maxlen=window)\n",
    "            \n",
    "        # Prepare output list if return_sequences\n",
    "        outputs = [] if self.return_sequences == True else None\n",
    "        \n",
    "        # Forward propagate through time\n",
    "        for t in range(seq_len):\n",
    "            \n",
    "            # Crcord input at time t, shape (batch, input_size)\n",
    "            xt = x[:, t, ...]  \n",
    "            x_cache.append(xt)\n",
    "\n",
    "            # Compute h_lin = W_ih * x_t + W_hh * h_{t-1} + b_ih + b_hh\n",
    "            h_lin = (xt @ self._parameters[\"weight_ih\"].data.transpose()) + (h @ self._parameters[\"weight_hh\"].data.transpose())\n",
    "            if self.has_bias:\n",
    "                h_lin = h_lin + self._parameters[\"bias_ih\"].data\n",
    "                h_lin = h_lin + self._parameters[\"bias_hh\"].data\n",
    "\n",
    "            # Apply nonlinearity\n",
    "            if self.actv == 'relu':\n",
    "                h_new = h_lin.relu()\n",
    "            else:  # default 'tanh'\n",
    "                h_new = h_lin.tanh()\n",
    "\n",
    "            # Store pre-activation and post-activation hidden for backprop\n",
    "            h_lin_cache.append(h_lin)\n",
    "            h_post_cache.append(h_new)\n",
    "\n",
    "            # Append to outputs if needed\n",
    "            if self.return_sequences == True:\n",
    "                outputs.append(deepcopy(h_new))\n",
    "\n",
    "            # Prepare next step\n",
    "            h = h_new  # update hidden state\n",
    "\n",
    "            # TBPTT: Truncated Back Propagation Through Time\n",
    "            # if we've reached tbptt_steps, detach hidden state\n",
    "            if self.tbptt_steps is not None and self.tbptt_steps > 0:\n",
    "                \n",
    "                # Detach at the boundary of each segment\n",
    "                if (t + 1) % self.tbptt_steps == 0:\n",
    "                    # Detach the hidden state to prevent backprop beyond this point\n",
    "                    # It is only used for autograd purposes\n",
    "                    h = h.detach()  \n",
    "                    \n",
    "        # Save cache for backward\n",
    "        self._cache['x'] = x_cache\n",
    "        self._cache['h_lin'] = h_lin_cache\n",
    "        self._cache['h'] = h_post_cache\n",
    "        self._cache['seq_len'] = seq_len\n",
    "        self._cache['batch_size'] = batch_size\n",
    "        self._cache['hidden_final'] = h  \n",
    "        # h: final hidden state after last step (stored for continuation reuse)\n",
    "\n",
    "        # Prepare output data\n",
    "        if self.return_sequences == True:\n",
    "            # Stack outputs list to tensor: (batch, seq_len, hidden)\n",
    "            outputs_tensor = outputs[0].stack(*outputs[1:], axis=1)\n",
    "        else:\n",
    "            # Last output is the last hidden state\n",
    "            outputs_tensor = h\n",
    "\n",
    "        if self.return_state:\n",
    "            # Return output and final hidden state\n",
    "            return outputs_tensor, h\n",
    "        else:\n",
    "            # Return just the output tensor\n",
    "            return outputs_tensor\n",
    "    \n",
    "    def backward(self, grad_output: Tensor, grad_h: Tensor | None = None) -> Tensor | Tuple[Tensor, Tensor]:\n",
    "        \"\"\"\n",
    "        Backward pass: compute gradients for weight, bias, and input.\n",
    "\n",
    "        This method performs the gradient computation for a dense layer during backpropagation. \n",
    "        It calculates the gradients of the loss with respect to the weights, biases, and input tensor,\n",
    "        based on the provided `grad_output` (gradient from the next layer). The implementation\n",
    "        supports both PyTorch autograd and manual gradient calculation modes.\n",
    "\n",
    "        Args:\n",
    "            grad_output (Tensor): Gradient tensor resulting from the output of the layer, used as input for backpropagation.\n",
    "                     - If return_sequences=True, shape is (batch, seq_len, hidden_size)\n",
    "                     - If return_sequences=False, shape is (batch, hidden_size) for last output.\n",
    "            grad_h (Tensor | None): Gradient of loss w.r.t. the final hidden state.\n",
    "                     - This is usually None unless the RNN's final state is fed into another layer.\n",
    "\n",
    "        Returns:\n",
    "            Tuple of \n",
    "                (Tensor: Gradient with respect to the input tensor, for recursive backward calculations in previous layers, shape (batch, seq_len, input_size) tensor.\n",
    "                 Tensor: Gradient with respect to the final hidden state, ∂L/∂h0.\n",
    "                 )\n",
    "        Raises:\n",
    "            ValueError: If `grad_output` is not a valid MML.Tensor object.\n",
    "            ValueError: If `grad_h` is given but not a valid MML.Tensor object.\n",
    "        \"\"\"\n",
    "        \n",
    "        # If use autograd, pass; if manual mode, then calculate\n",
    "        if self.autograd == True:\n",
    "            return None\n",
    "        \n",
    "        # Type check, grad_output must be an instance of Tensor\n",
    "        if isinstance(grad_output, Tensor) == False:\n",
    "            raise ValueError(f\"In performing backward(), `grad_output` must be in a MML `Tensor` format but you have {type(grad_output)}\")\n",
    "        \n",
    "        # Type check, if given, grad_h must be an instance of Tensor\n",
    "        if grad_h is not None:\n",
    "            if isinstance(grad_h, Tensor) == False:\n",
    "                raise ValueError(f\"In performing backward(), if given `grad_h`, it must be in a MML `Tensor` format but you have {type(grad_h)}\")\n",
    "        \n",
    "        # Retrieve cached values\n",
    "        x_cache = self._cache.get('x')\n",
    "        h_lin_cache = self._cache.get('h_lin')\n",
    "        h_post_cache = self._cache.get('h')\n",
    "        seq_len = self._cache.get('seq_len')\n",
    "        batch_size = self._cache.get('batch_size')\n",
    "        \n",
    "        # storage for dL/dx_t\n",
    "        grad_x_all = Tensor.zeros((batch_size, seq_len, self.in_features), backend=self.backend, dtype=self.dtype, device=self.device)\n",
    "\n",
    "        # Initialize gradient for hidden state (dL/dh) at step t (post-activation)\n",
    "        # Start from final output gradient.\n",
    "        if self.return_sequences == True:\n",
    "            # If the output was the full sequence, grad_output is a tensor of shape (batch, seq_len, hidden)\n",
    "            # We will accumulate gradient from each time step's output.\n",
    "            # Start with zero grad for final hidden; contributions will come from grad_output at each step.\n",
    "            grad_h_t = Tensor.zeros((batch_size, self.hid_features), backend=self.backend, dtype=self.dtype, device=self.device)\n",
    "        else:\n",
    "            # If output was last hidden only, grad_output is dL/dh_final\n",
    "            grad_h_t = grad_output.clone()\n",
    "            grad_output = None  # this is not used for per-step outputs in this case\n",
    "\n",
    "        # If an external grad_h is provided, add it\n",
    "        if grad_h is not None:\n",
    "            grad_h_t += grad_h\n",
    "        \n",
    "        # maximum number of steps to walk back\n",
    "        max_steps = self.tbptt_steps if (self.tbptt_steps and self.tbptt_steps > 0) else seq_len\n",
    "        \n",
    "        # Truncated Backpropagate Through Time (TBPTT)\n",
    "        # We will iterate backwards through each time step.\n",
    "        # If return_sequences, include grad from that step's output as well.\n",
    "        steps_done = 0\n",
    "        for t in range(seq_len - 1, -1, -1):\n",
    "            \n",
    "            if steps_done >= max_steps:\n",
    "                break\n",
    "            steps_done += 1\n",
    "            \n",
    "            # If return_sequences, accumulate gradient from output at time t\n",
    "            if self.return_sequences == True:\n",
    "                # grad_output[:, t, ...] is gradient of loss w.r.t output at time t\n",
    "                grad_h_t += grad_output[:, t, ...]\n",
    "\n",
    "            # h_post_cache[t] is h_t (post-nonlinearity), h_lin_cache[t] is pre-nonlinearity\n",
    "            # grad_h_t is dL/dh_t (gradient wrt post-activation hidden state at time t)\n",
    "            xt = x_cache[t]\n",
    "            h_lin = h_lin_cache[t]\n",
    "            h_post = h_post_cache[t]\n",
    "\n",
    "            # Apply activation derivative\n",
    "            if self.actv == 'relu':\n",
    "                # derivative of ReLU: 1 for h_lin > 0, else 0\n",
    "                grad_h_lin = grad_h_t * Tensor.where_as(h_lin.data > 0, 1.0, 0.0, backend=self.backend, dtype=self.dtype, device=self.device)\n",
    "            else:\n",
    "                # derivative of tanh: (1 - tanh^2) = 1 - h_post^2\n",
    "                grad_h_lin = grad_h_t * (1.0 - h_lin.tanh() ** 2)\n",
    "\n",
    "            # Now grad_h_lin is dL/d(h_lin) which is dL/d(W_ih x + W_hh h_prev + b)\n",
    "            self._parameters[\"weight_ih\"].grad += grad_h_lin.transpose() @ xt\n",
    "            \n",
    "            # Grad w.rt bias_ih: just sum grad_h_lin over batch\n",
    "            if self.has_bias == True:\n",
    "                self._parameters[\"bias_ih\"].grad += grad_h_lin.sum(axis=0)\n",
    "            \n",
    "            # Grad w.rt weight_hh: uses previous hidden state (post-activation) at t-1 (or h0 for t=0)\n",
    "            if t == 0:\n",
    "                # previous hidden is either provided h0 or zero (we treat it as constant, so no gradient beyond it)\n",
    "                h_prev = Tensor.zeros_like(h_post, backend=self.backend, dtype=self.dtype, device=self.device)  \n",
    "            else:\n",
    "                h_prev = h_post_cache[t-1] \n",
    "            \n",
    "            # Now grad_h_lin is dL/d(h_lin) which is dL/d(W_hh x + W_hh h_prev + b)\n",
    "            self._parameters[\"weight_hh\"].grad += grad_h_lin.transpose() @ h_prev\n",
    "            \n",
    "            # Grad w.rt bias_hh: just sum grad_h_lin over batch\n",
    "            if self.has_bias == True:\n",
    "                self._parameters[\"bias_hh\"].grad += grad_h_lin.sum(axis=0)\n",
    "\n",
    "            # Gradients w.r.t. inputs and h_{t-1} grad_x_t = grad_h_lin * W_ih (matrix multiply)\n",
    "            grad_x_all[:, t, ...] = grad_h_lin @ self._parameters[\"weight_ih\"].data\n",
    "\n",
    "            # grad_h_prev (dL/dh_{t-1}, post-activation) = grad_h_lin * W_hh\n",
    "            grad_h_prev = grad_h_lin @ self._parameters[\"weight_hh\"].data\n",
    "            \n",
    "            # Set grad_h_t for next iteration (previous time step)\n",
    "            grad_h_t = grad_h_prev\n",
    "            \n",
    "        # After accumulating all gradients\n",
    "        def _clip_gradients_inplace(params: nn_Parameter, max_norm: float) -> None:\n",
    "            \n",
    "            # Gather all gradient Tensors from nn_Parameter\n",
    "            grads = [p.grad for p in params if p is not None and p.grad is not None]\n",
    "            # List\n",
    "            \n",
    "            # Compute squared norms of each gradient and sum them\n",
    "            total_norm_sq = sum(((g ** 2).sum()) for g in grads)\n",
    "            # Tensor\n",
    "            \n",
    "            # Take the square root for the global norm\n",
    "            total_norm = total_norm_sq ** 0.5\n",
    "            \n",
    "            # If outside threshold, scale each gradient in place\n",
    "            if total_norm.to_list() > max_norm:\n",
    "                scale = max_norm / (total_norm + 1e-12)\n",
    "                for g in grads:\n",
    "                    g *= scale\n",
    "                    \n",
    "            return\n",
    "                    \n",
    "        # If self.gradient_clipping is not None, then perform gradient clipping\n",
    "        if self.gradient_clipping is not None and self.gradient_clipping > 0:\n",
    "            parameters = [self._parameters[\"weight_ih\"], self._parameters[\"weight_hh\"], self._parameters[\"bias_ih\"], self._parameters[\"bias_hh\"]]\n",
    "            _clip_gradients_inplace(parameters, self.gradient_clipping)\n",
    "        \n",
    "        return grad_x_all, grad_h_t\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"nn_Layer_RNNCell(shape: ({self.in_features}, {self.hid_features}) with{'out' if self.has_bias == False else ''} bias).\"\n",
    "    \n",
    "    \n",
    "# Implementation of a Stacked RNN Layer using TBPTT  \n",
    "class nn_Layer_StackedRNN(nn_Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    Stacked RNN Implementation\n",
    "    \n",
    "    This class implements a stacked Recurrent Neural Network (RNN) layer, \n",
    "    designed to process sequential data by maintaining and updating a hidden state \n",
    "    at each time step. It contains weight matrices and bias vectors stored \n",
    "    in nn_Parameter containers, enabling the computation of forward passes through \n",
    "    sequences while supporting gradient-based learning via backward passes. \n",
    "    \n",
    "    Note, TBPTT should be implemented externally with segmentation and carrying (manually reset_hidden).\n",
    "    \"\"\"\n",
    "\n",
    "    __attr__ = \"MML.nn_Layer_StackedRNN\"   \n",
    "\n",
    "    def __init__(self, \n",
    "                 in_features: int = 1, \n",
    "                 hid_features: int = 1, \n",
    "                 num_layers: int = 1,\n",
    "                 has_bias: str = True,\n",
    "                 init_scale: float = 0.1,\n",
    "                 actv: Literal['tanh', 'relu'] = 'tanh', \n",
    "                 tbptt_steps: int | None = None,\n",
    "                 return_sequences: bool = False, \n",
    "                 return_state: bool = False,\n",
    "                 accumulate_hidden: bool = False,\n",
    "                 gradient_clipping: float = 5.0,\n",
    "                 *,\n",
    "                 module_name: str = \"nn_Layer_StackedRNN\", \n",
    "                 backend: Literal[\"torch\", \"numpy\"] = \"torch\",\n",
    "                 dtype: type | str = None,\n",
    "                 device: str | None = None,\n",
    "                 autograd: bool = False,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        A Stacked RNN (Recurrent Neural Network) cell processes sequences by maintaining\n",
    "            a hidden state that is updated at each time step.\n",
    "            \n",
    "        Structure: h_t = \\tanh(W_{ih} x_t + b_{ih} + W_{hh} h_{t-1} + b_{hh}) \\tag{1}\n",
    "        \n",
    "        Parameters:\n",
    "            in_features: int, The number of input features for this layer. Defaults to 1.\n",
    "            hid_features: int, The number of hidden features for this layer. Defaults to 1.\n",
    "            num_layers: int, The number of RNN cells stacked together (h(n-1) becomes x in stack n).\n",
    "            has_bias: str, A flag indicating whether to include a bias term. Valid values are \"True\" or \"False\".\n",
    "                    If set to \"True\", the layer includes an additive bias term (b). Defaults to \"True\".\n",
    "            actv: str, activation type, must be in 'tanh' and 'relu'. Defaults to 'tanh'.\n",
    "            tbptt_steps: int | None, the number of steps performing Truncated BPTT (TBPTT), \n",
    "                    if None, then use true BPTT. Defaults: None.\n",
    "                    Note, TBPTT is not fully implemented. Please use BPTT instead.\n",
    "            return_sequences: bool, should the layers return only the final hidden state, or the full sequence \n",
    "                    of hidden states as well. Defaults to False.\n",
    "            return_state: bool, should the layers return output and final hidden state. Defaults to False.\n",
    "            accumulate_hidden: bool, should the network accumulate hidden state (when processing segments) or clear it before next forward.\n",
    "            gradient_clipping: float, The factor of performing gradient clipping to avoid gradient expolosion. Defaults to 5.0.\n",
    "            module_name: str, The name of the module instance. Defaults to \"nn_Layer_StackedRNN\".\n",
    "            backend: Literal[\"torch\", \"numpy\"], The computational backend to use. Defaults to \"torch\".\n",
    "            dtype: type, The data type for the tensor values. Defaults to None (auto detection). \n",
    "                    For PyTorch, this corresponds to torch.dtype; for NumPy, it corresponds to np.dtype.\n",
    "            device: str | None, The target device (e.g., \"cpu\", \"cuda\") where the layer's parameters will be placed. \n",
    "                    If None, uses the default device. Defaults to None (auto detection).\n",
    "            autograd: bool, A flag indicating whether to use PyTorch's autograd for gradient computation. \n",
    "                    If True, manual gradient tracking is disabled. Defaults to False.\n",
    "\n",
    "        Attributes:\n",
    "            self.in_features: int, The number of input features for this layer.\n",
    "            self.hid_features: int, The number of output features for this layer.\n",
    "            self.num_layers: int, The number of stacked RNN cells for this layer.\n",
    "            self.has_bias: str, A flag indicating whether a bias term is included (\"True\" or \"False\").\n",
    "            self.init_scale: float, A floatting number indicating the maximum value of initial random weights.\n",
    "            self.actv: Literal[\"tanh\", \"relu\"], the activation type, either \"tanh\" or \"relu\".\n",
    "            self.tbptt_steps: int | None, the number of TBPTT size, or left None.\n",
    "            self.return_sequences: bool, indicator of whether returns full sequence of hidden or not.\n",
    "            self.return_state: bool, indicator of whether returns final state or not.\n",
    "            self.accumulate_hidden: bool, indicator of whether accumulate states or reinitialize when next forward.\n",
    "            self.gradient_clipping: float, The threshold for gradient clipping.\n",
    "            self.backend: Literal[\"torch\", \"numpy\"], The computational backend used by the layer.\n",
    "            self.dtype: type, The data type of the tensor values.\n",
    "            self.device: str | None, The device where the parameters are placed.\n",
    "            self.autograd: bool, Whether PyTorch's autograd is enabled for this layer.\n",
    "            self._modules[...]: dict, a dict of stacked RNN cells sequentially.\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__(module_name = module_name, backend = backend, dtype = dtype, device = device, autograd = autograd)\n",
    "        \n",
    "        # If actv is not in 'tanh' or 'relu', raise ValueError\n",
    "        if actv not in {'tanh', 'relu'}:\n",
    "            raise ValueError(f\"In initializing a Stacked RNN, you have to ensure `actv` in 'relu' or 'tanh', but you have {actv}\")\n",
    "        \n",
    "        # Record shapes etc\n",
    "        self.__setattr__(\"in_features\", in_features)\n",
    "        self.__setattr__(\"hid_features\", hid_features)\n",
    "        self.__setattr__(\"num_layers\", num_layers)\n",
    "        self.__setattr__(\"has_bias\", has_bias)\n",
    "        self.__setattr__(\"init_scale\", init_scale)\n",
    "        self.__setattr__(\"actv\", actv)\n",
    "        self.__setattr__(\"tbptt_steps\", tbptt_steps)\n",
    "        self.__setattr__(\"return_sequences\", return_sequences)\n",
    "        self.__setattr__(\"return_state\", return_state)\n",
    "        self.__setattr__(\"accumulate_hidden\", accumulate_hidden)\n",
    "        self.__setattr__(\"gradient_clipping\", gradient_clipping)\n",
    "        \n",
    "        # Initialize stacked RNN components, each returning full sequence + its final state\n",
    "        for i in range(num_layers):\n",
    "            self.__setattr__(\"RNN_Layer_\"+str(i), \n",
    "                             nn_Layer_RNNCell(\n",
    "                                 in_features=in_features if i == 0 else hid_features,\n",
    "                                 hid_features=hid_features,\n",
    "                                 has_bias=has_bias,\n",
    "                                 init_scale=init_scale,\n",
    "                                 actv=actv,\n",
    "                                 tbptt_steps=tbptt_steps,\n",
    "                                 return_sequences=True,\n",
    "                                 return_state=True,\n",
    "                                 accumulate_hidden=accumulate_hidden,\n",
    "                                 gradient_clipping=gradient_clipping,\n",
    "                                 module_name=module_name+\"_\"+\"RNN_Layer_\"+str(i),\n",
    "                                 backend=backend,\n",
    "                                 dtype=dtype,\n",
    "                                 device=device,\n",
    "                                 autograd=autograd,\n",
    "                                 **kwargs\n",
    "                                 )\n",
    "                             )\n",
    "\n",
    "    def reset_hidden(self) -> None:\n",
    "        \"\"\"\n",
    "        Reset the stored hidden state if done TBPTT.\n",
    "        If invoked, when calling forward() next time, hiddden state will be starting from 0s.\n",
    "        \"\"\"\n",
    "        for k in self._modules.keys():\n",
    "            self._modules[k].reset_hidden()\n",
    "\n",
    "    def forward(self, x: Tensor, h0: List[Tensor] | None = None) -> Tensor| Tuple[Tensor, List[Tensor]]:\n",
    "        \"\"\"\n",
    "        Compute the layer output: h_t = \\tanh(W_{ih} x_t + b_{ih} + W_{hh} h_{t-1} + b_{hh}) \\tag{1} and return it.\n",
    "        You can safely choose NOT to return hidden states since we have memorized them if you do not pass.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape (batch_size, seq_len, input_size)) to be transformed by the layer.\n",
    "            h0 (List[Tensor] | None): initial hidden state, List of Tensors of shape (batch, hidden_size) or left None.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Output tensor of shape (batch_size, hid_features) after applying the RNN transformation.\n",
    "            or \n",
    "            A tuple of 2 having outputs and the List of Tensors containing hidden state.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the input `x` is not a valid MML.Tensor object.\n",
    "            ValueError: If the input `h0` is given but not a list of Tensors.\n",
    "\n",
    "        \"\"\"\n",
    "        # Type check, x must be an instance of Tensor\n",
    "        if isinstance(x, Tensor) == False:\n",
    "            raise ValueError(f\"In performing forward(), input `x` must be in a MML `Tensor` format but you have {type(x)}\")\n",
    "        \n",
    "        # If h0 is given, it must have (batch_size, hid_features) size\n",
    "        if h0 is not None:\n",
    "            if isinstance(h0, list) == False:\n",
    "                raise ValueError(f\"In performing forward(), input `h0` must be in a List of MML `Tensor` format but you have {type(h0)}\")\n",
    "            if isinstance(h0[0], Tensor) == False:\n",
    "                raise ValueError(f\"In performing forward(), if given an input `h0`, it must be a list of MML `Tensor`s but you have {type(h0[0])}\")\n",
    "            \n",
    "        # Prepare output and final states\n",
    "        output = x\n",
    "        final_states = []\n",
    "\n",
    "        # Perform RNN Layer by layer\n",
    "        for idx, key in enumerate(self._modules.keys()):\n",
    "            h0 = h0[idx] if (h0 is not None) else None\n",
    "            # Perform a forward pass for each layer sequentially\n",
    "            output, h_n = self._modules[key].forward(output, h0)\n",
    "            final_states.append(h_n)\n",
    "            \n",
    "        # if only last step is wanted, slice it off the sequence\n",
    "        if self.return_sequences == False:\n",
    "            # (batch, hidden_size)\n",
    "            output = output[:, -1, ...] \n",
    "            \n",
    "        # If return state, then return a list of states\n",
    "        if self.return_state:\n",
    "            return output, final_states\n",
    "        # Otherwise, only the output\n",
    "        else:\n",
    "            return output\n",
    "        \n",
    "    def backward(self, grad_output: Tensor, grad_h: List[Tensor] | None = None) -> Tensor | Tuple[Tensor, List[Tensor]]:\n",
    "        \"\"\"\n",
    "        Backward pass: compute gradients for weight, bias, and input.\n",
    "\n",
    "        This method performs the gradient computation for a dense layer during backpropagation. \n",
    "        It calculates the gradients of the loss with respect to the weights, biases, and input tensor,\n",
    "        based on the provided `grad_output` (gradient from the next layer). The implementation\n",
    "        supports both PyTorch autograd and manual gradient calculation modes.\n",
    "\n",
    "        Args:\n",
    "            grad_output (Tensor): Gradient tensor resulting from the output of the layer, used as input for backpropagation.\n",
    "                     - If return_sequences=True, shape is (batch, seq_len, hidden_size)\n",
    "                     - If return_sequences=False, shape is (batch, hidden_size) for last output.\n",
    "            grad_h (List[Tensor] | None): Gradient of loss w.r.t. the final hidden state.\n",
    "                     - This is usually None unless the RNN's final state is fed into another layer.\n",
    "                     - If used, it is List of Tensors with shape (batch, hidden_size)\n",
    "        Returns:\n",
    "            Tuple of \n",
    "                (Tensor: Gradient with respect to the input tensor, for recursive backward calculations in previous layers, shape (batch, seq_len, input_size) tensor.\n",
    "                 List[Tensor]: Gradients with respect to each final hidden state, List[∂L/∂h0] (batch, hidden).\n",
    "                 )\n",
    "        Raises:\n",
    "            ValueError: If `grad_output` is not a valid MML.Tensor object.\n",
    "            ValueError: If `grad_h` is given but not a valid list of MML.Tensor object.\n",
    "        \"\"\"\n",
    "        \n",
    "        # If use autograd, pass; if manual mode, then calculate\n",
    "        if self.autograd == True:\n",
    "            return None\n",
    "        \n",
    "        # Type check, grad_output must be an instance of Tensor\n",
    "        if isinstance(grad_output, Tensor) == False:\n",
    "            raise ValueError(f\"In performing backward(), `grad_output` must be in a MML `Tensor` format but you have {type(grad_output)}\")\n",
    "        \n",
    "        # Type check, if given, grad_h must be an instance of Tensor\n",
    "        if grad_h is not None:\n",
    "            if isinstance(grad_h, list) == False:\n",
    "                raise ValueError(f\"In performing backward(), if given `grad_h`, it must be in a List of MML `Tensor`s format but you have {type(grad_h)} with each {type(grad_h[0])}\")\n",
    "            if isinstance(grad_h[0], Tensor) == False:\n",
    "                raise ValueError(f\"In performing backward(), if given `grad_h`, it must be in a List of MML `Tensor`s format but you have {type(grad_h)} with each {type(grad_h[0])}\")\n",
    "        \n",
    "        # prepare per-layer state gradients\n",
    "        if grad_h is None:\n",
    "            grad_h = [None] * self.num_layers\n",
    "            \n",
    "        # grad flowing into top layer's output\n",
    "        next_grad_out = grad_output\n",
    "        grad_h0_list = [None] * self.num_layers\n",
    "\n",
    "        # walk backward through layers\n",
    "        keys = list(self._modules.keys())\n",
    "        for i in reversed(range(self.num_layers)):\n",
    "            layer     = self._modules[keys[i]]\n",
    "            grad_h_n  = grad_h[0]\n",
    "            grad_out  = next_grad_out\n",
    "\n",
    "            # if only last-step output was used externally, expand it\n",
    "            if self.return_sequences == False and i == self.num_layers - 1:\n",
    "                seq_len = layer._cache[\"seq_len\"]\n",
    "                batch_size = layer._cache[\"batch_size\"]\n",
    "                hidden = layer.hid_features\n",
    "                full = Tensor.zeros((batch_size, seq_len, hidden), backend=self.backend, dtype=self.dtype, device=self.device)\n",
    "                full[:, -1, ...] = grad_out\n",
    "                grad_out = full\n",
    "\n",
    "            # call sub-layer backward (must return two)\n",
    "            grad_x, grad_h0 = layer.backward(grad_out, grad_h_n)\n",
    "\n",
    "            grad_h0_list[i] = grad_h0\n",
    "            next_grad_out = grad_x\n",
    "\n",
    "        # next_grad_out is gradient w.rt the very input x\n",
    "        # grad_h0_list is gradient w.rt the hidden state of each layer\n",
    "        return next_grad_out, grad_h0_list        \n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"nn_Layer_StackedRNN(shape: ({self.in_features}, {self.hid_features}), num_layers: {self.num_layers} with{'out' if self.has_bias == False else ''} bias).\"\n",
    "  \n",
    "  \n",
    "# Alias for nn_Layer_StackedRNN\n",
    "StackedRNN = nn_Layer_StackedRNN\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Evauator is self implemented and open-sourced\n",
    "# Available at https://github.com/dof-studio/MML/\n",
    "# By Nathmath Huang (bh2821)\n",
    "# License: Apache License Version 2.0\n",
    "\n",
    "# Neural Network Fast Evaluation Pipeline\n",
    "class nn_SInterf_Evaluator(nn_Base, Regression, Classification):\n",
    "    \"\"\"\n",
    "    Neural Network Simple Interface - Evaluation pipeline.\n",
    "    \n",
    "    This evaluator accepts a neural network module, a ctiterion module (loss function),\n",
    "    an optimizer instance and conduct controlled automatic training and evaluation job.\n",
    "    You may use fit(), predict(), or other APIs to experience an easy-to-use and optimized\n",
    "    neural network training process with full automation loaded.\n",
    "    \"\"\"\n",
    "    \n",
    "    __attr__ = \"MML.nn_SInterf_Evaluator\"\n",
    "    \n",
    "    def __init__(self, name: str = \"Evaluator\",\n",
    "                       task: str = \"classification\",\n",
    "                       module: nn_Module | None = None,\n",
    "                       criterion: nn_Loss_BaseLoss | None = None,\n",
    "                       optimizer: nn_Optm_BaseOptimizer | None = None,\n",
    "                       **kwargs) -> None:\n",
    "        \"\"\"\n",
    "        Initialize an easy-interface evaluator pipeline object by passing in modules.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "            task: str, one of {\"classification\", \"regression\"}, showing the learning task.\n",
    "            module: nn_Module | None, you must pass an nn_Module which is the root node of your neural network structure.\n",
    "            criterion: nn_Loss_BaseLoss, you must pass an instance of a loss function that is the child of the base class.\n",
    "            optimizer: nn_Optm_BaseOptimizer, you must pass an instance of an optimizer that is the child of the base class.\n",
    "            Optional:\n",
    "                **kwargs: other key word arguments, reserved for compatibility use.\n",
    "            \n",
    "        Raise\n",
    "        ----------\n",
    "            ValueError, if task it not valid.\n",
    "            TypeError, if any of the parameter is None or does not have the correct type.\n",
    "        \"\"\"\n",
    "        # Task check\n",
    "        if task not in {\"classification\", \"regression\"}:\n",
    "            raise ValueError(f\"In initializing an evaluator, `task` must be either 'classification' or 'regression', but got {task}\")\n",
    "\n",
    "        # Type check (must be the type specified but not None or others)\n",
    "        if module is not None and not isinstance(module, nn_Module):\n",
    "            raise TypeError(\"In initializing an evaluator, `module` must be an instance of nn_Module\")\n",
    "        elif module is None:\n",
    "            raise TypeError(\"In initializing an evaluator, `module` must be initialized and cannot be None\")\n",
    "    \n",
    "        if criterion is not None and not isinstance(criterion, nn_Loss_BaseLoss):\n",
    "            raise TypeError(\"In initializing an evaluator, `criterion` must be an instance of nn_Loss_BaseLoss\")\n",
    "        elif criterion is None:\n",
    "            raise TypeError(\"In initializing an evaluator, `criterion` must be initialized and cannot be None\")\n",
    "        \n",
    "        if optimizer is not None and not isinstance(optimizer, nn_Optm_BaseOptimizer):\n",
    "            raise TypeError(\"In initializing an evaluator, `optimizer` must be an instance of nn_Optm_BaseOptimizer\")\n",
    "        elif optimizer is None:\n",
    "            raise TypeError(\"In initializing an evaluator, `optimizer` must be initialized and cannot be None\")\n",
    "            \n",
    "        # Call the nn_Base to keep the format consistent\n",
    "        super().__init__()\n",
    "        \n",
    "        # Record name, task, module, criterion, optimizer\n",
    "        self.name = name\n",
    "        self.task = task\n",
    "        self.module = module\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        # Create a reference of reference_X and reference_y WITHOUT COPY\n",
    "        self.reference_X = None   # NO COPY\n",
    "        self.reference_y = None   # ON COPY\n",
    "        \n",
    "        # Create a recoder of batch_size\n",
    "        self.batch_size = None\n",
    "        \n",
    "        # Create a counter of how may epoches and steps trainned\n",
    "        self.n_epoch = 0\n",
    "        self.n_step = 0\n",
    "        \n",
    "        # Create a dictionary to collect training loss and evaluation information (if any)\n",
    "        self.losses_ = {}    # Stepwise, index: step number\n",
    "        self.evalhist_ = {}  # Some_epoch-wise, index: epoch number\n",
    "        \n",
    "        # Create a record of random state\n",
    "        self.random_state = None\n",
    "        \n",
    "        # Create a record of to_device which means we need to redevice the data before training/testing\n",
    "        self.to_device = None\n",
    "        \n",
    "    def _fit_epoch_prep(self, X: Tensor, y: Tensor,\n",
    "                        batch_size: int | None = None,\n",
    "                        shuffle: bool = True, \n",
    "                        random_state: int | None = None, \n",
    "                        **kwargs) -> Tuple[Tensor, Tensor]:\n",
    "        \"\"\"\n",
    "        Prepare datasets and calculate initial values (for regression tasks and classification tasks).\n",
    "\n",
    "        Parameters:\n",
    "            ----------\n",
    "            X: Tensor, the feature tensor (the 1st dimension is sample).\n",
    "            y: Tensor, the target values (for regression, numerical; for classification, one-hot or multi-label).\n",
    "            batch_size: int, the number of samples trained each time. Must be greater than 1. If None, then use all.\n",
    "            shuffle: bool, whether data will be shuffled for each round (same device same type). By default, True (if batch_size is None then omitted).\n",
    "            random_seed: int | None, the random seed set to perform shuffle, can be None which means to randomly choose one.\n",
    "\n",
    "        Returns:\n",
    "            ----------\n",
    "            Tuple(X, y) shuffled copy or original reference\n",
    "        \"\"\"\n",
    "        \n",
    "        # We don't conduct type checks but checks if X or y are None\n",
    "        if X is None or y is None:\n",
    "            raise ValueError(\"In _fit_epoch_prep(), input `X` or `y` is/are None-type.\")\n",
    "        \n",
    "        # If no need to shuffle, then JUST RETURN without shuffling and copying\n",
    "        if shuffle == False or batch_size is None:\n",
    "            return X,y\n",
    "        elif batch_size >= X.shape[0]:\n",
    "            return X,y\n",
    "        \n",
    "        # Else, we shuffle based on the seed\n",
    "        if random_state is not None:\n",
    "            np.random.seed(random_state)\n",
    "        idx = list(range(X.shape[0]))\n",
    "        np.random.shuffle(idx)\n",
    "        \n",
    "        return X[idx], y[idx]\n",
    "     \n",
    "    def _fit_slice_batch(self, X: Tensor, y: Tensor,\n",
    "                         start: int | None = None,\n",
    "                         batch_size: int | None = None,\n",
    "                         to_device: str | None = None,\n",
    "                         **kwargs) -> Tuple[Tensor, Tensor]:\n",
    "        \"\"\"\n",
    "        Slice the input to create one mini-batch for training/testing.\n",
    "        \n",
    "        Parameters:\n",
    "            ----------\n",
    "            X: Tensor, the feature tensor (the 1st dimension is sample).\n",
    "            y: Tensor, the target values (for regression, numerical; for classification, one-hot or multi-label).\n",
    "            start: int | None, the starting index (begining) to be sliced for this round.\n",
    "            batch_size: int | None, the number of samples trained each time. Must be greater than 1. If None, then use all.\n",
    "            to_device: str | None, if non-None, we will perform device transformation after slicing them.\n",
    "            \n",
    "        Returns:\n",
    "            ----------\n",
    "            Tuple(X, y) sliced copy or original reference.\n",
    "        \"\"\"\n",
    "        \n",
    "        # We don't conduct type checks but checks if X or y are None\n",
    "        if X is None or y is None:\n",
    "            raise ValueError(\"In _fit_slice_batch(), input `X` or `y` is/are None-type.\")\n",
    "            \n",
    "        # If no need to slice, then JUST RETURN without shuffling and copying\n",
    "        if batch_size is None and start is None:\n",
    "            if to_device is None:\n",
    "                return X,y\n",
    "            else:\n",
    "                return X.to(backend=X._backend, dtype=X.dtype, device=to_device), y.to(backend=y._backend, dtype=y.dtype, device=to_device)\n",
    "        elif batch_size >= X.shape[0] and start is None:\n",
    "            if to_device is None:\n",
    "                return X,y\n",
    "            else:\n",
    "                return X.to(backend=X._backend, dtype=X.dtype, device=to_device), y.to(backend=y._backend, dtype=y.dtype, device=to_device)\n",
    "        \n",
    "        # Then, we need to slice.\n",
    "        if start is None:\n",
    "            raise ValueError(\"In _fit_slice_batch(), input `start` is None while a small batch_size is specified\")\n",
    "        if batch_size is None:\n",
    "            raise ValueError(\"In _fit_slice_batch(), input `batch_size` is None while a start is specified\")\n",
    "        if start >= X.shape[0]:\n",
    "            raise ValueError(f\"In _fit_slice_batch(), input `start` {start} is greater than the number of samples in `X`\")\n",
    "        \n",
    "        # Slice and return a copy.\n",
    "        if start + batch_size <= X.shape[0]:\n",
    "            idx = list(range(start, start + batch_size))\n",
    "        else:\n",
    "            idx = list(range(start, X.shape[0]))\n",
    "        if to_device is None:\n",
    "            return X[idx],y[idx]\n",
    "        else:\n",
    "            return X[idx].to(backend=X._backend, dtype=X.dtype, device=to_device), y[idx].to(backend=y._backend, dtype=y.dtype, device=to_device)\n",
    "    \n",
    "    def _fit_slice_batch_X(self, X: Tensor,\n",
    "                         start: int | None = None,\n",
    "                         batch_size: int | None = None,\n",
    "                         to_device: str | None = None,\n",
    "                         **kwargs) -> Tensor:\n",
    "        \"\"\"\n",
    "        Slice the input to create one mini-batch for training/testing.\n",
    "        \n",
    "        Parameters:\n",
    "            ----------\n",
    "            X: Tensor, the feature tensor (the 1st dimension is sample).\n",
    "            y: Tensor, the target values (for regression, numerical; for classification, one-hot or multi-label).\n",
    "            start: int | None, the starting index (begining) to be sliced for this round.\n",
    "            batch_size: int | None, the number of samples trained each time. Must be greater than 1. If None, then use all.\n",
    "            to_device: str | None, if non-None, we will perform device transformation after slicing them.\n",
    "            \n",
    "        Returns:\n",
    "            ----------\n",
    "            X sliced copy or original reference.\n",
    "        \"\"\"\n",
    "        \n",
    "        # We don't conduct type checks but checks if X or y are None\n",
    "        if X is None:\n",
    "            raise ValueError(\"In _fit_slice_batch_X(), input `X` is/are None-type.\")\n",
    "            \n",
    "        # If no need to slice, then JUST RETURN without shuffling and copying\n",
    "        if batch_size is None and start is None:\n",
    "            if to_device is None:\n",
    "                return X\n",
    "            else:\n",
    "                return X.to(backend=X._backend, dtype=X.dtype, device=to_device)\n",
    "        elif batch_size >= X.shape[0] and start is None:\n",
    "            if to_device is None:\n",
    "                return X\n",
    "            else:\n",
    "                return X.to(backend=X._backend, dtype=X.dtype, device=to_device)\n",
    "        \n",
    "        # Then, we need to slice.\n",
    "        if start is None:\n",
    "            raise ValueError(\"In _fit_slice_batch(), input `start` is None while a small batch_size is specified\")\n",
    "        if batch_size is None:\n",
    "            raise ValueError(\"In _fit_slice_batch(), input `batch_size` is None while a start is specified\")\n",
    "        if start >= X.shape[0]:\n",
    "            raise ValueError(f\"In _fit_slice_batch(), input `start` {start} is greater than the number of samples in `X`\")\n",
    "        \n",
    "        # Slice and return a copy.\n",
    "        if start + batch_size <= X.shape[0]:\n",
    "            idx = list(range(start, start + batch_size))\n",
    "        else:\n",
    "            idx = list(range(start, X.shape[0]))\n",
    "        if to_device is None:\n",
    "            return X[idx]\n",
    "        else:\n",
    "            return X[idx]    \n",
    "    \n",
    "    def _fit_train_one_step(self, X: Tensor, y: Tensor, **kwargs) -> Tuple[int, int, float]:\n",
    "        \"\"\"\n",
    "        Train the model for 1 complete step without switching to evaluation mode.\n",
    "        \n",
    "        Parameters:\n",
    "            ----------\n",
    "            X: Tensor, the mini-batch feature tensor (the 1st dimension is sample).\n",
    "            y: Tensor, the mini-batch target values (for regression, numerical; for classification, one-hot or multi-label).\n",
    "            \n",
    "        Returns:\n",
    "            ----------\n",
    "            Tuple[int, int, float]: (epoch, step, train_loss)\n",
    "\n",
    "        \"\"\"\n",
    "        # We don't conduct type checks but checks if X or y are None\n",
    "        if X is None or y is None:\n",
    "            raise ValueError(\"In _fit_slice_batch(), input `X` or `y` is/are None-type.\")\n",
    "        \n",
    "        # Module must be in training mode\n",
    "        if self.module.training == False:\n",
    "            raise RuntimeError(\"Called _fit_train_one_step() to perform one step training but the module is in non-training mode.\")\n",
    "        \n",
    "        # Module must have the same dtype, device with X\n",
    "        if self.module.dtype != X.dtype or self.module.device != X.device:\n",
    "            raise RuntimeError(\"Called _fit_train_one_step() to perform one step training but the module and your data have different dtype/device.\")\n",
    "        \n",
    "        # Perform a forward pass on the inputs\n",
    "        out = self.module.forward(X)\n",
    "        \n",
    "        # Calculate loss of this step\n",
    "        loss = self.criterion(out, y)\n",
    "        \n",
    "        # Perform the backward propagation of the loss function\n",
    "        lossgrad = self.criterion.backward()\n",
    "    \n",
    "        # Perform the backward propagation of the neural network module\n",
    "        self.module.backward(lossgrad)\n",
    "        \n",
    "        # Apply one step on optimizer to update the parameters\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Apply zero grad to clear the gradients\n",
    "        self.module.zero_grad()\n",
    "        \n",
    "        # Increment the step += 1\n",
    "        self.n_step += 1\n",
    "        return self.n_epoch, self.n_step, loss.to_list()\n",
    "        \n",
    "    def _fit_switch_to_mode(self, mode: Literal[\"train\", \"eval\"] = \"train\", **kwargs) -> None:\n",
    "        \"\"\"\n",
    "        Switch the module to train mode or evaluation mode.\n",
    "        \"\"\"\n",
    "        if mode not in {\"train\", \"eval\"}:\n",
    "            raise ValueError(f\"In _fit_switch_to_mode(), you gave a mode {mode} which is neither `train` nor `eval`.\")\n",
    "    \n",
    "        if mode == \"train\":\n",
    "            self.module.train()\n",
    "        elif mode == \"eval\":\n",
    "            self.module.eval()\n",
    "        return\n",
    "    \n",
    "    def _eval_one_batch(self, evalset: Dict[str, Tuple[Tensor, Tensor]] | None = None, evalmetrics: List[str] | str | None = None, one_hot: bool = True, **kwargs):\n",
    "        \"\"\"\n",
    "        Evaluate the `evalset` after training for one batch.    \n",
    "\n",
    "        Returns\n",
    "            -------\n",
    "            result_dict : dict  # Key: evalset name\n",
    "                                # Value dict {metric_name: metric_value}\n",
    "            or \n",
    "            {} if failed or did not evaluate\n",
    "        \"\"\"\n",
    "        # First switch to evaluation mode\n",
    "        self._fit_switch_to_mode(mode = \"eval\")\n",
    "        \n",
    "        # If:\n",
    "        # 1. sequential_batch non-None\n",
    "        # 2. evalset is at least len = 1\n",
    "        # 3. evalmetrics is non-None and at least len = 1\n",
    "        # Do evaluation\n",
    "        result_dict = {}\n",
    "        if evalmetrics is not None and evalset is not None:\n",
    "            if len(evalset) > 0 and len(evalmetrics) > 0:\n",
    "                # Record the result for each eval group\n",
    "                result_dict = {}    # Key: evalset name\n",
    "                                    # Value dict {metric_name: metric_value}\n",
    "                for eval_name in evalset.keys():\n",
    "                    X_sub, y_sub = evalset[eval_name]\n",
    "                    y_pred = self.predict(X_sub)\n",
    "                    \n",
    "                    # Inner metric dict, for values of result dict\n",
    "                    metrics = {}\n",
    "                    for metric_name in evalmetrics:\n",
    "                        \n",
    "                        # Evaluation: regression\n",
    "                        if self.task == \"regression\":\n",
    "                            eval_metric = RegressionMetrics(y_pred, y_sub, metric_type = metric_name).compute()\n",
    "                            # Matrix | Tensor\n",
    "                        \n",
    "                        # Evaluation: this is classification\n",
    "                        else:\n",
    "                            if (y_pred.shape[1] == 2 and one_hot == False) or y_pred.shape[1] == 1:\n",
    "                                # Binary and non-one hot\n",
    "                                eval_metric = BinaryClassificationMetrics(self._to_binary_prob(y_pred), y_sub, metric_type = metric_name).compute()\n",
    "                            else:\n",
    "                                # Since the aggregation output is alway one-hot, use Multiple then\n",
    "                                eval_metric = MultiClassificationMetrics(y_pred, y_sub, metric_type = metric_name).compute()\n",
    "                            # Matrix | Tensor\n",
    "                        metrics[metric_name] = eval_metric\n",
    "                    # For all metrics, put them into result_dict\n",
    "                    result_dict[eval_name] = metrics\n",
    "        \n",
    "        # If it is empty, then exit since nothing valid\n",
    "        if len(result_dict) == 0:\n",
    "            return {}\n",
    "        \n",
    "        # Else, return the dict\n",
    "        else:\n",
    "            return result_dict\n",
    "\n",
    "    def fit(self, \n",
    "            X: Tensor, \n",
    "            y: Tensor,\n",
    "            epoches: int = 100,\n",
    "            batch_size: int | None = None,\n",
    "            shuffle: bool = True,\n",
    "            random_state: int | None = None,\n",
    "            to_device: str | None = None,\n",
    "            *,\n",
    "            one_hot: bool = True,\n",
    "            verbosity: int | None = None,\n",
    "            evalper: int = 1,\n",
    "            evalset: Dict[str, Tuple[Tensor, Tensor]] | None = None,\n",
    "            evalmetrics: List[str] | str | None = None,\n",
    "            early_stop: int | None = None,\n",
    "            early_stop_logic: str = \"some\",\n",
    "            continue_to_train: bool | None = True,\n",
    "            **kwargs):\n",
    "        \"\"\"\n",
    "        Train n_estimators gradient boosting trees sequentially.\n",
    "        \n",
    "        Evaluation Remark:\n",
    "            ----------\n",
    "            You may want to evaluate datasets while training. If so, please do the following things:\n",
    "                1. set `verbosity` = 1 to print the evaluation\n",
    "                2. set the `evalset` to a dict of tuples of your dataset that is going to be evaluated\n",
    "                3. set the `evalmetrics` either to a string of metrics or a list of strings\n",
    "            You may want the algorithm to decide to stop training automatically. If so, please do things above, plus:\n",
    "                1. set `early_stop` to a number of batches, like 1 or 2, which acts like: \n",
    "                    if the metrics for all/some/any/most of the evaluation sets do not decrease anymore, \n",
    "                    the training process will be terminated and return\n",
    "                2. set `early_stop_logic` to determine the way of processing non-decreasing datasets/metrics\n",
    "                3. If you hope to continue to train again, call this `fit` again with `continue_to_train` set to True\n",
    "\n",
    "        Parameters:\n",
    "            ----------\n",
    "            X: Tensor, the feature tensor (the 1st dimension is sample).\n",
    "            y: Tensor, the target values (for regression, numerical; for classification, one-hot or multi-label).\n",
    "            epoches: int, the number of rounds (maximum rounds) to be trainned. Default is 100.\n",
    "            batch_size: int, the number of samples trained each time. Must be greater than 1. If None, then use all.\n",
    "            shuffle: bool, whether data will be shuffled for each round (same device same type). By default, True (if batch_size is None then omitted).\n",
    "            random_seed: int | None, the random seed set to perform shuffle, can be None which means to randomly choose one.\n",
    "            Optional:\n",
    "                one_hot : bool, if y is one-hot encoded for classification tasks.\n",
    "                verbosity: int | None, if >= 1 and having `evalset`, then will report metrics each batch.\n",
    "                evalper: int, the number of rounds to perform before evaluation conducted again.\n",
    "                evalset: Dict[name : Tuple[X, y],\n",
    "                              ...], | None, if provided, it may be used as evaluation set. XGBoost style.\n",
    "                evalmetrics: list of str | str | None, metrics used to do the evaluation. Will be printed.\n",
    "                early_stop: int | None, if non-None, then if metrics NOT gained for `early_stop` times, the forest will stop training.\n",
    "                early_stop_logic: str, the logic when deciding on multiple metrics, can be {\"any\", \"some\", \"most\", \"all\"}.\n",
    "                continue_to_train: bool | None, if non-None and True, the machine will try to restore the place it was and continue\n",
    "                                   to train new estimators until a new stopping criterion meets or until reaches the max number of allowed estimators.\n",
    "                \n",
    "        Returns:\n",
    "            ----------\n",
    "            self\n",
    "        \"\"\"\n",
    "\n",
    "        # Type Check (must be an Tensor type).\n",
    "        if isinstance(X, Tensor) == False or isinstance(y, Tensor) == False:\n",
    "            raise ValueError(\"Input dataset must be Tensor for neural networks. Use Tensor(data) or Tensor(data) to convert.\")\n",
    "        if type(X) != type(y):\n",
    "            raise ValueError(\"Input feature `X` and target `y` must have the same type, use Tensor instead.\")\n",
    "        \n",
    "        # Dimension Check.\n",
    "        if len(X.shape) < 2:\n",
    "            raise ValueError(\"Input feature `X` must be at least 2 dimensional (the first is for samples).\")\n",
    "        if len(y.shape) == 1:\n",
    "            raise ValueError(\"Input target `y` must also be a 2d data. If only one label or value, use data.reshape([-1, 1])\")\n",
    "                    \n",
    "        # Batch size Check.\n",
    "        if batch_size is not None:\n",
    "            if int(batch_size) < 1:\n",
    "                raise ValueError(\"Input `batch_size` must be an interger which is greater or equal to 1.\")\n",
    "                        \n",
    "        # Stopping Logic Check.\n",
    "        if early_stop_logic not in (\"any\", \"some\", \"most\", \"all\"):\n",
    "            raise ValueError(\"Stopping logic `early_stop_logic` must be one of ('any', 'some', 'most', 'all')\")\n",
    "    \n",
    "        # Record the original data, random seeds, and to_device\n",
    "        self.reference_X = X\n",
    "        self.reference_y = y\n",
    "        self.random_state = random_state\n",
    "        self.to_device = to_device\n",
    "        \n",
    "        # Record batch size\n",
    "        self.batch_size = int(batch_size) if batch_size is not None else None\n",
    "        \n",
    "        # Special evalmetrics type conversion\n",
    "        if isinstance(evalmetrics, str) == True:\n",
    "            evalmetrics = [evalmetrics]\n",
    "            \n",
    "        # Verbosity Conversion\n",
    "        verbosity = verbosity if verbosity is not None else 0\n",
    "        \n",
    "        # Create Evaluation Related Objects\n",
    "        undecreased_no = 0\n",
    "        last_eval_dict = {} # Please use deepcopy() here to avoid being errorly referred\n",
    "        \n",
    "        # Helper: Print and decide the evaluated results\n",
    "        def _decide_stop_with_print(batch: int, undecreased_no: int, eval_dict: dict, last_eval_dict: dict, **kwargs):\n",
    "            \"\"\"\n",
    "            Compare the metics and decide if stop or not.\n",
    "\n",
    "            Parameters\n",
    "                ----------\n",
    "                batch: int, batch no, for printing uses.\n",
    "                undecreased_no : int, cumulative number that loss did NOT decrease before evaluation.\n",
    "                eval_dict : dict, the passed evaluation dict.\n",
    "\n",
    "            Returns\n",
    "                -------\n",
    "                Tuple of (int, bool):\n",
    "                    int, updated undecreased_no\n",
    "                    bool, whether to stop (True) training or continue (False)\n",
    "\n",
    "            \"\"\"\n",
    "            # Dict is empty, abort\n",
    "            if len(eval_dict) == 0:\n",
    "                return undecreased_no, False\n",
    "            if len(last_eval_dict) == 0:\n",
    "                return undecreased_no, False\n",
    "            \n",
    "            # Difference dict copy\n",
    "            diff_dict = deepcopy(eval_dict)\n",
    "            \n",
    "            # Calculate the difference (this - last)\n",
    "            # and\n",
    "            # If verbosity, print the new evaluation dict\n",
    "            undes_count = 0\n",
    "            allmetric_count = 0\n",
    "            for evalset_name in eval_dict.keys():\n",
    "                eval_result = eval_dict[evalset_name]\n",
    "                if verbosity >= 1:\n",
    "                    print(\"Evalset: [\", evalset_name, \" : Metrics {\", end = \" \", sep = \"\")\n",
    "                for metric_name in eval_result.keys():\n",
    "                    metric_value = eval_result[metric_name]\n",
    "                    diff_dict[evalset_name][metric_name] = metric_value - last_eval_dict[evalset_name][metric_name]\n",
    "                    if diff_dict[evalset_name][metric_name].to_list() > 0:\n",
    "                        undes_count += 1\n",
    "                    allmetric_count += 1\n",
    "                    if verbosity >= 1:\n",
    "                        print(metric_name, \":\", round(metric_value.to_list(), 4), \", \", end = \" \", sep = \"\")\n",
    "                if verbosity >= 1:\n",
    "                    print(\"}]\", end = \"\\n\")\n",
    "                    \n",
    "            # If no early stop, directly return 0, False\n",
    "            if early_stop is None:\n",
    "                return 0, False\n",
    "                    \n",
    "            # If meets the requirement, stop training\n",
    "            if early_stop_logic == \"any\":\n",
    "                if undes_count > 0:\n",
    "                    undecreased_no += 1\n",
    "                    if undecreased_no >= early_stop:\n",
    "                        return undecreased_no, True\n",
    "                    else:\n",
    "                        return undecreased_no, False\n",
    "            elif early_stop_logic == \"some\":\n",
    "                if undes_count * 3 >= allmetric_count:\n",
    "                    undecreased_no += 1\n",
    "                    if undecreased_no >= early_stop:\n",
    "                        return undecreased_no, True\n",
    "                    else:\n",
    "                        return undecreased_no, False\n",
    "            elif early_stop_logic == \"most\":\n",
    "                if undes_count * 2 >= allmetric_count:\n",
    "                    undecreased_no += 1\n",
    "                    if undecreased_no >= early_stop:\n",
    "                        return undecreased_no, True\n",
    "                    else:\n",
    "                        return undecreased_no, False\n",
    "            elif early_stop_logic == \"all\":\n",
    "                if undes_count * 1 >= allmetric_count:\n",
    "                    undecreased_no += 1\n",
    "                    if undecreased_no >= early_stop:\n",
    "                        return undecreased_no, True\n",
    "                    else:\n",
    "                        return undecreased_no, False\n",
    "                    \n",
    "            # If survives here, return 0, False to refresh the undecreased_no\n",
    "            return 0, False\n",
    "                \n",
    "        #######################################################################\n",
    "        #        \n",
    "        # Iteratively train the neural network\n",
    "        rounds = 0\n",
    "        while rounds < epoches:\n",
    "            \n",
    "            # Verbosity\n",
    "            if verbosity >= 1:\n",
    "                print(f\"Training on Total Epoch: {self.n_epoch}, Round: {rounds}\")\n",
    "\n",
    "            ###################################################################\n",
    "            #\n",
    "            # If needs to shuffle the data, then shuffle it\n",
    "            epo_X, epo_y = self._fit_epoch_prep(X, y, \n",
    "                    batch_size=batch_size, shuffle=shuffle, random_state=self._random_state_next(), **kwargs)\n",
    "            \n",
    "            # Calculate number of steps in this round\n",
    "            if batch_size is None:\n",
    "                this_steps = 1\n",
    "            elif batch_size >= epo_X.shape[0]:\n",
    "                this_steps = 1\n",
    "            else:\n",
    "                this_steps = int(np.ceil(epo_X.shape[0] / batch_size))\n",
    "            \n",
    "            ###################################################################\n",
    "            #\n",
    "            # Formally strat to train this epoch, we first transfer to train mode\n",
    "            self._fit_switch_to_mode(mode = \"train\")\n",
    "            \n",
    "            # For steps in one epoch, train iteratively\n",
    "            if this_steps == 1:\n",
    "                # Just train and get the results\n",
    "                stp_X, stp_y = self._fit_slice_batch(epo_X, epo_y, start = None, batch_size = None, to_device = to_device, **kwargs)\n",
    "                _epoch, _step, _loss = self._fit_train_one_step(stp_X, stp_y, **kwargs)\n",
    "                # Record the step loss\n",
    "                self.losses_[_step] = _loss\n",
    "                \n",
    "            else:\n",
    "                for step in range(this_steps):\n",
    "                    # Prepare step-sliced data\n",
    "                    stp_X, stp_y = self._fit_slice_batch(epo_X, epo_y, start = step * batch_size, \n",
    "                         batch_size = batch_size, to_device = to_device, **kwargs)\n",
    "                    # Train one step\n",
    "                    _epoch, _step, _loss = self._fit_train_one_step(stp_X, stp_y, **kwargs)\n",
    "                    # Record the step loss\n",
    "                    self.losses_[_step] = _loss\n",
    "                                        \n",
    "            # Self-increment epoch\n",
    "            self.n_epoch += 1\n",
    "            \n",
    "            ###################################################################\n",
    "            #\n",
    "            # Evaluation if needed\n",
    "            if rounds % evalper != 0 or rounds == 0:\n",
    "                # Count self increasing\n",
    "                rounds += 1\n",
    "                continue\n",
    "            \n",
    "            # Evaluate and decide if stop training from now\n",
    "            eval_dict = self._eval_one_batch(evalset = evalset, evalmetrics = evalmetrics, one_hot = one_hot, **kwargs)\n",
    "            \n",
    "            # Try stop maker and receive the advice\n",
    "            undecreased_no, decision = _decide_stop_with_print(rounds, undecreased_no = undecreased_no, eval_dict = eval_dict, last_eval_dict = last_eval_dict)\n",
    "            \n",
    "            # Record the evaluation result\n",
    "            self.evalhist_[self.n_epoch] = deepcopy(eval_dict)\n",
    "            \n",
    "            # Copy last evaluated dict\n",
    "            last_eval_dict = deepcopy(eval_dict)\n",
    "            \n",
    "            # Count self increasing\n",
    "            rounds += 1\n",
    "            \n",
    "            # Make decision to terminate or not\n",
    "            if decision == True:\n",
    "                break\n",
    "            \n",
    "        return self\n",
    "            \n",
    "    def eval(self) -> None:\n",
    "        \"\"\"\n",
    "        Switch the module to evaluation mode.\n",
    "        \"\"\"\n",
    "        self.module.eval()\n",
    "        return\n",
    "    \n",
    "    def predict(self, X: Tensor, **kwargs) -> Tensor:\n",
    "        \"\"\"\n",
    "        Predict target values for samples in X in batches.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor, output of predictions.\n",
    "            \n",
    "        Raises:\n",
    "            RuntimeError: if you did NOT switched to evalation mode.\n",
    "        \"\"\"\n",
    "        # Check the module training status\n",
    "        if self.module.training == True:\n",
    "            raise RuntimeError(\"In predict(), you called this in training mode. Please call .eval() to make the model safe in evaluation mode.\")\n",
    "        \n",
    "        # Type Check (must be an Tensor type).\n",
    "        if isinstance(X, Tensor) == False:\n",
    "            raise ValueError(\"Input dataset must be a Tensor. Use Tensor(data) to convert.\")\n",
    "        \n",
    "        # If does not need mini-batch, directly go with X to deviced\n",
    "        if self.batch_size is None:\n",
    "            epo_X = X if self.to_device is None else X.to(X._backend, dtype=X.dtype, device=self.to_device)\n",
    "            return self.module.forward(epo_X)\n",
    "        elif self.batch_size >= X.shape[0]:\n",
    "            epo_X = X if self.to_device is None else X.to(X._backend, dtype=X.dtype, device=self.to_device)\n",
    "            return self.module.forward(epo_X)\n",
    "        \n",
    "        # We have to batchly predict to avoid exceeding the limit of memory\n",
    "        else:\n",
    "            pred = None\n",
    "            start = 0\n",
    "            while start < X.shape[0]:\n",
    "                stp_X = self._fit_slice_batch_X(X, start = start, batch_size = self.batch_size, to_device = self.to_device, **kwargs)\n",
    "                stp_pred = self.module.forward(stp_X)\n",
    "                if pred is None:\n",
    "                    pred = stp_pred\n",
    "                else:\n",
    "                    pred = pred.vstack(stp_pred)\n",
    "                start += self.batch_size\n",
    "            return pred\n",
    "\n",
    "    def predict_loss(self, X: Tensor, y: Tensor, **kwargs) -> Tuple[Tensor, Tensor]:\n",
    "        \"\"\"\n",
    "        Predict target values for samples in X and calculate the loss by a given target y.\n",
    "        \n",
    "        Returns:\n",
    "            Tuple[Tensor, Tensor]: output of predictions, loss.\n",
    "            \n",
    "        Raises:\n",
    "            RuntimeError: if you did NOT switched to evalation mode.\n",
    "        \"\"\"\n",
    "        # Check the module training status\n",
    "        if self.module.training == True:\n",
    "            raise RuntimeError(\"In predict(), you called this in training mode. Please call .eval() to make the model safe in evaluation mode.\")\n",
    "        \n",
    "        # Type Check (must be an Tensor type).\n",
    "        if isinstance(X, Tensor) == False:\n",
    "            raise ValueError(\"Input dataset must be either Matrix and Tensor. Use Matrix(data) or Tensor(data) to convert.\")\n",
    "        \n",
    "        # Conduct a forward pass and return result\n",
    "        pred = self.predict(X, kwargs)    \n",
    "        \n",
    "        # Compute loss function\n",
    "        loss = self.criterion.forward(pred, y if self.to_device is None else y.to(backend=y._backend, dtype=y.dtype, device=self.to_device))\n",
    "        return pred, loss\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"NN Simple Interf Evaluator(name = {self.name}, task = {self.task}, has trained n_epoch = {self.n_epoch}, n_step = {self.n_step}).\"\n",
    "    \n",
    "    \n",
    "# Alias for nn_SInterf_Evaluator\n",
    "Evaluator = nn_SInterf_Evaluator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2.1 Download the S&P 500 daily prices into the Notebook Environment do Pre-processing`\n",
    "\n",
    "and\n",
    "\n",
    "`2.2 Create the labels`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Open_Rate</th>\n",
       "      <th>High_Rate</th>\n",
       "      <th>Low_Rate</th>\n",
       "      <th>Close_Rate</th>\n",
       "      <th>Open_MA</th>\n",
       "      <th>High_MA</th>\n",
       "      <th>Low_MA</th>\n",
       "      <th>Close_MA</th>\n",
       "      <th>Open_Rate_MA</th>\n",
       "      <th>High_Rate_MA</th>\n",
       "      <th>Low_Rate_MA</th>\n",
       "      <th>Close_Rate_MA</th>\n",
       "      <th>Next Day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5/6/2005</th>\n",
       "      <td>1172.630005</td>\n",
       "      <td>1177.750000</td>\n",
       "      <td>1170.500000</td>\n",
       "      <td>1171.349976</td>\n",
       "      <td>-0.0026</td>\n",
       "      <td>-0.0007</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>-0.0011</td>\n",
       "      <td>1160.942993</td>\n",
       "      <td>1167.095947</td>\n",
       "      <td>1153.374512</td>\n",
       "      <td>1160.450439</td>\n",
       "      <td>-0.000335</td>\n",
       "      <td>-0.000245</td>\n",
       "      <td>-0.000330</td>\n",
       "      <td>-0.000390</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5/9/2005</th>\n",
       "      <td>1171.349976</td>\n",
       "      <td>1178.869995</td>\n",
       "      <td>1169.380005</td>\n",
       "      <td>1178.839966</td>\n",
       "      <td>-0.0011</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>-0.0010</td>\n",
       "      <td>0.0064</td>\n",
       "      <td>1160.450439</td>\n",
       "      <td>1166.836060</td>\n",
       "      <td>1152.909058</td>\n",
       "      <td>1160.332031</td>\n",
       "      <td>-0.000390</td>\n",
       "      <td>-0.000195</td>\n",
       "      <td>-0.000380</td>\n",
       "      <td>-0.000070</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5/10/2005</th>\n",
       "      <td>1178.839966</td>\n",
       "      <td>1178.839966</td>\n",
       "      <td>1162.979980</td>\n",
       "      <td>1166.219971</td>\n",
       "      <td>0.0064</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.0055</td>\n",
       "      <td>-0.0107</td>\n",
       "      <td>1160.332031</td>\n",
       "      <td>1166.269531</td>\n",
       "      <td>1152.515503</td>\n",
       "      <td>1159.255005</td>\n",
       "      <td>-0.000070</td>\n",
       "      <td>-0.000455</td>\n",
       "      <td>-0.000320</td>\n",
       "      <td>-0.000880</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5/11/2005</th>\n",
       "      <td>1166.219971</td>\n",
       "      <td>1171.770020</td>\n",
       "      <td>1157.709961</td>\n",
       "      <td>1171.109985</td>\n",
       "      <td>-0.0107</td>\n",
       "      <td>-0.0060</td>\n",
       "      <td>-0.0045</td>\n",
       "      <td>0.0042</td>\n",
       "      <td>1159.255005</td>\n",
       "      <td>1165.469971</td>\n",
       "      <td>1151.831055</td>\n",
       "      <td>1159.120972</td>\n",
       "      <td>-0.000880</td>\n",
       "      <td>-0.000655</td>\n",
       "      <td>-0.000570</td>\n",
       "      <td>-0.000080</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5/12/2005</th>\n",
       "      <td>1171.109985</td>\n",
       "      <td>1173.369995</td>\n",
       "      <td>1157.760010</td>\n",
       "      <td>1159.359985</td>\n",
       "      <td>0.0042</td>\n",
       "      <td>0.0014</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.0100</td>\n",
       "      <td>1159.120972</td>\n",
       "      <td>1165.405029</td>\n",
       "      <td>1151.634033</td>\n",
       "      <td>1158.986450</td>\n",
       "      <td>-0.000080</td>\n",
       "      <td>-0.000035</td>\n",
       "      <td>-0.000155</td>\n",
       "      <td>-0.000080</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5/2/2025</th>\n",
       "      <td>5645.879883</td>\n",
       "      <td>5700.700195</td>\n",
       "      <td>5642.279785</td>\n",
       "      <td>5686.669922</td>\n",
       "      <td>0.0037</td>\n",
       "      <td>0.0074</td>\n",
       "      <td>0.0080</td>\n",
       "      <td>0.0147</td>\n",
       "      <td>5351.241699</td>\n",
       "      <td>5436.217773</td>\n",
       "      <td>5273.058105</td>\n",
       "      <td>5367.482910</td>\n",
       "      <td>0.001875</td>\n",
       "      <td>0.001945</td>\n",
       "      <td>0.002595</td>\n",
       "      <td>0.003045</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5/5/2025</th>\n",
       "      <td>5655.319824</td>\n",
       "      <td>5683.379883</td>\n",
       "      <td>5634.479980</td>\n",
       "      <td>5650.379883</td>\n",
       "      <td>0.0017</td>\n",
       "      <td>-0.0030</td>\n",
       "      <td>-0.0014</td>\n",
       "      <td>-0.0064</td>\n",
       "      <td>5369.400391</td>\n",
       "      <td>5455.779785</td>\n",
       "      <td>5301.287109</td>\n",
       "      <td>5396.297852</td>\n",
       "      <td>0.003785</td>\n",
       "      <td>0.003680</td>\n",
       "      <td>0.005500</td>\n",
       "      <td>0.005710</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5/6/2025</th>\n",
       "      <td>5605.870117</td>\n",
       "      <td>5649.580078</td>\n",
       "      <td>5586.040039</td>\n",
       "      <td>5606.910156</td>\n",
       "      <td>-0.0087</td>\n",
       "      <td>-0.0059</td>\n",
       "      <td>-0.0086</td>\n",
       "      <td>-0.0077</td>\n",
       "      <td>5402.004395</td>\n",
       "      <td>5475.930664</td>\n",
       "      <td>5338.836914</td>\n",
       "      <td>5423.530762</td>\n",
       "      <td>0.006545</td>\n",
       "      <td>0.003815</td>\n",
       "      <td>0.007385</td>\n",
       "      <td>0.005440</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5/7/2025</th>\n",
       "      <td>5614.180176</td>\n",
       "      <td>5654.729980</td>\n",
       "      <td>5578.640137</td>\n",
       "      <td>5631.279785</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.0009</td>\n",
       "      <td>-0.0013</td>\n",
       "      <td>0.0043</td>\n",
       "      <td>5423.035156</td>\n",
       "      <td>5495.293457</td>\n",
       "      <td>5372.248047</td>\n",
       "      <td>5455.956543</td>\n",
       "      <td>0.004200</td>\n",
       "      <td>0.003660</td>\n",
       "      <td>0.006540</td>\n",
       "      <td>0.006440</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5/8/2025</th>\n",
       "      <td>5663.600098</td>\n",
       "      <td>5720.100098</td>\n",
       "      <td>5635.379883</td>\n",
       "      <td>5663.939941</td>\n",
       "      <td>0.0088</td>\n",
       "      <td>0.0116</td>\n",
       "      <td>0.0102</td>\n",
       "      <td>0.0058</td>\n",
       "      <td>5457.951172</td>\n",
       "      <td>5507.231445</td>\n",
       "      <td>5406.595703</td>\n",
       "      <td>5466.308594</td>\n",
       "      <td>0.006840</td>\n",
       "      <td>0.002210</td>\n",
       "      <td>0.006665</td>\n",
       "      <td>0.001970</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10088 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Open         High          Low        Close  Open_Rate  \\\n",
       "5/6/2005   1172.630005  1177.750000  1170.500000  1171.349976    -0.0026   \n",
       "5/9/2005   1171.349976  1178.869995  1169.380005  1178.839966    -0.0011   \n",
       "5/10/2005  1178.839966  1178.839966  1162.979980  1166.219971     0.0064   \n",
       "5/11/2005  1166.219971  1171.770020  1157.709961  1171.109985    -0.0107   \n",
       "5/12/2005  1171.109985  1173.369995  1157.760010  1159.359985     0.0042   \n",
       "...                ...          ...          ...          ...        ...   \n",
       "5/2/2025   5645.879883  5700.700195  5642.279785  5686.669922     0.0037   \n",
       "5/5/2025   5655.319824  5683.379883  5634.479980  5650.379883     0.0017   \n",
       "5/6/2025   5605.870117  5649.580078  5586.040039  5606.910156    -0.0087   \n",
       "5/7/2025   5614.180176  5654.729980  5578.640137  5631.279785     0.0015   \n",
       "5/8/2025   5663.600098  5720.100098  5635.379883  5663.939941     0.0088   \n",
       "\n",
       "           High_Rate  Low_Rate  Close_Rate      Open_MA      High_MA  \\\n",
       "5/6/2005     -0.0007    0.0032     -0.0011  1160.942993  1167.095947   \n",
       "5/9/2005      0.0010   -0.0010      0.0064  1160.450439  1166.836060   \n",
       "5/10/2005     0.0000   -0.0055     -0.0107  1160.332031  1166.269531   \n",
       "5/11/2005    -0.0060   -0.0045      0.0042  1159.255005  1165.469971   \n",
       "5/12/2005     0.0014    0.0000     -0.0100  1159.120972  1165.405029   \n",
       "...              ...       ...         ...          ...          ...   \n",
       "5/2/2025      0.0074    0.0080      0.0147  5351.241699  5436.217773   \n",
       "5/5/2025     -0.0030   -0.0014     -0.0064  5369.400391  5455.779785   \n",
       "5/6/2025     -0.0059   -0.0086     -0.0077  5402.004395  5475.930664   \n",
       "5/7/2025      0.0009   -0.0013      0.0043  5423.035156  5495.293457   \n",
       "5/8/2025      0.0116    0.0102      0.0058  5457.951172  5507.231445   \n",
       "\n",
       "                Low_MA     Close_MA  Open_Rate_MA  High_Rate_MA  Low_Rate_MA  \\\n",
       "5/6/2005   1153.374512  1160.450439     -0.000335     -0.000245    -0.000330   \n",
       "5/9/2005   1152.909058  1160.332031     -0.000390     -0.000195    -0.000380   \n",
       "5/10/2005  1152.515503  1159.255005     -0.000070     -0.000455    -0.000320   \n",
       "5/11/2005  1151.831055  1159.120972     -0.000880     -0.000655    -0.000570   \n",
       "5/12/2005  1151.634033  1158.986450     -0.000080     -0.000035    -0.000155   \n",
       "...                ...          ...           ...           ...          ...   \n",
       "5/2/2025   5273.058105  5367.482910      0.001875      0.001945     0.002595   \n",
       "5/5/2025   5301.287109  5396.297852      0.003785      0.003680     0.005500   \n",
       "5/6/2025   5338.836914  5423.530762      0.006545      0.003815     0.007385   \n",
       "5/7/2025   5372.248047  5455.956543      0.004200      0.003660     0.006540   \n",
       "5/8/2025   5406.595703  5466.308594      0.006840      0.002210     0.006665   \n",
       "\n",
       "           Close_Rate_MA  Next Day  \n",
       "5/6/2005       -0.000390       1.0  \n",
       "5/9/2005       -0.000070       0.0  \n",
       "5/10/2005      -0.000880       1.0  \n",
       "5/11/2005      -0.000080       0.0  \n",
       "5/12/2005      -0.000080       0.0  \n",
       "...                  ...       ...  \n",
       "5/2/2025        0.003045       0.0  \n",
       "5/5/2025        0.005710       0.0  \n",
       "5/6/2025        0.005440       1.0  \n",
       "5/7/2025        0.006440       1.0  \n",
       "5/8/2025        0.001970       0.0  \n",
       "\n",
       "[10088 rows x 17 columns]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and preprocess the S&P 500 Dataset\n",
    "#\n",
    "\n",
    "# Load the stock index dataset into the environment.\n",
    "url = \"https://raw.githubusercontent.com/dof-studio/dtafina/refs/heads/main/MachineLearning/spx-choice-daily-2005-processed.csv\"\n",
    "raw_index = pd.read_csv(url)\n",
    "raw_index.index = raw_index[\"Date\"].to_list()\n",
    "\n",
    "# Astype into float32.\n",
    "raw_index = raw_index.drop([\"Date\"], axis=1).astype(\"float32\")\n",
    "raw_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Open_Rate</th>\n",
       "      <th>High_Rate</th>\n",
       "      <th>Low_Rate</th>\n",
       "      <th>Close_Rate</th>\n",
       "      <th>Open_MA</th>\n",
       "      <th>High_MA</th>\n",
       "      <th>Low_MA</th>\n",
       "      <th>Close_MA</th>\n",
       "      <th>Open_Rate_MA</th>\n",
       "      <th>High_Rate_MA</th>\n",
       "      <th>Low_Rate_MA</th>\n",
       "      <th>Close_Rate_MA</th>\n",
       "      <th>Next Day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5/6/2005</th>\n",
       "      <td>-0.460007</td>\n",
       "      <td>-0.462017</td>\n",
       "      <td>-0.456197</td>\n",
       "      <td>-0.461608</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>0.32</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>-0.466263</td>\n",
       "      <td>-0.467711</td>\n",
       "      <td>-0.465388</td>\n",
       "      <td>-0.467453</td>\n",
       "      <td>-0.0335</td>\n",
       "      <td>-0.0245</td>\n",
       "      <td>-0.0330</td>\n",
       "      <td>-0.0390</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5/9/2005</th>\n",
       "      <td>-0.460692</td>\n",
       "      <td>-0.461418</td>\n",
       "      <td>-0.456798</td>\n",
       "      <td>-0.457592</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>0.64</td>\n",
       "      <td>-0.466526</td>\n",
       "      <td>-0.467850</td>\n",
       "      <td>-0.465638</td>\n",
       "      <td>-0.467517</td>\n",
       "      <td>-0.0390</td>\n",
       "      <td>-0.0195</td>\n",
       "      <td>-0.0380</td>\n",
       "      <td>-0.0070</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5/10/2005</th>\n",
       "      <td>-0.456683</td>\n",
       "      <td>-0.461434</td>\n",
       "      <td>-0.460233</td>\n",
       "      <td>-0.464359</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.55</td>\n",
       "      <td>-1.07</td>\n",
       "      <td>-0.466590</td>\n",
       "      <td>-0.468152</td>\n",
       "      <td>-0.465849</td>\n",
       "      <td>-0.468094</td>\n",
       "      <td>-0.0070</td>\n",
       "      <td>-0.0455</td>\n",
       "      <td>-0.0320</td>\n",
       "      <td>-0.0880</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5/11/2005</th>\n",
       "      <td>-0.463438</td>\n",
       "      <td>-0.465213</td>\n",
       "      <td>-0.463061</td>\n",
       "      <td>-0.461737</td>\n",
       "      <td>-1.07</td>\n",
       "      <td>-0.60</td>\n",
       "      <td>-0.45</td>\n",
       "      <td>0.42</td>\n",
       "      <td>-0.467166</td>\n",
       "      <td>-0.468580</td>\n",
       "      <td>-0.466216</td>\n",
       "      <td>-0.468166</td>\n",
       "      <td>-0.0880</td>\n",
       "      <td>-0.0655</td>\n",
       "      <td>-0.0570</td>\n",
       "      <td>-0.0080</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5/12/2005</th>\n",
       "      <td>-0.460820</td>\n",
       "      <td>-0.464357</td>\n",
       "      <td>-0.463034</td>\n",
       "      <td>-0.468038</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-0.467238</td>\n",
       "      <td>-0.468614</td>\n",
       "      <td>-0.466322</td>\n",
       "      <td>-0.468238</td>\n",
       "      <td>-0.0080</td>\n",
       "      <td>-0.0035</td>\n",
       "      <td>-0.0155</td>\n",
       "      <td>-0.0080</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5/2/2025</th>\n",
       "      <td>1.934423</td>\n",
       "      <td>1.955277</td>\n",
       "      <td>1.943844</td>\n",
       "      <td>1.959838</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.80</td>\n",
       "      <td>1.47</td>\n",
       "      <td>1.776710</td>\n",
       "      <td>1.813925</td>\n",
       "      <td>1.745680</td>\n",
       "      <td>1.788667</td>\n",
       "      <td>0.1875</td>\n",
       "      <td>0.1945</td>\n",
       "      <td>0.2595</td>\n",
       "      <td>0.3045</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5/5/2025</th>\n",
       "      <td>1.939476</td>\n",
       "      <td>1.946020</td>\n",
       "      <td>1.939658</td>\n",
       "      <td>1.940377</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>-0.64</td>\n",
       "      <td>1.786430</td>\n",
       "      <td>1.824379</td>\n",
       "      <td>1.760831</td>\n",
       "      <td>1.804120</td>\n",
       "      <td>0.3785</td>\n",
       "      <td>0.3680</td>\n",
       "      <td>0.5500</td>\n",
       "      <td>0.5710</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5/6/2025</th>\n",
       "      <td>1.913007</td>\n",
       "      <td>1.927956</td>\n",
       "      <td>1.913660</td>\n",
       "      <td>1.917065</td>\n",
       "      <td>-0.87</td>\n",
       "      <td>-0.59</td>\n",
       "      <td>-0.86</td>\n",
       "      <td>-0.77</td>\n",
       "      <td>1.803882</td>\n",
       "      <td>1.835149</td>\n",
       "      <td>1.780984</td>\n",
       "      <td>1.818724</td>\n",
       "      <td>0.6545</td>\n",
       "      <td>0.3815</td>\n",
       "      <td>0.7385</td>\n",
       "      <td>0.5440</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5/7/2025</th>\n",
       "      <td>1.917455</td>\n",
       "      <td>1.930709</td>\n",
       "      <td>1.909688</td>\n",
       "      <td>1.930134</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.09</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.815139</td>\n",
       "      <td>1.845498</td>\n",
       "      <td>1.798916</td>\n",
       "      <td>1.836113</td>\n",
       "      <td>0.4200</td>\n",
       "      <td>0.3660</td>\n",
       "      <td>0.6540</td>\n",
       "      <td>0.6440</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5/8/2025</th>\n",
       "      <td>1.943908</td>\n",
       "      <td>1.965646</td>\n",
       "      <td>1.940141</td>\n",
       "      <td>1.947649</td>\n",
       "      <td>0.88</td>\n",
       "      <td>1.16</td>\n",
       "      <td>1.02</td>\n",
       "      <td>0.58</td>\n",
       "      <td>1.833829</td>\n",
       "      <td>1.851878</td>\n",
       "      <td>1.817351</td>\n",
       "      <td>1.841665</td>\n",
       "      <td>0.6840</td>\n",
       "      <td>0.2210</td>\n",
       "      <td>0.6665</td>\n",
       "      <td>0.1970</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10088 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Open      High       Low     Close  Open_Rate  High_Rate  \\\n",
       "5/6/2005  -0.460007 -0.462017 -0.456197 -0.461608      -0.26      -0.07   \n",
       "5/9/2005  -0.460692 -0.461418 -0.456798 -0.457592      -0.11       0.10   \n",
       "5/10/2005 -0.456683 -0.461434 -0.460233 -0.464359       0.64       0.00   \n",
       "5/11/2005 -0.463438 -0.465213 -0.463061 -0.461737      -1.07      -0.60   \n",
       "5/12/2005 -0.460820 -0.464357 -0.463034 -0.468038       0.42       0.14   \n",
       "...             ...       ...       ...       ...        ...        ...   \n",
       "5/2/2025   1.934423  1.955277  1.943844  1.959838       0.37       0.74   \n",
       "5/5/2025   1.939476  1.946020  1.939658  1.940377       0.17      -0.30   \n",
       "5/6/2025   1.913007  1.927956  1.913660  1.917065      -0.87      -0.59   \n",
       "5/7/2025   1.917455  1.930709  1.909688  1.930134       0.15       0.09   \n",
       "5/8/2025   1.943908  1.965646  1.940141  1.947649       0.88       1.16   \n",
       "\n",
       "           Low_Rate  Close_Rate   Open_MA   High_MA    Low_MA  Close_MA  \\\n",
       "5/6/2005       0.32       -0.11 -0.466263 -0.467711 -0.465388 -0.467453   \n",
       "5/9/2005      -0.10        0.64 -0.466526 -0.467850 -0.465638 -0.467517   \n",
       "5/10/2005     -0.55       -1.07 -0.466590 -0.468152 -0.465849 -0.468094   \n",
       "5/11/2005     -0.45        0.42 -0.467166 -0.468580 -0.466216 -0.468166   \n",
       "5/12/2005      0.00       -1.00 -0.467238 -0.468614 -0.466322 -0.468238   \n",
       "...             ...         ...       ...       ...       ...       ...   \n",
       "5/2/2025       0.80        1.47  1.776710  1.813925  1.745680  1.788667   \n",
       "5/5/2025      -0.14       -0.64  1.786430  1.824379  1.760831  1.804120   \n",
       "5/6/2025      -0.86       -0.77  1.803882  1.835149  1.780984  1.818724   \n",
       "5/7/2025      -0.13        0.43  1.815139  1.845498  1.798916  1.836113   \n",
       "5/8/2025       1.02        0.58  1.833829  1.851878  1.817351  1.841665   \n",
       "\n",
       "           Open_Rate_MA  High_Rate_MA  Low_Rate_MA  Close_Rate_MA  Next Day  \n",
       "5/6/2005        -0.0335       -0.0245      -0.0330        -0.0390       1.0  \n",
       "5/9/2005        -0.0390       -0.0195      -0.0380        -0.0070       0.0  \n",
       "5/10/2005       -0.0070       -0.0455      -0.0320        -0.0880       1.0  \n",
       "5/11/2005       -0.0880       -0.0655      -0.0570        -0.0080       0.0  \n",
       "5/12/2005       -0.0080       -0.0035      -0.0155        -0.0080       0.0  \n",
       "...                 ...           ...          ...            ...       ...  \n",
       "5/2/2025         0.1875        0.1945       0.2595         0.3045       0.0  \n",
       "5/5/2025         0.3785        0.3680       0.5500         0.5710       0.0  \n",
       "5/6/2025         0.6545        0.3815       0.7385         0.5440       1.0  \n",
       "5/7/2025         0.4200        0.3660       0.6540         0.6440       1.0  \n",
       "5/8/2025         0.6840        0.2210       0.6665         0.1970       0.0  \n",
       "\n",
       "[10088 rows x 17 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocess the data to make prices lies in a proper range\n",
    "\n",
    "merged_index = raw_index.copy()\n",
    "\n",
    "# Calculate the robust scaler\n",
    "merged_index_median = merged_index.median()\n",
    "merged_index_25q = merged_index.quantile(0.25)\n",
    "merged_index_75q = merged_index.quantile(0.75)\n",
    "\n",
    "# Make them only open,high,low,close\n",
    "merged_index_median = merged_index_median.iloc[0:4,].to_list()\n",
    "merged_index_25q = merged_index_25q.iloc[0:4,]\n",
    "merged_index_75q = merged_index_75q.iloc[0:4,]\n",
    "merged_index_range = merged_index_75q - merged_index_25q\n",
    "merged_index_range = merged_index_range.to_list()\n",
    "\n",
    "# De-median all the prices and prices\n",
    "merged_index[[\"Open\", \"High\", \"Low\", \"Close\"]] = merged_index[[\"Open\", \"High\", \"Low\", \"Close\"]] - merged_index_median\n",
    "merged_index[[\"Open_MA\", \"High_MA\", \"Low_MA\", \"Close_MA\"]] = merged_index[[\"Open_MA\", \"High_MA\", \"Low_MA\", \"Close_MA\"]] - merged_index_median\n",
    "\n",
    "# Standardize all prices to make them lies in the range\n",
    "merged_index[[\"Open\", \"High\", \"Low\", \"Close\"]] = merged_index[[\"Open\", \"High\", \"Low\", \"Close\"]] / merged_index_range\n",
    "merged_index[[\"Open_MA\", \"High_MA\", \"Low_MA\", \"Close_MA\"]] = merged_index[[\"Open_MA\", \"High_MA\", \"Low_MA\", \"Close_MA\"]] / merged_index_range\n",
    "\n",
    "# Standardize all rates\n",
    "merged_index[[\"Open_Rate\", \"High_Rate\", \"Low_Rate\", \"Close_Rate\"]] = merged_index[[\"Open_Rate\", \"High_Rate\", \"Low_Rate\", \"Close_Rate\"]] * 100\n",
    "merged_index[[\"Open_Rate_MA\", \"High_Rate_MA\", \"Low_Rate_MA\", \"Close_Rate_MA\"]] = merged_index[[\"Open_Rate_MA\", \"High_Rate_MA\", \"Low_Rate_MA\", \"Close_Rate_MA\"]] * 100\n",
    "\n",
    "merged_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(backend=torch, shape=torch.Size([10088, 16]), data=\n",
      "tensor([[-0.4600, -0.4620, -0.4562,  ..., -0.0245, -0.0330, -0.0390],\n",
      "        [-0.4607, -0.4614, -0.4568,  ..., -0.0195, -0.0380, -0.0070],\n",
      "        [-0.4567, -0.4614, -0.4602,  ..., -0.0455, -0.0320, -0.0880],\n",
      "        ...,\n",
      "        [ 1.9130,  1.9280,  1.9137,  ...,  0.3815,  0.7385,  0.5440],\n",
      "        [ 1.9175,  1.9307,  1.9097,  ...,  0.3660,  0.6540,  0.6440],\n",
      "        [ 1.9439,  1.9656,  1.9401,  ...,  0.2210,  0.6665,  0.1970]],\n",
      "       device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "# Let's split the data into features and target \n",
    "\n",
    "# Create the Features Tensor\n",
    "features_index_org = Tensor(merged_index.drop([\"Next Day\"], axis = 1).to_numpy(), backend=backend, device=device, dtype=torch.float32)\n",
    "\n",
    "# Create the Targets Tensor\n",
    "targets_index = Tensor(merged_index[[\"Next Day\"]].to_numpy(), backend=backend, device=device, dtype=torch.float32)\n",
    "\n",
    "# Show how is the feature Tensor like\n",
    "print(features_index_org)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2.3 Build input sequences of 20 days of past prices (sample) to predict the next day's movement`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build input sequences of 20 days of past prices (sample) to predict the next day's movement\n",
    "\n",
    "features_index_w, targets_index_w = MLBase.make_rolling_window(features_index_org, targets_index, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2.4 Split the dataset: 80% training / 20% testing`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(backend=torch, shape=torch.Size([8063, 10, 16]), data=\n",
      "tensor([[[-0.4600, -0.4620, -0.4562,  ..., -0.0245, -0.0330, -0.0390],\n",
      "         [-0.4607, -0.4614, -0.4568,  ..., -0.0195, -0.0380, -0.0070],\n",
      "         [-0.4567, -0.4614, -0.4602,  ..., -0.0455, -0.0320, -0.0880],\n",
      "         ...,\n",
      "         [-0.4637, -0.4638, -0.4619,  ...,  0.0860,  0.0615,  0.0940],\n",
      "         [-0.4594, -0.4566, -0.4544,  ...,  0.1400,  0.1645,  0.2105],\n",
      "         [-0.4531, -0.4549, -0.4487,  ...,  0.1340,  0.2040,  0.1355]],\n",
      "\n",
      "        [[-0.4607, -0.4614, -0.4568,  ..., -0.0195, -0.0380, -0.0070],\n",
      "         [-0.4567, -0.4614, -0.4602,  ..., -0.0455, -0.0320, -0.0880],\n",
      "         [-0.4634, -0.4652, -0.4631,  ..., -0.0655, -0.0570, -0.0080],\n",
      "         ...,\n",
      "         [-0.4594, -0.4566, -0.4544,  ...,  0.1400,  0.1645,  0.2105],\n",
      "         [-0.4531, -0.4549, -0.4487,  ...,  0.1340,  0.2040,  0.1355],\n",
      "         [-0.4501, -0.4548, -0.4483,  ...,  0.1345,  0.1830,  0.1620]],\n",
      "\n",
      "        [[-0.4567, -0.4614, -0.4602,  ..., -0.0455, -0.0320, -0.0880],\n",
      "         [-0.4634, -0.4652, -0.4631,  ..., -0.0655, -0.0570, -0.0080],\n",
      "         [-0.4608, -0.4644, -0.4630,  ..., -0.0035, -0.0155, -0.0080],\n",
      "         ...,\n",
      "         [-0.4531, -0.4549, -0.4487,  ...,  0.1340,  0.2040,  0.1355],\n",
      "         [-0.4501, -0.4548, -0.4483,  ...,  0.1345,  0.1830,  0.1620],\n",
      "         [-0.4511, -0.4515, -0.4464,  ...,  0.1430,  0.1580,  0.1380]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.1620,  0.1643,  0.1656,  ..., -0.0750, -0.1025, -0.0605],\n",
      "         [ 0.1653,  0.1640,  0.1683,  ..., -0.0635, -0.0745, -0.0650],\n",
      "         [ 0.1693,  0.1659,  0.1688,  ..., -0.0615, -0.0145, -0.0115],\n",
      "         ...,\n",
      "         [ 0.1907,  0.1902,  0.1963,  ...,  0.0720,  0.1035,  0.0620],\n",
      "         [ 0.1914,  0.1870,  0.1941,  ...,  0.0610,  0.0625,  0.0595],\n",
      "         [ 0.1925,  0.1878,  0.1942,  ...,  0.0495,  0.0515,  0.0355]],\n",
      "\n",
      "        [[ 0.1653,  0.1640,  0.1683,  ..., -0.0635, -0.0745, -0.0650],\n",
      "         [ 0.1693,  0.1659,  0.1688,  ..., -0.0615, -0.0145, -0.0115],\n",
      "         [ 0.1674,  0.1707,  0.1705,  ...,  0.0205,  0.0035,  0.0170],\n",
      "         ...,\n",
      "         [ 0.1914,  0.1870,  0.1941,  ...,  0.0610,  0.0625,  0.0595],\n",
      "         [ 0.1925,  0.1878,  0.1942,  ...,  0.0495,  0.0515,  0.0355],\n",
      "         [ 0.1913,  0.1883,  0.1952,  ...,  0.0500,  0.0455,  0.0555]],\n",
      "\n",
      "        [[ 0.1693,  0.1659,  0.1688,  ..., -0.0615, -0.0145, -0.0115],\n",
      "         [ 0.1674,  0.1707,  0.1705,  ...,  0.0205,  0.0035,  0.0170],\n",
      "         [ 0.1728,  0.1678,  0.1738,  ..., -0.0060,  0.0045,  0.0075],\n",
      "         ...,\n",
      "         [ 0.1925,  0.1878,  0.1942,  ...,  0.0495,  0.0515,  0.0355],\n",
      "         [ 0.1913,  0.1883,  0.1952,  ...,  0.0500,  0.0455,  0.0555],\n",
      "         [ 0.1925,  0.1874,  0.1960,  ...,  0.0570,  0.0885,  0.0695]]],\n",
      "       device='cuda:0'))\n",
      "Tensor(backend=torch, shape=torch.Size([8063, 1]), data=\n",
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        ...,\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.]], device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset: 80% training / 20% testing\n",
    "\n",
    "# We must use time-series to avoid data leakage\n",
    "feature_train, feature_test, target_train, target_test = MLBase.train_test_split_for_timeseries(\n",
    "    features_index_w, targets_index_w, 0.2)\n",
    "\n",
    "print(feature_train)\n",
    "print(target_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2.3 Train your Simple RNN`\n",
    "\n",
    "and\n",
    "\n",
    "`2.3.1 Report Training and testing accuracy`\n",
    "\n",
    "and \n",
    "\n",
    "`2.3.2 Compare your RNN results to a simple logistic regression baseline.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an Evaluation Pipeline\n",
    "def eval_pipeline(evaluator: nn_SInterf_Evaluator, X: Tensor, y_true: Tensor, plot_roc = True) -> tuple:\n",
    "\n",
    "    pred = evaluator.predict(X)\n",
    "    acc = BinaryClassificationMetrics(pred, y_true, \"accuracy\").compute().to_list()\n",
    "    prec = BinaryClassificationMetrics(pred, y_true, \"precision\").compute().to_list()\n",
    "    recall = BinaryClassificationMetrics(pred, y_true, \"recall\").compute().to_list()\n",
    "    f1s = BinaryClassificationMetrics(pred, y_true, \"f1\").compute().to_list()\n",
    "    cfm = BinaryClassificationMetrics(pred, y_true, \"confusion_matrix\").compute()\n",
    "    # Normalize the confusion matrix\n",
    "    cfm = cfm.to(backend=backend,device=\"cpu\",dtype=int)\n",
    "    # Plot ROC curve\n",
    "    if plot_roc == True:\n",
    "        from sklearn.metrics import roc_curve, auc\n",
    "        false_positive_rate, true_positive_rate, _ = roc_curve(y_true.flatten().to_numpy_array(), pred.flatten().to_numpy_array())\n",
    "        auc_score = auc(false_positive_rate, true_positive_rate)\n",
    "        plt.figure()\n",
    "        lw = 2\n",
    "        plt.title(f'ROC curve (area = {round(auc_score, 4)})')\n",
    "        plt.xlabel(\"FPR\")\n",
    "        plt.ylabel(\"TPR\")\n",
    "        plt.plot(false_positive_rate, true_positive_rate, color='darkorange', lw=lw)\n",
    "    return pred, acc, prec, recall, f1s, cfm, \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct A Simple but Well Crafted Stacked RNN Model\n",
    "\n",
    "class Model_1(nn_Module):\n",
    "\n",
    "    def __init__(self, input_num, **kwargs):\n",
    "        # Nathmath Huang (This is a template definition)\n",
    "        super().__init__(module_name=\"Model_1\", **kwargs)\n",
    "        # Stacked RNNs\n",
    "        self.rnn_1 = StackedRNN(input_num, hid_features=128, num_layers=1, has_bias=True,\n",
    "                                init_scale=0.2, actv='tanh', module_name=\"RNN_1\", **kwargs)\n",
    "\n",
    "        # Output Layers\n",
    "        self.dense_1 = Dense(128, 64, init_scale=0.2, **kwargs)\n",
    "        self.actv_1 = Sigmoid(**kwargs)\n",
    "        self.dense_2 = Dense(64, 1, init_scale=0.2, **kwargs)\n",
    "        self.actv_2 = Sigmoid(**kwargs)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        out = self.rnn_1(inputs)\n",
    "        out = self.dense_1(out)\n",
    "        out = self.actv_1(out)\n",
    "        out = self.dense_2(out)\n",
    "        out = self.actv_2(out)\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a Simple Logistic Model\n",
    "\n",
    "class Logistic_1(nn_Module):\n",
    "\n",
    "    def __init__(self, flatten_size, **kwargs):\n",
    "        # Nathmath Huang (This is a template definition)\n",
    "        super().__init__(module_name=\"Logistic_1\", **kwargs)\n",
    "\n",
    "        self.flatten_1 = Flatten(**kwargs)\n",
    "        self.dense_1 = Dense(flatten_size, 1, True, **kwargs)\n",
    "        self.actv_1 = Sigmoid(**kwargs)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        out = self.flatten_1(inputs)\n",
    "        out = self.dense_1(out)\n",
    "        out = self.actv_1(out)\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histogram of Errors\n",
    "\n",
    "def plot_error_histograms(data1, data2, bins1=100, bins2=100, cmap_name='plasma', which='Target', names=[\"Data1\", \"Data2\"]):\n",
    "\n",
    "    # Flatten and clean both datasets\n",
    "    data1 = data1.flatten()\n",
    "    data2 = data2.flatten()\n",
    "    data1 = data1[~np.isnan(data1)]\n",
    "    data2 = data2[~np.isnan(data2)]\n",
    "    data1 = data1[data1 > 0]\n",
    "    data2 = data2[data2 > 0]\n",
    "\n",
    "    # Define binning shared across both datasets\n",
    "    combined_data = np.concatenate((data1, data2))\n",
    "    bins = np.logspace(np.log10(combined_data.min()), np.log10(combined_data.max()), num=50)\n",
    "\n",
    "    # Plot first histogram\n",
    "    plt.hist(data1, bins=bins1, color='skyblue', edgecolor='black', alpha=0.6, label=names[0])\n",
    "\n",
    "    # Plot second histogram\n",
    "    plt.hist(data2, bins=bins2, color='salmon', edgecolor='darkred', alpha=0.6, label=names[1])\n",
    "\n",
    "    # Set log scale\n",
    "    plt.xscale('log')\n",
    "    plt.yscale('log')\n",
    "\n",
    "    # Labeling and aesthetics\n",
    "    plt.xlabel(which)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Comparison of Two Distributions of ' + which)\n",
    "    plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3.1.1 Results from the RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on Total Epoch: 10705, Round: 0\n",
      "Training on Total Epoch: 10706, Round: 1\n",
      "Training on Total Epoch: 10707, Round: 2\n",
      "Evalset: [Train : Metrics { log-loss:0.3024,  logloss:0.3024,  accuracy:0.8579,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.5073,  logloss:0.5073,  accuracy:0.7222,  }]\n",
      "Training on Total Epoch: 10708, Round: 3\n",
      "Evalset: [Train : Metrics { log-loss:0.3024,  logloss:0.3024,  accuracy:0.8586,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.5073,  logloss:0.5073,  accuracy:0.7222,  }]\n",
      "Training on Total Epoch: 10709, Round: 4\n",
      "Evalset: [Train : Metrics { log-loss:0.3024,  logloss:0.3024,  accuracy:0.8584,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.5073,  logloss:0.5073,  accuracy:0.7222,  }]\n",
      "Training on Total Epoch: 10710, Round: 5\n",
      "Evalset: [Train : Metrics { log-loss:0.3024,  logloss:0.3024,  accuracy:0.8581,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.5073,  logloss:0.5073,  accuracy:0.7222,  }]\n",
      "Training on Total Epoch: 10711, Round: 6\n",
      "Evalset: [Train : Metrics { log-loss:0.3024,  logloss:0.3024,  accuracy:0.8596,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.5073,  logloss:0.5073,  accuracy:0.7252,  }]\n",
      "Training on Total Epoch: 10712, Round: 7\n",
      "Evalset: [Train : Metrics { log-loss:0.3023,  logloss:0.3023,  accuracy:0.8584,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.5072,  logloss:0.5072,  accuracy:0.7222,  }]\n",
      "Training on Total Epoch: 10713, Round: 8\n",
      "Evalset: [Train : Metrics { log-loss:0.3023,  logloss:0.3023,  accuracy:0.8584,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.5072,  logloss:0.5072,  accuracy:0.7222,  }]\n",
      "Training on Total Epoch: 10714, Round: 9\n",
      "Evalset: [Train : Metrics { log-loss:0.3023,  logloss:0.3023,  accuracy:0.8592,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.5072,  logloss:0.5072,  accuracy:0.7237,  }]\n"
     ]
    }
   ],
   "source": [
    "# Train and Evaluate the model - RNN Model with 1 Stacked Layers\n",
    "# \n",
    "# Train the Hybrid RNN Model\n",
    "model1 = Model_1(input_num=16, backend=backend, device=device, autograd=False)\n",
    "crit1 = BinaryCrossEntropy(backend=backend, device=device, autograd=False)\n",
    "optm1 = AdamW(model1.parameters(), lr = 1e-3, eps = 1e-10, backend=backend, device=device, autograd=False)\n",
    "eval1 = Evaluator(\"Eval_1\", task = \"classification\", module=model1, criterion=crit1, optimizer=optm1)\n",
    "\n",
    "# Start to train the RNN Model\n",
    "eval1.fit(\n",
    "    X = feature_train, y = target_train,\n",
    "    epoches = 10,\n",
    "    batch_size = 4096,\n",
    "    shuffle = True,\n",
    "    one_hot = False,\n",
    "    random_state = None,\n",
    "    verbosity = 1,\n",
    "    evalper = 1,\n",
    "    evalset = {\n",
    "        \"Train\": (feature_train, target_train),\n",
    "        \"Test\" : (feature_test, target_test)\n",
    "    },\n",
    "    evalmetrics = [\"log-loss\", \"logloss\", \"accuracy\"],\n",
    "    early_stop = 5,\n",
    "    early_stop_logic = \"most\"\n",
    ")\n",
    "\n",
    "eval1.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and Evaluate the model - RNN Model with 1 Stacked Layers\n",
    "# \n",
    "\n",
    "# Save the model and evaluator\n",
    "model1.save(model1, \"./models/RNN_Model_ID01_ckpt9.bin\")\n",
    "eval1.save(eval1, \"./models/RNN_Eval_ID01_ckpt9.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Config: {Benchmark Evaluation: RNN with 1 Stacked Layers }\n",
      "ACC    : 0.72371\n",
      "PREC   : 0.732037\n",
      "RECALL : 0.783002\n",
      "F1     : 0.756662\n",
      "CONFUSION MATRIX:\n",
      "[[TP, FP],\n",
      "[FN, TN]]\n",
      " [[866 317]\n",
      " [240 593]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train and Evaluate the model - RNN Model with 1 Stacked Layers\n",
    "# \n",
    "\n",
    "# Print the results for the test-set\n",
    "eval1.eval()\n",
    "pred, acc, prec, recall, f1, cfm = eval_pipeline(eval1, feature_test, target_test)\n",
    "print(\"Model Config: {Benchmark Evaluation: RNN with 1 Stacked Layers }\")\n",
    "print(\"ACC    :\", round(acc, 6))\n",
    "print(\"PREC   :\", round(prec, 6))\n",
    "print(\"RECALL :\", round(recall, 6))\n",
    "print(\"F1     :\", round(f1, 6))\n",
    "print(\"CONFUSION MATRIX:\\n[[TP, FP],\\n[FN, TN]]\\n\", cfm.to_numpy_array())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yeah, I have felt this result surprising.\n",
    "# And I have double checked that there are NOT any obvious leakage or issues leading to this high level of accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3.1.3 Results from the Logistic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on Total Epoch: 73956, Round: 0\n",
      "Training on Total Epoch: 73957, Round: 1\n",
      "Training on Total Epoch: 73958, Round: 2\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 73959, Round: 3\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 73960, Round: 4\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 73961, Round: 5\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 73962, Round: 6\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 73963, Round: 7\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 73964, Round: 8\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 73965, Round: 9\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 73966, Round: 10\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 73967, Round: 11\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 73968, Round: 12\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 73969, Round: 13\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 73970, Round: 14\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 73971, Round: 15\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 73972, Round: 16\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 73973, Round: 17\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 73974, Round: 18\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 73975, Round: 19\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 73976, Round: 20\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 73977, Round: 21\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 73978, Round: 22\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 73979, Round: 23\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 73980, Round: 24\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 73981, Round: 25\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 73982, Round: 26\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 73983, Round: 27\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 73984, Round: 28\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 73985, Round: 29\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 73986, Round: 30\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 73987, Round: 31\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 73988, Round: 32\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 73989, Round: 33\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 73990, Round: 34\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 73991, Round: 35\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 73992, Round: 36\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 73993, Round: 37\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 73994, Round: 38\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 73995, Round: 39\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 73996, Round: 40\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 73997, Round: 41\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 73998, Round: 42\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 73999, Round: 43\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74000, Round: 44\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74001, Round: 45\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74002, Round: 46\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74003, Round: 47\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74004, Round: 48\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74005, Round: 49\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74006, Round: 50\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74007, Round: 51\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74008, Round: 52\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74009, Round: 53\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74010, Round: 54\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74011, Round: 55\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74012, Round: 56\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74013, Round: 57\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74014, Round: 58\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74015, Round: 59\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74016, Round: 60\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74017, Round: 61\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74018, Round: 62\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74019, Round: 63\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74020, Round: 64\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74021, Round: 65\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74022, Round: 66\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74023, Round: 67\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74024, Round: 68\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74025, Round: 69\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74026, Round: 70\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74027, Round: 71\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74028, Round: 72\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74029, Round: 73\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74030, Round: 74\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74031, Round: 75\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74032, Round: 76\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74033, Round: 77\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74034, Round: 78\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74035, Round: 79\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74036, Round: 80\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74037, Round: 81\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74038, Round: 82\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74039, Round: 83\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74040, Round: 84\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74041, Round: 85\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74042, Round: 86\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74043, Round: 87\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74044, Round: 88\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74045, Round: 89\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74046, Round: 90\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74047, Round: 91\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74048, Round: 92\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74049, Round: 93\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74050, Round: 94\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74051, Round: 95\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74052, Round: 96\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74053, Round: 97\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74054, Round: 98\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74055, Round: 99\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74056, Round: 100\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74057, Round: 101\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74058, Round: 102\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74059, Round: 103\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74060, Round: 104\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74061, Round: 105\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74062, Round: 106\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74063, Round: 107\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74064, Round: 108\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74065, Round: 109\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74066, Round: 110\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74067, Round: 111\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74068, Round: 112\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74069, Round: 113\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74070, Round: 114\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74071, Round: 115\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74072, Round: 116\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74073, Round: 117\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74074, Round: 118\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74075, Round: 119\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74076, Round: 120\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74077, Round: 121\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74078, Round: 122\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74079, Round: 123\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74080, Round: 124\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74081, Round: 125\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74082, Round: 126\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74083, Round: 127\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74084, Round: 128\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74085, Round: 129\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74086, Round: 130\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74087, Round: 131\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74088, Round: 132\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74089, Round: 133\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74090, Round: 134\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74091, Round: 135\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74092, Round: 136\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74093, Round: 137\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74094, Round: 138\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74095, Round: 139\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74096, Round: 140\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74097, Round: 141\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74098, Round: 142\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74099, Round: 143\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74100, Round: 144\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74101, Round: 145\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74102, Round: 146\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74103, Round: 147\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74104, Round: 148\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74105, Round: 149\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74106, Round: 150\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74107, Round: 151\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74108, Round: 152\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74109, Round: 153\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74110, Round: 154\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74111, Round: 155\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74112, Round: 156\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74113, Round: 157\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74114, Round: 158\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74115, Round: 159\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74116, Round: 160\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74117, Round: 161\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74118, Round: 162\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74119, Round: 163\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74120, Round: 164\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74121, Round: 165\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74122, Round: 166\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74123, Round: 167\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74124, Round: 168\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74125, Round: 169\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74126, Round: 170\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74127, Round: 171\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74128, Round: 172\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74129, Round: 173\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74130, Round: 174\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74131, Round: 175\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74132, Round: 176\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74133, Round: 177\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74134, Round: 178\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74135, Round: 179\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74136, Round: 180\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74137, Round: 181\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74138, Round: 182\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74139, Round: 183\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74140, Round: 184\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74141, Round: 185\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74142, Round: 186\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74143, Round: 187\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74144, Round: 188\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74145, Round: 189\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74146, Round: 190\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74147, Round: 191\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74148, Round: 192\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74149, Round: 193\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74150, Round: 194\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74151, Round: 195\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74152, Round: 196\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74153, Round: 197\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74154, Round: 198\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74155, Round: 199\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74156, Round: 200\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74157, Round: 201\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74158, Round: 202\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74159, Round: 203\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74160, Round: 204\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74161, Round: 205\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74162, Round: 206\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74163, Round: 207\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74164, Round: 208\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74165, Round: 209\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74166, Round: 210\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74167, Round: 211\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74168, Round: 212\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74169, Round: 213\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74170, Round: 214\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74171, Round: 215\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74172, Round: 216\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74173, Round: 217\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74174, Round: 218\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74175, Round: 219\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74176, Round: 220\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74177, Round: 221\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74178, Round: 222\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74179, Round: 223\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74180, Round: 224\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74181, Round: 225\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74182, Round: 226\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74183, Round: 227\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74184, Round: 228\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74185, Round: 229\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74186, Round: 230\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74187, Round: 231\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74188, Round: 232\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74189, Round: 233\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74190, Round: 234\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74191, Round: 235\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74192, Round: 236\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74193, Round: 237\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74194, Round: 238\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74195, Round: 239\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74196, Round: 240\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74197, Round: 241\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74198, Round: 242\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74199, Round: 243\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74200, Round: 244\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74201, Round: 245\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74202, Round: 246\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74203, Round: 247\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74204, Round: 248\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74205, Round: 249\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74206, Round: 250\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74207, Round: 251\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74208, Round: 252\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74209, Round: 253\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74210, Round: 254\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74211, Round: 255\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74212, Round: 256\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74213, Round: 257\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74214, Round: 258\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74215, Round: 259\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74216, Round: 260\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74217, Round: 261\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74218, Round: 262\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74219, Round: 263\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74220, Round: 264\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74221, Round: 265\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74222, Round: 266\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74223, Round: 267\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74224, Round: 268\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74225, Round: 269\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74226, Round: 270\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74227, Round: 271\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74228, Round: 272\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74229, Round: 273\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74230, Round: 274\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74231, Round: 275\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74232, Round: 276\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74233, Round: 277\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74234, Round: 278\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74235, Round: 279\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74236, Round: 280\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74237, Round: 281\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74238, Round: 282\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74239, Round: 283\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74240, Round: 284\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74241, Round: 285\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74242, Round: 286\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74243, Round: 287\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74244, Round: 288\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74245, Round: 289\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74246, Round: 290\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74247, Round: 291\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74248, Round: 292\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74249, Round: 293\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74250, Round: 294\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74251, Round: 295\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74252, Round: 296\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74253, Round: 297\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74254, Round: 298\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74255, Round: 299\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74256, Round: 300\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74257, Round: 301\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74258, Round: 302\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74259, Round: 303\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74260, Round: 304\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74261, Round: 305\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74262, Round: 306\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74263, Round: 307\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74264, Round: 308\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74265, Round: 309\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74266, Round: 310\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74267, Round: 311\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74268, Round: 312\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74269, Round: 313\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74270, Round: 314\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74271, Round: 315\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74272, Round: 316\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74273, Round: 317\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74274, Round: 318\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74275, Round: 319\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74276, Round: 320\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74277, Round: 321\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74278, Round: 322\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74279, Round: 323\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74280, Round: 324\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74281, Round: 325\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74282, Round: 326\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74283, Round: 327\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74284, Round: 328\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74285, Round: 329\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74286, Round: 330\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74287, Round: 331\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74288, Round: 332\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74289, Round: 333\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74290, Round: 334\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74291, Round: 335\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74292, Round: 336\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74293, Round: 337\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74294, Round: 338\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74295, Round: 339\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74296, Round: 340\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74297, Round: 341\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74298, Round: 342\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74299, Round: 343\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74300, Round: 344\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74301, Round: 345\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74302, Round: 346\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74303, Round: 347\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74304, Round: 348\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74305, Round: 349\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74306, Round: 350\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74307, Round: 351\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74308, Round: 352\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74309, Round: 353\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74310, Round: 354\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74311, Round: 355\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74312, Round: 356\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74313, Round: 357\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74314, Round: 358\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74315, Round: 359\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74316, Round: 360\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74317, Round: 361\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74318, Round: 362\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74319, Round: 363\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74320, Round: 364\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74321, Round: 365\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74322, Round: 366\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74323, Round: 367\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74324, Round: 368\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74325, Round: 369\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74326, Round: 370\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74327, Round: 371\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74328, Round: 372\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74329, Round: 373\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74330, Round: 374\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74331, Round: 375\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74332, Round: 376\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74333, Round: 377\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74334, Round: 378\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74335, Round: 379\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74336, Round: 380\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74337, Round: 381\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74338, Round: 382\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74339, Round: 383\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74340, Round: 384\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74341, Round: 385\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74342, Round: 386\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74343, Round: 387\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74344, Round: 388\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74345, Round: 389\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74346, Round: 390\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74347, Round: 391\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74348, Round: 392\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74349, Round: 393\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74350, Round: 394\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74351, Round: 395\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74352, Round: 396\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74353, Round: 397\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74354, Round: 398\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74355, Round: 399\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74356, Round: 400\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74357, Round: 401\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74358, Round: 402\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74359, Round: 403\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74360, Round: 404\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74361, Round: 405\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74362, Round: 406\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74363, Round: 407\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74364, Round: 408\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74365, Round: 409\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74366, Round: 410\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74367, Round: 411\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74368, Round: 412\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74369, Round: 413\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74370, Round: 414\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74371, Round: 415\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74372, Round: 416\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74373, Round: 417\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74374, Round: 418\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74375, Round: 419\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74376, Round: 420\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74377, Round: 421\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74378, Round: 422\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74379, Round: 423\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74380, Round: 424\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74381, Round: 425\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74382, Round: 426\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74383, Round: 427\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74384, Round: 428\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74385, Round: 429\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74386, Round: 430\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74387, Round: 431\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74388, Round: 432\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74389, Round: 433\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74390, Round: 434\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74391, Round: 435\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74392, Round: 436\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74393, Round: 437\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74394, Round: 438\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74395, Round: 439\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74396, Round: 440\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74397, Round: 441\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74398, Round: 442\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74399, Round: 443\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74400, Round: 444\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74401, Round: 445\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74402, Round: 446\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74403, Round: 447\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74404, Round: 448\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74405, Round: 449\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74406, Round: 450\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74407, Round: 451\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74408, Round: 452\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74409, Round: 453\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74410, Round: 454\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74411, Round: 455\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74412, Round: 456\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74413, Round: 457\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74414, Round: 458\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74415, Round: 459\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74416, Round: 460\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74417, Round: 461\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74418, Round: 462\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74419, Round: 463\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74420, Round: 464\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74421, Round: 465\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74422, Round: 466\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74423, Round: 467\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74424, Round: 468\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74425, Round: 469\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74426, Round: 470\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74427, Round: 471\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74428, Round: 472\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74429, Round: 473\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74430, Round: 474\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74431, Round: 475\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74432, Round: 476\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74433, Round: 477\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74434, Round: 478\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74435, Round: 479\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74436, Round: 480\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74437, Round: 481\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74438, Round: 482\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74439, Round: 483\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74440, Round: 484\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74441, Round: 485\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74442, Round: 486\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74443, Round: 487\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74444, Round: 488\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74445, Round: 489\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74446, Round: 490\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74447, Round: 491\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74448, Round: 492\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74449, Round: 493\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74450, Round: 494\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74451, Round: 495\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74452, Round: 496\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74453, Round: 497\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74454, Round: 498\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74455, Round: 499\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74456, Round: 500\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74457, Round: 501\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74458, Round: 502\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74459, Round: 503\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74460, Round: 504\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74461, Round: 505\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74462, Round: 506\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74463, Round: 507\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74464, Round: 508\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74465, Round: 509\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74466, Round: 510\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74467, Round: 511\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74468, Round: 512\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74469, Round: 513\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74470, Round: 514\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74471, Round: 515\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74472, Round: 516\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74473, Round: 517\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74474, Round: 518\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74475, Round: 519\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74476, Round: 520\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74477, Round: 521\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74478, Round: 522\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74479, Round: 523\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74480, Round: 524\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74481, Round: 525\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74482, Round: 526\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74483, Round: 527\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74484, Round: 528\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74485, Round: 529\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74486, Round: 530\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74487, Round: 531\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74488, Round: 532\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74489, Round: 533\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74490, Round: 534\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74491, Round: 535\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74492, Round: 536\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74493, Round: 537\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74494, Round: 538\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74495, Round: 539\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74496, Round: 540\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74497, Round: 541\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74498, Round: 542\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74499, Round: 543\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74500, Round: 544\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74501, Round: 545\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74502, Round: 546\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74503, Round: 547\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74504, Round: 548\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74505, Round: 549\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74506, Round: 550\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74507, Round: 551\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74508, Round: 552\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74509, Round: 553\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74510, Round: 554\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74511, Round: 555\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74512, Round: 556\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74513, Round: 557\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74514, Round: 558\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74515, Round: 559\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74516, Round: 560\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74517, Round: 561\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74518, Round: 562\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74519, Round: 563\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74520, Round: 564\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74521, Round: 565\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74522, Round: 566\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74523, Round: 567\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74524, Round: 568\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74525, Round: 569\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74526, Round: 570\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74527, Round: 571\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74528, Round: 572\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74529, Round: 573\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74530, Round: 574\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74531, Round: 575\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74532, Round: 576\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74533, Round: 577\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74534, Round: 578\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74535, Round: 579\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74536, Round: 580\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74537, Round: 581\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74538, Round: 582\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74539, Round: 583\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74540, Round: 584\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74541, Round: 585\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74542, Round: 586\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74543, Round: 587\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74544, Round: 588\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74545, Round: 589\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74546, Round: 590\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74547, Round: 591\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74548, Round: 592\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74549, Round: 593\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74550, Round: 594\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74551, Round: 595\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74552, Round: 596\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74553, Round: 597\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74554, Round: 598\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74555, Round: 599\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74556, Round: 600\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74557, Round: 601\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74558, Round: 602\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74559, Round: 603\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74560, Round: 604\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74561, Round: 605\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74562, Round: 606\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74563, Round: 607\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74564, Round: 608\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74565, Round: 609\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74566, Round: 610\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74567, Round: 611\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74568, Round: 612\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74569, Round: 613\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74570, Round: 614\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74571, Round: 615\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74572, Round: 616\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74573, Round: 617\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74574, Round: 618\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74575, Round: 619\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74576, Round: 620\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74577, Round: 621\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74578, Round: 622\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74579, Round: 623\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74580, Round: 624\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74581, Round: 625\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74582, Round: 626\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74583, Round: 627\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74584, Round: 628\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74585, Round: 629\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74586, Round: 630\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74587, Round: 631\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74588, Round: 632\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74589, Round: 633\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74590, Round: 634\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74591, Round: 635\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74592, Round: 636\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74593, Round: 637\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74594, Round: 638\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74595, Round: 639\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74596, Round: 640\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74597, Round: 641\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74598, Round: 642\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74599, Round: 643\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74600, Round: 644\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74601, Round: 645\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74602, Round: 646\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74603, Round: 647\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74604, Round: 648\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74605, Round: 649\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74606, Round: 650\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74607, Round: 651\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74608, Round: 652\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74609, Round: 653\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74610, Round: 654\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74611, Round: 655\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74612, Round: 656\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74613, Round: 657\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74614, Round: 658\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74615, Round: 659\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74616, Round: 660\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74617, Round: 661\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74618, Round: 662\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74619, Round: 663\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74620, Round: 664\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74621, Round: 665\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74622, Round: 666\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74623, Round: 667\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74624, Round: 668\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74625, Round: 669\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74626, Round: 670\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74627, Round: 671\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74628, Round: 672\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74629, Round: 673\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74630, Round: 674\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 74631, Round: 675\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74632, Round: 676\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74633, Round: 677\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74634, Round: 678\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74635, Round: 679\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74636, Round: 680\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74637, Round: 681\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74638, Round: 682\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74639, Round: 683\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74640, Round: 684\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74641, Round: 685\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74642, Round: 686\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74643, Round: 687\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74644, Round: 688\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74645, Round: 689\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74646, Round: 690\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74647, Round: 691\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74648, Round: 692\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74649, Round: 693\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74650, Round: 694\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74651, Round: 695\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74652, Round: 696\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74653, Round: 697\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74654, Round: 698\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74655, Round: 699\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74656, Round: 700\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74657, Round: 701\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74658, Round: 702\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74659, Round: 703\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74660, Round: 704\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74661, Round: 705\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74662, Round: 706\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74663, Round: 707\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74664, Round: 708\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74665, Round: 709\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74666, Round: 710\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74667, Round: 711\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74668, Round: 712\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74669, Round: 713\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74670, Round: 714\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74671, Round: 715\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74672, Round: 716\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74673, Round: 717\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74674, Round: 718\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74675, Round: 719\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74676, Round: 720\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74677, Round: 721\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74678, Round: 722\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74679, Round: 723\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74680, Round: 724\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74681, Round: 725\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74682, Round: 726\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74683, Round: 727\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74684, Round: 728\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74685, Round: 729\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74686, Round: 730\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74687, Round: 731\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74688, Round: 732\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74689, Round: 733\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74690, Round: 734\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74691, Round: 735\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74692, Round: 736\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74693, Round: 737\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74694, Round: 738\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74695, Round: 739\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74696, Round: 740\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74697, Round: 741\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74698, Round: 742\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74699, Round: 743\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74700, Round: 744\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74701, Round: 745\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74702, Round: 746\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74703, Round: 747\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74704, Round: 748\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74705, Round: 749\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74706, Round: 750\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74707, Round: 751\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74708, Round: 752\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74709, Round: 753\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74710, Round: 754\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74711, Round: 755\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74712, Round: 756\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74713, Round: 757\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74714, Round: 758\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74715, Round: 759\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74716, Round: 760\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74717, Round: 761\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74718, Round: 762\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74719, Round: 763\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74720, Round: 764\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74721, Round: 765\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74722, Round: 766\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74723, Round: 767\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74724, Round: 768\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74725, Round: 769\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74726, Round: 770\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74727, Round: 771\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74728, Round: 772\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74729, Round: 773\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74730, Round: 774\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74731, Round: 775\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74732, Round: 776\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74733, Round: 777\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74734, Round: 778\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74735, Round: 779\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74736, Round: 780\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74737, Round: 781\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74738, Round: 782\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74739, Round: 783\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74740, Round: 784\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74741, Round: 785\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74742, Round: 786\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74743, Round: 787\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74744, Round: 788\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74745, Round: 789\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74746, Round: 790\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74747, Round: 791\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74748, Round: 792\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74749, Round: 793\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74750, Round: 794\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74751, Round: 795\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74752, Round: 796\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74753, Round: 797\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74754, Round: 798\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74755, Round: 799\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74756, Round: 800\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74757, Round: 801\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74758, Round: 802\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74759, Round: 803\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74760, Round: 804\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74761, Round: 805\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74762, Round: 806\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74763, Round: 807\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74764, Round: 808\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74765, Round: 809\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74766, Round: 810\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74767, Round: 811\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74768, Round: 812\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74769, Round: 813\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74770, Round: 814\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74771, Round: 815\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74772, Round: 816\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74773, Round: 817\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74774, Round: 818\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74775, Round: 819\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74776, Round: 820\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74777, Round: 821\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74778, Round: 822\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74779, Round: 823\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74780, Round: 824\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74781, Round: 825\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74782, Round: 826\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74783, Round: 827\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74784, Round: 828\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74785, Round: 829\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74786, Round: 830\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74787, Round: 831\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74788, Round: 832\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74789, Round: 833\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74790, Round: 834\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74791, Round: 835\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74792, Round: 836\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74793, Round: 837\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74794, Round: 838\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74795, Round: 839\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74796, Round: 840\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74797, Round: 841\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74798, Round: 842\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74799, Round: 843\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74800, Round: 844\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74801, Round: 845\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74802, Round: 846\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74803, Round: 847\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74804, Round: 848\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74805, Round: 849\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74806, Round: 850\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74807, Round: 851\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74808, Round: 852\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74809, Round: 853\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74810, Round: 854\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74811, Round: 855\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74812, Round: 856\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74813, Round: 857\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74814, Round: 858\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74815, Round: 859\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74816, Round: 860\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74817, Round: 861\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74818, Round: 862\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74819, Round: 863\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74820, Round: 864\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74821, Round: 865\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74822, Round: 866\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74823, Round: 867\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74824, Round: 868\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74825, Round: 869\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74826, Round: 870\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74827, Round: 871\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74828, Round: 872\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74829, Round: 873\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74830, Round: 874\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74831, Round: 875\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74832, Round: 876\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74833, Round: 877\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74834, Round: 878\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74835, Round: 879\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74836, Round: 880\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74837, Round: 881\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74838, Round: 882\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74839, Round: 883\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74840, Round: 884\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74841, Round: 885\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74842, Round: 886\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74843, Round: 887\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74844, Round: 888\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74845, Round: 889\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74846, Round: 890\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74847, Round: 891\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74848, Round: 892\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74849, Round: 893\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74850, Round: 894\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74851, Round: 895\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74852, Round: 896\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74853, Round: 897\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74854, Round: 898\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74855, Round: 899\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74856, Round: 900\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74857, Round: 901\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74858, Round: 902\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74859, Round: 903\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74860, Round: 904\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74861, Round: 905\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74862, Round: 906\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74863, Round: 907\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74864, Round: 908\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74865, Round: 909\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74866, Round: 910\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74867, Round: 911\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74868, Round: 912\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74869, Round: 913\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74870, Round: 914\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74871, Round: 915\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74872, Round: 916\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74873, Round: 917\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74874, Round: 918\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74875, Round: 919\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74876, Round: 920\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74877, Round: 921\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74878, Round: 922\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74879, Round: 923\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74880, Round: 924\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74881, Round: 925\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74882, Round: 926\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74883, Round: 927\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74884, Round: 928\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74885, Round: 929\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74886, Round: 930\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74887, Round: 931\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74888, Round: 932\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74889, Round: 933\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74890, Round: 934\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74891, Round: 935\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74892, Round: 936\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74893, Round: 937\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74894, Round: 938\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74895, Round: 939\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74896, Round: 940\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74897, Round: 941\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74898, Round: 942\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74899, Round: 943\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74900, Round: 944\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74901, Round: 945\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74902, Round: 946\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74903, Round: 947\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74904, Round: 948\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74905, Round: 949\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74906, Round: 950\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74907, Round: 951\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74908, Round: 952\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74909, Round: 953\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74910, Round: 954\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74911, Round: 955\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74912, Round: 956\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74913, Round: 957\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74914, Round: 958\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74915, Round: 959\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74916, Round: 960\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74917, Round: 961\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74918, Round: 962\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74919, Round: 963\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74920, Round: 964\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74921, Round: 965\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74922, Round: 966\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74923, Round: 967\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74924, Round: 968\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74925, Round: 969\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74926, Round: 970\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74927, Round: 971\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74928, Round: 972\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74929, Round: 973\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74930, Round: 974\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74931, Round: 975\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74932, Round: 976\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74933, Round: 977\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74934, Round: 978\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74935, Round: 979\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74936, Round: 980\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74937, Round: 981\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74938, Round: 982\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74939, Round: 983\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74940, Round: 984\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74941, Round: 985\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74942, Round: 986\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74943, Round: 987\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74944, Round: 988\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74945, Round: 989\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74946, Round: 990\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74947, Round: 991\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74948, Round: 992\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74949, Round: 993\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74950, Round: 994\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74951, Round: 995\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74952, Round: 996\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74953, Round: 997\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74954, Round: 998\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74955, Round: 999\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74956, Round: 1000\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74957, Round: 1001\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74958, Round: 1002\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74959, Round: 1003\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74960, Round: 1004\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74961, Round: 1005\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74962, Round: 1006\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74963, Round: 1007\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74964, Round: 1008\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74965, Round: 1009\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74966, Round: 1010\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74967, Round: 1011\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74968, Round: 1012\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74969, Round: 1013\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74970, Round: 1014\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74971, Round: 1015\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74972, Round: 1016\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74973, Round: 1017\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74974, Round: 1018\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74975, Round: 1019\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74976, Round: 1020\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74977, Round: 1021\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74978, Round: 1022\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74979, Round: 1023\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74980, Round: 1024\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74981, Round: 1025\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74982, Round: 1026\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74983, Round: 1027\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74984, Round: 1028\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74985, Round: 1029\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74986, Round: 1030\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74987, Round: 1031\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74988, Round: 1032\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74989, Round: 1033\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74990, Round: 1034\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74991, Round: 1035\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74992, Round: 1036\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74993, Round: 1037\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74994, Round: 1038\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74995, Round: 1039\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74996, Round: 1040\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74997, Round: 1041\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74998, Round: 1042\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 74999, Round: 1043\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75000, Round: 1044\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75001, Round: 1045\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75002, Round: 1046\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75003, Round: 1047\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75004, Round: 1048\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75005, Round: 1049\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75006, Round: 1050\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75007, Round: 1051\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75008, Round: 1052\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75009, Round: 1053\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75010, Round: 1054\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75011, Round: 1055\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75012, Round: 1056\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75013, Round: 1057\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75014, Round: 1058\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75015, Round: 1059\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75016, Round: 1060\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75017, Round: 1061\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75018, Round: 1062\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75019, Round: 1063\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75020, Round: 1064\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75021, Round: 1065\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75022, Round: 1066\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75023, Round: 1067\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75024, Round: 1068\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75025, Round: 1069\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75026, Round: 1070\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75027, Round: 1071\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75028, Round: 1072\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75029, Round: 1073\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75030, Round: 1074\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75031, Round: 1075\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75032, Round: 1076\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75033, Round: 1077\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75034, Round: 1078\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75035, Round: 1079\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75036, Round: 1080\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75037, Round: 1081\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75038, Round: 1082\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75039, Round: 1083\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75040, Round: 1084\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75041, Round: 1085\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75042, Round: 1086\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75043, Round: 1087\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75044, Round: 1088\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75045, Round: 1089\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75046, Round: 1090\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75047, Round: 1091\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75048, Round: 1092\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75049, Round: 1093\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75050, Round: 1094\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75051, Round: 1095\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75052, Round: 1096\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75053, Round: 1097\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75054, Round: 1098\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75055, Round: 1099\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75056, Round: 1100\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75057, Round: 1101\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75058, Round: 1102\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75059, Round: 1103\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75060, Round: 1104\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75061, Round: 1105\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75062, Round: 1106\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75063, Round: 1107\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75064, Round: 1108\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75065, Round: 1109\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75066, Round: 1110\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75067, Round: 1111\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75068, Round: 1112\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75069, Round: 1113\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75070, Round: 1114\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75071, Round: 1115\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75072, Round: 1116\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75073, Round: 1117\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75074, Round: 1118\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75075, Round: 1119\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75076, Round: 1120\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75077, Round: 1121\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75078, Round: 1122\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75079, Round: 1123\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75080, Round: 1124\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75081, Round: 1125\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75082, Round: 1126\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75083, Round: 1127\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75084, Round: 1128\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75085, Round: 1129\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75086, Round: 1130\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75087, Round: 1131\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75088, Round: 1132\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75089, Round: 1133\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75090, Round: 1134\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75091, Round: 1135\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75092, Round: 1136\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75093, Round: 1137\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75094, Round: 1138\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75095, Round: 1139\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75096, Round: 1140\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75097, Round: 1141\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75098, Round: 1142\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75099, Round: 1143\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75100, Round: 1144\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75101, Round: 1145\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75102, Round: 1146\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75103, Round: 1147\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75104, Round: 1148\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75105, Round: 1149\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75106, Round: 1150\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75107, Round: 1151\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75108, Round: 1152\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75109, Round: 1153\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75110, Round: 1154\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75111, Round: 1155\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75112, Round: 1156\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75113, Round: 1157\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75114, Round: 1158\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75115, Round: 1159\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75116, Round: 1160\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75117, Round: 1161\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75118, Round: 1162\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75119, Round: 1163\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75120, Round: 1164\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75121, Round: 1165\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75122, Round: 1166\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75123, Round: 1167\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75124, Round: 1168\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75125, Round: 1169\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75126, Round: 1170\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75127, Round: 1171\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75128, Round: 1172\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75129, Round: 1173\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75130, Round: 1174\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75131, Round: 1175\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5634,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75132, Round: 1176\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5636,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75133, Round: 1177\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5636,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75134, Round: 1178\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5636,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75135, Round: 1179\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5636,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75136, Round: 1180\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5636,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75137, Round: 1181\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5636,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75138, Round: 1182\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5636,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75139, Round: 1183\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5636,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75140, Round: 1184\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5636,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75141, Round: 1185\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5636,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75142, Round: 1186\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5636,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75143, Round: 1187\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5636,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75144, Round: 1188\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5636,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75145, Round: 1189\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5636,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75146, Round: 1190\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5636,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75147, Round: 1191\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5636,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75148, Round: 1192\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5636,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75149, Round: 1193\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5636,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75150, Round: 1194\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5636,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75151, Round: 1195\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5636,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75152, Round: 1196\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5636,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75153, Round: 1197\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5636,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75154, Round: 1198\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5636,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75155, Round: 1199\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5636,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75156, Round: 1200\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5636,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75157, Round: 1201\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5636,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75158, Round: 1202\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5636,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75159, Round: 1203\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5636,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75160, Round: 1204\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5636,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75161, Round: 1205\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5636,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75162, Round: 1206\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5636,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75163, Round: 1207\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5636,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75164, Round: 1208\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5636,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75165, Round: 1209\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5636,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75166, Round: 1210\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5636,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75167, Round: 1211\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5636,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75168, Round: 1212\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5636,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75169, Round: 1213\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5636,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75170, Round: 1214\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5636,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75171, Round: 1215\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5636,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75172, Round: 1216\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5636,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75173, Round: 1217\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5636,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75174, Round: 1218\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5636,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75175, Round: 1219\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5636,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75176, Round: 1220\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5636,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75177, Round: 1221\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5636,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75178, Round: 1222\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5636,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75179, Round: 1223\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5636,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75180, Round: 1224\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5636,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75181, Round: 1225\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5636,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75182, Round: 1226\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5636,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75183, Round: 1227\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5636,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75184, Round: 1228\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5636,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75185, Round: 1229\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5636,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75186, Round: 1230\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5636,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75187, Round: 1231\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5636,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75188, Round: 1232\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5636,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75189, Round: 1233\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5636,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75190, Round: 1234\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5636,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75191, Round: 1235\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5636,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75192, Round: 1236\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5636,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75193, Round: 1237\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5636,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75194, Round: 1238\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5636,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75195, Round: 1239\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5636,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75196, Round: 1240\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5636,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75197, Round: 1241\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5636,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75198, Round: 1242\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5636,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75199, Round: 1243\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5636,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75200, Round: 1244\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5636,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75201, Round: 1245\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5636,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75202, Round: 1246\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5636,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75203, Round: 1247\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5636,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75204, Round: 1248\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5636,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75205, Round: 1249\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5636,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75206, Round: 1250\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5636,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75207, Round: 1251\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5636,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75208, Round: 1252\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5636,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75209, Round: 1253\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5636,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75210, Round: 1254\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5636,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75211, Round: 1255\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5636,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75212, Round: 1256\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5636,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75213, Round: 1257\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5636,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75214, Round: 1258\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5636,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75215, Round: 1259\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5636,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75216, Round: 1260\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5636,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75217, Round: 1261\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5636,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75218, Round: 1262\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5636,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75219, Round: 1263\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5636,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75220, Round: 1264\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5636,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75221, Round: 1265\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75222, Round: 1266\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75223, Round: 1267\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75224, Round: 1268\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75225, Round: 1269\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75226, Round: 1270\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75227, Round: 1271\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75228, Round: 1272\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75229, Round: 1273\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75230, Round: 1274\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75231, Round: 1275\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75232, Round: 1276\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75233, Round: 1277\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75234, Round: 1278\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75235, Round: 1279\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75236, Round: 1280\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75237, Round: 1281\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75238, Round: 1282\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75239, Round: 1283\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75240, Round: 1284\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75241, Round: 1285\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75242, Round: 1286\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75243, Round: 1287\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75244, Round: 1288\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75245, Round: 1289\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75246, Round: 1290\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75247, Round: 1291\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75248, Round: 1292\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75249, Round: 1293\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75250, Round: 1294\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75251, Round: 1295\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75252, Round: 1296\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75253, Round: 1297\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75254, Round: 1298\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75255, Round: 1299\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75256, Round: 1300\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75257, Round: 1301\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75258, Round: 1302\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75259, Round: 1303\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75260, Round: 1304\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75261, Round: 1305\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75262, Round: 1306\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75263, Round: 1307\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75264, Round: 1308\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75265, Round: 1309\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75266, Round: 1310\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75267, Round: 1311\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75268, Round: 1312\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75269, Round: 1313\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75270, Round: 1314\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75271, Round: 1315\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75272, Round: 1316\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75273, Round: 1317\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75274, Round: 1318\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75275, Round: 1319\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75276, Round: 1320\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75277, Round: 1321\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75278, Round: 1322\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75279, Round: 1323\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75280, Round: 1324\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75281, Round: 1325\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75282, Round: 1326\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75283, Round: 1327\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75284, Round: 1328\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75285, Round: 1329\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75286, Round: 1330\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75287, Round: 1331\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75288, Round: 1332\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75289, Round: 1333\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75290, Round: 1334\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75291, Round: 1335\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75292, Round: 1336\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75293, Round: 1337\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75294, Round: 1338\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75295, Round: 1339\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75296, Round: 1340\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75297, Round: 1341\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75298, Round: 1342\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75299, Round: 1343\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75300, Round: 1344\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75301, Round: 1345\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75302, Round: 1346\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75303, Round: 1347\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75304, Round: 1348\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75305, Round: 1349\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75306, Round: 1350\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75307, Round: 1351\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75308, Round: 1352\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75309, Round: 1353\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75310, Round: 1354\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75311, Round: 1355\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75312, Round: 1356\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75313, Round: 1357\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75314, Round: 1358\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75315, Round: 1359\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75316, Round: 1360\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75317, Round: 1361\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75318, Round: 1362\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75319, Round: 1363\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75320, Round: 1364\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75321, Round: 1365\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75322, Round: 1366\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75323, Round: 1367\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75324, Round: 1368\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75325, Round: 1369\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75326, Round: 1370\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75327, Round: 1371\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75328, Round: 1372\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75329, Round: 1373\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75330, Round: 1374\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75331, Round: 1375\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75332, Round: 1376\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75333, Round: 1377\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75334, Round: 1378\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75335, Round: 1379\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75336, Round: 1380\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75337, Round: 1381\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75338, Round: 1382\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75339, Round: 1383\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75340, Round: 1384\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75341, Round: 1385\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75342, Round: 1386\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75343, Round: 1387\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75344, Round: 1388\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75345, Round: 1389\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75346, Round: 1390\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75347, Round: 1391\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75348, Round: 1392\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75349, Round: 1393\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75350, Round: 1394\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75351, Round: 1395\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75352, Round: 1396\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75353, Round: 1397\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75354, Round: 1398\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75355, Round: 1399\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75356, Round: 1400\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75357, Round: 1401\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75358, Round: 1402\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75359, Round: 1403\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75360, Round: 1404\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75361, Round: 1405\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75362, Round: 1406\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75363, Round: 1407\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75364, Round: 1408\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75365, Round: 1409\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75366, Round: 1410\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75367, Round: 1411\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75368, Round: 1412\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75369, Round: 1413\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75370, Round: 1414\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75371, Round: 1415\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75372, Round: 1416\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75373, Round: 1417\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75374, Round: 1418\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75375, Round: 1419\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75376, Round: 1420\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75377, Round: 1421\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75378, Round: 1422\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75379, Round: 1423\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75380, Round: 1424\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75381, Round: 1425\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75382, Round: 1426\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75383, Round: 1427\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75384, Round: 1428\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75385, Round: 1429\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75386, Round: 1430\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75387, Round: 1431\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75388, Round: 1432\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75389, Round: 1433\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75390, Round: 1434\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75391, Round: 1435\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75392, Round: 1436\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75393, Round: 1437\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75394, Round: 1438\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75395, Round: 1439\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75396, Round: 1440\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75397, Round: 1441\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75398, Round: 1442\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75399, Round: 1443\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75400, Round: 1444\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75401, Round: 1445\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75402, Round: 1446\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75403, Round: 1447\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75404, Round: 1448\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75405, Round: 1449\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75406, Round: 1450\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75407, Round: 1451\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75408, Round: 1452\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75409, Round: 1453\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75410, Round: 1454\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75411, Round: 1455\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75412, Round: 1456\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75413, Round: 1457\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75414, Round: 1458\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75415, Round: 1459\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75416, Round: 1460\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75417, Round: 1461\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75418, Round: 1462\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75419, Round: 1463\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75420, Round: 1464\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75421, Round: 1465\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75422, Round: 1466\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75423, Round: 1467\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75424, Round: 1468\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75425, Round: 1469\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75426, Round: 1470\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75427, Round: 1471\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75428, Round: 1472\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75429, Round: 1473\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75430, Round: 1474\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75431, Round: 1475\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75432, Round: 1476\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75433, Round: 1477\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75434, Round: 1478\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75435, Round: 1479\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75436, Round: 1480\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75437, Round: 1481\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75438, Round: 1482\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75439, Round: 1483\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75440, Round: 1484\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75441, Round: 1485\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75442, Round: 1486\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75443, Round: 1487\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75444, Round: 1488\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75445, Round: 1489\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75446, Round: 1490\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75447, Round: 1491\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75448, Round: 1492\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75449, Round: 1493\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75450, Round: 1494\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75451, Round: 1495\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75452, Round: 1496\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75453, Round: 1497\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75454, Round: 1498\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75455, Round: 1499\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75456, Round: 1500\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75457, Round: 1501\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75458, Round: 1502\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75459, Round: 1503\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75460, Round: 1504\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75461, Round: 1505\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75462, Round: 1506\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75463, Round: 1507\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75464, Round: 1508\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75465, Round: 1509\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75466, Round: 1510\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75467, Round: 1511\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75468, Round: 1512\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75469, Round: 1513\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75470, Round: 1514\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75471, Round: 1515\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75472, Round: 1516\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75473, Round: 1517\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75474, Round: 1518\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75475, Round: 1519\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75476, Round: 1520\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75477, Round: 1521\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75478, Round: 1522\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75479, Round: 1523\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75480, Round: 1524\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75481, Round: 1525\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75482, Round: 1526\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75483, Round: 1527\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75484, Round: 1528\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75485, Round: 1529\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75486, Round: 1530\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75487, Round: 1531\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75488, Round: 1532\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75489, Round: 1533\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75490, Round: 1534\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75491, Round: 1535\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75492, Round: 1536\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75493, Round: 1537\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75494, Round: 1538\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75495, Round: 1539\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75496, Round: 1540\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75497, Round: 1541\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75498, Round: 1542\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75499, Round: 1543\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75500, Round: 1544\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75501, Round: 1545\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75502, Round: 1546\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75503, Round: 1547\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75504, Round: 1548\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75505, Round: 1549\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75506, Round: 1550\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75507, Round: 1551\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75508, Round: 1552\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75509, Round: 1553\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75510, Round: 1554\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75511, Round: 1555\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75512, Round: 1556\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75513, Round: 1557\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75514, Round: 1558\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75515, Round: 1559\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75516, Round: 1560\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75517, Round: 1561\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75518, Round: 1562\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75519, Round: 1563\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75520, Round: 1564\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75521, Round: 1565\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75522, Round: 1566\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75523, Round: 1567\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75524, Round: 1568\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75525, Round: 1569\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75526, Round: 1570\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75527, Round: 1571\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75528, Round: 1572\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75529, Round: 1573\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75530, Round: 1574\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75531, Round: 1575\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75532, Round: 1576\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75533, Round: 1577\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75534, Round: 1578\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75535, Round: 1579\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75536, Round: 1580\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75537, Round: 1581\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75538, Round: 1582\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75539, Round: 1583\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75540, Round: 1584\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75541, Round: 1585\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75542, Round: 1586\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75543, Round: 1587\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75544, Round: 1588\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75545, Round: 1589\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75546, Round: 1590\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75547, Round: 1591\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75548, Round: 1592\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75549, Round: 1593\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75550, Round: 1594\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75551, Round: 1595\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75552, Round: 1596\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75553, Round: 1597\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75554, Round: 1598\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75555, Round: 1599\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75556, Round: 1600\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75557, Round: 1601\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75558, Round: 1602\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75559, Round: 1603\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75560, Round: 1604\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75561, Round: 1605\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75562, Round: 1606\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75563, Round: 1607\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75564, Round: 1608\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75565, Round: 1609\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75566, Round: 1610\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75567, Round: 1611\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75568, Round: 1612\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75569, Round: 1613\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75570, Round: 1614\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75571, Round: 1615\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75572, Round: 1616\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75573, Round: 1617\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75574, Round: 1618\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75575, Round: 1619\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75576, Round: 1620\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75577, Round: 1621\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75578, Round: 1622\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75579, Round: 1623\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75580, Round: 1624\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75581, Round: 1625\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75582, Round: 1626\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75583, Round: 1627\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75584, Round: 1628\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75585, Round: 1629\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75586, Round: 1630\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75587, Round: 1631\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75588, Round: 1632\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75589, Round: 1633\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75590, Round: 1634\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75591, Round: 1635\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75592, Round: 1636\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75593, Round: 1637\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75594, Round: 1638\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75595, Round: 1639\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75596, Round: 1640\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75597, Round: 1641\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75598, Round: 1642\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75599, Round: 1643\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75600, Round: 1644\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75601, Round: 1645\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75602, Round: 1646\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75603, Round: 1647\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75604, Round: 1648\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75605, Round: 1649\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75606, Round: 1650\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75607, Round: 1651\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75608, Round: 1652\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75609, Round: 1653\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75610, Round: 1654\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75611, Round: 1655\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75612, Round: 1656\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75613, Round: 1657\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75614, Round: 1658\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75615, Round: 1659\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75616, Round: 1660\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75617, Round: 1661\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75618, Round: 1662\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75619, Round: 1663\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75620, Round: 1664\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75621, Round: 1665\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75622, Round: 1666\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75623, Round: 1667\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75624, Round: 1668\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75625, Round: 1669\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75626, Round: 1670\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75627, Round: 1671\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75628, Round: 1672\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75629, Round: 1673\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75630, Round: 1674\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75631, Round: 1675\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75632, Round: 1676\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75633, Round: 1677\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75634, Round: 1678\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75635, Round: 1679\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75636, Round: 1680\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75637, Round: 1681\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75638, Round: 1682\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75639, Round: 1683\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75640, Round: 1684\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75641, Round: 1685\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75642, Round: 1686\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75643, Round: 1687\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75644, Round: 1688\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75645, Round: 1689\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75646, Round: 1690\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75647, Round: 1691\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75648, Round: 1692\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75649, Round: 1693\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75650, Round: 1694\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75651, Round: 1695\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75652, Round: 1696\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75653, Round: 1697\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75654, Round: 1698\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75655, Round: 1699\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75656, Round: 1700\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75657, Round: 1701\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75658, Round: 1702\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75659, Round: 1703\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75660, Round: 1704\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75661, Round: 1705\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75662, Round: 1706\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75663, Round: 1707\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75664, Round: 1708\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75665, Round: 1709\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75666, Round: 1710\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75667, Round: 1711\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75668, Round: 1712\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75669, Round: 1713\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75670, Round: 1714\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75671, Round: 1715\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75672, Round: 1716\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75673, Round: 1717\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75674, Round: 1718\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75675, Round: 1719\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75676, Round: 1720\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75677, Round: 1721\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75678, Round: 1722\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75679, Round: 1723\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75680, Round: 1724\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75681, Round: 1725\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75682, Round: 1726\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75683, Round: 1727\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75684, Round: 1728\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75685, Round: 1729\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75686, Round: 1730\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75687, Round: 1731\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75688, Round: 1732\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75689, Round: 1733\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75690, Round: 1734\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75691, Round: 1735\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75692, Round: 1736\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75693, Round: 1737\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75694, Round: 1738\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75695, Round: 1739\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75696, Round: 1740\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75697, Round: 1741\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75698, Round: 1742\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75699, Round: 1743\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75700, Round: 1744\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75701, Round: 1745\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75702, Round: 1746\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75703, Round: 1747\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75704, Round: 1748\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75705, Round: 1749\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75706, Round: 1750\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75707, Round: 1751\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75708, Round: 1752\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75709, Round: 1753\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75710, Round: 1754\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75711, Round: 1755\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75712, Round: 1756\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75713, Round: 1757\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75714, Round: 1758\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75715, Round: 1759\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75716, Round: 1760\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75717, Round: 1761\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75718, Round: 1762\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75719, Round: 1763\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75720, Round: 1764\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75721, Round: 1765\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75722, Round: 1766\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75723, Round: 1767\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75724, Round: 1768\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75725, Round: 1769\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75726, Round: 1770\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75727, Round: 1771\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75728, Round: 1772\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75729, Round: 1773\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75730, Round: 1774\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75731, Round: 1775\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75732, Round: 1776\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75733, Round: 1777\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75734, Round: 1778\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 75735, Round: 1779\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75736, Round: 1780\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75737, Round: 1781\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75738, Round: 1782\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75739, Round: 1783\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75740, Round: 1784\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75741, Round: 1785\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75742, Round: 1786\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75743, Round: 1787\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75744, Round: 1788\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75745, Round: 1789\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75746, Round: 1790\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75747, Round: 1791\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75748, Round: 1792\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75749, Round: 1793\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75750, Round: 1794\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75751, Round: 1795\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75752, Round: 1796\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75753, Round: 1797\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75754, Round: 1798\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75755, Round: 1799\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75756, Round: 1800\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75757, Round: 1801\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75758, Round: 1802\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75759, Round: 1803\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75760, Round: 1804\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75761, Round: 1805\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75762, Round: 1806\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75763, Round: 1807\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75764, Round: 1808\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75765, Round: 1809\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75766, Round: 1810\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75767, Round: 1811\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75768, Round: 1812\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75769, Round: 1813\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75770, Round: 1814\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75771, Round: 1815\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75772, Round: 1816\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75773, Round: 1817\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75774, Round: 1818\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75775, Round: 1819\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75776, Round: 1820\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75777, Round: 1821\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75778, Round: 1822\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75779, Round: 1823\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75780, Round: 1824\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75781, Round: 1825\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75782, Round: 1826\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75783, Round: 1827\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75784, Round: 1828\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75785, Round: 1829\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75786, Round: 1830\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75787, Round: 1831\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75788, Round: 1832\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75789, Round: 1833\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75790, Round: 1834\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75791, Round: 1835\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75792, Round: 1836\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75793, Round: 1837\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75794, Round: 1838\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75795, Round: 1839\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75796, Round: 1840\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75797, Round: 1841\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75798, Round: 1842\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75799, Round: 1843\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75800, Round: 1844\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75801, Round: 1845\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75802, Round: 1846\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75803, Round: 1847\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75804, Round: 1848\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75805, Round: 1849\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75806, Round: 1850\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75807, Round: 1851\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75808, Round: 1852\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75809, Round: 1853\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75810, Round: 1854\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75811, Round: 1855\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75812, Round: 1856\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75813, Round: 1857\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75814, Round: 1858\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75815, Round: 1859\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75816, Round: 1860\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75817, Round: 1861\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75818, Round: 1862\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75819, Round: 1863\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75820, Round: 1864\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75821, Round: 1865\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75822, Round: 1866\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75823, Round: 1867\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75824, Round: 1868\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75825, Round: 1869\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75826, Round: 1870\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75827, Round: 1871\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75828, Round: 1872\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75829, Round: 1873\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75830, Round: 1874\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75831, Round: 1875\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75832, Round: 1876\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75833, Round: 1877\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75834, Round: 1878\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75835, Round: 1879\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75836, Round: 1880\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75837, Round: 1881\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75838, Round: 1882\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75839, Round: 1883\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75840, Round: 1884\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75841, Round: 1885\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75842, Round: 1886\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75843, Round: 1887\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75844, Round: 1888\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75845, Round: 1889\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75846, Round: 1890\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75847, Round: 1891\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75848, Round: 1892\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75849, Round: 1893\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75850, Round: 1894\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75851, Round: 1895\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75852, Round: 1896\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75853, Round: 1897\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75854, Round: 1898\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75855, Round: 1899\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75856, Round: 1900\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75857, Round: 1901\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75858, Round: 1902\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75859, Round: 1903\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75860, Round: 1904\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75861, Round: 1905\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75862, Round: 1906\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75863, Round: 1907\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75864, Round: 1908\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75865, Round: 1909\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75866, Round: 1910\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75867, Round: 1911\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75868, Round: 1912\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75869, Round: 1913\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75870, Round: 1914\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75871, Round: 1915\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75872, Round: 1916\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75873, Round: 1917\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75874, Round: 1918\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75875, Round: 1919\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75876, Round: 1920\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75877, Round: 1921\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75878, Round: 1922\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75879, Round: 1923\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75880, Round: 1924\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75881, Round: 1925\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75882, Round: 1926\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75883, Round: 1927\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75884, Round: 1928\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75885, Round: 1929\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75886, Round: 1930\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75887, Round: 1931\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75888, Round: 1932\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75889, Round: 1933\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75890, Round: 1934\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75891, Round: 1935\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75892, Round: 1936\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75893, Round: 1937\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75894, Round: 1938\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75895, Round: 1939\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75896, Round: 1940\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75897, Round: 1941\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75898, Round: 1942\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75899, Round: 1943\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75900, Round: 1944\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75901, Round: 1945\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75902, Round: 1946\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75903, Round: 1947\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75904, Round: 1948\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75905, Round: 1949\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75906, Round: 1950\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75907, Round: 1951\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75908, Round: 1952\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75909, Round: 1953\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75910, Round: 1954\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75911, Round: 1955\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75912, Round: 1956\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75913, Round: 1957\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75914, Round: 1958\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75915, Round: 1959\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75916, Round: 1960\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75917, Round: 1961\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75918, Round: 1962\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75919, Round: 1963\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75920, Round: 1964\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75921, Round: 1965\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75922, Round: 1966\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75923, Round: 1967\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75924, Round: 1968\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75925, Round: 1969\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75926, Round: 1970\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75927, Round: 1971\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75928, Round: 1972\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75929, Round: 1973\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75930, Round: 1974\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75931, Round: 1975\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75932, Round: 1976\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75933, Round: 1977\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75934, Round: 1978\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75935, Round: 1979\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75936, Round: 1980\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75937, Round: 1981\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75938, Round: 1982\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75939, Round: 1983\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75940, Round: 1984\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75941, Round: 1985\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75942, Round: 1986\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75943, Round: 1987\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75944, Round: 1988\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75945, Round: 1989\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75946, Round: 1990\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75947, Round: 1991\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75948, Round: 1992\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75949, Round: 1993\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75950, Round: 1994\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75951, Round: 1995\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75952, Round: 1996\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75953, Round: 1997\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75954, Round: 1998\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75955, Round: 1999\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75956, Round: 2000\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75957, Round: 2001\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75958, Round: 2002\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75959, Round: 2003\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75960, Round: 2004\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75961, Round: 2005\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75962, Round: 2006\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75963, Round: 2007\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75964, Round: 2008\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75965, Round: 2009\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75966, Round: 2010\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75967, Round: 2011\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75968, Round: 2012\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75969, Round: 2013\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75970, Round: 2014\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75971, Round: 2015\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75972, Round: 2016\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75973, Round: 2017\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75974, Round: 2018\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75975, Round: 2019\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75976, Round: 2020\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75977, Round: 2021\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75978, Round: 2022\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75979, Round: 2023\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75980, Round: 2024\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75981, Round: 2025\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75982, Round: 2026\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75983, Round: 2027\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75984, Round: 2028\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75985, Round: 2029\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75986, Round: 2030\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75987, Round: 2031\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75988, Round: 2032\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75989, Round: 2033\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75990, Round: 2034\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75991, Round: 2035\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75992, Round: 2036\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75993, Round: 2037\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75994, Round: 2038\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75995, Round: 2039\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75996, Round: 2040\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75997, Round: 2041\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75998, Round: 2042\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 75999, Round: 2043\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76000, Round: 2044\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76001, Round: 2045\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76002, Round: 2046\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76003, Round: 2047\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76004, Round: 2048\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76005, Round: 2049\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76006, Round: 2050\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76007, Round: 2051\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76008, Round: 2052\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76009, Round: 2053\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76010, Round: 2054\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76011, Round: 2055\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76012, Round: 2056\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76013, Round: 2057\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76014, Round: 2058\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76015, Round: 2059\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76016, Round: 2060\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76017, Round: 2061\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76018, Round: 2062\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76019, Round: 2063\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76020, Round: 2064\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76021, Round: 2065\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76022, Round: 2066\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76023, Round: 2067\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76024, Round: 2068\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76025, Round: 2069\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76026, Round: 2070\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76027, Round: 2071\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76028, Round: 2072\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76029, Round: 2073\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76030, Round: 2074\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76031, Round: 2075\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76032, Round: 2076\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76033, Round: 2077\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76034, Round: 2078\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76035, Round: 2079\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76036, Round: 2080\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76037, Round: 2081\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76038, Round: 2082\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76039, Round: 2083\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76040, Round: 2084\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76041, Round: 2085\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76042, Round: 2086\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76043, Round: 2087\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76044, Round: 2088\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76045, Round: 2089\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76046, Round: 2090\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76047, Round: 2091\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76048, Round: 2092\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76049, Round: 2093\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76050, Round: 2094\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76051, Round: 2095\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76052, Round: 2096\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76053, Round: 2097\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76054, Round: 2098\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76055, Round: 2099\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76056, Round: 2100\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76057, Round: 2101\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76058, Round: 2102\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76059, Round: 2103\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76060, Round: 2104\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76061, Round: 2105\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76062, Round: 2106\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76063, Round: 2107\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76064, Round: 2108\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76065, Round: 2109\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76066, Round: 2110\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76067, Round: 2111\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76068, Round: 2112\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76069, Round: 2113\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76070, Round: 2114\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76071, Round: 2115\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76072, Round: 2116\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76073, Round: 2117\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76074, Round: 2118\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76075, Round: 2119\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76076, Round: 2120\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76077, Round: 2121\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76078, Round: 2122\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76079, Round: 2123\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76080, Round: 2124\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76081, Round: 2125\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76082, Round: 2126\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76083, Round: 2127\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76084, Round: 2128\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76085, Round: 2129\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76086, Round: 2130\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76087, Round: 2131\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76088, Round: 2132\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76089, Round: 2133\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76090, Round: 2134\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76091, Round: 2135\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76092, Round: 2136\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76093, Round: 2137\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76094, Round: 2138\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76095, Round: 2139\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76096, Round: 2140\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76097, Round: 2141\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76098, Round: 2142\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76099, Round: 2143\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76100, Round: 2144\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76101, Round: 2145\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76102, Round: 2146\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76103, Round: 2147\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76104, Round: 2148\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76105, Round: 2149\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76106, Round: 2150\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76107, Round: 2151\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76108, Round: 2152\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76109, Round: 2153\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76110, Round: 2154\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76111, Round: 2155\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76112, Round: 2156\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76113, Round: 2157\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76114, Round: 2158\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76115, Round: 2159\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76116, Round: 2160\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76117, Round: 2161\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76118, Round: 2162\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76119, Round: 2163\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76120, Round: 2164\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76121, Round: 2165\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76122, Round: 2166\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76123, Round: 2167\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76124, Round: 2168\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76125, Round: 2169\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76126, Round: 2170\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76127, Round: 2171\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76128, Round: 2172\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76129, Round: 2173\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76130, Round: 2174\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76131, Round: 2175\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76132, Round: 2176\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76133, Round: 2177\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76134, Round: 2178\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76135, Round: 2179\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76136, Round: 2180\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76137, Round: 2181\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76138, Round: 2182\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76139, Round: 2183\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76140, Round: 2184\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76141, Round: 2185\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76142, Round: 2186\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76143, Round: 2187\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76144, Round: 2188\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76145, Round: 2189\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76146, Round: 2190\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76147, Round: 2191\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76148, Round: 2192\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76149, Round: 2193\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76150, Round: 2194\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76151, Round: 2195\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76152, Round: 2196\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76153, Round: 2197\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76154, Round: 2198\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76155, Round: 2199\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76156, Round: 2200\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76157, Round: 2201\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76158, Round: 2202\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76159, Round: 2203\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76160, Round: 2204\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76161, Round: 2205\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76162, Round: 2206\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76163, Round: 2207\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76164, Round: 2208\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76165, Round: 2209\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76166, Round: 2210\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76167, Round: 2211\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76168, Round: 2212\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76169, Round: 2213\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76170, Round: 2214\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76171, Round: 2215\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76172, Round: 2216\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76173, Round: 2217\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76174, Round: 2218\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76175, Round: 2219\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76176, Round: 2220\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76177, Round: 2221\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76178, Round: 2222\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76179, Round: 2223\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76180, Round: 2224\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76181, Round: 2225\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76182, Round: 2226\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76183, Round: 2227\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76184, Round: 2228\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76185, Round: 2229\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76186, Round: 2230\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76187, Round: 2231\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76188, Round: 2232\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76189, Round: 2233\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76190, Round: 2234\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76191, Round: 2235\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76192, Round: 2236\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76193, Round: 2237\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76194, Round: 2238\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76195, Round: 2239\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76196, Round: 2240\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76197, Round: 2241\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76198, Round: 2242\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76199, Round: 2243\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76200, Round: 2244\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76201, Round: 2245\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76202, Round: 2246\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76203, Round: 2247\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76204, Round: 2248\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76205, Round: 2249\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76206, Round: 2250\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76207, Round: 2251\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76208, Round: 2252\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76209, Round: 2253\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76210, Round: 2254\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76211, Round: 2255\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76212, Round: 2256\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76213, Round: 2257\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76214, Round: 2258\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76215, Round: 2259\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76216, Round: 2260\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76217, Round: 2261\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76218, Round: 2262\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76219, Round: 2263\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76220, Round: 2264\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76221, Round: 2265\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76222, Round: 2266\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76223, Round: 2267\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76224, Round: 2268\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76225, Round: 2269\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76226, Round: 2270\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76227, Round: 2271\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76228, Round: 2272\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76229, Round: 2273\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76230, Round: 2274\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76231, Round: 2275\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76232, Round: 2276\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76233, Round: 2277\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76234, Round: 2278\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76235, Round: 2279\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76236, Round: 2280\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76237, Round: 2281\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76238, Round: 2282\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76239, Round: 2283\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76240, Round: 2284\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76241, Round: 2285\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76242, Round: 2286\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76243, Round: 2287\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76244, Round: 2288\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76245, Round: 2289\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76246, Round: 2290\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76247, Round: 2291\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76248, Round: 2292\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76249, Round: 2293\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76250, Round: 2294\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76251, Round: 2295\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76252, Round: 2296\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76253, Round: 2297\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76254, Round: 2298\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76255, Round: 2299\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76256, Round: 2300\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76257, Round: 2301\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76258, Round: 2302\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76259, Round: 2303\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76260, Round: 2304\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76261, Round: 2305\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76262, Round: 2306\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76263, Round: 2307\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76264, Round: 2308\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76265, Round: 2309\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76266, Round: 2310\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76267, Round: 2311\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76268, Round: 2312\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76269, Round: 2313\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76270, Round: 2314\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76271, Round: 2315\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76272, Round: 2316\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76273, Round: 2317\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76274, Round: 2318\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76275, Round: 2319\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76276, Round: 2320\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76277, Round: 2321\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76278, Round: 2322\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76279, Round: 2323\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76280, Round: 2324\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76281, Round: 2325\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76282, Round: 2326\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76283, Round: 2327\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76284, Round: 2328\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76285, Round: 2329\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76286, Round: 2330\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76287, Round: 2331\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76288, Round: 2332\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76289, Round: 2333\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76290, Round: 2334\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76291, Round: 2335\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76292, Round: 2336\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76293, Round: 2337\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76294, Round: 2338\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76295, Round: 2339\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76296, Round: 2340\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76297, Round: 2341\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76298, Round: 2342\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76299, Round: 2343\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76300, Round: 2344\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76301, Round: 2345\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76302, Round: 2346\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76303, Round: 2347\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76304, Round: 2348\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76305, Round: 2349\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76306, Round: 2350\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76307, Round: 2351\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76308, Round: 2352\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76309, Round: 2353\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76310, Round: 2354\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76311, Round: 2355\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76312, Round: 2356\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76313, Round: 2357\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76314, Round: 2358\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76315, Round: 2359\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76316, Round: 2360\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76317, Round: 2361\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76318, Round: 2362\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76319, Round: 2363\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76320, Round: 2364\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76321, Round: 2365\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76322, Round: 2366\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76323, Round: 2367\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76324, Round: 2368\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76325, Round: 2369\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76326, Round: 2370\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76327, Round: 2371\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76328, Round: 2372\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76329, Round: 2373\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76330, Round: 2374\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76331, Round: 2375\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76332, Round: 2376\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76333, Round: 2377\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76334, Round: 2378\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76335, Round: 2379\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76336, Round: 2380\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76337, Round: 2381\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76338, Round: 2382\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76339, Round: 2383\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76340, Round: 2384\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76341, Round: 2385\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76342, Round: 2386\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76343, Round: 2387\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76344, Round: 2388\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76345, Round: 2389\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76346, Round: 2390\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76347, Round: 2391\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76348, Round: 2392\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76349, Round: 2393\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76350, Round: 2394\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76351, Round: 2395\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76352, Round: 2396\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76353, Round: 2397\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76354, Round: 2398\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76355, Round: 2399\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76356, Round: 2400\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76357, Round: 2401\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76358, Round: 2402\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76359, Round: 2403\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76360, Round: 2404\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76361, Round: 2405\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76362, Round: 2406\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76363, Round: 2407\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76364, Round: 2408\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76365, Round: 2409\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76366, Round: 2410\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76367, Round: 2411\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76368, Round: 2412\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76369, Round: 2413\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76370, Round: 2414\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76371, Round: 2415\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76372, Round: 2416\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76373, Round: 2417\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76374, Round: 2418\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76375, Round: 2419\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76376, Round: 2420\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76377, Round: 2421\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76378, Round: 2422\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76379, Round: 2423\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76380, Round: 2424\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76381, Round: 2425\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76382, Round: 2426\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76383, Round: 2427\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76384, Round: 2428\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76385, Round: 2429\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76386, Round: 2430\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76387, Round: 2431\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76388, Round: 2432\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76389, Round: 2433\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76390, Round: 2434\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76391, Round: 2435\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76392, Round: 2436\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76393, Round: 2437\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76394, Round: 2438\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76395, Round: 2439\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76396, Round: 2440\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76397, Round: 2441\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76398, Round: 2442\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76399, Round: 2443\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76400, Round: 2444\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76401, Round: 2445\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76402, Round: 2446\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76403, Round: 2447\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76404, Round: 2448\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76405, Round: 2449\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76406, Round: 2450\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76407, Round: 2451\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76408, Round: 2452\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76409, Round: 2453\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76410, Round: 2454\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76411, Round: 2455\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76412, Round: 2456\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76413, Round: 2457\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76414, Round: 2458\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76415, Round: 2459\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76416, Round: 2460\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76417, Round: 2461\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76418, Round: 2462\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76419, Round: 2463\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76420, Round: 2464\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76421, Round: 2465\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76422, Round: 2466\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76423, Round: 2467\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76424, Round: 2468\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76425, Round: 2469\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76426, Round: 2470\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76427, Round: 2471\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76428, Round: 2472\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76429, Round: 2473\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76430, Round: 2474\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76431, Round: 2475\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76432, Round: 2476\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76433, Round: 2477\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76434, Round: 2478\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76435, Round: 2479\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76436, Round: 2480\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76437, Round: 2481\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76438, Round: 2482\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76439, Round: 2483\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76440, Round: 2484\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76441, Round: 2485\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76442, Round: 2486\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76443, Round: 2487\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76444, Round: 2488\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76445, Round: 2489\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76446, Round: 2490\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76447, Round: 2491\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76448, Round: 2492\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76449, Round: 2493\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76450, Round: 2494\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76451, Round: 2495\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76452, Round: 2496\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76453, Round: 2497\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76454, Round: 2498\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76455, Round: 2499\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76456, Round: 2500\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76457, Round: 2501\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76458, Round: 2502\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76459, Round: 2503\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76460, Round: 2504\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76461, Round: 2505\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76462, Round: 2506\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76463, Round: 2507\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76464, Round: 2508\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76465, Round: 2509\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76466, Round: 2510\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76467, Round: 2511\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76468, Round: 2512\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76469, Round: 2513\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76470, Round: 2514\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76471, Round: 2515\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76472, Round: 2516\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5627,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76473, Round: 2517\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5627,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76474, Round: 2518\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5627,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76475, Round: 2519\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5627,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76476, Round: 2520\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5627,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76477, Round: 2521\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5627,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76478, Round: 2522\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5627,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76479, Round: 2523\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5627,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76480, Round: 2524\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5627,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76481, Round: 2525\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5627,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76482, Round: 2526\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5627,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76483, Round: 2527\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5627,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76484, Round: 2528\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5627,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76485, Round: 2529\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5627,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76486, Round: 2530\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5627,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76487, Round: 2531\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5627,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76488, Round: 2532\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5627,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76489, Round: 2533\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5627,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76490, Round: 2534\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76491, Round: 2535\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76492, Round: 2536\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76493, Round: 2537\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76494, Round: 2538\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76495, Round: 2539\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76496, Round: 2540\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76497, Round: 2541\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76498, Round: 2542\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76499, Round: 2543\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76500, Round: 2544\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76501, Round: 2545\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76502, Round: 2546\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76503, Round: 2547\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76504, Round: 2548\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76505, Round: 2549\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76506, Round: 2550\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76507, Round: 2551\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76508, Round: 2552\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76509, Round: 2553\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76510, Round: 2554\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76511, Round: 2555\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76512, Round: 2556\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76513, Round: 2557\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76514, Round: 2558\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76515, Round: 2559\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76516, Round: 2560\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76517, Round: 2561\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76518, Round: 2562\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76519, Round: 2563\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76520, Round: 2564\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76521, Round: 2565\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76522, Round: 2566\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76523, Round: 2567\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76524, Round: 2568\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76525, Round: 2569\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76526, Round: 2570\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76527, Round: 2571\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76528, Round: 2572\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76529, Round: 2573\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76530, Round: 2574\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76531, Round: 2575\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76532, Round: 2576\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76533, Round: 2577\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76534, Round: 2578\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76535, Round: 2579\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76536, Round: 2580\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76537, Round: 2581\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76538, Round: 2582\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76539, Round: 2583\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76540, Round: 2584\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76541, Round: 2585\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76542, Round: 2586\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76543, Round: 2587\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76544, Round: 2588\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76545, Round: 2589\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76546, Round: 2590\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76547, Round: 2591\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76548, Round: 2592\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76549, Round: 2593\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76550, Round: 2594\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76551, Round: 2595\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76552, Round: 2596\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76553, Round: 2597\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76554, Round: 2598\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76555, Round: 2599\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76556, Round: 2600\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76557, Round: 2601\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76558, Round: 2602\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76559, Round: 2603\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76560, Round: 2604\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76561, Round: 2605\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76562, Round: 2606\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76563, Round: 2607\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76564, Round: 2608\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76565, Round: 2609\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76566, Round: 2610\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76567, Round: 2611\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76568, Round: 2612\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76569, Round: 2613\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76570, Round: 2614\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76571, Round: 2615\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76572, Round: 2616\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76573, Round: 2617\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76574, Round: 2618\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76575, Round: 2619\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76576, Round: 2620\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76577, Round: 2621\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76578, Round: 2622\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76579, Round: 2623\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76580, Round: 2624\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76581, Round: 2625\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76582, Round: 2626\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76583, Round: 2627\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76584, Round: 2628\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76585, Round: 2629\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76586, Round: 2630\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76587, Round: 2631\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76588, Round: 2632\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76589, Round: 2633\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76590, Round: 2634\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76591, Round: 2635\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76592, Round: 2636\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76593, Round: 2637\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76594, Round: 2638\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76595, Round: 2639\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76596, Round: 2640\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76597, Round: 2641\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76598, Round: 2642\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76599, Round: 2643\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76600, Round: 2644\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76601, Round: 2645\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76602, Round: 2646\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76603, Round: 2647\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76604, Round: 2648\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76605, Round: 2649\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76606, Round: 2650\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76607, Round: 2651\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76608, Round: 2652\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76609, Round: 2653\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76610, Round: 2654\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76611, Round: 2655\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76612, Round: 2656\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76613, Round: 2657\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76614, Round: 2658\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76615, Round: 2659\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76616, Round: 2660\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76617, Round: 2661\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76618, Round: 2662\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 76619, Round: 2663\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76620, Round: 2664\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76621, Round: 2665\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76622, Round: 2666\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76623, Round: 2667\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76624, Round: 2668\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76625, Round: 2669\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76626, Round: 2670\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76627, Round: 2671\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76628, Round: 2672\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76629, Round: 2673\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76630, Round: 2674\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76631, Round: 2675\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76632, Round: 2676\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76633, Round: 2677\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76634, Round: 2678\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76635, Round: 2679\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76636, Round: 2680\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76637, Round: 2681\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76638, Round: 2682\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76639, Round: 2683\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76640, Round: 2684\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76641, Round: 2685\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76642, Round: 2686\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76643, Round: 2687\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76644, Round: 2688\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76645, Round: 2689\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76646, Round: 2690\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76647, Round: 2691\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76648, Round: 2692\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76649, Round: 2693\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76650, Round: 2694\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76651, Round: 2695\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76652, Round: 2696\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76653, Round: 2697\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76654, Round: 2698\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76655, Round: 2699\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76656, Round: 2700\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76657, Round: 2701\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76658, Round: 2702\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76659, Round: 2703\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76660, Round: 2704\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76661, Round: 2705\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76662, Round: 2706\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76663, Round: 2707\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76664, Round: 2708\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76665, Round: 2709\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76666, Round: 2710\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76667, Round: 2711\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76668, Round: 2712\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76669, Round: 2713\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76670, Round: 2714\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76671, Round: 2715\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76672, Round: 2716\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76673, Round: 2717\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76674, Round: 2718\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76675, Round: 2719\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76676, Round: 2720\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76677, Round: 2721\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76678, Round: 2722\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76679, Round: 2723\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76680, Round: 2724\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76681, Round: 2725\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76682, Round: 2726\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76683, Round: 2727\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76684, Round: 2728\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76685, Round: 2729\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76686, Round: 2730\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76687, Round: 2731\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76688, Round: 2732\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76689, Round: 2733\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76690, Round: 2734\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76691, Round: 2735\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76692, Round: 2736\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76693, Round: 2737\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76694, Round: 2738\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76695, Round: 2739\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76696, Round: 2740\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76697, Round: 2741\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76698, Round: 2742\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76699, Round: 2743\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76700, Round: 2744\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76701, Round: 2745\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76702, Round: 2746\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76703, Round: 2747\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76704, Round: 2748\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76705, Round: 2749\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76706, Round: 2750\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76707, Round: 2751\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76708, Round: 2752\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76709, Round: 2753\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76710, Round: 2754\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76711, Round: 2755\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76712, Round: 2756\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76713, Round: 2757\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76714, Round: 2758\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76715, Round: 2759\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76716, Round: 2760\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76717, Round: 2761\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76718, Round: 2762\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76719, Round: 2763\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76720, Round: 2764\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76721, Round: 2765\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76722, Round: 2766\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76723, Round: 2767\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76724, Round: 2768\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76725, Round: 2769\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76726, Round: 2770\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76727, Round: 2771\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76728, Round: 2772\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76729, Round: 2773\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76730, Round: 2774\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76731, Round: 2775\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76732, Round: 2776\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76733, Round: 2777\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76734, Round: 2778\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76735, Round: 2779\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76736, Round: 2780\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76737, Round: 2781\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76738, Round: 2782\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76739, Round: 2783\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76740, Round: 2784\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76741, Round: 2785\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76742, Round: 2786\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76743, Round: 2787\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76744, Round: 2788\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76745, Round: 2789\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76746, Round: 2790\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76747, Round: 2791\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76748, Round: 2792\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76749, Round: 2793\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76750, Round: 2794\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76751, Round: 2795\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76752, Round: 2796\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76753, Round: 2797\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76754, Round: 2798\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76755, Round: 2799\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76756, Round: 2800\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76757, Round: 2801\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76758, Round: 2802\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76759, Round: 2803\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76760, Round: 2804\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76761, Round: 2805\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76762, Round: 2806\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76763, Round: 2807\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76764, Round: 2808\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76765, Round: 2809\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76766, Round: 2810\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76767, Round: 2811\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76768, Round: 2812\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76769, Round: 2813\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76770, Round: 2814\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76771, Round: 2815\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76772, Round: 2816\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76773, Round: 2817\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76774, Round: 2818\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76775, Round: 2819\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76776, Round: 2820\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76777, Round: 2821\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76778, Round: 2822\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76779, Round: 2823\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76780, Round: 2824\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76781, Round: 2825\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76782, Round: 2826\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76783, Round: 2827\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76784, Round: 2828\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76785, Round: 2829\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76786, Round: 2830\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76787, Round: 2831\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76788, Round: 2832\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76789, Round: 2833\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76790, Round: 2834\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76791, Round: 2835\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76792, Round: 2836\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76793, Round: 2837\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76794, Round: 2838\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76795, Round: 2839\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76796, Round: 2840\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76797, Round: 2841\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76798, Round: 2842\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76799, Round: 2843\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76800, Round: 2844\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76801, Round: 2845\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76802, Round: 2846\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76803, Round: 2847\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76804, Round: 2848\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76805, Round: 2849\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76806, Round: 2850\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76807, Round: 2851\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76808, Round: 2852\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76809, Round: 2853\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76810, Round: 2854\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76811, Round: 2855\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76812, Round: 2856\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76813, Round: 2857\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76814, Round: 2858\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76815, Round: 2859\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76816, Round: 2860\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76817, Round: 2861\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76818, Round: 2862\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76819, Round: 2863\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76820, Round: 2864\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76821, Round: 2865\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76822, Round: 2866\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76823, Round: 2867\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76824, Round: 2868\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76825, Round: 2869\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76826, Round: 2870\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76827, Round: 2871\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76828, Round: 2872\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76829, Round: 2873\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76830, Round: 2874\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76831, Round: 2875\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76832, Round: 2876\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76833, Round: 2877\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76834, Round: 2878\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76835, Round: 2879\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76836, Round: 2880\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76837, Round: 2881\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76838, Round: 2882\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76839, Round: 2883\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76840, Round: 2884\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76841, Round: 2885\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76842, Round: 2886\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76843, Round: 2887\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76844, Round: 2888\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76845, Round: 2889\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76846, Round: 2890\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76847, Round: 2891\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76848, Round: 2892\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76849, Round: 2893\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76850, Round: 2894\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76851, Round: 2895\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76852, Round: 2896\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76853, Round: 2897\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76854, Round: 2898\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76855, Round: 2899\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76856, Round: 2900\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76857, Round: 2901\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76858, Round: 2902\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76859, Round: 2903\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76860, Round: 2904\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76861, Round: 2905\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76862, Round: 2906\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76863, Round: 2907\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76864, Round: 2908\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76865, Round: 2909\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76866, Round: 2910\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76867, Round: 2911\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76868, Round: 2912\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76869, Round: 2913\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76870, Round: 2914\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76871, Round: 2915\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76872, Round: 2916\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.563,  }]\n",
      "Training on Total Epoch: 76873, Round: 2917\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76874, Round: 2918\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76875, Round: 2919\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76876, Round: 2920\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76877, Round: 2921\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76878, Round: 2922\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76879, Round: 2923\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76880, Round: 2924\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76881, Round: 2925\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76882, Round: 2926\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76883, Round: 2927\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76884, Round: 2928\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76885, Round: 2929\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76886, Round: 2930\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76887, Round: 2931\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76888, Round: 2932\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76889, Round: 2933\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76890, Round: 2934\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76891, Round: 2935\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76892, Round: 2936\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76893, Round: 2937\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76894, Round: 2938\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76895, Round: 2939\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76896, Round: 2940\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76897, Round: 2941\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76898, Round: 2942\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76899, Round: 2943\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76900, Round: 2944\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76901, Round: 2945\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76902, Round: 2946\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76903, Round: 2947\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76904, Round: 2948\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76905, Round: 2949\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76906, Round: 2950\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76907, Round: 2951\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76908, Round: 2952\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76909, Round: 2953\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76910, Round: 2954\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76911, Round: 2955\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76912, Round: 2956\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76913, Round: 2957\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76914, Round: 2958\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76915, Round: 2959\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76916, Round: 2960\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76917, Round: 2961\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76918, Round: 2962\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76919, Round: 2963\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76920, Round: 2964\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76921, Round: 2965\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76922, Round: 2966\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76923, Round: 2967\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76924, Round: 2968\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76925, Round: 2969\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76926, Round: 2970\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76927, Round: 2971\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76928, Round: 2972\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76929, Round: 2973\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76930, Round: 2974\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76931, Round: 2975\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76932, Round: 2976\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76933, Round: 2977\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76934, Round: 2978\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76935, Round: 2979\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76936, Round: 2980\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76937, Round: 2981\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76938, Round: 2982\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76939, Round: 2983\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76940, Round: 2984\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76941, Round: 2985\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76942, Round: 2986\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76943, Round: 2987\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76944, Round: 2988\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76945, Round: 2989\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76946, Round: 2990\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76947, Round: 2991\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76948, Round: 2992\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76949, Round: 2993\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76950, Round: 2994\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76951, Round: 2995\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76952, Round: 2996\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76953, Round: 2997\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76954, Round: 2998\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76955, Round: 2999\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76956, Round: 3000\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76957, Round: 3001\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76958, Round: 3002\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76959, Round: 3003\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76960, Round: 3004\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76961, Round: 3005\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76962, Round: 3006\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76963, Round: 3007\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76964, Round: 3008\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76965, Round: 3009\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76966, Round: 3010\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76967, Round: 3011\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76968, Round: 3012\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76969, Round: 3013\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76970, Round: 3014\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76971, Round: 3015\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76972, Round: 3016\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76973, Round: 3017\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76974, Round: 3018\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76975, Round: 3019\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76976, Round: 3020\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76977, Round: 3021\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76978, Round: 3022\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76979, Round: 3023\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76980, Round: 3024\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76981, Round: 3025\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76982, Round: 3026\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76983, Round: 3027\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76984, Round: 3028\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76985, Round: 3029\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76986, Round: 3030\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76987, Round: 3031\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76988, Round: 3032\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76989, Round: 3033\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76990, Round: 3034\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76991, Round: 3035\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76992, Round: 3036\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76993, Round: 3037\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76994, Round: 3038\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76995, Round: 3039\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76996, Round: 3040\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76997, Round: 3041\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76998, Round: 3042\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 76999, Round: 3043\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77000, Round: 3044\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77001, Round: 3045\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77002, Round: 3046\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77003, Round: 3047\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77004, Round: 3048\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77005, Round: 3049\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77006, Round: 3050\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77007, Round: 3051\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77008, Round: 3052\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77009, Round: 3053\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77010, Round: 3054\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77011, Round: 3055\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77012, Round: 3056\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77013, Round: 3057\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77014, Round: 3058\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77015, Round: 3059\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77016, Round: 3060\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77017, Round: 3061\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77018, Round: 3062\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77019, Round: 3063\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77020, Round: 3064\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77021, Round: 3065\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77022, Round: 3066\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77023, Round: 3067\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77024, Round: 3068\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77025, Round: 3069\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77026, Round: 3070\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77027, Round: 3071\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77028, Round: 3072\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77029, Round: 3073\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77030, Round: 3074\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77031, Round: 3075\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77032, Round: 3076\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77033, Round: 3077\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77034, Round: 3078\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77035, Round: 3079\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77036, Round: 3080\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77037, Round: 3081\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77038, Round: 3082\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77039, Round: 3083\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77040, Round: 3084\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77041, Round: 3085\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77042, Round: 3086\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77043, Round: 3087\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77044, Round: 3088\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77045, Round: 3089\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77046, Round: 3090\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77047, Round: 3091\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77048, Round: 3092\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77049, Round: 3093\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77050, Round: 3094\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77051, Round: 3095\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77052, Round: 3096\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77053, Round: 3097\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77054, Round: 3098\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77055, Round: 3099\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77056, Round: 3100\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77057, Round: 3101\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77058, Round: 3102\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77059, Round: 3103\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77060, Round: 3104\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77061, Round: 3105\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77062, Round: 3106\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77063, Round: 3107\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77064, Round: 3108\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77065, Round: 3109\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77066, Round: 3110\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77067, Round: 3111\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77068, Round: 3112\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77069, Round: 3113\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77070, Round: 3114\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77071, Round: 3115\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77072, Round: 3116\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77073, Round: 3117\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77074, Round: 3118\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77075, Round: 3119\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77076, Round: 3120\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77077, Round: 3121\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77078, Round: 3122\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77079, Round: 3123\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77080, Round: 3124\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77081, Round: 3125\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77082, Round: 3126\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77083, Round: 3127\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77084, Round: 3128\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77085, Round: 3129\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77086, Round: 3130\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77087, Round: 3131\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77088, Round: 3132\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77089, Round: 3133\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77090, Round: 3134\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77091, Round: 3135\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77092, Round: 3136\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77093, Round: 3137\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77094, Round: 3138\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77095, Round: 3139\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77096, Round: 3140\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77097, Round: 3141\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77098, Round: 3142\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77099, Round: 3143\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77100, Round: 3144\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77101, Round: 3145\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77102, Round: 3146\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77103, Round: 3147\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77104, Round: 3148\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77105, Round: 3149\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77106, Round: 3150\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77107, Round: 3151\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77108, Round: 3152\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77109, Round: 3153\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77110, Round: 3154\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77111, Round: 3155\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77112, Round: 3156\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77113, Round: 3157\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77114, Round: 3158\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77115, Round: 3159\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77116, Round: 3160\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77117, Round: 3161\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77118, Round: 3162\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77119, Round: 3163\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77120, Round: 3164\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77121, Round: 3165\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77122, Round: 3166\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77123, Round: 3167\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77124, Round: 3168\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77125, Round: 3169\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77126, Round: 3170\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77127, Round: 3171\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77128, Round: 3172\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77129, Round: 3173\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77130, Round: 3174\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77131, Round: 3175\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77132, Round: 3176\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77133, Round: 3177\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77134, Round: 3178\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77135, Round: 3179\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77136, Round: 3180\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77137, Round: 3181\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77138, Round: 3182\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77139, Round: 3183\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77140, Round: 3184\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77141, Round: 3185\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77142, Round: 3186\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77143, Round: 3187\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77144, Round: 3188\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77145, Round: 3189\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77146, Round: 3190\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77147, Round: 3191\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77148, Round: 3192\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77149, Round: 3193\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77150, Round: 3194\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77151, Round: 3195\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77152, Round: 3196\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77153, Round: 3197\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77154, Round: 3198\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77155, Round: 3199\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77156, Round: 3200\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77157, Round: 3201\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77158, Round: 3202\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77159, Round: 3203\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77160, Round: 3204\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77161, Round: 3205\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77162, Round: 3206\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77163, Round: 3207\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77164, Round: 3208\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77165, Round: 3209\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77166, Round: 3210\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77167, Round: 3211\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77168, Round: 3212\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77169, Round: 3213\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77170, Round: 3214\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77171, Round: 3215\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77172, Round: 3216\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77173, Round: 3217\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77174, Round: 3218\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77175, Round: 3219\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77176, Round: 3220\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77177, Round: 3221\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77178, Round: 3222\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77179, Round: 3223\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77180, Round: 3224\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77181, Round: 3225\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77182, Round: 3226\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77183, Round: 3227\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77184, Round: 3228\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77185, Round: 3229\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 77186, Round: 3230\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77187, Round: 3231\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77188, Round: 3232\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77189, Round: 3233\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77190, Round: 3234\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77191, Round: 3235\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77192, Round: 3236\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77193, Round: 3237\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77194, Round: 3238\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77195, Round: 3239\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77196, Round: 3240\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77197, Round: 3241\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77198, Round: 3242\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77199, Round: 3243\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77200, Round: 3244\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77201, Round: 3245\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77202, Round: 3246\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77203, Round: 3247\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77204, Round: 3248\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77205, Round: 3249\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77206, Round: 3250\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77207, Round: 3251\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77208, Round: 3252\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77209, Round: 3253\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77210, Round: 3254\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77211, Round: 3255\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77212, Round: 3256\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77213, Round: 3257\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77214, Round: 3258\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77215, Round: 3259\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77216, Round: 3260\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77217, Round: 3261\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77218, Round: 3262\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77219, Round: 3263\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77220, Round: 3264\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77221, Round: 3265\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77222, Round: 3266\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77223, Round: 3267\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77224, Round: 3268\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77225, Round: 3269\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77226, Round: 3270\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77227, Round: 3271\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77228, Round: 3272\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77229, Round: 3273\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77230, Round: 3274\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77231, Round: 3275\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77232, Round: 3276\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77233, Round: 3277\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77234, Round: 3278\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77235, Round: 3279\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77236, Round: 3280\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77237, Round: 3281\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77238, Round: 3282\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77239, Round: 3283\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77240, Round: 3284\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77241, Round: 3285\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77242, Round: 3286\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77243, Round: 3287\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77244, Round: 3288\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77245, Round: 3289\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77246, Round: 3290\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77247, Round: 3291\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77248, Round: 3292\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77249, Round: 3293\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77250, Round: 3294\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77251, Round: 3295\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77252, Round: 3296\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77253, Round: 3297\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77254, Round: 3298\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77255, Round: 3299\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77256, Round: 3300\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77257, Round: 3301\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77258, Round: 3302\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77259, Round: 3303\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77260, Round: 3304\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77261, Round: 3305\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77262, Round: 3306\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77263, Round: 3307\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77264, Round: 3308\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77265, Round: 3309\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77266, Round: 3310\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77267, Round: 3311\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77268, Round: 3312\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77269, Round: 3313\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77270, Round: 3314\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77271, Round: 3315\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77272, Round: 3316\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77273, Round: 3317\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77274, Round: 3318\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77275, Round: 3319\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77276, Round: 3320\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77277, Round: 3321\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77278, Round: 3322\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77279, Round: 3323\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77280, Round: 3324\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77281, Round: 3325\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77282, Round: 3326\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77283, Round: 3327\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77284, Round: 3328\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77285, Round: 3329\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77286, Round: 3330\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77287, Round: 3331\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77288, Round: 3332\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77289, Round: 3333\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77290, Round: 3334\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77291, Round: 3335\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77292, Round: 3336\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77293, Round: 3337\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77294, Round: 3338\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77295, Round: 3339\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77296, Round: 3340\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77297, Round: 3341\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77298, Round: 3342\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77299, Round: 3343\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77300, Round: 3344\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77301, Round: 3345\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77302, Round: 3346\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77303, Round: 3347\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77304, Round: 3348\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77305, Round: 3349\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77306, Round: 3350\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77307, Round: 3351\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77308, Round: 3352\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77309, Round: 3353\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77310, Round: 3354\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77311, Round: 3355\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77312, Round: 3356\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77313, Round: 3357\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77314, Round: 3358\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77315, Round: 3359\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77316, Round: 3360\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77317, Round: 3361\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77318, Round: 3362\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77319, Round: 3363\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77320, Round: 3364\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77321, Round: 3365\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77322, Round: 3366\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77323, Round: 3367\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77324, Round: 3368\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77325, Round: 3369\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77326, Round: 3370\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77327, Round: 3371\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77328, Round: 3372\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77329, Round: 3373\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77330, Round: 3374\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77331, Round: 3375\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77332, Round: 3376\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77333, Round: 3377\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77334, Round: 3378\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77335, Round: 3379\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77336, Round: 3380\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77337, Round: 3381\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77338, Round: 3382\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77339, Round: 3383\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77340, Round: 3384\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77341, Round: 3385\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77342, Round: 3386\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77343, Round: 3387\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77344, Round: 3388\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77345, Round: 3389\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77346, Round: 3390\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77347, Round: 3391\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77348, Round: 3392\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77349, Round: 3393\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77350, Round: 3394\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77351, Round: 3395\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77352, Round: 3396\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77353, Round: 3397\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77354, Round: 3398\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77355, Round: 3399\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77356, Round: 3400\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77357, Round: 3401\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77358, Round: 3402\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77359, Round: 3403\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77360, Round: 3404\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77361, Round: 3405\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77362, Round: 3406\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77363, Round: 3407\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77364, Round: 3408\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77365, Round: 3409\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77366, Round: 3410\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77367, Round: 3411\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77368, Round: 3412\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77369, Round: 3413\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77370, Round: 3414\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77371, Round: 3415\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77372, Round: 3416\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77373, Round: 3417\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77374, Round: 3418\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77375, Round: 3419\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77376, Round: 3420\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77377, Round: 3421\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77378, Round: 3422\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77379, Round: 3423\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77380, Round: 3424\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77381, Round: 3425\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77382, Round: 3426\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77383, Round: 3427\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77384, Round: 3428\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77385, Round: 3429\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77386, Round: 3430\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77387, Round: 3431\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77388, Round: 3432\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77389, Round: 3433\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77390, Round: 3434\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77391, Round: 3435\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77392, Round: 3436\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77393, Round: 3437\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77394, Round: 3438\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77395, Round: 3439\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77396, Round: 3440\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77397, Round: 3441\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77398, Round: 3442\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77399, Round: 3443\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77400, Round: 3444\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77401, Round: 3445\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77402, Round: 3446\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77403, Round: 3447\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77404, Round: 3448\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77405, Round: 3449\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77406, Round: 3450\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77407, Round: 3451\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77408, Round: 3452\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77409, Round: 3453\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77410, Round: 3454\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77411, Round: 3455\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77412, Round: 3456\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77413, Round: 3457\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77414, Round: 3458\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77415, Round: 3459\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77416, Round: 3460\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77417, Round: 3461\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77418, Round: 3462\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77419, Round: 3463\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77420, Round: 3464\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77421, Round: 3465\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77422, Round: 3466\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77423, Round: 3467\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77424, Round: 3468\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77425, Round: 3469\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77426, Round: 3470\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77427, Round: 3471\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77428, Round: 3472\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77429, Round: 3473\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77430, Round: 3474\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77431, Round: 3475\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77432, Round: 3476\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77433, Round: 3477\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77434, Round: 3478\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77435, Round: 3479\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77436, Round: 3480\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77437, Round: 3481\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77438, Round: 3482\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77439, Round: 3483\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77440, Round: 3484\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77441, Round: 3485\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77442, Round: 3486\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77443, Round: 3487\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77444, Round: 3488\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77445, Round: 3489\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77446, Round: 3490\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77447, Round: 3491\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77448, Round: 3492\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77449, Round: 3493\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77450, Round: 3494\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77451, Round: 3495\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77452, Round: 3496\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77453, Round: 3497\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77454, Round: 3498\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77455, Round: 3499\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77456, Round: 3500\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77457, Round: 3501\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77458, Round: 3502\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77459, Round: 3503\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77460, Round: 3504\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77461, Round: 3505\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77462, Round: 3506\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77463, Round: 3507\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77464, Round: 3508\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77465, Round: 3509\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77466, Round: 3510\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77467, Round: 3511\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77468, Round: 3512\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77469, Round: 3513\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77470, Round: 3514\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77471, Round: 3515\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77472, Round: 3516\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77473, Round: 3517\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77474, Round: 3518\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77475, Round: 3519\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77476, Round: 3520\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77477, Round: 3521\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77478, Round: 3522\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77479, Round: 3523\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77480, Round: 3524\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77481, Round: 3525\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77482, Round: 3526\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77483, Round: 3527\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77484, Round: 3528\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77485, Round: 3529\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77486, Round: 3530\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77487, Round: 3531\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77488, Round: 3532\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77489, Round: 3533\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77490, Round: 3534\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77491, Round: 3535\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77492, Round: 3536\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77493, Round: 3537\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77494, Round: 3538\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77495, Round: 3539\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77496, Round: 3540\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77497, Round: 3541\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77498, Round: 3542\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77499, Round: 3543\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77500, Round: 3544\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77501, Round: 3545\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77502, Round: 3546\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77503, Round: 3547\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77504, Round: 3548\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77505, Round: 3549\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77506, Round: 3550\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77507, Round: 3551\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77508, Round: 3552\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77509, Round: 3553\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77510, Round: 3554\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77511, Round: 3555\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77512, Round: 3556\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77513, Round: 3557\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77514, Round: 3558\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77515, Round: 3559\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77516, Round: 3560\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77517, Round: 3561\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77518, Round: 3562\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77519, Round: 3563\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77520, Round: 3564\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77521, Round: 3565\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77522, Round: 3566\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77523, Round: 3567\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77524, Round: 3568\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77525, Round: 3569\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77526, Round: 3570\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77527, Round: 3571\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77528, Round: 3572\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77529, Round: 3573\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77530, Round: 3574\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77531, Round: 3575\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77532, Round: 3576\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77533, Round: 3577\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77534, Round: 3578\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77535, Round: 3579\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77536, Round: 3580\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77537, Round: 3581\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77538, Round: 3582\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77539, Round: 3583\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77540, Round: 3584\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77541, Round: 3585\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77542, Round: 3586\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77543, Round: 3587\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77544, Round: 3588\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77545, Round: 3589\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77546, Round: 3590\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77547, Round: 3591\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77548, Round: 3592\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77549, Round: 3593\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77550, Round: 3594\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77551, Round: 3595\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77552, Round: 3596\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77553, Round: 3597\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77554, Round: 3598\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77555, Round: 3599\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77556, Round: 3600\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77557, Round: 3601\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77558, Round: 3602\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77559, Round: 3603\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77560, Round: 3604\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77561, Round: 3605\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77562, Round: 3606\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77563, Round: 3607\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5632,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77564, Round: 3608\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77565, Round: 3609\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77566, Round: 3610\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77567, Round: 3611\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77568, Round: 3612\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77569, Round: 3613\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77570, Round: 3614\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77571, Round: 3615\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77572, Round: 3616\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77573, Round: 3617\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77574, Round: 3618\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77575, Round: 3619\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77576, Round: 3620\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77577, Round: 3621\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77578, Round: 3622\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77579, Round: 3623\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77580, Round: 3624\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77581, Round: 3625\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77582, Round: 3626\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77583, Round: 3627\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77584, Round: 3628\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77585, Round: 3629\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77586, Round: 3630\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77587, Round: 3631\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77588, Round: 3632\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77589, Round: 3633\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77590, Round: 3634\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77591, Round: 3635\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77592, Round: 3636\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77593, Round: 3637\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77594, Round: 3638\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77595, Round: 3639\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77596, Round: 3640\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77597, Round: 3641\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77598, Round: 3642\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77599, Round: 3643\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77600, Round: 3644\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77601, Round: 3645\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77602, Round: 3646\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77603, Round: 3647\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77604, Round: 3648\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77605, Round: 3649\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77606, Round: 3650\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77607, Round: 3651\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77608, Round: 3652\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77609, Round: 3653\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77610, Round: 3654\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77611, Round: 3655\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77612, Round: 3656\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77613, Round: 3657\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77614, Round: 3658\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77615, Round: 3659\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77616, Round: 3660\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77617, Round: 3661\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77618, Round: 3662\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77619, Round: 3663\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77620, Round: 3664\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77621, Round: 3665\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77622, Round: 3666\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77623, Round: 3667\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77624, Round: 3668\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77625, Round: 3669\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77626, Round: 3670\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77627, Round: 3671\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77628, Round: 3672\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77629, Round: 3673\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77630, Round: 3674\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77631, Round: 3675\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77632, Round: 3676\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77633, Round: 3677\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77634, Round: 3678\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77635, Round: 3679\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77636, Round: 3680\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77637, Round: 3681\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77638, Round: 3682\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77639, Round: 3683\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77640, Round: 3684\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77641, Round: 3685\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77642, Round: 3686\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77643, Round: 3687\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77644, Round: 3688\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77645, Round: 3689\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77646, Round: 3690\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77647, Round: 3691\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77648, Round: 3692\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77649, Round: 3693\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77650, Round: 3694\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77651, Round: 3695\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77652, Round: 3696\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77653, Round: 3697\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77654, Round: 3698\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77655, Round: 3699\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77656, Round: 3700\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77657, Round: 3701\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77658, Round: 3702\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77659, Round: 3703\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77660, Round: 3704\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77661, Round: 3705\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77662, Round: 3706\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77663, Round: 3707\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77664, Round: 3708\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77665, Round: 3709\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77666, Round: 3710\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77667, Round: 3711\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77668, Round: 3712\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77669, Round: 3713\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77670, Round: 3714\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77671, Round: 3715\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77672, Round: 3716\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77673, Round: 3717\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77674, Round: 3718\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77675, Round: 3719\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77676, Round: 3720\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77677, Round: 3721\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77678, Round: 3722\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77679, Round: 3723\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77680, Round: 3724\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77681, Round: 3725\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77682, Round: 3726\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77683, Round: 3727\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77684, Round: 3728\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77685, Round: 3729\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77686, Round: 3730\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77687, Round: 3731\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77688, Round: 3732\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77689, Round: 3733\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77690, Round: 3734\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77691, Round: 3735\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77692, Round: 3736\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77693, Round: 3737\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77694, Round: 3738\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77695, Round: 3739\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77696, Round: 3740\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77697, Round: 3741\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77698, Round: 3742\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77699, Round: 3743\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77700, Round: 3744\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77701, Round: 3745\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77702, Round: 3746\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77703, Round: 3747\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77704, Round: 3748\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77705, Round: 3749\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77706, Round: 3750\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77707, Round: 3751\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77708, Round: 3752\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77709, Round: 3753\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77710, Round: 3754\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77711, Round: 3755\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77712, Round: 3756\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77713, Round: 3757\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77714, Round: 3758\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77715, Round: 3759\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77716, Round: 3760\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77717, Round: 3761\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77718, Round: 3762\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77719, Round: 3763\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77720, Round: 3764\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77721, Round: 3765\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77722, Round: 3766\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77723, Round: 3767\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77724, Round: 3768\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77725, Round: 3769\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77726, Round: 3770\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77727, Round: 3771\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77728, Round: 3772\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77729, Round: 3773\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77730, Round: 3774\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77731, Round: 3775\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77732, Round: 3776\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77733, Round: 3777\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77734, Round: 3778\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77735, Round: 3779\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77736, Round: 3780\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77737, Round: 3781\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77738, Round: 3782\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77739, Round: 3783\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77740, Round: 3784\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77741, Round: 3785\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77742, Round: 3786\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77743, Round: 3787\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77744, Round: 3788\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77745, Round: 3789\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77746, Round: 3790\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77747, Round: 3791\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77748, Round: 3792\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77749, Round: 3793\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77750, Round: 3794\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77751, Round: 3795\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77752, Round: 3796\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77753, Round: 3797\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77754, Round: 3798\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77755, Round: 3799\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77756, Round: 3800\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77757, Round: 3801\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77758, Round: 3802\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77759, Round: 3803\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77760, Round: 3804\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77761, Round: 3805\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77762, Round: 3806\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77763, Round: 3807\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77764, Round: 3808\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77765, Round: 3809\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77766, Round: 3810\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77767, Round: 3811\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77768, Round: 3812\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77769, Round: 3813\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77770, Round: 3814\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77771, Round: 3815\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77772, Round: 3816\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77773, Round: 3817\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77774, Round: 3818\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77775, Round: 3819\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77776, Round: 3820\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77777, Round: 3821\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77778, Round: 3822\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77779, Round: 3823\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77780, Round: 3824\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77781, Round: 3825\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77782, Round: 3826\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77783, Round: 3827\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77784, Round: 3828\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77785, Round: 3829\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77786, Round: 3830\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77787, Round: 3831\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77788, Round: 3832\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77789, Round: 3833\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77790, Round: 3834\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77791, Round: 3835\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77792, Round: 3836\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77793, Round: 3837\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77794, Round: 3838\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77795, Round: 3839\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77796, Round: 3840\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77797, Round: 3841\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77798, Round: 3842\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77799, Round: 3843\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77800, Round: 3844\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77801, Round: 3845\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77802, Round: 3846\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77803, Round: 3847\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77804, Round: 3848\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77805, Round: 3849\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77806, Round: 3850\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77807, Round: 3851\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77808, Round: 3852\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77809, Round: 3853\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5633,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77810, Round: 3854\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77811, Round: 3855\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77812, Round: 3856\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77813, Round: 3857\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77814, Round: 3858\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77815, Round: 3859\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77816, Round: 3860\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77817, Round: 3861\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77818, Round: 3862\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77819, Round: 3863\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77820, Round: 3864\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77821, Round: 3865\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77822, Round: 3866\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77823, Round: 3867\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77824, Round: 3868\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77825, Round: 3869\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77826, Round: 3870\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77827, Round: 3871\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77828, Round: 3872\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77829, Round: 3873\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77830, Round: 3874\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77831, Round: 3875\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77832, Round: 3876\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77833, Round: 3877\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77834, Round: 3878\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77835, Round: 3879\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77836, Round: 3880\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77837, Round: 3881\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77838, Round: 3882\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77839, Round: 3883\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77840, Round: 3884\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77841, Round: 3885\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77842, Round: 3886\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77843, Round: 3887\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77844, Round: 3888\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77845, Round: 3889\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77846, Round: 3890\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77847, Round: 3891\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77848, Round: 3892\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77849, Round: 3893\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77850, Round: 3894\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77851, Round: 3895\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77852, Round: 3896\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77853, Round: 3897\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77854, Round: 3898\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77855, Round: 3899\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77856, Round: 3900\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77857, Round: 3901\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77858, Round: 3902\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77859, Round: 3903\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77860, Round: 3904\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77861, Round: 3905\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77862, Round: 3906\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77863, Round: 3907\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77864, Round: 3908\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77865, Round: 3909\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77866, Round: 3910\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77867, Round: 3911\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77868, Round: 3912\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77869, Round: 3913\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77870, Round: 3914\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77871, Round: 3915\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77872, Round: 3916\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77873, Round: 3917\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77874, Round: 3918\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77875, Round: 3919\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77876, Round: 3920\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77877, Round: 3921\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77878, Round: 3922\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77879, Round: 3923\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77880, Round: 3924\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77881, Round: 3925\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77882, Round: 3926\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77883, Round: 3927\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77884, Round: 3928\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77885, Round: 3929\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77886, Round: 3930\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77887, Round: 3931\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77888, Round: 3932\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77889, Round: 3933\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77890, Round: 3934\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77891, Round: 3935\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77892, Round: 3936\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77893, Round: 3937\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77894, Round: 3938\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77895, Round: 3939\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77896, Round: 3940\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77897, Round: 3941\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77898, Round: 3942\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77899, Round: 3943\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77900, Round: 3944\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77901, Round: 3945\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77902, Round: 3946\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77903, Round: 3947\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77904, Round: 3948\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77905, Round: 3949\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77906, Round: 3950\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77907, Round: 3951\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77908, Round: 3952\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77909, Round: 3953\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77910, Round: 3954\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77911, Round: 3955\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77912, Round: 3956\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77913, Round: 3957\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77914, Round: 3958\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77915, Round: 3959\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77916, Round: 3960\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77917, Round: 3961\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77918, Round: 3962\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77919, Round: 3963\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77920, Round: 3964\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77921, Round: 3965\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77922, Round: 3966\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77923, Round: 3967\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77924, Round: 3968\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77925, Round: 3969\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77926, Round: 3970\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77927, Round: 3971\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77928, Round: 3972\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77929, Round: 3973\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77930, Round: 3974\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77931, Round: 3975\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77932, Round: 3976\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77933, Round: 3977\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77934, Round: 3978\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77935, Round: 3979\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77936, Round: 3980\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77937, Round: 3981\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77938, Round: 3982\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77939, Round: 3983\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77940, Round: 3984\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77941, Round: 3985\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77942, Round: 3986\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77943, Round: 3987\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77944, Round: 3988\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77945, Round: 3989\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77946, Round: 3990\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77947, Round: 3991\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77948, Round: 3992\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77949, Round: 3993\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77950, Round: 3994\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77951, Round: 3995\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77952, Round: 3996\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77953, Round: 3997\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77954, Round: 3998\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77955, Round: 3999\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77956, Round: 4000\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77957, Round: 4001\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77958, Round: 4002\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77959, Round: 4003\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77960, Round: 4004\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77961, Round: 4005\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77962, Round: 4006\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77963, Round: 4007\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77964, Round: 4008\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77965, Round: 4009\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77966, Round: 4010\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77967, Round: 4011\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77968, Round: 4012\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77969, Round: 4013\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 77970, Round: 4014\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77971, Round: 4015\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77972, Round: 4016\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77973, Round: 4017\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77974, Round: 4018\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77975, Round: 4019\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77976, Round: 4020\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77977, Round: 4021\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77978, Round: 4022\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77979, Round: 4023\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77980, Round: 4024\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77981, Round: 4025\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77982, Round: 4026\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77983, Round: 4027\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77984, Round: 4028\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77985, Round: 4029\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77986, Round: 4030\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77987, Round: 4031\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77988, Round: 4032\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77989, Round: 4033\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77990, Round: 4034\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77991, Round: 4035\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77992, Round: 4036\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77993, Round: 4037\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77994, Round: 4038\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77995, Round: 4039\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77996, Round: 4040\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77997, Round: 4041\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77998, Round: 4042\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 77999, Round: 4043\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78000, Round: 4044\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78001, Round: 4045\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78002, Round: 4046\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78003, Round: 4047\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78004, Round: 4048\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78005, Round: 4049\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78006, Round: 4050\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78007, Round: 4051\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78008, Round: 4052\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78009, Round: 4053\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78010, Round: 4054\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78011, Round: 4055\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78012, Round: 4056\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78013, Round: 4057\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78014, Round: 4058\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78015, Round: 4059\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78016, Round: 4060\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78017, Round: 4061\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78018, Round: 4062\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78019, Round: 4063\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78020, Round: 4064\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78021, Round: 4065\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78022, Round: 4066\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78023, Round: 4067\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78024, Round: 4068\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78025, Round: 4069\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78026, Round: 4070\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78027, Round: 4071\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78028, Round: 4072\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78029, Round: 4073\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78030, Round: 4074\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78031, Round: 4075\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78032, Round: 4076\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78033, Round: 4077\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78034, Round: 4078\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78035, Round: 4079\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78036, Round: 4080\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78037, Round: 4081\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78038, Round: 4082\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78039, Round: 4083\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78040, Round: 4084\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78041, Round: 4085\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78042, Round: 4086\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78043, Round: 4087\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78044, Round: 4088\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78045, Round: 4089\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78046, Round: 4090\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78047, Round: 4091\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78048, Round: 4092\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78049, Round: 4093\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78050, Round: 4094\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78051, Round: 4095\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78052, Round: 4096\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78053, Round: 4097\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78054, Round: 4098\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78055, Round: 4099\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78056, Round: 4100\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78057, Round: 4101\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78058, Round: 4102\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78059, Round: 4103\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78060, Round: 4104\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78061, Round: 4105\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78062, Round: 4106\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78063, Round: 4107\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78064, Round: 4108\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78065, Round: 4109\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78066, Round: 4110\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78067, Round: 4111\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78068, Round: 4112\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78069, Round: 4113\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78070, Round: 4114\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78071, Round: 4115\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78072, Round: 4116\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78073, Round: 4117\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78074, Round: 4118\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78075, Round: 4119\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78076, Round: 4120\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78077, Round: 4121\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78078, Round: 4122\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78079, Round: 4123\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78080, Round: 4124\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78081, Round: 4125\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78082, Round: 4126\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78083, Round: 4127\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78084, Round: 4128\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78085, Round: 4129\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78086, Round: 4130\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78087, Round: 4131\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78088, Round: 4132\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78089, Round: 4133\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78090, Round: 4134\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78091, Round: 4135\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78092, Round: 4136\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78093, Round: 4137\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78094, Round: 4138\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78095, Round: 4139\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78096, Round: 4140\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78097, Round: 4141\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78098, Round: 4142\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78099, Round: 4143\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78100, Round: 4144\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78101, Round: 4145\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78102, Round: 4146\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78103, Round: 4147\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78104, Round: 4148\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78105, Round: 4149\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78106, Round: 4150\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78107, Round: 4151\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78108, Round: 4152\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78109, Round: 4153\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78110, Round: 4154\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78111, Round: 4155\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78112, Round: 4156\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78113, Round: 4157\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78114, Round: 4158\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78115, Round: 4159\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78116, Round: 4160\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78117, Round: 4161\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78118, Round: 4162\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78119, Round: 4163\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78120, Round: 4164\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78121, Round: 4165\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78122, Round: 4166\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78123, Round: 4167\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78124, Round: 4168\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78125, Round: 4169\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78126, Round: 4170\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78127, Round: 4171\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78128, Round: 4172\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78129, Round: 4173\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78130, Round: 4174\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78131, Round: 4175\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78132, Round: 4176\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78133, Round: 4177\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78134, Round: 4178\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78135, Round: 4179\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78136, Round: 4180\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78137, Round: 4181\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78138, Round: 4182\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78139, Round: 4183\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78140, Round: 4184\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78141, Round: 4185\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78142, Round: 4186\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78143, Round: 4187\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78144, Round: 4188\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78145, Round: 4189\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78146, Round: 4190\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78147, Round: 4191\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78148, Round: 4192\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78149, Round: 4193\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78150, Round: 4194\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78151, Round: 4195\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78152, Round: 4196\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78153, Round: 4197\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78154, Round: 4198\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78155, Round: 4199\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78156, Round: 4200\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78157, Round: 4201\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78158, Round: 4202\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78159, Round: 4203\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78160, Round: 4204\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78161, Round: 4205\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78162, Round: 4206\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78163, Round: 4207\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78164, Round: 4208\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78165, Round: 4209\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78166, Round: 4210\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78167, Round: 4211\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78168, Round: 4212\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78169, Round: 4213\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78170, Round: 4214\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78171, Round: 4215\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78172, Round: 4216\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78173, Round: 4217\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78174, Round: 4218\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78175, Round: 4219\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78176, Round: 4220\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78177, Round: 4221\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78178, Round: 4222\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78179, Round: 4223\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78180, Round: 4224\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78181, Round: 4225\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78182, Round: 4226\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78183, Round: 4227\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78184, Round: 4228\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78185, Round: 4229\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78186, Round: 4230\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78187, Round: 4231\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78188, Round: 4232\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78189, Round: 4233\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78190, Round: 4234\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78191, Round: 4235\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78192, Round: 4236\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78193, Round: 4237\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78194, Round: 4238\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78195, Round: 4239\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78196, Round: 4240\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78197, Round: 4241\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78198, Round: 4242\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78199, Round: 4243\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78200, Round: 4244\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78201, Round: 4245\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78202, Round: 4246\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78203, Round: 4247\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78204, Round: 4248\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78205, Round: 4249\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78206, Round: 4250\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78207, Round: 4251\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78208, Round: 4252\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78209, Round: 4253\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78210, Round: 4254\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78211, Round: 4255\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78212, Round: 4256\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78213, Round: 4257\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78214, Round: 4258\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78215, Round: 4259\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78216, Round: 4260\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78217, Round: 4261\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78218, Round: 4262\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78219, Round: 4263\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78220, Round: 4264\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78221, Round: 4265\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78222, Round: 4266\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78223, Round: 4267\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78224, Round: 4268\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78225, Round: 4269\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78226, Round: 4270\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78227, Round: 4271\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78228, Round: 4272\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78229, Round: 4273\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78230, Round: 4274\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78231, Round: 4275\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78232, Round: 4276\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78233, Round: 4277\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78234, Round: 4278\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78235, Round: 4279\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5631,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78236, Round: 4280\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78237, Round: 4281\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78238, Round: 4282\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78239, Round: 4283\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78240, Round: 4284\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78241, Round: 4285\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78242, Round: 4286\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78243, Round: 4287\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78244, Round: 4288\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78245, Round: 4289\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78246, Round: 4290\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78247, Round: 4291\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78248, Round: 4292\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78249, Round: 4293\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78250, Round: 4294\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78251, Round: 4295\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78252, Round: 4296\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78253, Round: 4297\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78254, Round: 4298\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78255, Round: 4299\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78256, Round: 4300\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78257, Round: 4301\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78258, Round: 4302\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78259, Round: 4303\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78260, Round: 4304\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78261, Round: 4305\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78262, Round: 4306\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78263, Round: 4307\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78264, Round: 4308\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78265, Round: 4309\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78266, Round: 4310\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78267, Round: 4311\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78268, Round: 4312\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78269, Round: 4313\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78270, Round: 4314\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78271, Round: 4315\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78272, Round: 4316\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78273, Round: 4317\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78274, Round: 4318\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78275, Round: 4319\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78276, Round: 4320\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78277, Round: 4321\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78278, Round: 4322\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78279, Round: 4323\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78280, Round: 4324\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78281, Round: 4325\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78282, Round: 4326\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78283, Round: 4327\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78284, Round: 4328\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78285, Round: 4329\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78286, Round: 4330\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78287, Round: 4331\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78288, Round: 4332\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78289, Round: 4333\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78290, Round: 4334\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78291, Round: 4335\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78292, Round: 4336\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78293, Round: 4337\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78294, Round: 4338\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78295, Round: 4339\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78296, Round: 4340\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78297, Round: 4341\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78298, Round: 4342\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78299, Round: 4343\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78300, Round: 4344\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78301, Round: 4345\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78302, Round: 4346\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78303, Round: 4347\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78304, Round: 4348\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78305, Round: 4349\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78306, Round: 4350\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78307, Round: 4351\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78308, Round: 4352\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78309, Round: 4353\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78310, Round: 4354\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78311, Round: 4355\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78312, Round: 4356\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78313, Round: 4357\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78314, Round: 4358\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78315, Round: 4359\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78316, Round: 4360\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78317, Round: 4361\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78318, Round: 4362\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78319, Round: 4363\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78320, Round: 4364\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78321, Round: 4365\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78322, Round: 4366\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78323, Round: 4367\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78324, Round: 4368\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78325, Round: 4369\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78326, Round: 4370\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78327, Round: 4371\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78328, Round: 4372\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78329, Round: 4373\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78330, Round: 4374\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78331, Round: 4375\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78332, Round: 4376\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78333, Round: 4377\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78334, Round: 4378\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78335, Round: 4379\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78336, Round: 4380\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78337, Round: 4381\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78338, Round: 4382\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78339, Round: 4383\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78340, Round: 4384\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78341, Round: 4385\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78342, Round: 4386\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78343, Round: 4387\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78344, Round: 4388\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78345, Round: 4389\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78346, Round: 4390\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78347, Round: 4391\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78348, Round: 4392\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78349, Round: 4393\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78350, Round: 4394\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78351, Round: 4395\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78352, Round: 4396\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78353, Round: 4397\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78354, Round: 4398\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78355, Round: 4399\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78356, Round: 4400\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78357, Round: 4401\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78358, Round: 4402\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78359, Round: 4403\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78360, Round: 4404\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78361, Round: 4405\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78362, Round: 4406\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78363, Round: 4407\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78364, Round: 4408\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78365, Round: 4409\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78366, Round: 4410\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78367, Round: 4411\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78368, Round: 4412\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78369, Round: 4413\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78370, Round: 4414\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78371, Round: 4415\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78372, Round: 4416\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78373, Round: 4417\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78374, Round: 4418\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78375, Round: 4419\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78376, Round: 4420\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78377, Round: 4421\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78378, Round: 4422\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78379, Round: 4423\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78380, Round: 4424\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78381, Round: 4425\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78382, Round: 4426\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78383, Round: 4427\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78384, Round: 4428\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78385, Round: 4429\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78386, Round: 4430\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78387, Round: 4431\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78388, Round: 4432\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78389, Round: 4433\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78390, Round: 4434\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78391, Round: 4435\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78392, Round: 4436\n",
      "Evalset: [Train : Metrics { log-loss:0.6789,  logloss:0.6789,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78393, Round: 4437\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78394, Round: 4438\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78395, Round: 4439\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78396, Round: 4440\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78397, Round: 4441\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78398, Round: 4442\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78399, Round: 4443\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78400, Round: 4444\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78401, Round: 4445\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78402, Round: 4446\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78403, Round: 4447\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78404, Round: 4448\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78405, Round: 4449\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78406, Round: 4450\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78407, Round: 4451\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78408, Round: 4452\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78409, Round: 4453\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78410, Round: 4454\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78411, Round: 4455\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78412, Round: 4456\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78413, Round: 4457\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78414, Round: 4458\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78415, Round: 4459\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78416, Round: 4460\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78417, Round: 4461\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78418, Round: 4462\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78419, Round: 4463\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78420, Round: 4464\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78421, Round: 4465\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78422, Round: 4466\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78423, Round: 4467\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78424, Round: 4468\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78425, Round: 4469\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78426, Round: 4470\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78427, Round: 4471\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78428, Round: 4472\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78429, Round: 4473\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78430, Round: 4474\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78431, Round: 4475\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78432, Round: 4476\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78433, Round: 4477\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78434, Round: 4478\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78435, Round: 4479\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78436, Round: 4480\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78437, Round: 4481\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78438, Round: 4482\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78439, Round: 4483\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78440, Round: 4484\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78441, Round: 4485\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78442, Round: 4486\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78443, Round: 4487\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78444, Round: 4488\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78445, Round: 4489\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78446, Round: 4490\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78447, Round: 4491\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78448, Round: 4492\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78449, Round: 4493\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78450, Round: 4494\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78451, Round: 4495\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78452, Round: 4496\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78453, Round: 4497\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78454, Round: 4498\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78455, Round: 4499\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78456, Round: 4500\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78457, Round: 4501\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78458, Round: 4502\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78459, Round: 4503\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78460, Round: 4504\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78461, Round: 4505\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78462, Round: 4506\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78463, Round: 4507\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78464, Round: 4508\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78465, Round: 4509\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78466, Round: 4510\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78467, Round: 4511\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78468, Round: 4512\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78469, Round: 4513\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78470, Round: 4514\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78471, Round: 4515\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78472, Round: 4516\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78473, Round: 4517\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78474, Round: 4518\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78475, Round: 4519\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78476, Round: 4520\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78477, Round: 4521\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78478, Round: 4522\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78479, Round: 4523\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78480, Round: 4524\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78481, Round: 4525\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78482, Round: 4526\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78483, Round: 4527\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78484, Round: 4528\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78485, Round: 4529\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78486, Round: 4530\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78487, Round: 4531\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78488, Round: 4532\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78489, Round: 4533\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78490, Round: 4534\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78491, Round: 4535\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78492, Round: 4536\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78493, Round: 4537\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78494, Round: 4538\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78495, Round: 4539\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78496, Round: 4540\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78497, Round: 4541\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78498, Round: 4542\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78499, Round: 4543\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78500, Round: 4544\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78501, Round: 4545\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78502, Round: 4546\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78503, Round: 4547\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78504, Round: 4548\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5628,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78505, Round: 4549\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78506, Round: 4550\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78507, Round: 4551\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78508, Round: 4552\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78509, Round: 4553\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78510, Round: 4554\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78511, Round: 4555\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78512, Round: 4556\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78513, Round: 4557\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78514, Round: 4558\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78515, Round: 4559\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78516, Round: 4560\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78517, Round: 4561\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78518, Round: 4562\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78519, Round: 4563\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78520, Round: 4564\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78521, Round: 4565\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78522, Round: 4566\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78523, Round: 4567\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78524, Round: 4568\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78525, Round: 4569\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78526, Round: 4570\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5629,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78527, Round: 4571\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5627,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78528, Round: 4572\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5627,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78529, Round: 4573\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5627,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78530, Round: 4574\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5627,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78531, Round: 4575\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5627,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78532, Round: 4576\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5627,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78533, Round: 4577\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5627,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78534, Round: 4578\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5627,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78535, Round: 4579\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5627,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78536, Round: 4580\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5627,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78537, Round: 4581\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78538, Round: 4582\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78539, Round: 4583\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78540, Round: 4584\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78541, Round: 4585\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78542, Round: 4586\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78543, Round: 4587\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78544, Round: 4588\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78545, Round: 4589\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78546, Round: 4590\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78547, Round: 4591\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78548, Round: 4592\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78549, Round: 4593\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78550, Round: 4594\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78551, Round: 4595\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78552, Round: 4596\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78553, Round: 4597\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78554, Round: 4598\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78555, Round: 4599\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78556, Round: 4600\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78557, Round: 4601\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78558, Round: 4602\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78559, Round: 4603\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78560, Round: 4604\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78561, Round: 4605\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78562, Round: 4606\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78563, Round: 4607\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78564, Round: 4608\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78565, Round: 4609\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78566, Round: 4610\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78567, Round: 4611\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78568, Round: 4612\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78569, Round: 4613\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78570, Round: 4614\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78571, Round: 4615\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78572, Round: 4616\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78573, Round: 4617\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78574, Round: 4618\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78575, Round: 4619\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78576, Round: 4620\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78577, Round: 4621\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78578, Round: 4622\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78579, Round: 4623\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78580, Round: 4624\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78581, Round: 4625\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78582, Round: 4626\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78583, Round: 4627\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78584, Round: 4628\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78585, Round: 4629\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78586, Round: 4630\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78587, Round: 4631\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78588, Round: 4632\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78589, Round: 4633\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78590, Round: 4634\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78591, Round: 4635\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78592, Round: 4636\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6808,  logloss:0.6808,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78593, Round: 4637\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78594, Round: 4638\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78595, Round: 4639\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78596, Round: 4640\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78597, Round: 4641\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78598, Round: 4642\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78599, Round: 4643\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78600, Round: 4644\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78601, Round: 4645\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78602, Round: 4646\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78603, Round: 4647\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78604, Round: 4648\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78605, Round: 4649\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78606, Round: 4650\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78607, Round: 4651\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78608, Round: 4652\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78609, Round: 4653\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78610, Round: 4654\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78611, Round: 4655\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78612, Round: 4656\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78613, Round: 4657\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78614, Round: 4658\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78615, Round: 4659\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78616, Round: 4660\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78617, Round: 4661\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78618, Round: 4662\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78619, Round: 4663\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78620, Round: 4664\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78621, Round: 4665\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78622, Round: 4666\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78623, Round: 4667\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78624, Round: 4668\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78625, Round: 4669\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78626, Round: 4670\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78627, Round: 4671\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78628, Round: 4672\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78629, Round: 4673\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78630, Round: 4674\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78631, Round: 4675\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78632, Round: 4676\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78633, Round: 4677\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78634, Round: 4678\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78635, Round: 4679\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78636, Round: 4680\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78637, Round: 4681\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78638, Round: 4682\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78639, Round: 4683\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78640, Round: 4684\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78641, Round: 4685\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78642, Round: 4686\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78643, Round: 4687\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78644, Round: 4688\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78645, Round: 4689\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78646, Round: 4690\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78647, Round: 4691\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78648, Round: 4692\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78649, Round: 4693\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78650, Round: 4694\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78651, Round: 4695\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78652, Round: 4696\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78653, Round: 4697\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78654, Round: 4698\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78655, Round: 4699\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78656, Round: 4700\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78657, Round: 4701\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78658, Round: 4702\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78659, Round: 4703\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78660, Round: 4704\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78661, Round: 4705\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78662, Round: 4706\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78663, Round: 4707\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78664, Round: 4708\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78665, Round: 4709\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78666, Round: 4710\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78667, Round: 4711\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78668, Round: 4712\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78669, Round: 4713\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78670, Round: 4714\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78671, Round: 4715\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78672, Round: 4716\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78673, Round: 4717\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78674, Round: 4718\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78675, Round: 4719\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78676, Round: 4720\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78677, Round: 4721\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78678, Round: 4722\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78679, Round: 4723\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78680, Round: 4724\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78681, Round: 4725\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78682, Round: 4726\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78683, Round: 4727\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78684, Round: 4728\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78685, Round: 4729\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78686, Round: 4730\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78687, Round: 4731\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78688, Round: 4732\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78689, Round: 4733\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78690, Round: 4734\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78691, Round: 4735\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78692, Round: 4736\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78693, Round: 4737\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78694, Round: 4738\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78695, Round: 4739\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78696, Round: 4740\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78697, Round: 4741\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78698, Round: 4742\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78699, Round: 4743\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78700, Round: 4744\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78701, Round: 4745\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78702, Round: 4746\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78703, Round: 4747\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78704, Round: 4748\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78705, Round: 4749\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78706, Round: 4750\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78707, Round: 4751\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78708, Round: 4752\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78709, Round: 4753\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78710, Round: 4754\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78711, Round: 4755\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78712, Round: 4756\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78713, Round: 4757\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78714, Round: 4758\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78715, Round: 4759\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78716, Round: 4760\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78717, Round: 4761\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78718, Round: 4762\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78719, Round: 4763\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78720, Round: 4764\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78721, Round: 4765\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78722, Round: 4766\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78723, Round: 4767\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78724, Round: 4768\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78725, Round: 4769\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78726, Round: 4770\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78727, Round: 4771\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78728, Round: 4772\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78729, Round: 4773\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78730, Round: 4774\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78731, Round: 4775\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78732, Round: 4776\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78733, Round: 4777\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78734, Round: 4778\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78735, Round: 4779\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 78736, Round: 4780\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78737, Round: 4781\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 78738, Round: 4782\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5622,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5605,  }]\n",
      "Training on Total Epoch: 78739, Round: 4783\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5627,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 78740, Round: 4784\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5622,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5605,  }]\n",
      "Training on Total Epoch: 78741, Round: 4785\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78742, Round: 4786\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5622,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5605,  }]\n",
      "Training on Total Epoch: 78743, Round: 4787\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78744, Round: 4788\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5622,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5605,  }]\n",
      "Training on Total Epoch: 78745, Round: 4789\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78746, Round: 4790\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5622,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5605,  }]\n",
      "Training on Total Epoch: 78747, Round: 4791\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5627,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 78748, Round: 4792\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78749, Round: 4793\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78750, Round: 4794\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78751, Round: 4795\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78752, Round: 4796\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 78753, Round: 4797\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78754, Round: 4798\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 78755, Round: 4799\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78756, Round: 4800\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 78757, Round: 4801\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78758, Round: 4802\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5622,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78759, Round: 4803\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 78760, Round: 4804\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78761, Round: 4805\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 78762, Round: 4806\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78763, Round: 4807\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 78764, Round: 4808\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78765, Round: 4809\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 78766, Round: 4810\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5622,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78767, Round: 4811\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78768, Round: 4812\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 78769, Round: 4813\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78770, Round: 4814\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 78771, Round: 4815\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78772, Round: 4816\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 78773, Round: 4817\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78774, Round: 4818\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78775, Round: 4819\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5622,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78776, Round: 4820\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5622,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78777, Round: 4821\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 78778, Round: 4822\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78779, Round: 4823\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 78780, Round: 4824\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78781, Round: 4825\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 78782, Round: 4826\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5622,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78783, Round: 4827\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78784, Round: 4828\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5622,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78785, Round: 4829\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5622,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78786, Round: 4830\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 78787, Round: 4831\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5622,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78788, Round: 4832\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 78789, Round: 4833\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5622,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78790, Round: 4834\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 78791, Round: 4835\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5622,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78792, Round: 4836\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78793, Round: 4837\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5622,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78794, Round: 4838\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78795, Round: 4839\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78796, Round: 4840\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5622,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78797, Round: 4841\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 78798, Round: 4842\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5622,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78799, Round: 4843\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 78800, Round: 4844\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5622,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78801, Round: 4845\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78802, Round: 4846\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78803, Round: 4847\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78804, Round: 4848\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78805, Round: 4849\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78806, Round: 4850\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78807, Round: 4851\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78808, Round: 4852\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78809, Round: 4853\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78810, Round: 4854\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78811, Round: 4855\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78812, Round: 4856\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78813, Round: 4857\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78814, Round: 4858\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78815, Round: 4859\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78816, Round: 4860\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78817, Round: 4861\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78818, Round: 4862\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78819, Round: 4863\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78820, Round: 4864\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78821, Round: 4865\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78822, Round: 4866\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78823, Round: 4867\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78824, Round: 4868\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78825, Round: 4869\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78826, Round: 4870\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78827, Round: 4871\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78828, Round: 4872\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78829, Round: 4873\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78830, Round: 4874\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78831, Round: 4875\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78832, Round: 4876\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78833, Round: 4877\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78834, Round: 4878\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78835, Round: 4879\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78836, Round: 4880\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78837, Round: 4881\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78838, Round: 4882\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78839, Round: 4883\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78840, Round: 4884\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78841, Round: 4885\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78842, Round: 4886\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78843, Round: 4887\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78844, Round: 4888\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78845, Round: 4889\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78846, Round: 4890\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78847, Round: 4891\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78848, Round: 4892\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78849, Round: 4893\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78850, Round: 4894\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78851, Round: 4895\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78852, Round: 4896\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78853, Round: 4897\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78854, Round: 4898\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78855, Round: 4899\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78856, Round: 4900\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78857, Round: 4901\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78858, Round: 4902\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78859, Round: 4903\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78860, Round: 4904\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78861, Round: 4905\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78862, Round: 4906\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78863, Round: 4907\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78864, Round: 4908\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78865, Round: 4909\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78866, Round: 4910\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78867, Round: 4911\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78868, Round: 4912\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78869, Round: 4913\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78870, Round: 4914\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78871, Round: 4915\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78872, Round: 4916\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78873, Round: 4917\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78874, Round: 4918\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78875, Round: 4919\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78876, Round: 4920\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78877, Round: 4921\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78878, Round: 4922\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78879, Round: 4923\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78880, Round: 4924\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78881, Round: 4925\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78882, Round: 4926\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78883, Round: 4927\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78884, Round: 4928\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78885, Round: 4929\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 78886, Round: 4930\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78887, Round: 4931\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 78888, Round: 4932\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78889, Round: 4933\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 78890, Round: 4934\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78891, Round: 4935\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 78892, Round: 4936\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78893, Round: 4937\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 78894, Round: 4938\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78895, Round: 4939\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78896, Round: 4940\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.561,  }]\n",
      "Training on Total Epoch: 78897, Round: 4941\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78898, Round: 4942\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5622,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5605,  }]\n",
      "Training on Total Epoch: 78899, Round: 4943\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78900, Round: 4944\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5622,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5605,  }]\n",
      "Training on Total Epoch: 78901, Round: 4945\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78902, Round: 4946\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78903, Round: 4947\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 78904, Round: 4948\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 78905, Round: 4949\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78906, Round: 4950\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78907, Round: 4951\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5615,  }]\n",
      "Training on Total Epoch: 78908, Round: 4952\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 78909, Round: 4953\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78910, Round: 4954\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 78911, Round: 4955\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 78912, Round: 4956\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78913, Round: 4957\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 78914, Round: 4958\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5626,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78915, Round: 4959\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 78916, Round: 4960\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78917, Round: 4961\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78918, Round: 4962\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 78919, Round: 4963\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78920, Round: 4964\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 78921, Round: 4965\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78922, Round: 4966\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 78923, Round: 4967\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78924, Round: 4968\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78925, Round: 4969\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 78926, Round: 4970\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78927, Round: 4971\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 78928, Round: 4972\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78929, Round: 4973\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 78930, Round: 4974\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78931, Round: 4975\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78932, Round: 4976\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 78933, Round: 4977\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78934, Round: 4978\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 78935, Round: 4979\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78936, Round: 4980\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78937, Round: 4981\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78938, Round: 4982\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78939, Round: 4983\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 78940, Round: 4984\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78941, Round: 4985\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5624,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.5625,  }]\n",
      "Training on Total Epoch: 78942, Round: 4986\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78943, Round: 4987\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78944, Round: 4988\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78945, Round: 4989\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78946, Round: 4990\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78947, Round: 4991\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78948, Round: 4992\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78949, Round: 4993\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78950, Round: 4994\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78951, Round: 4995\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78952, Round: 4996\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78953, Round: 4997\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78954, Round: 4998\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n",
      "Training on Total Epoch: 78955, Round: 4999\n",
      "Evalset: [Train : Metrics { log-loss:0.6788,  logloss:0.6788,  accuracy:0.5623,  }]\n",
      "Evalset: [Test : Metrics { log-loss:0.6807,  logloss:0.6807,  accuracy:0.562,  }]\n"
     ]
    }
   ],
   "source": [
    "# Train and Evaluate the model - Simple Logistic Model\n",
    "# \n",
    "model2 = Logistic_1(16 * 10, backend=backend, device=device, autograd=False)\n",
    "crit2 = BinaryCrossEntropy(backend=backend, device=device, autograd=False)\n",
    "optm2 = AdamW(model2.parameters(), lr = 1e-3, eps = 1e-10, backend=backend, device=device, autograd=False)\n",
    "eval2 = Evaluator(\"Eval_2\", task = \"classification\", module=model2, criterion=crit2, optimizer=optm2)\n",
    "\n",
    "# Start to train the Logistic Model\n",
    "eval2.fit(\n",
    "    X = feature_train, y = target_train,\n",
    "    epoches = 5000,\n",
    "    batch_size = 10240,\n",
    "    shuffle = True,\n",
    "    one_hot = False,\n",
    "    random_state = None,\n",
    "    verbosity = 1,\n",
    "    evalper = 1,\n",
    "    evalset = {\n",
    "        \"Train\": (feature_train, target_train),\n",
    "        \"Test\" : (feature_test, target_test)\n",
    "    },\n",
    "    evalmetrics = [\"log-loss\", \"logloss\", \"accuracy\"],\n",
    "    early_stop = 50,\n",
    "    early_stop_logic = \"most\"\n",
    ")\n",
    "\n",
    "eval2.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and Evaluate the model - Simple Logistic Model\n",
    "# \n",
    "\n",
    "# Save the model and evaluator\n",
    "model2.save(model2, \"./models/RNN_Model_ID02_ckpt0.bin\")\n",
    "eval2.save(eval2, \"./models/RNN_Eval_ID02_ckpt0.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Config: {Benchmark Evaluation: Simple Logistic Model }\n",
      "ACC    : 0.562004\n",
      "PREC   : 0.573404\n",
      "RECALL : 0.787523\n",
      "F1     : 0.663619\n",
      "CONFUSION MATRIX:\n",
      "[[TP, FP],\n",
      "[FN, TN]]\n",
      " [[871 648]\n",
      " [235 262]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train and Evaluate the model - Simple Logistic Model\n",
    "# \n",
    "\n",
    "# Print the results for the test-set\n",
    "eval2.eval()\n",
    "pred, acc, prec, recall, f1, cfm = eval_pipeline(eval2, feature_test, target_test)\n",
    "print(\"Model Config: {Benchmark Evaluation: Simple Logistic Model }\")\n",
    "print(\"ACC    :\", round(acc, 6))\n",
    "print(\"PREC   :\", round(prec, 6))\n",
    "print(\"RECALL :\", round(recall, 6))\n",
    "print(\"F1     :\", round(f1, 6))\n",
    "print(\"CONFUSION MATRIX:\\n[[TP, FP],\\n[FN, TN]]\\n\", cfm.to_numpy_array())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A short Comparision between RNN and Logistic Model\n",
    "\n",
    "# The RNN with one stacked layer and a contraction dense architecture outperforms the simple logistic model across most metrics. \n",
    "#   The recurrent network scores 72.37 % accuracy against 56.20 %, and its precision rises to 73.20 % rather than 57.34 %, \n",
    "#   while maintain­ing a similarly strong recall (78.30 % vs. 78.75 %). Its F1 measure reaches 75.67 %, \n",
    "#   surpassing the logistic’s 66.36 %. Examining the confusion matrices confirms this: the RNN commits 317 false positives compared to 648,\n",
    "#   with a marginal increase of only five more false negatives (240 vs. 235), reflecting a substantially \n",
    "#   cleaner positive prediction profile.\n",
    "#   To be honest, I was surprised at this RNN architecture and I would say I have checked that there is no obvious data leakage:\n",
    "#   1. I used time-series train-test split which makes NO overlap between training and test data.\n",
    "#   2. All features are computed from historical data (including MA) and no looking into future.\n",
    "\n",
    "# Dive more. The RNN offers a more balanced decision boundary, trading a negligible rise in missed positives \n",
    "#   for a drastic reduction in spurious positive calls. The logistic model, despite matching recall, sacrifices precision \n",
    "#   and overall accuracy, yielding a flood of false alarms. In scenarios where both precision and accuracy matter, \n",
    "#   the RNN’s superior F1 and reduced false positive rate make it the more reliable choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2.3.4 Plot the training loss and accuracy curves over epochs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training/validation loss over epochs\n",
    "\n",
    "# Collect train-valid losses\n",
    "def collect_losses(evaluator: nn_SInterf_Evaluator, \n",
    "                   metric = \"logloss\",\n",
    "                   collect = [\"Train\", \"Valid\"]) -> Tuple[dict, pd.DataFrame]:\n",
    "\n",
    "    evalhist = deepcopy(evaluator.evalhist_)\n",
    "    evaldict = {}\n",
    "    evaldict[\"No\"] = []\n",
    "    for c in collect:\n",
    "        evaldict[c] = []\n",
    "\n",
    "    # For each, extract data as float types\n",
    "    for k in evalhist.keys():\n",
    "        obj = evalhist[k]\n",
    "        # No self-increment\n",
    "        evaldict[\"No\"].append(k)\n",
    "        for c in collect:\n",
    "            data = obj[c][metric].to_list()\n",
    "            evaldict[c].append(data)\n",
    "\n",
    "    return evaldict, pd.DataFrame(evaldict)\n",
    "\n",
    "# Plot train/train-valid losses\n",
    "def plot_loss(df, xlabel_: str | None = None, what: str = \"Loss\", log_y: bool = True):\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    from matplotlib.collections import LineCollection\n",
    "    from matplotlib.colors import Normalize\n",
    "\n",
    "    if 'No' in df.columns:\n",
    "        x = df['No'].values\n",
    "        xlabel = 'Epoch'\n",
    "    else:\n",
    "        x = np.arange(len(df))\n",
    "        xlabel = 'Index' if xlabel_ is None else xlabel_\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "    # Plot training loss with gradient color\n",
    "    y_train = df['Train'].values\n",
    "    pts_train = np.array([x, y_train]).T.reshape(-1, 1, 2)\n",
    "    segs_train = np.concatenate([pts_train[:-1], pts_train[1:]], axis=1)\n",
    "    norm_train = Normalize(vmin=y_train.min(), vmax=y_train.max())\n",
    "    lc_train = LineCollection(segs_train, cmap='viridis', norm=norm_train)\n",
    "    lc_train.set_array(y_train)\n",
    "    lc_train.set_linewidth(2)\n",
    "    ax.add_collection(lc_train)\n",
    "    cbar_train = fig.colorbar(lc_train, ax=ax, pad=0.02)\n",
    "    cbar_train.set_label(what)\n",
    "\n",
    "    # Plot validation loss if available\n",
    "    if 'Valid' in df.columns:\n",
    "        y_valid = df['Valid'].values\n",
    "        pts_valid = np.array([x, y_valid]).T.reshape(-1, 1, 2)\n",
    "        segs_valid = np.concatenate([pts_valid[:-1], pts_valid[1:]], axis=1)\n",
    "        norm_valid = Normalize(vmin=y_valid.min(), vmax=y_valid.max())\n",
    "        lc_valid = LineCollection(segs_valid, cmap='plasma', norm=norm_valid)\n",
    "        lc_valid.set_array(y_valid)\n",
    "        lc_valid.set_linewidth(2)\n",
    "        ax.add_collection(lc_valid)\n",
    "\n",
    "    ax.set_xlim(x.min(), x.max())\n",
    "    y_min = y_train.min()\n",
    "    y_max = y_train.max()\n",
    "    if 'Valid' in df.columns:\n",
    "        y_min = min(y_min, y_valid.min())\n",
    "        y_max = max(y_max, y_valid.max())\n",
    "    ax.set_ylim(y_min, y_max)\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(what)\n",
    "    if log_y == True:\n",
    "        ax.set_yscale('log')\n",
    "    ax.set_title(what + ' Over {}s'.format(xlabel))\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the Train/Test Logloss\n",
    "\n",
    "losses_, losses_df = collect_losses(eval1, collect=[\"Train\", \"Test\"])\n",
    "losses_df.columns = [\"No\", \"Train\", \"Valid\"]\n",
    "plot_loss(losses_df, xlabel_=\"Epoches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the Train/Test Accuracy\n",
    "\n",
    "acc_, acc_df = collect_losses(eval1, metric=\"accuracy\", collect=[\"Train\", \"Test\"])\n",
    "acc_df.columns = [\"No\", \"Train\", \"Valid\"]\n",
    "plot_loss(acc_df, xlabel_=\"Epoches\", what=\"Accuracy\", log_y=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`End of Homework 4 Q1 by Nathmath Huang (bh2821)`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
